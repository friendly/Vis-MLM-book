```{r include=FALSE}
source(here::here("R", "common.R"))
```

# PCA and Biplots {#sec-pca-biplot}

## _Flatland_ and _Spaceland_ {#sec-spaceland}

> It is high time that I should pass from these brief and discursive notes about Flatland to the central event of this book, my initiation into the mysteries of Space. THAT is my subject; all that has gone before is merely preface --- Edwin Abbott, _Flatland_, p. 57.

There was a cloud in the sky above _Flatland_ one day. But it was a huge, multidimensional cloud of sparkly points that might contain some important message, perhaps like the hidden EUREKA (@fig-pollen-eureka-fig), or perhaps forecasting the upcoming harvest, if only Flatlanders could appreciate it. 

A leading citizen, A SQUARE, who had traveled once to 
Spaceland and therefore had an inkling of its majesty beyond the simple world of his life in the plane looked at that cloud and had a brilliant thought, an OMG moment:

> "Oh, can I, in my imagination, rotate that cloud and squeeze its' juice so that it rains down on Flatland with greatest joy?"

As it happened, our Square friend, although he could never really _see_ in three dimensions, he could now 
at least _think_ of a world described by **height** as well as breadth and width, and think of the
**shadow** cast by a cloud as something mutable, changing size and shape depending on its' orientation over Flatland.

And what a world it was, inhabited by
Pryamids, Cubes and wondrous creatures called Polyhedrons with many $C$orners, $F$aces and $E$dges.
Not only that, but all those Polyhedra were forced in Spaceland to obey a magic formula:
$C + F - E = 2$.[^1-euler] How cool was that!

[^1-euler]: This is Euler's [-@Euler:1758] formula, which states that any convex polyheron must obey the formula $V + F - E = 2$ where $V$ is the number of vertexes (corners), $F$ is the number of faces and $E$ is the number of edges.  For example, a tetrahedron
or pyramid has $(V, F, E) = (4, 4, 6)$ and a cube has $(V, F, E) = (8, 6, 12)$. Stated in words,
for all solid bodies confined by planes, the sum of the number of vertexes and the number of faces is two less than the number of edges.

Indeed, there were even exalted Spheres,
having so many faces that its surface became as smooth as a baby's bottom with no need for pointed corners or edges, just as Circles were the smoothest occupants of his world with far too many sides to count.
It was his dream of a Sphere passing through Flatland (@fig-flatland-spheres) that first awakened him to 
a third dimension.

He also marveled at Ellipsoids, as smooth as Spheres, but in Spaceland having three natural axes of different extent
and capable of being appearing fatter or slimmer when rotated from different views. An Ellipsoid
had magical properties: it could appear as so thin in one or more dimensions that it became a simple
2D ellipse, or a 1D line, or even a 0D point [@Friendly-etal:ellipses:2013].
<!-- **TODO**: somehow mention the `gellipsoid` package here. -->

All of these now arose in Square's richer 3D imagination. 
And, all of this came from just one more dimension than his life in Flatland.


### Multivariate juicers

Up to now, we have also been living in Flatland. We have been trying to understand data in
**data space** of possibly many dimensions, but confined to the 2D plane of a graph window.
Scatterplot matrices and parallel coordinate plots provided some relief. 
The former did so by **projecting** the data into sets of 2D views in the coordinates of data
space; the latter did so by providing multiple axes in a 2D space along which we could trace
the paths of individual observations.

This chapter is about seeing data in a different space, a low-dimensional, usually 2D, space
that which squeezes out the most juice
from multidimensional data for a particular purpose (@fig-MV-juicer), where what we want to
understand can be more easily seen.

```{r}
#| label: fig-MV-juicer
#| echo: false
#| fig-align: center
#| out-width: "90%"
#| fig-cap: "A multivariate juicer takes data from possibly high-dimensional data space and transforms it to a lower-dimenional space in which important effects can be more easily seen."
knitr::include_graphics("images/MV-juicer.png")
```

Here, I concentrate on **principal components analysis** (PCA), whose goal reflects A Square's desire to see that sparkly cloud of points 
in $nD$ space in the plane showing the greatest variation (squeezing the most juice)
among all other possible views.

This appealed to his sense of geometry, but left him wondering how the variables in that 
high-D cloud were related to the dimensions he could see in a best-fitting plane.
The idea of a **biplot**, showing the data points in the plane, together with thick pointed
arrows---variable vectors--- in one view is the other topic explained in this chapter (@sec-biplot).

**Packages**

In this chapter I use the following packages. Load them now:

```{r load-pkgs}
library(ggplot2)
library(dplyr)
library(tidyr)
library(patchwork)
library(ggbiplot)
```


## Principal components analysis {#sec-pca}

When Francis Galton [-@Galton:1886] first discovered the idea of regression toward the mean
and presented his famous diagram (@fig-galton-corr), he had little thought that he had provided a
window to a higher-dimensional world, beyond what even A Square could imagine. 
His friend, Karl Pearson [-@Pearson:1896] took that idea and developed it into a theory of
regression and a measure of correlation that would bear his name, Pearson's $r$.

But then Pearson [-@Pearson:1901] had a further inspiration, akin to that of A Square. 
If he also had a cloud of sparkly points in $2, 3, 4, ..., p$ dimensions, could he find a
point ($0D$), or line ($1D$), or plane ($2D$), or even a hyperplane ($nD$) that best summarized ---
squeezed out the most juice---from multivariate data? This was the first trully multivariate
problem in the history of statistics [@FriendlyWainer:2021:TOGS, p. 186].

The best $0D$ point was easy--- it was simply the centroid, the means of each of the
variables in the data, $(\bar{x}_1, \bar{x}_2, ..., \bar{x}_p)$, because that was "closest"
to the data in the sense of minimizing the sum of squared differences, $\Sigma_i\Sigma_j (x_{ij} - \bar{x}_j)^2$.
In higher dimensions, his solution was also an application of the method of least squares, but he argued
it geometrically and visually as shown in @fig-Pearson1901.

```{r}
#| label: fig-Pearson1901
#| echo: false
#| fig-align: center
#| out-width: "70%"
#| fig-cap: "Karl Pearson's (1901) geometric, visual argument for finding the line or plane of closest fit to a collection of points, P1, P2, P3, ... "
knitr::include_graphics("images/Pearson1901.png")
```

For a $1D$ summary, the line of best fit to the points $P_1, P_2, \dots P_n$ is the line that goes through the
centroid and made the average squared length of the _perpendicular_ segments from those points to a line as small
as possible. This was different from the case in linear regression, for fitting $y$ from $x$,
where the average squared length of the _vertical_ segments, $\Sigma_i (y_i - \hat{y}_i)^2$ was minimized by
least squares.

He went on to prove the visual insights from simple smoothing of @Galton:1886 (shown in @fig-galton-corr) 
regarding the regression lines of
`y ~ x` and `x ~ y`. More importantly, he proved that the cloud of points is captured,
for the purpose of finding a best line, plane or hyperplane, by
the ellipsoid that encloses it, as seen in his diagram, @fig-Pearson1901-2. The major axis of the 
2D ellipse is the line of best fit, along which the data points have the smallest average squared
distance from the line. The axis at right angles to that---the minor axis--- is labeled "line of worst fit"
with the largest average squared distance.

```{r}
#| label: fig-Pearson1901-2
#| echo: false
#| fig-align: center
#| out-width: "80%"
#| fig-cap: "Karl Pearson's diagram showing the elliptical geometry of regression and principal components analysis ... _Source_: Pearson (1901), p. 566."
knitr::include_graphics("images/Pearson1901_2.png")
```

Even more importantly--- and this is the basis for what we call **principal components analysis** (PCA)--- he recognized that the two orthogonal axes of the ellipse gave new coordinates for the data which were uncorrelated, whatever the correlation of $x$ and $y$.

> Physically, the axes of the correlation type-ellipse are the directions of independent and uncorrelated variation. --- @Pearson:1901, p. 566.

It was but a small step to recognize that for two variables, $x$ and $y$:

* the line of best fit, the major axis (PC1) had the greatest variance of points projected onto it;
* the line of worst fit, the minor axis (PC2), had the least variance;
* these could be seen as a rotation of the data space of $(x, y)$ to a new space (PC1, PC2) with uncorrelated variables;
* the total variation of the points in data space, $\text{Var}(x) + \text{Var}(y)$, being unchanged by rotation, was equally well expressed as the total variation $\text{Var}(PC1) + \text{Var}(PC2)$ of the scores on what are
now called the principal component axes.



### PCA by springs

Before delving into the mathematics of PCA, it is useful to see how Pearson's problem, and fitting by least squares generally, could be solved in a physical realization. 

From elementary statistics, you may be familiar with a
physical demonstration that the mean, $\bar{x}$, of a sample is the value for which the sum of deviations,
$\Sigma_i (x_i - \bar{x})$ is zero, so the mean can be visualized as the point of balance on a line where
those differences $(x_i - \bar{x})$ are placed. Equally well, there is a physical realization of the mean
as the point along an axis where weights connected by springs will minimize the sum of squared differences,
because springs with a constant stiffness, $k$, exert forces proportional to $k (x_i - \bar{x}) ^2$. That's
the reason is useful as a measure of central tendency: it minimizes the average squared error.

In two dimensions, imagine that we have points, $(x_i, y_i)$ and these are attached by springs of equal stiffness $k$, to a line anchored at the centroid, $(\bar{x}, \bar{y})$ as shown in @fig-pca-springs. 
If we rotate the line to some initial position and release it, the springs will pull the line clockwise or counterclockwise and the line will bounce around until the forces, proportional to the squares of the lengths of the springs, will eventually balance out at the position 
(shown by the `r colorize("red")` of the fixed line segments at the ends). This is the position that minimizes the
the sum of squared lengths of the connecting springs, and also minimizes the kinetic energy in the system.

If you look closely at @fig-pca-springs you will see something else: When the line is at its' final position of minimum
squared length and energy, the positions of the `r colorize("red")` points on this line are spread out furthest, i.e., have **maximum** variance. Conversely, when the line line is at right angles to its' final position (shown by the black line) the projected points have the smallest possible variance.

::: {#fig-pca-springs}
<div align="center">
<iframe width="412" height="364" src="images/pca-springs-cropped.gif"></iframe>
</div>
Animation of PCA fitted by springs. The `r colorize("blue")` data points are connected to their projections on the `r colorize("red")` line by springs perpendicular to that line. From an initial position, the springs pull that line in proportion to their squared distances, until the line finally settles down to the position where the forces are balanced and the minimum is achieved. _Source_: https://bit.ly/46tAicu.

:::

<!-- See: https://joshualoftus.com/posts/2020-11-23-least-squares-as-springs/least-squares-as-springs.html -->

**TODO**: Simple PCA example

### Mathematics and geometry of PCA

### Finding principal components

In R, principal components analysis is most easily carried out using `stats::prcomp()` or `stats::princomp()`
or similar functions in other packages such as `FactomineR::PCA()`.
The **FactoMineR** package [@R-FactoMineR] 
has extensive capabilities for exploratory analysis of multivariate data (PCA, correspondence analysis, cluster analysis, ...). 

Unfortunately, although all of these performing similar calculations, the options for
analysis and the details of the result they return differ ...

The important options for analysis include: 

* whether or not the data variables are **centered**, to a mean of 0
* whether or not the data variables are **scaled**, to a variance of 1.

#### Example: Crime data {.unnumbered}

The dataset `crime`, analysed in @sec-corrgram, showed all positive correlations among the rates of various
crimes in the corrgram, @fig-crime-corrplot. What can we see from a principal components analysis?
Is it possible that a few dimensions can account for most of the juice in this data?

In this example, you can easily find the PCA solution using `prcomp()` in a single line in base-R.
You need to specify the numeric variables to analyze by their columns in the data frame.
The most important option here is `scale. = TRUE` ...

```{r crime-pca0}
data(crime, package = "ggbiplot")
crime.pca <- prcomp(crime[, 2:7], scale. = TRUE)
```

The tidy equivalent is more verbose, but also more expressive about what is being done.
It selects the variables to analyze by a function, `is.numeric()` applied to each of the columns and feeds
the result to `prcomp()`.
```{r crime-pca}
crime.pca <- 
  crime |> 
  dplyr::select(where(is.numeric)) |>
  prcomp(scale. = TRUE)
```

As is typical with models in R, the result, `crime.pca` of `prcomp()` is an object of class `"prcomp"`,
a list of components, and there are a variety of methods for `"prcomp"` objects. Among the simplest
is `summary()`, which gives the contributions of each component to the total variance in the dataset.

```{r crime-pca-summary}
summary(crime.pca) |> print(digits=2)
```

The object, `crime.pca` returned by `prcomp()` is a list of the following the following elements:
```{r crime-pca-components}
names(crime.pca)
```

Of these, for $n$ observations and $p$ variables,

* `sdev` is the length $p$ vector of the standard deviations of the principal components (i.e., the square roots $\sqrt{\lambda_i}$ of the eigenvalues of the covariance/correlation matrix);
* `rotation` is the $p \times p$ matrix of weights or **loadings** of the variables on the components; the columns are the eigenvectors of the covariance or correlation matrix of the data;
* `x` is the $n \times p$ matrix of **scores** for the observations on the components, the result of multiplying (rotating) the data matrix by the loadings. These are uncorrelated, so `cov(x)` is a $p \times p$ diagonal matrix whose diagonal elements are the eigenvalues $\lambda_i$ = `sdev^2`.

### Visualizing variance proportions: screeplots

For a high-D dataset, such as the crime data in seven dimensions, a natural question is how much of the
variation in the data can be captured in 1D, 2D, 3D, ... summaries and views. This is answered by
considering the proportions of variance accounted by each of the dimensions, or their cumulative values.
The components returned by various PCA methods have (confusingly) different names, so
`broom::tidy()` provides methods to unify extraction of these values.

```{r crime-pca-tidy}
(crime.eig <- crime.pca |> 
  broom::tidy(matrix = "eigenvalues"))
```

Then, a simple visualization is a plot of the proportion of variance for each component (or cumulative proportion)
against the component number, usually called a **screeplot**. The idea, introduced by @Cattell1966, is that
after the largest, dominant components, the remainder should resemble the rubble, or scree formed by rocks falling
from a cliff. From this plot, imagine
drawing a straight line through the plotted eigenvalues, starting with the largest one. The typical rough guidance is that the last point to fall on this line represents the last component to extract, the idea being that beyond this, the amount of additional variance explained is non-meaningful. Another rule of thumb is to choose the number
of components to extract a desired proportion of total variance, usually in the range of 80 - 90%.

`stats::plot(crime.pca)` would give a bar plot of the variances of the components, however ggbiplot::ggscreeplot()` gives nicer and more flexible displays as shown in @fig-crime-ggscreeplot.

```{r}
#| label: fig-crime-ggscreeplot
#| out-width: "100%"
#| fig-cap: "Screeplots for the PCA of the crime data. The left panel shows the traditional version, plotting variance proportions against component number, with linear guideline for the scree rule of thumb. The right panel plots cumulative proportions, showing cutoffs of 80%, 90%."
p1 <- ggscreeplot(crime.pca) +
  stat_smooth(data = crime.eig |> filter(PC>=4), 
              aes(x=PC, y=percent), method = "lm", 
              se = FALSE,
              fullrange = TRUE) +
  theme_bw(base_size = 14)

p2 <- ggscreeplot(crime.pca, type = "cev") +
  geom_hline(yintercept = c(0.8, 0.9), color = "blue") +
  theme_bw(base_size = 14)

p1 + p2
```

From this we might conclude that four components are necessary to satisfy the scree criterion or to
account for 90% of the total variation in these crime statistics. However two components, giving
76.5%, might be enough juice to tell a reasonable story.

### Visualizing PCA scores and variable vectors

To see and attempt to understand PCA results, it is useful to plot both the scores for the observations
on a few of the largest components and also the loadings or variable vectors that give the weights
for the variables in determining the principal components.

In @sec-biplot I discuss the biplot technique that plots both in a single display. However,
I do this directly here, using tidy processing to explain what is going on in PCA
and in these graphical displays.

#### Scores {.unnumbered}
The (uncorrelated) principal component scores can be extracted as `crime.pca$x` or using `purrr::pluck("x")`. As noted above, these are uncorrelated and have variances
equal to the eigenvalues of the correlation matrix.

```{r scores}
scores <- crime.pca |> purrr::pluck("x") 
cov(scores) |> zapsmall()
```

For plotting, it is more convenient to use `broom::augment()` which 
extracts the scores (named `.fittedPC*`)
and appends these to the variables in the dataset.

```{r}
crime.pca |>
  broom::augment(crime) |> head()
```

Then, we can use `ggplot()` to plot and pair of components.
To aid interpretation, I label the points by their state abbreviation and color them
by `region` of the U.S.. A geometric interpretation of the plot requires 
an aspect ratio of 1.0 (via `coord_fixed()`)
so that a unit distance on the horizontal axis is the
same length as a unit distance on the vertical.
To demonstrate that the components are uncorrelated, I also added their data
ellipse.

```{r}
#| label: fig-crime-scores-plot12
#| out-width: "80%"
#| fig-cap: "Plot of component scores on the first two principal components for the `crime` data. States are colored by `region`."
crime.pca |>
  broom::augment(crime) |> # add original dataset back in
  ggplot(aes(.fittedPC1, .fittedPC2, color = region)) + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(size = 1.5) +
  geom_text(aes(label = st), nudge_x = 0.2) +
  stat_ellipse(color = "grey") +
  coord_fixed() +
  labs(x = "PC Dimension 1", y = "PC Dimnension 2") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top") 
```

To interpret such plots, it is useful consider the observations that are a high
and low on each of the axes as well as other information, such as region here,
and ask how these differ on the crime statistics.
The first component, PC1, contrasts Nevada and California with North Dakota, South Dakota
and West Virginia. The second component has most of the southern states on the low end
and Massachusetts, Rhode Island and Hawaii on the high end. However, interpretation is
easier when we also consider how the various crimes contribute to these dimensions.

#### Variable vectors {.unnumbered}

You can extract the variable loadings using either `crime.pca$rotation` or
`purrr::pluck("rotation")`, similar to what I did with the scores.

```{r rotation}
crime.pca |> purrr::pluck("rotation")
```

But note something important in this output: All of the weights for the first component are negative. In PCA, the directions of the eigenvectors are completely arbitrary, in the sense that the vector $-\mathbf{v}_i$ gives the same linear combination as $\mathbf{v}_i$,
but with its' sign reversed. For interpretation, it is useful (and usually recommended) to reflect the loadings to a positive orientation by multiplying them by -1.

To reflect the PCA loadings and get them into a convenient format for plotting with `ggplot()`, it is necessary to do a bit of processing, including making the `row.names()` into an explicit variable for the purpose of labeling.

```{r vectors}
vectors <- crime.pca |> 
  purrr::pluck("rotation") |>
  as.data.frame() |>
  mutate(PC1 = -1 * PC1, PC2 = -1 * PC2) |>      # reflect axes
  tibble::rownames_to_column(var = "label") 

vectors[, 1:3]
```

Then, I plot these using `geom_segment()`, taking some care to use arrows
from the origin with a nice shape and add `geom_text()` labels for the variables positioned slightly to the right. Again, `coord_fixed()` ensures equal scales for the axes, which is important because we want to interpret the angles between the variable vectors and the 
PCA coordinate axes.

```{r}
#| label: fig-crime-vectors
#| out-width: "80%"
#| fig-cap: "Plot of component loadings the first two principal components for the `crime` data."
arrow_style <- arrow(
  angle = 20, ends = "first", type = "closed", 
  length = grid::unit(8, "pt")
)

vectors |>
  ggplot(aes(PC1, PC2)) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_segment(xend = 0, yend = 0, 
               linewidth = 1, 
               arrow = arrow_style) +
  geom_text(aes(label = label), 
            size = 5,
            hjust = "outward",
            nudge_x = 0.05, 
            color = "brown") +
  xlim(-0.4, 0.9) + 
  ylim(-0.8, 0.8) +
  coord_fixed() + 
  theme_minimal(base_size = 14)
```

What is shown in @fig-crime-vectors has the following interpretations:

* the lengths of the variable vectors, $||\mathbf{v}_i|| = \sqrt{\Sigma_{j = 1:2} \; v_{ij}^2}$ give the proportion of variance of each variable accounted for in a two-dimensional display.

* the value, $v_{ij}$, of the vector for variable $\mathbf{x}_i$ on component $j$ gives
the correlation of that variable with the $j$th principal component.

* the angle between two variable vectors, $\mathbf{v}_i$ and $\mathbf{v}_j$
gives the approximation of the correlation between $\mathbf{x}_i$ and $\mathbf{x}_j$
that is shown in this space.



## Biplots {#sec-biplot}

<!-- Elliptical insights: Outliers -->
```{r child="child/04-outliers.qmd"}
```


