---
editor: 
  markdown: 
    wrap: 120
---

# Preface

> However, to do this it is useful to set the stage for multivariate thinking, with a grand scheme for statistics and data visualization, a parable, and an example of multivariate discovery.

- However, to do this it is useful to set the stage for multivariate thinking with a grand scheme for statistics, data visualization, a parable, and an example of multivariate discovery.

> scatterplot can show other variables using color, shape or other
> visual attributes
> His point in this classification was that once you've reached three variables, all higher

scatterplot can show other variables using color, shape or other
visual attributes.
His point in this classification was that once you've reached three variables, all higher

> the plane he inhabits. It is a large circle when seen at the moment of its' greatest extent. As the Spehere rises, it becomes progressively smaller, until it becomes a point, and then vanishes.

the plane he inhabits. It is a large circle when seen at the moment of its' greatest extent. As the Sphere rises, it becomes progressively smaller, until it becomes a point, and then vanishes.

> To really see a tesseract it helps to view it in an animation over time (@fig-tesseract).

- Rendering of `@fig-tesseract` is broken


The last parts of this section have a few `TODO`s that need to be filled in.

# Part I: Orienting Ideas

## Chapter 1: Introduction

> This material may or may not survive; it was taken from an earlier article.

- Could delete [done]

> A particular research outcome (e.g., depression, neuro-cognitive functioning, academic achievement, self-concept, attention deficit hyperactivity disorders) might take on a multivariate form if it has several observed measurement scales or related aspects by which it is quantified, or if there are multiple theoretically distinct outcomes that should be assessed in conjunction with each other (e.g., using depression, generalized anxiety, and stress inventories to model overall happiness).

- Could be simplified + broken down, built up to the concrete example from simpler principles/motivations. I think emphasizing the contrast and also the potential cognitive blindspot with univariate-response models is key. 

[This calls for a re-write of the intro to Ch1]

::: {.callout-note title="SEM"}
Structural equation modeling (SEM) offers another route to explore and analyze the relationships among multiple predictors and multiple responses. They have the advantage of being able to test potentially complex systems of linear equations in very flexible ways. However, these methods are often far removed from data analysis *per se* because they typically start with data summaries such as means and
covariance matrices.
Except for path diagrams they usually offer little in the way of visualization methods to aid in understanding and communicating the results. The graphical methods we describe here can also be useful in a SEM context.
\ix{Structural equation model}
:::

- I think what's important to emphasize is the confirmatory/exploratory distinction for in comparing SEM with multivariate linear models.

- It's may also be worthwhile to emphasize why it's a problem that SEM emphasizes `data summaries such as means and covariance matrices", as the downstream implications of this fact may not be immediately relevant to everyone

[added some text to address these pts]


> The aim of this book is to describe and illustrate some central methods that we have developed over the last ten years that aid in the understanding and communication of the results of multivariate linear models [@Friendly-07-manova; 
<!-- @Friendly-etal:ellipses:2013;  --> 
@FriendlyMeyer:2016:DDAR]. These methods rely on *data ellipsoids* as simple, minimally sufficient visualizations of variance that can be shown in 2D and 3D plots. As will be demonstrated, the *Hypothesis-Error (HE) plot* framework applies this idea to the results of multivariate tests of linear hypotheses.

- I think it'd be a good idea to put this in bold, or some stronger way to emphasize how this is the key of this chapter. Perhaps changing the heading of this section from "Visualization is harder" would also help. 

> Not only is this confusing and awkward to report, but it is largely unnecessary because the multivariate tests provide protection for multiple testing.

- I think future readers would appreciate some a citation or a cross-reference to the relevant section in your book regarding protection for multiple testing, especially as this is a hot topic. [DONE]

## Chapter 2: Getting Started

For length, I think you should pick between the pop-out boxes of  `Datasaurus Dozen` and `Quartets`.

For length, I think you should pick between the Davis example and the 1970 Draft Lottery Data.

# Part II: Exploratory Methods

## Chapter 3: Plots of Multivariate Data

> Confidence bands allow us to visualize the uncertainty around a fitted regression curve,
which can be of two types: _pointwise intervals_ or _simultaneous intervals_.
The default setting in ``ggplot2::geom_smooth()` calculates pointwise intervals 
(using `stats::predict.lm(..., interval="confidence")` at a confidence level $1-\alpha$ for the predicted response at _each value_ $x_i$ of a predictor, and have the frequentist interpretation that over repeated sampling only $100\;\alpha$ of the predictions at $x_i$ will be outside that interval. 
In contrast, simultaneous intervals are calculated so that $1 - \alpha$ is the probability that _all of them_ cover their corresponding true values simultaneously. These are necessarily wider than pointwise intervals.
Commonly used methods for constructing simultaneous confidence bands in regression are the Bonferroni and Scheffé methods, which control the family-wise error rate over all values of $x_i$.
See [](https://en.wikipedia.org/wiki/Confidence_and_prediction_bands) for precise definitions of these terms.
These are different from a _prediction band_, which is used to represent the uncertainty about the value of a **new** data-point on the curve, but subject to the additional variance reflected in one observation.

- The wikipedia reference doesn't render properly, perhaps due to lack of text. 
- I think this is a bit confusing -- perhaps shorter sentence and vertical spacing between groups of thought could help? 

> **Grouping**: Identify subgroups in the data by assigning different
    visual attributes, such as color, shape, line style, etc. within a
    single plot. This is quite natural for factors; quantitative
    predictors can be accommodated by cutting their range into ordered
    intervals. Grouping has the
    advantage that the levels of a grouping variable can be shown within
    the same plot, facilitating direct comparison.

>  **Conditioning**: Showing subgroups in different plot panels. This
    has the advantages that relations for the individual groups more
    easily discerned and one can easily stratify by two (or more) other
    variables jointly, but visual comparison is more difficult because
    the eye must scan from one panel to another.
    
Perhaps explicitly referring to example plots within each of these definitions would make things more readily understood? Perhaps even adding separate sub-section for grouping itself would also make things easier

> The most obvious variable that affects academic salary is `rank`,
because faculty typically get an increase in salary with a promotion
that carries through in their future salary. What can we see if we group
by `rank` and fit a separate smoothed curve for each?

- The transition to this seems quite abrupt

> Mathematically, Euclidean (squared) distance for $p$ variables

- Would be helpful to clarify that $D_E^2(y)$ is the Euclidean squared distance. For example, "Mathematically, the Euclidean (squared) distance $(D_E^2(y))$ for $p$ variables.
- Some transition to the discussion of Euclidean squared distance would help
- Wouldn't hurt to also emphasize the difference between $\mathbf{y}$ and $y$ for clarity.
- Explicitly stating the relationship between the Euclidean-squared distance and Mahalanobis' distance would help

> For $p$ variables, the data _ellipsoid_ $\mathcal{E}_c$ of
size $c$ is a $p$-dimensional ellipse,
defined as the set of points $\mathbf{y} = (y_1, y_2, \dots y_p)$
whose squared Mahalanobis distance, $D_M^2 ( \mathbf{y} )$ is less than or equal
to $c^2$,

- Defining or showing what $c$ is would help

> where $\mathbf{S}^{1/2}$ represents a rotation and scaling and the notation $\oplus$ represents translation to a new centroid, $\bar{\mathbf{y}}$ here. The matrix $\mathbf{S}^{1/2}$ is commonly computed
as the Cholesky factor of $\mathbf{S}$.

- Perhaps state the Cholesky factor first before stating its rotational and scaling effects?


- Some motivation or narrative for why we must discuss the following equation at all 
$\mathcal{E}_c (\bar{\mathbf{y}}, \mathbf{S}) := \{ D_M^2 (\mathbf{y}) \le c^2 \}$, instead of just $\mathcal{E}_c (\bar{\mathbf{y}}, \mathbf{S}) = \bar{\mathbf{y}} \; \oplus \; c\, \sqrt{\mathbf{S}}$ would be helpful.

[Revised the notation and discussion to address these points]


> the lines of means of ($Y | X$) and ($X | Y$) corresponded approximately to
the loci of  horizontal and vertical tangents to the concentric ellipses. 

- Perhaps annotating an ellipse for what these correspond to would improve comprehensibility? I see that you briefly touch on what exactly this means/refers to in the immediately following section, so then perhaps a re-ordering would be helpful? Nevertheless, I think the specific and clear labeling between the definition and some visualization is iimportant.

> Points with the 4 largest $D^2$ values are labeled."

- Perhaps worth emphasizing whether you're referring to Mahalanobis or Euclidean-squared distance [FIXED]

> The data ellipse, centered at the means encloses approximately 68% of the data points. It adds visual information about the correlation and precision of the linear regression; but here, the non-linear trend for higher incomes strongly suggests a different approach.

- Perhaps good to re-emphasize why the number `68%` is special?

> #| label: fig-Prestige-scatterplot3
> #| out-width: "80%"
> #| fig-cap: "Scatterplot of prestige vs. income, stratified by occupational type. This implies a > different interpretation, where occupation type is a moderator variable."

- Perhaps good to use a different linetype per kind of smoother? Thing seem crowded... [DONE]

Many of the section headers in this chapter state the kind of plot they discuss. Perhaps it would help guide the reader by putting a short blurb for how that kind of plot is uniquely helpful? For example, "Plotting on a log scale: handling non-linearity in scatterplots". I know that you describe the utility of the plot in the main text following the header, but I feel that section headers would be extra helpful as guideposts.

> Because it provides a visual representation of means, variances, and correlations,

- Perhaps change correlations to covariances? 

- The rendering of the mathematics is overlapping with the texts sometimes.

- I am sometimes confused by the switch between $D^2$ and $D_M^2$ -- does $D^2$ always refer to Mahalanobis distances? Similarly, it's unclear to me whether $\chi_p^2$ and $\chi^2$ refer to the same variables or how they differ? Does $\chi_2^2$ refer to the quantiles for $p = 2$, or...?

- When referring to other chapter numbers, also giving the title of the chapter number might make things clearer
[Can't do this in Quarto]

> #| fig-cap: "Customized `ggpairs()` plot of penguin size variables, together with species, island and sex."

- The x-axis seems to be horizontally mis-aligned [Because they're rotated]

> Those who don't know history are doomed to plagarize it ---The author

- Plagiarize

- Somehow I feel like this chapter should be called `Plots of Multivariable Data`, rather than `Plots of Multivariate Data` per-se...

- This chapter is also ~ 70 pages -- it's probably the one that needs to be intercalated with the other chapters.

- My personal opinion is that the multivariate linear model is particularly well suited as an exploratory method, so there isn't a need to separate the section on Exploratory Methods from everything else. Something like probabilistic graphical models, (Bayesian networks), and SEM is more well-suited for a confirmatory analysis.

> More generally, statistical tours are a type of dynamic projections onto orthogonal axes (called a _basis_)
that embed data in a
$p$−dimensional space into a $d$−dimensional viewing subspace.

- Perhaps emphasize that $d$ is a lower-dimensional subspace, so that the purpose comes across even more clearly?

> In $\mathbf{P}_1$, the projected variable $\mathbf{y}_1$ is related only to $\mathbf{x}_1$, while $\mathbf{y}_2$ is related only to $\mathbf{x}_2$
$\mathbf{x}_3$ makes no contribution, and appears at the origin.

- Perhaps a period after $\mathbf{x}_2$ to separate the sentences?

## Chapter 4: Dimension Reduction

I appreciate the references to Flatland and Spaceland. At the same time, I feel like the repeated references to the opening material and the metaphorical style of writing could be making the abstract mathematical topic even more abstract.  

> 3. The columns $\mathbf{v}_i$ are the weights applied to the variables to produce the scores on
the principal components. For example, the first principal component is the weighted sum:

- Perhaps it'd be helpful to re-emphasize that $\mathbf{v}_i$ is a column $i$ of $\mathbf{V}$? 


$$
\text{PC}_1 = v_{11} \mathbf{x}_1 + v_{12} \mathbf{x}_2 + \cdots + v_{1p} \mathbf{x}_p
$$

- I understand what you mean, but perhaps it'd be helpful to first re-define what a principal component is, in this context before discussing how you can compute it?

> to extract principal componends whose eigenvalues exceed 1.0,

- components

The following seems odd

```{r, eval=FALSE}
crime.pca |>
  broom::augment(crime) |> # add original dataset back in
  ggplot(aes(.fittedPC1, .fittedPC2, color = region)) + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(size = 1.5) +
  geom_text(aes(label = st), nudge_x = 0.2) +
  stat_ellipse(color = "grey") +
  coord_fixed() +
  labs(x = "PC Dimension 1", y = "PC Dimnension 2") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top") 
```

Perhaps `coord_fixed` needs to be adjusted? Also, the I fixed the y-axis dimension labelling [c(-3, 3) looks better]

```{r, eval = FALSE}
crime.pca |>
  broom::augment(crime) |> # add original dataset back in
  ggplot(aes(.fittedPC1, .fittedPC2, color = region)) + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(size = 1.5) +
  geom_text(aes(label = st), nudge_x = 0.2) +
  stat_ellipse(color = "grey") +
  coord_fixed(ylim = c(-3,3), ratio = 1) +
  labs(x = "PC Dimension 1", y = "PC Dimension 2") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top") 
```

> that is shown in this space. This means that: 
  * two variable vectors that point in the same direction are highly correlated; $r = 1$ if they are completely aligned.
  * Variable vectors at right angles are approximately uncorrelated, while those
pointing in opposite directions are negatively correlated; $r = -1$ if they are at 180$^o$.

- This isn't rendering properly in the HTML version. Perhaps a new line is necessary? [Needed indent]

> these are also the eigenvectors of $\mathbf{X} \mathbf{X}'$,

- Perhaps it'd be good to clarify the special significance of what $\mathbf{X} \mathbf{X}'$ is?

```{r, eval = FALSE}
ggbiplot(crime.pca,
   obs.scale = 1, var.scale = 1,
   labels = crime$st ,
   circle = TRUE,
   varname.size = 4,
   varname.color = "brown") +
  theme_minimal(base_size = 14) 
```

- Something seems off about the scales here, I suspect that adjusting the `ylim` is necessary

> to help interpret what
what is shown in the biplot

- Double what [FIXED]


This doesn't seem very idiomatic to me...

```{r, eval = FALSE}
cor(reg.data[, 1:4], reg.data[, 5:7]) |>
  print() -> R
```

Perhaps the following would be more idiomatic? [OK]

```{r, eval = FALSE}
(R <- cor(reg.data[, 1:4], reg.data[, 5:7])) |>
  print()
```


On one hand, it would have been nice if the mathematics behind Multidimensional Scaling could be explained to the same depth as PCA, but on the other, there are page limit concerns.

# Part III: Univariate Linear Models

I have a radical suggestion that perhaps this entire part should be relegated to the online-only section. Although very cool and also a power display of the importance of data visualization, it's not really related to the multivariate response models that were advertised in the beginning of the book. I'm only bringing this up because of the page concerns, otherwise, I would've liked to keep it too. 

## Chapter 5: Overview of Linear Models

> The most common multivariate generalization is 
_multivariate multiple regression_ (MMRA), where each outcome is regressed on the predictors, as if done separately for each outcome,
but using multivariate tests that take correlations among the predictors into account.

- Perhaps emphasize that phase of hypothesis testing of the fitted model is done by taking the correlations of the predictors into account? When I read this, I get the impression that the correlations amongst the predictors is relevant to how the model is fitted. Which I don't think it is? [FIXED]

> From a study of the diameters of sweet peas in parent plants and their size in the next generation, and another on the relationship between heights of human parents and their offspring, he developed the fundamental ideas
of regression. Karl Pearson [-@Pearson:1896]

- Something about the citation format of this doesn't seem right. Should the citation of Pearson go within the sentence...? [It was incomplete]


> To establish notation and terminology, it is worthwhile to state the the general linear model formally.

- Extra `the` [FIXED]

```{r, eval = FALSE}
data(workers, package = "matlib")
workers.mod1 <- lm(Income ~ Experience, data=workers)
coef(workers.mod1) |> t() |> t()

workers.mod2 <- lm(Income ~ poly(Experience, 2), data=workers)
coef(workers.mod2) |> t() |> t()
```

- Is it really necessary to transpose the result twice? I think this might confuse beginners [Bizarre, but needed]

- I really like the explicated section on contrasts

## Chapter 6: Plots for univariate response models

This name for this section should be capitalized

> Less obvious is the relation between the marginal and AVP ellipses. In 3D, the marginal
data ellipse is the shadow of the ellipsoid for $(\mathbf{y}, \mathbf{x}_1, \mathbf{x}_2)$ on one of the coordinate planes,
while the AV plot is a slice through the ellipsoid where either $\mathbf{x}_1$ or $\mathbf{x}_2$
is held constant.

- I find this very hard to imagine and hence understand. Specifically, what do you mean by "In 3D, the marginal data ellipse is the shadow of the ellipsoid for..." ?


I like this chapter, something to bookmark.


## Chapter 7: Topics in Linear Models

I think the discussion of data space and beta-space should be re-ordered. It jumps a bit too directly into the meat of the duality before motivating why we might want to do this and how to even do the duality. In my reading, Figure 7.1 is introduced a bit abruptly -- it was difficult to understand how to interpret this graph and how the beta-space corresponds to the data spa.ce 

[Edited this to remove redundancy with Ch 06, and try to make this discussion clearer]

I'm also a bit lost on why does it matter that the points in beta space are where they are? What does this mean beyond their duality? Perhaps the motivation behind examining beta-space should be explicated earlier?

Why does the data space of Figure 7.4 show 40% and 68% intervals, whereas the beta space shows 95% intervals? Why not making the % of the intervals the same across both? [added a footnote]

> Errors in the response $y$ are accounted for in the model and measured by 
the mean squared error, $\text{MSE} = \hat{\sigma}_\epsilon^2$.
But in practice, of course, predictor variables are often also observed
indicators, subject to their own error. Indeed, in the behavioral sciences
it is rare that predictors are perfectly reliable and measured exactly.
This fact that is recognized in errors-in-variables
regression models [@Fuller2006]
and in more general structural equation models,
but often ignored otherwise.  Ellipsoids in data space and $\beta$ space
are well suited to showing the effect of measurement error in predictors on OLS estimates.

- Perhaps worth emphasizing that measurement error refers to random error that we assume is uncorrelated with anything else and that's normally distributed around the mean. Maybe emphasize that there may be other kinds of non-random measurement error that would require sophisticated modelling to handle correctly.

There's something about Figure 7.6 that makes me think that a table would be better... There's just something about the ordering of True/False on the x-axis, versus the ordering of True/False in the legend, versus the actual ordering of the lines that makes me confused about the meaning of the Model R plot.

I think it would be nice to make the code for the plots in "### Coffee data: $\beta$ space" available as a 'pop-up'. This seems like something that people would want to play with on their own and see with their own 'R'. [Maybe something interactive for the HTML version??]

## Chapter 8: Collinearity & Ridge Regression

- I think it's worthwhile to add a small note about issues with using VIF when modelling interactions

> These have the property that the resulting numbers
have common interpretations regardless of the number of predictors.

I think tableplots could've also been illustrated outside the context of plots of collinearity... perhaps?

- Perhaps change period to colon?

I'm still suspicious about whether the biplots are correctly scaled...?

> Dimension 5 has its' strongest relations to `weight`
and `horse`. 

- Extra (')?

> [genridge](https://CRAN.R-project.org/package=genridge) package and its [`pkgdown` documentation](https://friendly.github.io/genridge/) for visualization methods.

- Persumably you mean `genridge` documentation and not `pkgdown` documentation?

The text for the left plot Figure 8.8 is clipped

> Because there are multiple
degree of freedom terms in the model, `car::vif()` calculates GVIFs here.

- I think this deserves greater explanation as to what 'degrees of freedom' means here and why GVIF applies in this case, but not the others? 

- Also, I was under the impression that centering wasn't needed for improving standard errors in the case of interactions, is this not the case? Reading your text gives the impression that attempts at re-paramterizing the model (i.e., centering) will change the standard error of the interaction terms -- which I... will go out on a limb to say isn't true and would be very alarming if true? 

> The geometry of ridge regession is illustrated in

- Spelling

# Part IV: Multivariate Linear Models

## Chapter 9: Hotelling's $T^2$

## Chapter 10: Multivariate Linear Models

## Chapter 11: Visualizing Multivariate Models

## Chapter 12: Visualizing Equality of Covariance Matrices

## Chapter 13: Multivariate Influence and Robust Estimation

## Chapter 14: Case studies

This chapter section should be capitalized

# Part V: End matter

- The name for this part should be capitalized