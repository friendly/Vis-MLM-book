---
editor: 
  markdown: 
    wrap: 120
---

# Preface

> However, to do this it is useful to set the stage for multivariate thinking, with a grand scheme for statistics and data visualization, a parable, and an example of multivariate discovery.

- However, to do this it is useful to set the stage for multivariate thinking with a grand scheme for statistics, data visualization, a parable, and an example of multivariate discovery.

> scatterplot can show other variables using color, shape or other
> visual attributes
> His point in this classification was that once you've reached three variables, all higher

scatterplot can show other variables using color, shape or other
visual attributes.
His point in this classification was that once you've reached three variables, all higher

> the plane he inhabits. It is a large circle when seen at the moment of its' greatest extent. As the Spehere rises, it becomes progressively smaller, until it becomes a point, and then vanishes.

the plane he inhabits. It is a large circle when seen at the moment of its' greatest extent. As the Sphere rises, it becomes progressively smaller, until it becomes a point, and then vanishes.

> To really see a tesseract it helps to view it in an animation over time (@fig-tesseract).

- Rendering of `@fig-tesseract` is broken


The last parts of this section have a few `TODO`s that need to be filled in.

# Part I: Orienting Ideas

## Chapter 1: Introduction

> This material may or may not survive; it was taken from an earlier article.

- Could delete

> A particular research outcome (e.g., depression, neuro-cognitive functioning, academic achievement, self-concept, attention deficit hyperactivity disorders) might take on a multivariate form if it has several observed measurement scales or related aspects by which it is quantified, or if there are multiple theoretically distinct outcomes that should be assessed in conjunction with each other (e.g., using depression, generalized anxiety, and stress inventories to model overall happiness).

- Could be simplified + broken down, built up to the concrete example from simpler principles/motivations. I think emphasizing the contrast and also the potential cognitive blindspot with univariate-response models is key. 

::: {.callout-note title="SEM"}
Structural equation modeling (SEM) offers another route to explore and analyze the relationships among multiple predictors and multiple responses. They have the advantage of being able to test potentially complex systems of linear equations in very flexible ways. However, these methods are often far removed from data analysis *per se* because they typically start with data summaries such as means and
covariance matrices.
Except for path diagrams they usually offer little in the way of visualization methods to aid in understanding and communicating the results. The graphical methods we describe here can also be useful in a SEM context.
\ix{Structural equation model}
:::

- I think what's important to emphasize is the confirmatory/exploratory distinction for in comparing SEM with multivariate linear models.

- It's may also be worthwhile to emphasize why it's a problem that SEM emphasizes `data summaries such as means and covariance matrices", as the downstream implications of this fact may not be immediately relevant to everyone


> The aim of this book is to describe and illustrate some central methods that we have developed over the last ten years that aid in the understanding and communication of the results of multivariate linear models [@Friendly-07-manova; 
<!-- @Friendly-etal:ellipses:2013;  --> 
@FriendlyMeyer:2016:DDAR]. These methods rely on *data ellipsoids* as simple, minimally sufficient visualizations of variance that can be shown in 2D and 3D plots. As will be demonstrated, the *Hypothesis-Error (HE) plot* framework applies this idea to the results of multivariate tests of linear hypotheses.

- I think it'd be a good idea to put this in bold, or some stronger way to emphasize how this is the key of this chapter. Perhaps changing the heading of this section from "Visualization is harder" would also help. 

> Not only is this confusing and awkward to report, but it is largely unnecessary because the multivariate tests provide protection for multiple testing.

- I think future readers would appreciate some a citation or a cross-reference to the relevant section in your book regarding protection for multiple testing, especially as this is a hot topic.

## Chapter 2: Getting Started

For length, I think you should pick between the pop-out boxes of  `Datasaurus Dozen` and `Quartets`.

For length, I think you should pick between the Davis example and the 1970 Draft Lottery Data.

# Part II: Exploratory Methods

## Chapter 3: Plots of Multivariate Data

> Confidence bands allow us to visualize the uncertainty around a fitted regression curve,
which can be of two types: _pointwise intervals_ or _simultaneous intervals_.
The default setting in ``ggplot2::geom_smooth()` calculates pointwise intervals 
(using `stats::predict.lm(..., interval="confidence")` at a confidence level $1-\alpha$ for the predicted response at _each value_ $x_i$ of a predictor, and have the frequentist interpretation that over repeated sampling only $100\;\alpha$ of the predictions at $x_i$ will be outside that interval. 
In contrast, simultaneous intervals are calculated so that $1 - \alpha$ is the probability that _all of them_ cover their corresponding true values simultaneously. These are necessarily wider than pointwise intervals.
Commonly used methods for constructing simultaneous confidence bands in regression are the Bonferroni and ScheffÃ© methods, which control the family-wise error rate over all values of $x_i$.
See [](https://en.wikipedia.org/wiki/Confidence_and_prediction_bands) for precise definitions of these terms.
These are different from a _prediction band_, which is used to represent the uncertainty about the value of a **new** data-point on the curve, but subject to the additional variance reflected in one observation.

- The wikipedia reference doesn't render properly, perhaps due to lack of text. 
- I think this is a bit confusing -- perhaps shorter sentence and vertical spacing between groups of thought could help? 

> **Grouping**: Identify subgroups in the data by assigning different
    visual attributes, such as color, shape, line style, etc. within a
    single plot. This is quite natural for factors; quantitative
    predictors can be accommodated by cutting their range into ordered
    intervals. Grouping has the
    advantage that the levels of a grouping variable can be shown within
    the same plot, facilitating direct comparison.

>  **Conditioning**: Showing subgroups in different plot panels. This
    has the advantages that relations for the individual groups more
    easily discerned and one can easily stratify by two (or more) other
    variables jointly, but visual comparison is more difficult because
    the eye must scan from one panel to another.
    
Perhaps explicitly referring to example plots within each of these definitions would make things more readily understood? Perhaps even adding separate sub-section for grouping itself would also make things easier

> The most obvious variable that affects academic salary is `rank`,
because faculty typically get an increase in salary with a promotion
that carries through in their future salary. What can we see if we group
by `rank` and fit a separate smoothed curve for each?

- The transition to this seems quite abrupt

> Mathematically, Euclidean (squared) distance for $p$ variables

- Would be helpful to clarify that $D_E^2(y)$ is the Euclidean squared distance. For example, "Mathematically, the Euclidean (squared) distance $(D_E^2(y))$ for $p$ variables.
- Some transition to the discussion of Euclidean squared distance would help
- Wouldn't hurt to also emphasize the difference between $\mathbf{y}$ and $y$ for clarity.
- Explicitly stating the relationship between the Euclidean-squared distance and Mahalanobis' distance would help

> For $p$ variables, the data _ellipsoid_ $\mathcal{E}_c$ of
size $c$ is a $p$-dimensional ellipse,
defined as the set of points $\mathbf{y} = (y_1, y_2, \dots y_p)$
whose squared Mahalanobis distance, $D_M^2 ( \mathbf{y} )$ is less than or equal
to $c^2$,

- Defining or showing what $c$ is would help

> where $\mathbf{S}^{1/2}$ represents a rotation and scaling and the notation $\oplus$ represents translation to a new centroid, $\bar{\mathbf{y}}$ here. The matrix $\mathbf{S}^{1/2}$ is commonly computed
as the Cholesky factor of $\mathbf{S}$.

- Perhaps state the Cholesky factor first before stating its rotational and scaling effects?


- Some motivation or narrative for why we must discuss the following equation at all 
$\mathcal{E}_c (\bar{\mathbf{y}}, \mathbf{S}) := \{ D_M^2 (\mathbf{y}) \le c^2 \}$, instead of just $\mathcal{E}_c (\bar{\mathbf{y}}, \mathbf{S}) = \bar{\mathbf{y}} \; \oplus \; c\, \sqrt{\mathbf{S}}$ would be helpful.


> the lines of means of ($Y | X$) and ($X | Y$) corresponded approximately to
the loci of  horizontal and vertical tangents to the concentric ellipses. 

- Perhaps annotating an ellipse for what these correspond to would improve comprehensibility? I see that you briefly touch on what exactly this means/refers to in the immediately following section, so then perhaps a re-ordering would be helpful? Nevertheless, I think the specific and clear labeling between the definition and some visualization is iimportant.

> Points with the 4 largest $D^2$ values are labeled."

- Perhaps worth emphasizing whether you're referring to Mahalanobis or Euclidean-squared distance

> The data ellipse, centered at the means encloses approximately 68% of the data points. It adds visual information about the correlation and precision of the linear regression; but here, the non-linear trend for higher incomes strongly suggests a different approach.

- Perhaps good to re-emphasize why the number `68%` is special?

> #| label: fig-Prestige-scatterplot3
> #| out-width: "80%"
> #| fig-cap: "Scatterplot of prestige vs. income, stratified by occupational type. This implies a > different interpretation, where occupation type is a moderator variable."

- Perhaps good to use a different linetype per kind of smoother? Thing seem crowded...

Many of the section headers in this chapter state the kind of plot they discuss. Perhaps it would help guide the reader by putting a short blurb for how that kind of plot is uniquely helpful? For example, "Plotting on a log scale: handling non-linearity in scatterplots". I know that you describe the utility of the plot in the main text following the header, but I feel that section headers would be extra helpful as guideposts.

> Because it provides a visual representation of means, variances, and correlations,

- Perhaps change correlations to covariances? 

- The rendering of the mathematics is overlapping with the texts sometimes.

- I am sometimes confused by the switch between $D^2$ and $D_M^2$ -- does $D^2$ always refer to Mahalanobis distances? Similarly, it's unclear to me whether $\chi_p^2$ and $\chi^2$ refer to the same variables or how they differ? Does $\chi_2^2$ refer to the quantiles for $p = 2$, or...?

- When referring to other chapter numbers, also giving the title of the chapter number might make things clearer

> #| fig-cap: "Customized `ggpairs()` plot of penguin size variables, together with species, island and sex."

- The x-axis seems to be horizontally mis-aligned

> Those who don't know history are doomed to plagarize it ---The author

- Plagiarize

- Somehow I feel like this chapter should be called `Plots of Multivariable Data`, rather than `Plots of Multivariate Data` per-se...

- This chapter is also ~ 70 pages -- it's probably the one that needs to be intercalated with the other chapters.

- My personal opinion is that the multivariate linear model is particularly well suited as an exploratory method, so there isn't a need to separate the section on Exploratory Methods from everything else. Something like probabilistic graphical models, (Bayesian networks), and SEM is more well-suited for a confirmatory analysis.

> More generally, statistical tours are a type of dynamic projections onto orthogonal axes (called a _basis_)
that embed data in a
$p$âdimensional space into a $d$âdimensional viewing subspace.

- Perhaps emphasize that $d$ is a lower-dimensional subspace, so that the purpose comes across even more clearly?

> In $\mathbf{P}_1$, the projected variable $\mathbf{y}_1$ is related only to $\mathbf{x}_1$, while $\mathbf{y}_2$ is related only to $\mathbf{x}_2$
$\mathbf{x}_3$ makes no contribution, and appears at the origin.

- Perhaps a period after $\mathbf{x}_2$ to separate the sentences?

## Chapter 4: Dimension Reduction

I appreciate the references to Flatland and Spaceland. At the same time, I feel like the repeated references to the opening material and the metaphorical style of writing could be making the abstract mathematical topic even more abstract.  

> 3. The columns $\mathbf{v}_i$ are the weights applied to the variables to produce the scores on
the principal components. For example, the first principal component is the weighted sum:

- Perhaps it'd be helpful to re-emphasize that $\mathbf{v}_i$ is a column $i$ of $\mathbf{V}$? 


$$
\text{PC}_1 = v_{11} \mathbf{x}_1 + v_{12} \mathbf{x}_2 + \cdots + v_{1p} \mathbf{x}_p
$$

- I understand what you mean, but perhaps it'd be helpful to first re-define what a principal component is, in this context before discussing how you can compute it?

> to extract principal componends whose eigenvalues exceed 1.0,

- components

The following seems odd

```{r, eval=FALSE}
crime.pca |>
  broom::augment(crime) |> # add original dataset back in
  ggplot(aes(.fittedPC1, .fittedPC2, color = region)) + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(size = 1.5) +
  geom_text(aes(label = st), nudge_x = 0.2) +
  stat_ellipse(color = "grey") +
  coord_fixed() +
  labs(x = "PC Dimension 1", y = "PC Dimnension 2") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top") 
```

Perhaps `coord_fixed` needs to be adjusted? Also, the I fixed the y-axis dimension labelling

```{r, eval = FALSE}
crime.pca |>
  broom::augment(crime) |> # add original dataset back in
  ggplot(aes(.fittedPC1, .fittedPC2, color = region)) + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(size = 1.5) +
  geom_text(aes(label = st), nudge_x = 0.2) +
  stat_ellipse(color = "grey") +
  coord_fixed(ylim = c(-5,5), ratio = 1) +
  labs(x = "PC Dimension 1", y = "PC Dimension 2") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top") 
```

> that is shown in this space. This means that: 
  * two variable vectors that point in the same direction are highly correlated; $r = 1$ if they are completely aligned.
  * Variable vectors at right angles are approximately uncorrelated, while those
pointing in opposite directions are negatively correlated; $r = -1$ if they are at 180$^o$.

- This isn't rendering properly in the HTML version. Perhaps a new line is necessary?

> these are also the eigenvectors of $\mathbf{X} \mathbf{X}'$,

- Perhaps it'd be good to clarify the special significance of what $\mathbf{X} \mathbf{X}'$ is?

```{r, eval = FALSE}
ggbiplot(crime.pca,
   obs.scale = 1, var.scale = 1,
   labels = crime$st ,
   circle = TRUE,
   varname.size = 4,
   varname.color = "brown") +
  theme_minimal(base_size = 14) 
```

- Something seems off about the scales here, I suspect that adjusting the `ylim` is necessary

> to help interpret what
what is shown in the biplot

- Double what


This doesn't seem very idiomatic to me...

```{r, eval = FALSE}
cor(reg.data[, 1:4], reg.data[, 5:7]) |>
  print() -> R
```

Perhaps the following would be more idiomatic?

```{r, eval = FALSE}
(R <- cor(reg.data[, 1:4], reg.data[, 5:7])) |>
  print()
```


On one hand, it would have been nice if the mathematics behind Multidimensional Scaling could be explained to the same depth as PCA, but on the other, there are page limit concerns.

# Part III: Univariate Linear Models

## Chapter 5: Overview of Linear Models

## Chapter 6: Plots for univariate response models

This chapter section should be capitalized

## Chapter 7: Topics in Linear Models

## Chapter 8: Collinearity & Ridge Regression

# Part IV: Multivariate Linear Models

## Chapter 9: Hotelling's $T^2$

## Chapter 10: Multivariate Linear Models

## Chapter 11: Visualizing Multivariate Models

## Chapter 12: Visualizing Equality of Covariance Matrices

## Chapter 13: Multivariate Influence and Robust Estimation

## Chapter 14: Case studies

This chapter section should be capitalized

# Part V: End Matter

- This part section should be capitalized