---
editor: 
  markdown: 
    wrap: 120
---

# MT tasks

1.  Complete this review of the current draft as you've been doing, in this file.

-   Try to make refs to the text explicit, so I can find them

2.  Re-read at a high, overview level.

-   As perhaps another pass through the ms., try to read for coherence, structure, and placement of ideas/methods across
    chapters.
-   What needs to be explained earlier?
-   What things seem too deep in the weeds?

3.  Read through `working-text/general-issues.md`

-   What can you help with?

# Chapter 2 Comments

Section 2.1.1 -- a different word from 'rough' to better describe the juxtaposition? **DONE**

-   Maybe change the section header to better highlight the point that this is for debunking? These biases seems so
    implicit even today that without careful reading, one may be impressed that these notions are to be maintained
    @MT**:** **What would be better?**
-   Quarto formatting is a bit off in web-format, it's showing `::: {.cell layout-align=“center”}` **Fixed; need a blank
    line b4 code** But should check generally.
-   Someway to disable ligatures on R-pipe (\|\>)? I think this may confuse those unfamiliar with fonts **DONE: Use
    `monofont: "Fira mono"`, not `monofont: "Fira code"`**

Figure 2.3 - Maybe add an alpha, distinguish lines by linetype based on sex, and make points hollowed out?

Add TODO regarding Chapter XX, to put the actual number **DONE**

-   Fix *both* to actual formatting

-   Add TODO regarding sections 2.4, 2.5 and 2.6.

-   Packages used here? `report` library?

# Chapter 3

Section 3.1.1 - Definition of pointwise confidence bands and contrasting appropriate with inappropriate interpretations
would be nice \[**DONE**\]

Figure 3.3 - Maybe adjust the colors/linetype to better show how the LOESS smooth closely matches the quadratic fit?

Figure 3.4 and 3.5 and 3.7 - Distinguish by shape/linetype as well? - Also adjust using alpha? \[I get what you're
saying: better to distinguish by more than color. I'm reluctant to use `linetype`, b/c they vary in visual "weight", and
anyway, the lines don't overlap much here.\]

Mathematics in Section 3.2 - Perhaps greater elaboration on the mathematics and/or instantiating it in terms of concrete
R code could be helpful? Or perhaps where to look for a more elaborate description? It's a bit terse

Figure 3.10 - It would be nice if the graph could be elaborated, such that each of the things about 'horizontal/vertical
tangents' to the concentric ellipses, major and minor axes and more were labelled. - It's not easy for everyone to
immediately recognize what is meant by something like major/minor axes versus central cross. - It would be a lot more
work, but from a pedagogical point of view, I think it'd be nice if it was shown how the complete graph can be
constructed piece-by-piece and how each individual piece contributes to the meaningof the larger graph.

-   Latex right before section 3.2.2 is incomplete -- not rendering correctly:
    $\(\sqrt{\mathbf{v}_1}, \sqrt{\mathbf{v}_1\), …$ \[**FIXED**\]

-   Figure 3.1 can be made to be be the same text size and etc as the other graphs? It seems unusually small for some
    reason

Section 3.2.3 and 3.2.3.1 -- for the in-text R code perhaps the coefficients can be called by name instead of by number?
`coef(lm(prestige ~ income, data=Prestige))["COEFFICIENT NAME"]` instead of
`coef(lm(prestige ~ income, data=Prestige))[2]`. \[**DONE**\]

-   R code for Figure 3.14 seems a bit off? \[I'm using `source-line-numbers: "2"` to highlight line 2, but maybe not a
    good idea.\]

-   Why not make everything in ggplot2?

-   Maybe also add Representational Similarity Analysis and even network diagrams as another summary of bivariate
    relationships?

-   y-axis text is sqquished up in Figure 3.31 \[I don't think I can do much about this\]

## Chapter 3.8 Network Diagrams Comment October 30/2024

-   Figure 3.45 seems low resolution in my web browser. Is there a way we could increase it? Perhaps even make it into
    svg? \[Took a higher-res screen capture. Trying to avoid running his code.\]

> Here, edges are drawn only between nodes where the correlation is considered significant by a method (“glasso”)
> designed to make the graph optimally sparse.

-   Rather than use the loaded term 'significant', I would rather say the more neutral important. As far as I can tell,
    Rodrigues (2021) didn't use bootstrapping on qgraph, which is what's needed to say whether an edge is statistically
    significant or not.

> The edges shown in Figure 3.45 reflect the Pearson correlation between a given pair of items by the visual attributes
> of color and line style: magnitude is shown by both the thickness and transparency of the edge; the sign of the
> correlation is shown by color and line type: solid blue for positive correlations and dashed red for negative ones.

-   I'd say 'reflect the regularized partial correlations between a given pair of items, controlling for all other
    items, by the visual attributes of...

-   GLASSO is only necessary for the problem of partial correlations or else you could just visualize the marginal
    (Pearson) correlations using standard approaches. (With a helping of the `hetcor` and `correlation` libraries)

> Network diagrams stem from mathematical graph theory (Bondy & Murty, 2008; West, 2001) of the properties of nodes and
> edges used to represent pairwise relationships, …

-   Perhaps it may be nice to say that whereas multiple linear regression generally focuses on one response variable at
    a time, network diagrams blur the difference between a predictor and a response variable.

Everything else looks good to me AFAICT

# Chapter 4

## PCA

-   Generalized PCA?

-   The capitalization for each of the bullet points of
    `It was but a small step to recognize that for two variables, $x$ and $y$:...` is a bit odd/inconsistent
    \[**FIXED**\]

-   Perhaps making Figure 4.4 go into a continuous loop of a GIF would be better? The way it jumps from the beginning to
    the end makes it unclear for me what to focus my attention on and by the time I focus on something, the GIF resets
    and I forget what I was looking at.

    -   It's also a bit ambiguous what the 'horizontal and vertical' dimensions are in a moving 3D graph \[**added: in
        the final frame**\]

-   I like Figure 4.5. I wonder if it'd be possible to make everything in one visualization framework? GGplot2? Or is
    this unnecessary?

-   The language in Chapter 4 -- particularly 4.2.2 -- is quite metaphorical and vivid. I'm worried that this may
    distract the reader from what the main point is trying to be conveyed. Is there a way to balance style and
    directness?

-   The mathematics of PCA seems like a big of a leap

-   Perhaps supplementing it with visual explanations and a concrete example with a low number of dimensions might make
    it more intuitive? \[**TODO: Add R/workers-pca.R example**\]

## 4.2.3

-   Could be nice to clarify whether the data need to already be centered/scaled or whether the function can handle it.
    \[**DONE**\]
-   Would be nice to clarify what `center` and `scale` are in the output of `prcomp()`

## 4.2.4

> "The typical rough guidance is that the last point to fall on this line represents the last component to extract, the
> idea being that beyond this, the amount of additional variance explained is non-meaningful." - This could be rephrased

## 4.2.5

Figure 4.8 -- perhaps the legend could have the letter 'a' removed? \[**That's a ggplot2 gotcha**\]

-   Ore we could make the legend be similar with Figure 4.12

-   Shouldn't Figure 4.8 have its y-axis lengthened to really show that the two components are uncorrelated?

-   Y-axis seems to be mislabeled in Figure 4.8

-   I'm a bit confused about multiplying the principal component loadings by -1. Should I only multiply by -1 if all of
    the loadings are negative? And if I multiply by -1, should it be to all of the principal components? {\*\*DONE}

-   Because only the first 2 and 4th loadings are negative for PC2, the rest are positive.

-   Perhaps elaborate on what "the cosine of the angle between two variable vectors" means?

## 4.3

intro seems incomplete? `...` ? \[FIXED\]

### 4.3.1

-   `i`, `r` and `p` could be more explicitly defined?

-   Perhaps some concrete visual illustration of the ideas of eigenvectors, eigenvalues and orthonormality and how they
    relate to some data could be pedagogically helpful?

-   Could we consider going beyond the biplot, to cases where multiple principal components may be visualized at once?
    Perhaps via a parallel coordinate plot?

-   Perhaps we could use the idea of simulating your own data and show how to vary the simulation in order to get
    particular principal component results?

    -   For example, how to simulate data from a two factor model with 10 indicators each? What would these indicators
        look like with principal components?
    -   Or maybe generate data from a composite model (Cho and Choi, 2020; Behaviormetrika)?

-   TODO is missing a preceding `*`

### 4.3.3

I suspect that Figure 4.11 might not be scaled correctly?

-   It would be nice to have an explanation for why
    `The correlation circle indicates that these components are uncorrelated and have equal variance in the display.`
    and under what circumstances you would think that the components are correlated and not have equal variance.
    \[Fixed\]

Figure 4.12 seems a bit crowded between the mix of letters and lines, perhaps we could adjust alpha and do some
dodging - It may be nice if we could arrange the legend for the regions to correspond to the direction that they point
to. Because right now the 'Northeast' is at the 'Southeast of the plot' \[I generally prefer direct labels to legends,
but that couldn't work here\]

-   'Murder' is also cutoff \[ Re: cutoff text -- Added `clip` arg to `ggbiplot`; use `clip = "off"` in examples

Figure 4.13 -- 'larceny' is cutoff

### 4.3.5

Perhaps some more background for why Figure 4.15 is true and how it relates to multiple linear regression could be good?

-   Maybe elaborate on why the multivariate regression on the principal components excluded the intercept?
-   So is using supplementary variables the same or different from principal components regression? I'm confused.
    \[Different: PCR replaces predictors by their principal components. PCA and biplots have no response variable. We're
    just finding how the supp variables relate to the components\]
-   Because if it's the same, then why not just use `prcomp()`, then perform `lm()` on the principal components...? Or
    perhaps using the `pls::pcr()`?
    -   Is it because there's an easy transition into plotting the results and because it's not introducing a new
        package?
-   Could the following be elaborated?

> Note that these coefficients are the same as the correlations between the supplementary variables and the scores on
> the principal components, up to a scaling factor for each dimension. This provides a general way to relate dimensions
> found in other methods to the original data variables using vectors as in biplot techniques.

**Maybe it would be better to put all of the TODOs into the Github Issues page that way it's easier to sort through and
consider all of them?**

## Section 4.4

### Section 4.4.1

-   Perhaps explain what 'nonmetric' means and why that's advantageous?

### Section 4.4.2

> t-SNE defines a similar probability distribution over the points \_i\$ in the low-dimensional map, and it minimizes
> the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the
> points in the map

-   Perhaps there's a latex error? $q_{ij}$ maybe?

#### Section 4.4.2.1

So just to confirm, are the differences between PCA and t-SNE (rotation) specific to this data or in general? I thought
some stronger statement on when we should consider each technique would be nice.

## Section 4.5

Caption for Figure 4.26 is missing

## Section 4.7

-   The animation for Figure 4.32 is uncharacteristcally slow compared to all the other animations...

# Chapter 5

This chapter feels like it should come first in the book

So far, my impression is that restructuring the book to resemble a 'workflow' view of how to address particular
multivariate data problems would be helpful.

\[The general idea is that exploratory, graphical methods come first, followed by model-based methods, first univariate,
then multivariate\]

## Section 5.1

Can multivariate logistic regression not be used when p = 1... and is quantitative?

Isn't the bottom right for p \> 1 and q \>1 also the log-linear model?

Does ANCOVA refer to any GLM model with both continuous and discrete predictors? \[Strictly speaking, just where X
doesn't interact with a factor\]

## Section 5.2

I think the latex renderred a bit incorrectly?

### 5.2.1.1

-   Doesn't the default for factors involve a deviation from baseline as opposed to the individual mean for each level
    of the factor? Perhaps a note about how that affects interpretation of `y ~ x + A` would be helpful, or how to
    remove the intercept, maybe?

# Chapter 6

## 6.1

"This provides an assessment of homogeneity of variance, which appears as a tendency for scale to vary with location." -
Perhaps you mean, "an assessment of violations to homogeneity of variance. Violations appear as a tendency for scale to
vary with location."? The former implies that scale varying with location is a good thing? \[Fixed\]

### 6.1.1

Maybe make the unit diagonal in Figure 6.1 dashed and in some other color for contrast? \[Made it green\]

Figure 6.1 is quite amazing!

## 6.3

### 6.3.3

I wonder if it'd be excessive to try something like a specification curve of a multiverse analysis \[YES -- excessive\]

## 6.4

Is the added variable related to the partial correlations? It seems like the two ideas should be awfully related to each
other...

Oh it is, Section 6.4.1!. I think we'd bit a bit remiss to not at least mention the gaussian graphical model and mixed
graphical models, in passing then, because it shows all the partial correlations between all pairs of variables.

Arjun recently made a whole speech about accumulated local effects. Is this something pertinent to this chapter?

Although added-variable plots and its associations are well-explained, it doesn't seem like there was much backtracking
to the original MLR's mystifying coefficients? - Are the signs of the coefficients related to some sort of collider-bias
thing going on?

Perhaps you could discuss how average marginal effects compare with effects plots?

## 6.5

I get the impression that one should just skip AV plots and go to effect displays? Is this accurate?

### 6.6.1

"’;.\[;p;khnThe analogous formula is" \[Fixed\]

-   Typo

There's a real armada of regression diagnostics included here. Given all the tools that one takes with them before they
even begin modelling these days -- DAGs, scatterplot matrices, dimension reduction, etc--what do you make of regression
diagnostics within an analytical workflow? Are they still necessary?

# Chapter 7

## 7.1

-   I was a bit thrown off by "each line like $y= a + bx$ in data space corresponds to a point $(a, b)$ in $\beta$
    space.
    -   some how it made me think that $a$ and $b$ were data-points -- columns of $x$. Could we not use the notation of
        $beta_0$ and $beta_1$ here? That's similar to how LASSO vs ridge regression is generally illustrated...
-   Why does it switch later to $\alpha$ and $\gamma$?

"It is well to understand the underlying geometry" - Missing word between 'well' and 'to'?

I'm confused how we got from $\beta$ spaces to ellipsoids to the covariance matrix, perhaps further explanation could be
in place? \[explained better now\]

Perhaps some further discussion of $\oplus$ could be helpful? \[added description\]

### 7.1.1

What are $F_{d,v}^{0.95}$? Is that the $F$-statistic? $s_e$? Perhaps further discussion on how the $F$-statistic became
relevant would be illuminating.

### 7.2.2

This reminds a lot of Loken and Gelman 2017.

"Thus, as $\sigma^2_{\eta}$ increases, $\hat{\beta}_{x^*}$ becomes less than \$." \[fixed\] - Perhaps a missing symbol?

Figure 7.5 -- perhaps flip order of False and True in the legend? Maybe also differentiate by whether the lines solid or
dashed? \[Good idea! DONE\]

### 7.2.3

Figure 7.6 is pretty interesting.

Figure 7.7 is very interesting. I think it would be nice if the lines for the marginal estimates were solid, black and
quite large instead. Then the lines for the axes to be dashed and relatively small.

# Chapter 8

## Section 8.1

-   In my personal opinion, it'd be nice to denote $\sigma^2$ as $\sigma_\epsilon^2$. For a very long time, I was always
    very confused about the differences among $\sigma^2$, $\sigma_\epsilon^2$ and $\sigma_y^2$. A brief math-stat note
    about how all of these are related would not be un-appreciated.

> "or use ipsatized scores that sum to a constant..."

-   Defining what ipsatized scores explicitly would be nice

> More generally, collinearity refers to the case when there are very high multiple correlations among the predictors,
> such as...

-   When discussing 'multiple correlations', it would be nice to define and contrast it with simple correlation.

### Section 8.1.2

> Recall (Section #sec-data-beta) that the confidence ellipse for

-   Incomplete rendering?

## Section 8.2

I'm wondering if collinearity will be brought up in later chapters because it feels like we're getting quite deep into
this topic. Maybe, too deep?

\[Collinearity is the **same** problem in multivariate models. I need to state this explicitly.\]

### 8.2.1

Given the recurring call-outs to the inverse of correlation/covariance matrices, it seems like some discussion of
(partial) correlation networks seems awfully fitting as a way of visualizing the whole issue.

### Section 8.2.2

I can't help but intuit that there's a lot of ways to try to visualize collinearity, even outside of the network
literature. This section seems very mathematical/analytical.

### Section 8.2.3

> the condition indices, using using squares whose background color is red - Typo \[FIXED\]

-   Is it really necessary to separate section 8.2.2 and 8.2.3? Or perhaps we could integrate them together? Or perhaps
    rename their section headings to better describe their interrelationship?

### Section 8.2.4

I don't know if it would be overkill, but it seems like discussing factor analysis and contrasting it with PCA would be
nice. My recollection is that some multivariate statistics textbooks do discuss factor analysis, anyways. I suspect that
some of the content in this section may be redundant with the chapter/sections on PCA in "Dimension Reduction".

Figure 8.4 - Would it be helpful to useequal scales for the axes? \[NO: the axes must use asp=1 in a biplot\]

> An extreme outlier (case 20) appears in the lower left corner. - Do you mean lower right? \[FIXED\]

## Section 8.3

> Replace highly correlated regressors with linear combination(s) of them.

-   Perhaps it may be good to discuss the psychometric commitments that one may be making by choosing to do a linear
    combination of variables versus a factor model? I know this is a burgeoning literature, especially with composite
    structural equation modelling

Overall, I feel like there could have been more data-viz here and that there should probably be many options to choose
from. I suspect that one could place this chapter before the one on dimension reduction. Perhaps we could start the book
by introducing the central theme as "How to tackle large datasets with multiple response variables". Then, we could lead
into collinearity as one of the common issues that one faces in large datasets.

# Chapter 8 Nov 13/2024 Edition

> Another collection of graphical methods, generalized ridge trace plots, implemented 
in the `r pkg("genridge")``

- There's an extra '`'

The Where's Waldo is funny and apt

> In the limiting case, when one $x_i$ is _perfectly_
predictable from the other $x$s, i.e., $R^2 (x_i | \text{other }x) = 1$, 

- I'd put the extra emphasis right after wards like: "In the limiting case, collinearity becomes particularly problematic  when one $x_i$ is _perfectly_ predictable from the other $x$s, i.e., $R^2 (x_i | \text{other }x) = 1$. This is problematic because:"

- I'm biased towards putting the extra emphasis because it makes it easier to understand that the following list of sentences is a list of problems. 

> A more subtly case is the use _ipsatized_, defined as
scores that sum to a constant, such as proportions of a total.

- Perhaps, 'subtle'? And 'the use of _ipsatized_ scores, which are defined'

> You might have scores on
tests of reading, math, spelling and geography. With ipsatized scores, any one of these
is necessarily 1 $-$ sum of the others. 

- I'm not sure I understand, could you please give a concrete numerical example?

> Beyond this, the least squares solution may have poor numerical accurracy

- accuracy

> For example: predicting strength from the highly correlated height and weight

- Do you mean that it wouldn't make sense to try to separate height and weight from each other because since the two predictors are so highly correlated with each other, you'll be very limited in how many people will have a particularly high height at that weight? Perhaps this could be more simply described with a graph... Even verbally, I'm finding it complicated

```{r, eval = FALSE}
matrix(c(s[1],    r * prod(s),
         r * prod(s), s[2]), nrow = 2, ncol = 2)
```

Perhaps just do `r * s[1] * s[2]` instead of `r * prod(s)`? It's only two elements and I don't think many people use `prod()` frequently enough for them to know what it means... I thought it was a cross product


I really like Figure 8.4. I think it'd be nice if we could call it 'x1 $\beta$ coefficient' instead of x1 coefficient. It would've been great if the 1996 LASSO paper had something like this

> Note that when there are terms in the model with more than one degree of freedom, such as education with four levels
(and hence 3 df) or a polynomial term specified as `poly(age, 3)`, that variable, education or age
is represented by three separate $x$s in the model matrix, and the standard VIF calculation
gives results that vary with how those terms are coded in the model. 

- Perhaps there could be more direct emphasis that the VIF could be high simply because of how the terms are coded and not because there is something strictly 'problematic' regarding our data/model. ATM, it's coming across as neutral and it makes GVIF seem more like a neat side-trick than something that one should seriously consider in these cases.

> More generally, the matrix $\mathbf{R}^{-1}_{X} = (r^{ij})$, when standardized to a correlation matrix
as $-r^{ij} / \sqrt{r^{ii} \; r^{jj}}$ gives the matrix of all partial correlations,
$r_{ij} \,|\, \text{others}$.
}

- Dangling `}`
- I like and appreciate this informational pop-up

- I like the concrete explanation in section 8.2.2. 
- On my screen of the website, the printout of the `cd` object seems a bit broken. Not sure if this is a rendering issue? The thing is that it appears that the column names for columns 1 and 2 are not quite where they should be?

- Figure 8.5 is great

```{r colldiag2, eval = FALSE}
print(cd, fuzz = 0.5)
```

- Figure 8.6: 
    - I forget if I mentioned this, but I feel like the ggplot versions of the biplots might be a bit off because the magnitude of the horizontal distance between ticks on the x-axis are always very different from the magnitude of the vertical distance between ticks on the y-axis?
    - The dark blue text and the many black dots are making it a bit hard to read. Perhaps we could make the dots bigger, change shape, change to a color that might mix/blend with the text well? https://mjskay.github.io/ggblend/ ?
    - Maybe also adjust alpha?
    
> If we are only interested in predicting / explaining an outcome, 
and not the model coefficients or which are "significant", collinearity can be largely ignored.

- I understand what you mean, but in my consulting sessions, I feel that clients don't seem to understand what they're committing to when they want 'pure prediction'. This seems to be compounded by a sense that in the social sciences because causality is largely out of the question, the 'only thing we should care about is prediction' *ever*. Which I don't think is quite the same thing as what statisticians mean by 'if you only care about predictions'.
- I suspect that these days 'explaining an outcome' also has the connotation of causal inference, imo. Or I feel it really should, especially after Yarkoni & Westfall 2017.

> When some predictors share a common cause, as in GNP or population in time-series or cross-national data,

- Perhaps some context on what 'GNP', 'population in time'series' data are and whatthey have to do with common causes? Do you mean GDP? 

> use Bayesian regression; if multicollinearity prevents a regression coefficient from being estimated precisely, then a prior on that coefficient will help to reduce its posterior variance.

- Perhaps not a comment, but it's interesting to read the Stan documentation about collinearity: https://mc-stan.org/docs/stan-users-guide/problematic-posteriors.html#sampling-difficulties-with-problematic-priors and https://mc-stan.org/docs/stan-users-guide/problematic-posteriors.html#collinearity.section 

> The effect of centering here is remove the linear association in what is a purely quadratic relationship,
as can be seen by plotting `y1` and `y2` against `x`.

- This section was incredibly illuminating for me, especially the graph. I've heard this repeated so many times, but it was incredibly mystifying why it would be true in my head.

> This is far better, although
still not great in terms of VIF. But, how much have we improved the situation by the simple
act of centering the predictors? The square roots of the ratios of VIFs tell us the impact
of centering on the standard errors.

- Couldn't we use the tableplots and other diagrams from earlier? I don't think it's a waste to have another diagram... Especially over numerical output

- Figure 8.8
    - Could we have an animation maybe?
    - The sqrt{t} seems misplaced... maybe?
    - Maybe differentially have a color gradient to the red ellipses and have a legend to how they differ from each other?
    - I think it'd be nice to have append a pair of graphs for how the relationship between y ~ \beta_1 or y ~ \beta_2 varies with the path of beta^RR
    
> `glmnet::glmnet()` also implements a method for multivariate responses with
a `family="mgaussian".

- Missing '`' for "mgaussian"

> The dotted lines in @fig-longley-traceplot1
show choices for the ridge constant by two commonly used criteria to balance bias against precision due to 
@Hoerl-etal-1975 (`HKB`) and 
@LawlessWang:1976 (`LW`).
These values (along with a generalized cross-validation value `GCV`) are also stored in the "ridge" object:

- Perhaps elaborate on the HKB and LW criteria? I've never heard of them?

> For that, we need to consider the variances and covariances of the estimated coefficients. The univariate trace plot is the wrong graphic form for what is essentially a multivariate problem, where we would like to visualize how _both_ coefficients and their variances change with $k$.

- Very interesting

Figure 8.11 is very interesting... I don't think the Stanford people have done this before... Not in their textbooks?

Figure 8.12 is great! Perhaps if there was some way I could better see the numbers and the x and y-axes, that'd be nice. 

- Have you considered using something like the continuous scales from the color space package? 
- I'd really like a ggplot2 version of these bivariate ridge trace plots

- The axis text sizes in Figure 8.13 are quite small

I'm confused about what is being visualized in Figures 8.15 and 8.16? Are these the... What do they mean? What does each dot refer to? A beta parameter? Which one?

> Beyond these statistical considerations, the methods of this chapter highlight the roles of multivariate thinking and visualization in understanding these phenomena and the methods developed for solving them. …

- Trailing ...

# Chapter 9

## Section 9.1

> Hotelling’s $T^2$ (Hotelling, 1931) is an analog the square of a univariate statistic, extended to the

-   Missing "analog to the square"?
-   Incomplete sentence

> Consider the basic one-sample t-test, where we wish to test the hypothesis that the mean $\bar{x}$ of a set of $N$
> measures on a test of basic math, with standard deviation does not differ from an assumed mean $\mu_0 = 150$ for a
> population

-   Would it be better to call them measurements instead of measures in this context? As measures seems to imply
    multiple different response variables to me.

I suspect the denominator of your equation for $t^2$ may be missing a square

$$
t^2 = \frac{N(\bar{x}-\mu_0)^2}{s}
$$

to

$$
t^2 = \frac{N(\bar{x}-\mu_0)^2}{s^2}
$$

> Then, a hypothesis test for the means on basic math (BM) and word problems (WP) is the test of the means of these two
> variables jointly equal their separate values

-   Do you mean 'separate values', as in the null population values that we assumed *a priori*?

Figure 9.1 is missing the author source in the figure description

-   My immediate reaction upon reading $T^2$ is to wonder whether Stein's paradox may be relevant.

-   If I understand you correctly, $D^2_M$ is the squared Mahalonobis distances, right? Perhaps a note regarding this
    equivalence would be helpful.

    -   Perhaps even a separate note regarding why the squared Mahalonobis distance is equal to
        $(\bar{x}-\mu_0)^T S^{-1}(\bar{x} - \mu_0)$ might be nice

## Section 9.2

-   Just to clarify, would the matrix $X$ refer to different response variables or different predictor variables? I
    think you mean different response variables based on the context. But $X$ and especially $x_p$ are so often used to
    refer to predictor variables, that I think this is causing some sort of cognitive dissonance within me.

-   Is there a $T^2$ analogue for paired t-tests and Welch's t-test?

My understanding/recollection is that the dis-advantage of the ANOVA $F$ test is that it can be insignificant even when
one of the predictors have significant differences; and that it can be significant when one of the predictors does not
have significant differences (?). Does this also apply to $T^2$?

> You can carry out the test that the means for both variables are jointly equal using either

-   Do you mean that the means for both variables are jointly equal *between groups*?

> To see the differences between the groups on both variables together, we draw their data (68%) ellipses, using
> `heplots::covEllpses()`

-   Typo `heplots::covEllipses()`?

> We can see that: - Group 1 \> Group 2 on Basic Math, but worse on Word Problems - Group 2 \> Group 1 on Word Problems,
> but worse on Basic Math - Within each group, those who do better on Basic Math also do better on Word Problems

I guess what confuses me is, can you assign a p-value or effect size to how much "Group 1 \> Group 2 on Basic Math, but
worse on Word Problems"? And are these p-values/effect-sizes separate from "Group 2 \> Group 1 on Word Problems, but
worse on Basic Math" and "Within each group, those who do better on Basic Math also do better on Word Problems"?

Because my impression is that the $T^2$'s significance motivates the data visualization, but then what of the parameter
estimates (If any?). For meta-analyses, effect-size interpretation and decision-making, it seems like the parameter
estimates are quite important.

> Hotelling’s $T^2$, the “size” of the difference between the means (labeled “1” and “2”) is assessed relative to the
> pooled within-group covariance matrix $S_p$, which is just a size-weighted average of the two within-sample matrices,
> $S_1$ and $S_2$.

What would the more general formula for multiple response variables be?

## Section 9.3

Would there be a way to more clearly show the relevance of the $H$ and $E$ R objects to Figure 9.4?

```{r, eval = FALSE}
dot <- function(x, y) sum(x*y)
project_on <- function(a, p1, p2) {
    a <- as.numeric(a)
    p1 <- as.numeric(p1)
    p2 <- as.numeric(p2)
    dot <- function(x,y) sum( x * y)    
    t <- dot(p2-p1, a-p1) / dot(p2-p1, p2-p1)
    C <- p1 + t*(p2-p1)
    C
}
```

Why is the `dot` function written twice here?

> Then, we run the same code as before to plot the data ellipses, and follow this with a call to heplot() using the
> option add=TRUE which adds to an existing plot.

If I understand you correctly, is there a difference between a data ellipse and a 'model-based' ellipse? If so, I feel
like this could given extra emphasis.

```{r, eval = FALSE}
covEllipses(mathscore[,c("BM", "WP")], mathscore$group,
            pooled=FALSE, # ?
            col = colors,
            cex=2, cex.lab=1.5,
            asp=1, 
            xlab="Basic math", ylab="Word problems"
            )
```

Why is pooled set to false here? Intuitively, I would've thought that they would be pooled because Hotelling's $T^2$
uses the pooled variance?

## Section 9.4

> The coefficients give $w = -0.84BM + 0.75WP$.

-   Do you mean $-0.084BM + 0.075WP$?

> To round this out, we can calculate the discriminant scores by multiplying the matrix $X$ by the vector $a = ...$ of
> the discriminant weights.

-   I'm confused about why we would want to round this out and what this has to do with the vector of discriminant
    weights?

-   I do appreciate that it's related to $T^2$, but I guess I'm confused about what to take away from Fisher's
    Discriminant Analysis and how/when I should be using it.

## Section 9.5

### Section 9.5.1

-   Considering how we spent some time on t-SNE, perhaps we could explore whether t-SNE might also be a helpful
    complement to PCA biplots?

```{r, eval = FALSE}
banknote.pca <- reflect(banknote.pca)
```

I don't know how much work this would be, but I get the impression that explicitly referring what libraries these
functions come from, could help in replicating the textbook code quickly, as opposed to copying all the code and running
from the top to see what happens. So for example, having `ggbiplot::reflect()` as opposed to just `reflect()` would help
in knowing what is needed to replicate this exact code block.

This package seems promising: https://cran.rstudio.com/web/packages/origin/origin.pdf

> While all of the individual tests are highly significant, the average of the univariate $F$s is only 236. The
> multivariate test gains power by taking the correlations of the size measures into account.

-   Do we ever have to worry about conditioning on a mediator/confounder when running a multivariate test?

## Section 9.6

> and there are different different calculations for a single measure corresponding to the various test statistics

-   Double 'different'

# Chapter 10

## Chapter 10.1

> $\mathbf{B} = ( \mathbf{b}_0^\mathsf{T} , \mathbf{b}_1^\mathsf{T} , \dots \mathbf{b}_j^\mathsf{T} \dots, \mathbf{b}_p )^\mathsf{T}$,

For some reason, this is appearing in the html form of the book?

### Section 10.1.1

What is the difference between $\chi^2$ and $\chi^2_p$?

-   Perhaps a graph of the $\chi^2$ QQ plot of Mahalnobis squared distance for assessing multivariate normality of
    residuals could be good?

> Graphical methods to determine if this assumption is met are described in

Rather than 'described', perhaps 'illustrated' is a nice synonym that both captures the description and depiction of the
graph?

## Section 10.2

> In the presence of possible outliers, robust methods are available for univariate linear models (e.g.,
> `MASS::rlm()). So too`,

The backticks \` are overextended past the R code. The following heplots::robmlm() is also missing its pair of
back-ticks.

### Section 10.2.1

-   I don't think cross-products have been discussed yet? A more concrete representation/instantiation of the $SSP$
    matrix would be appreciated.

> The answer, “how big” is expressed in terms of the $p$

Better to repeat that "how big" refers to the ratio of $SS_H$ to $SS_E$.

Why does it matter that $\text{det}(\textbf{H}\textbf{E}^{-1} - \lambda \textbf{I}) = 0$?

> We exploit this in canonical discriminant analysis and the corresponding canonical HE plots (ref)

I think you have a dangling reference

### Section 10.2.2

Something that still confuses is why we might be interested in examining the multivariate response, as opposed to the
univariate response. I feel as if this could be more strongly emphasized.

Do the 'arithmetic, geometric, harmonic and supremum' means correspond to Wilks's $\lambda$, Pillai trace and so-on in
Table 10.1?

### Section 10.2.3

> For example, with three responses $y_1, y_2, y_3$ and three predictors $x_1, x_2, x_3$, we can test the hypothesis
> that neither $x_2$ nor $x_3$ contribute at all to the predicting the

Perhaps, 'at all to predicting the'?

> In MANOVA designs, it is often desirable to follow up a significant effect for a factor with subsequent tests to
> determine which groups differ. While you can simply test for all pairwise differences among groups (using Bonferonni
> or other corrections for multiplicity), a more substantively-driven approach uses planned comparisons or contrasts
> among the factor levels.

-   In that case, why not do the planned comparisons or contrasts in the first place?

> For example, with $g=4$ groups representing the combinations of two drugs, A and B given at low and high doses, we
> might want to compare

Perhaps you could elaborate what $\mu_1$, $\mu_2$ refer to?

> $L_1 & = (\mu_1 + \mu_2) - (\mu_1 + \mu_2)$

Perhaps you mean $L_1 & = (\mu_1 + \mu_2) - (\mu_3 + \mu_4)$ ?

`t(C) %*% C`

Perhaps you could discuss why the corresponding matrix shows that the contrasts are mutually orthogonal?

> Orthogonal contrasts correspond to statistically independent tests.

Perhaps double emphasis on why statistically independent tests versus statistically dependent tests is important?

## Section 10.3

> A multivariate analysis would show a highly difference among groups

Perhaps, 'A multivariate analysis would show a highly significant difference between the groups'?

> Doing separate ANOVAs on these variables would miss what is so obvious from Figure 10.1.

I think I'd be even more convinced by an explicit demonstration of the MANOVA versus ANOVA in R. I'm not sure I
understand how the conclusion would differ between MANOVA and multiple ANOVAs.

> to their differences can be ascribed to a single dimension

Perhaps 'so their differences can be ascribed to a single dimension'?

> Data like this might arise in a study of parental competency, where there are are measure of the degree of caring

Perhaps 'Data like this might arise in a study of paren tal competency, where there are measure of the degree of caring'

### Section 10.3.1

> Visualization methods using HE plots are discussed in (REF?)

Dangling reference

> ; each column is a contrast whose values sum to zero

<!-- Perhaps we could have a separate section elaborating on why the values need to sum to zero and tips-and-tricks for constructing any contrast of interest? This has always mystified me.  -->

For example, is `c(1, -.5, -.5)` equivalent to `c(1.2, -.6, -.6)`? When might we want to do `c(1, -.25, -.75)`?

> fathers of non-disabled children scoring highest

I understand the terming, but it'd be more consistent to also name the Figure X's axis to 'Non-disabled' to minimize
confusion

> If the covariance matrices were all the same, the data ellipses would have roughly the same size and orientation, but
> that is not the case in Figure 10.4

Perhaps an explicit graph of what data ellipses when covariance matrices are all the same would be good?

> These tests have $s = \min(p, \text{df}_h) = \min(3,2) = 2$

- What do each of $s$, $p$ and $df_h$ refer to?

> Note that this has a much smaller $p$-value than any of the univariate $F$ tests.

- I could use an added emphasis on what this statistical significance tell us that the univariate ANOVAs doesn't. If I understand correctly, it's testing the hypothesis that all of the beta coefficients across all of the models are equal to 0?

> They can be extracted from the object using `purrr::pluck()`

- I appreciate `pluck()`, but for our purposes, wouldn't it be simpler to just do `parenting.summary$multivariate.tests$group$SSPH`? 
#### Linear hypotheses & contrasts

> The function `car::linearHypothesis()` carries out the multivariate test that these are all zero.

- What would the corresponding contrast matrix look like?


>  Because the contrasts used here are orthogonal, they comprise the overall test of $\mathbf{B} = \mathbf{0}$ which implies that the means of the three groups are all equal. The test below gives the same results as `Anova(parenting.mlm)`.

- What do you mean by comprise and what does orthogonality have to do with it?

### Section 10.3.3

> How does the _association* of anxiety and depression vary with age?

- Perhaps _association_ ?

#### Fit the MLM

> or equivalently, that all coefficients except the intercept in the model \@ref(eq:AH-mod) are zero,

- Broken reference? It's not rendering in the html

# Chapter 11

I suppose I'll come back to this chapter once it's complete.

- What is canonical space?
- I think a facetted HE plot where the error ellipse is varied in terms of some more tangible $\beta$/$SE$ coefficient may be nice?

# Chapter 12

> How can we visualize differences among group variances and covariance matrices

So by this, do you mean the difference in the variances/covariances of some set of variables between groups? Or... Do we mean something like how the level 1 standard deviation of some level 2 groups are different from each other? I think it could be understood both ways given location-scale multilevel models.

> this topic can also be extended to the multivariate analysis of covaraiance (MANCOVA) setting

- Typo: covariance

$$
M = (N -g) \ln \;|\; \mathbf{S}_p \;|\; - \sum_{i=1}^g (n_i -1) \ln \;|\; \mathbf{S}_i \;|\; \; ,
$$ {eq-boxm}

- Dangling {eq-boxm} ?

## 12.4 Box's M test

- I think more clarification regarding whta each of the variables are could help. Is $g$ the number of groups? What is $p$?

> If group sizes are greatly unequal __and__ homogeneity of variance is violated, then the $F$ statistic is too liberal ($p$ values too large) when large sample variances are associated with small group sizes.

- I assume that by liberal you mean that the p-value approaches 0. Perhaps that could be clarified? I find it confusing when I read big or small $p$ values, it seems a bit ambiguous to me...