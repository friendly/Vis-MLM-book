---
editor: 
  markdown: 
    wrap: 72
---

# MT tasks

1. Complete this review of the current draft as you've been doing, in this file.
  - Try to make refs to the text explicit, so I can find them

2. Re-read at a high, overview level.
  - As perhaps another pass through the ms., try to read for coherence, structure, and placement of ideas/methods across chapters. 
  - What needs to be explained earlier?
  - What things seem too deep in the weeds?

3. Read through `working-text/general-issues.md`
  - What can you help with?
  

# Chapter 2 Comments

Section 2.1.1 -- a different word from 'rough' to better describe the
juxtaposition? **DONE**

-   Maybe change the section header to better highlight the point that
    this is for debunking? These biases seems so implicit even today
    that without careful reading, one may be impressed that these
    notions are to be maintained @MT**:** **What would be better?**
-   Quarto formatting is a bit off in web-format, it's showing
    `::: {.cell layout-align=“center”}` 
    **Fixed; need a blank line b4 code** But should check generally.
-   Someway to disable ligatures on R-pipe (\|\>)? I think this may
    confuse those unfamiliar with fonts 
    **DONE: Use `monofont: "Fira mono"`, not `monofont: "Fira code"`**

Figure 2.3 - Maybe add an alpha, distinguish lines by linetype based on
sex, and make points hollowed out? 

Add TODO regarding Chapter XX, to put the actual number **DONE**

-   Fix *both* to actual formatting

-   Add TODO regarding sections 2.4, 2.5 and 2.6. 

-   Packages used here? `report` library?

# Chapter 3

Section 3.1.1 - Definition of pointwise confidence bands and contrasting
appropriate with inappropriate interpretations would be nice [**DONE**]

Figure 3.3 - Maybe adjust the colors/linetype to better show how the
LOESS smooth closely matches the quadratic fit?

Figure 3.4 and 3.5 and 3.7 - Distinguish by shape/linetype as well? -
Also adjust using alpha? [I get what you're saying: better to distinguish
by more than color. I'm reluctant to use `linetype`, b/c they vary in visual "weight",
and anyway, the lines don't overlap much here.]

Mathematics in Section 3.2 - Perhaps greater elaboration on the
mathematics and/or instantiating it in terms of concrete R code could be
helpful? Or perhaps where to look for a more elaborate description? It's
a bit terse

Figure 3.10 - It would be nice if the graph could be elaborated, such
that each of the things about 'horizontal/vertical tangents' to the
concentric ellipses, major and minor axes and more were labelled. - It's
not easy for everyone to immediately recognize what is meant by
something like major/minor axes versus central cross. - It would be a
lot more work, but from a pedagogical point of view, I think it'd be
nice if it was shown how the complete graph can be constructed
piece-by-piece and how each individual piece contributes to the
meaningof the larger graph. 

- Latex right before section 3.2.2 is
incomplete -- not rendering correctly:
$\(\sqrt{\mathbf{v}_1}, \sqrt{\mathbf{v}_1\), …$ [**FIXED**]

-   Figure 3.1 can be made to be be the same text size and etc as the
    other graphs? It seems unusually small for some reason

Section 3.2.3 and 3.2.3.1 -- for the in-text R code perhaps the
coefficients can be called by name instead of by number?
`coef(lm(prestige ~ income, data=Prestige))["COEFFICIENT NAME"]` instead
of `coef(lm(prestige ~ income, data=Prestige))[2]`. [**DONE**]

-   R code for Figure 3.14 seems a bit off? [I'm using `source-line-numbers: "2"` to highlight line 2, but maybe not a good idea.]

-   Why not make everything in ggplot2?

-   Maybe also add Representational Similarity Analysis and even network
    diagrams as another summary of bivariate relationships?

-   y-axis text is sqquished up in Figure 3.31 [I don't think I can do much about this]

# Chapter 4

## PCA

-   Generalized PCA?
-   The capitalization for each of the bullet points of
    `It was but a small step to recognize that for two variables, $x$ and $y$:...`
    is a bit odd/inconsistent [**FIXED**]

-   Perhaps making Figure 4.4 go into a continuous loop of a GIF would
    be better? The way it jumps from the beginning to the end makes it
    unclear for me what to focus my attention on and by the time I focus
    on something, the GIF resets and I forget what I was looking at.
    -   It's also a bit ambiguous what the 'horizontal and vertical'
        dimensions are in a moving 3D graph [**added: in the final frame**]

-   I like Figure 4.5. I wonder if it'd be possible to make everything
    in one visualization framework? GGplot2? Or is this unnecessary?

-   The language in Chapter 4 -- particularly 4.2.2 -- is quite
    metaphorical and vivid. I'm worried that this may distract the
    reader from what the main point is trying to be conveyed. Is there a
    way to balance style and directness?

-   The mathematics of PCA seems like a big of a leap
-   Perhaps supplementing it with visual explanations and a concrete
    example with a low number of dimensions might make it more
    intuitive? [**TODO: Add R/workers-pca.R example**]

## 4.2.3

-   Could be nice to clarify whether the data need to already be
    centered/scaled or whether the function can handle it. [**DONE**]
-   Would be nice to clarify what `center` and `scale` are in the output
    of `prcomp()`

## 4.2.4

> "The typical rough guidance is that the last point to fall on this
> line represents the last component to extract, the idea being that
> beyond this, the amount of additional variance explained is
> non-meaningful." - This could be rephrased

## 4.2.5

Figure 4.8 -- perhaps the legend could have the letter 'a' removed? [**That's a ggplot2 gotcha**]

-   Ore we could make the legend be similar with Figure 4.12

-   Shouldn't Figure 4.8 have its y-axis lengthened to really show that
    the two components are uncorrelated?

-   Y-axis seems to be mislabeled in Figure 4.8

-   I'm a bit confused about multiplying the principal component
    loadings by -1. Should I only multiply by -1 if all of the loadings
    are negative? And if I multiply by -1, should it be to all of the
    principal components? {**DONE}

-   Because only the first 2 and 4th loadings are negative for PC2, the
    rest are positive.

-   Perhaps elaborate on what "the cosine of the angle between two
    variable vectors" means?

## 4.3

intro seems incomplete? `...` ? [FIXED]

### 4.3.1

-   `i`, `r` and `p` could be more explicitly defined?

-   Perhaps some concrete visual illustration of the ideas of
    eigenvectors, eigenvalues and orthonormality and how they relate to
    some data could be pedagogically helpful?

-   Could we consider going beyond the biplot, to cases where multiple
    principal components may be visualized at once? Perhaps via a
    parallel coordinate plot?

-   Perhaps we could use the idea of simulating your own data and show
    how to vary the simulation in order to get particular principal
    component results?
    -   For example, how to simulate data from a two factor model with
        10 indicators each? What would these indicators look like with
        principal components?
    -   Or maybe generate data from a composite model (Cho and Choi,
        2020; Behaviormetrika)?
-   TODO is missing a preceding `*`

### 4.3.3

I suspect that Figure 4.11 might not be scaled correctly?

-   It would be nice to have an explanation for why
    `The correlation circle indicates that these components are uncorrelated and have equal variance in the display.`
    and under what circumstances you would think that the components are
    correlated and not have equal variance. [Fixed]

Figure 4.12 seems a bit crowded between the mix of letters and lines,
perhaps we could adjust alpha and do some dodging - It may be nice if we
could arrange the legend for the regions to correspond to the direction
that they point to. Because right now the 'Northeast' is at the
'Southeast of the plot' 
[I generally prefer direct labels to legends, but that couldn't work here]

- 'Murder' is also cutoff 
[ Re: cutoff text -- Added `clip` arg to `ggbiplot`; use `clip = "off"` in examples

Figure 4.13 -- 'larceny' is cutoff

### 4.3.5

Perhaps some more background for why Figure 4.15 is true and how it
relates to multiple linear regression could be good?

-   Maybe elaborate on why the multivariate regression on the principal
    components excluded the intercept?
-   So is using supplementary variables the same or different from
    principal components regression? I'm confused.
    [Different: PCR replaces predictors by their principal components.
    PCA and biplots have no response variable. We're just finding how the supp variables relate to the components]
-   Because if it's the same, then why not just use `prcomp()`, then
    perform `lm()` on the principal components...? Or perhaps using the
    `pls::pcr()`?
    -   Is it because there's an easy transition into plotting the
        results and because it's not introducing a new package?

-   Could the following be elaborated?

> Note that these coefficients are the same as the correlations between
> the supplementary variables and the scores on the principal
> components, up to a scaling factor for each dimension. This provides a
> general way to relate dimensions found in other methods to the
> original data variables using vectors as in biplot techniques.

**Maybe it would be better to put all of the TODOs into the Github
Issues page that way it's easier to sort through and consider all of
them?**

## Section 4.4

### Section 4.4.1

-   Perhaps explain what 'nonmetric' means and why that's advantageous?

### Section 4.4.2

> t-SNE defines a similar probability distribution over the points \_i\$
> in the low-dimensional map, and it minimizes the Kullback–Leibler
> divergence (KL divergence) between the two distributions with respect
> to the locations of the points in the map

-   Perhaps there's a latex error? $q_{ij}$ maybe?

#### Section 4.4.2.1

So just to confirm, are the differences between PCA and t-SNE (rotation)
specific to this data or in general? I thought some stronger statement
on when we should consider each technique would be nice.

## Section 4.5

Caption for Figure 4.26 is missing

## Section 4.7

-   The animation for Figure 4.32 is uncharacteristcally slow compared
    to all the other animations...

# Chapter 5

This chapter feels like it should come first in the book

So far, my impression is that restructuring the book to resemble a
'workflow' view of how to address particular multivariate data problems
would be helpful.

[The general idea is that exploratory, graphical methods come first, followed by model-based methods,
first univariate, then multivariate]

## Section 5.1

Can multivariate logistic regression not be used when p = 1... and is
quantitative?

Isn't the bottom right for p \> 1 and q \>1 also the log-linear model?

Does ANCOVA refer to any GLM model with both continuous and discrete
predictors? [Strictly speaking, just where X doesn't interact with a factor]

## Section 5.2

I think the latex renderred a bit incorrectly?

### 5.2.1.1

-   Doesn't the default for factors involve a deviation from baseline as
    opposed to the individual mean for each level of the factor? Perhaps
    a note about how that affects interpretation of `y ~ x + A` would be
    helpful, or how to remove the intercept, maybe?

# Chapter 6

## 6.1

"This provides an assessment of homogeneity of variance, which appears
as a tendency for scale to vary with location." - Perhaps you mean, "an
assessment of violations to homogeneity of variance. Violations appear
as a tendency for scale to vary with location."? The former implies that
scale varying with location is a good thing? [Fixed]

### 6.1.1

Maybe make the unit diagonal in Figure 6.1 dashed and in some other
color for contrast? [Made it green]

Figure 6.1 is quite amazing!

## 6.3

### 6.3.3

I wonder if it'd be excessive to try something like a specification
curve of a multiverse analysis [YES -- excessive]

## 6.4

Is the added variable related to the partial correlations? It seems like
the two ideas should be awfully related to each other...

Oh it is, Section 6.4.1!. I think we'd bit a bit remiss to not at least
mention the gaussian graphical model and mixed graphical models, in
passing then, because it shows all the partial correlations between all
pairs of variables.

Arjun recently made a whole speech about accumulated local effects. Is
this something pertinent to this chapter?

Although added-variable plots and its associations are well-explained,
it doesn't seem like there was much backtracking to the original MLR's
mystifying coefficients? - Are the signs of the coefficients related to
some sort of collider-bias thing going on?

Perhaps you could discuss how average marginal effects compare with
effects plots?

## 6.5

I get the impression that one should just skip AV plots and go to effect
displays? Is this accurate?

### 6.6.1

"’;.\[;p;khnThe analogous formula is" [Fixed]

-   Typo

There's a real armada of regression diagnostics included here. Given all
the tools that one takes with them before they even begin modelling
these days -- DAGs, scatterplot matrices, dimension reduction, etc--what
do you make of regression diagnostics within an analytical workflow? Are
they still necessary?

# Chapter 7

## 7.1

-   I was a bit thrown off by "each line like $y= a + bx$ in data space
    corresponds to a point $(a, b)$ in $\beta$ space.
    -   some how it made me think that $a$ and $b$ were data-points --
        columns of $x$. Could we not use the notation of $beta_0$ and
        $beta_1$ here? That's similar to how LASSO vs ridge regression
        is generally illustrated...
-   Why does it switch later to $\alpha$ and $\gamma$?

"It is well to understand the underlying geometry" - Missing word
between 'well' and 'to'?

I'm confused how we got from $\beta$ spaces to ellipsoids to the
covariance matrix, perhaps further explanation could be in place?

Perhaps some further discussion of $\oplus$ could be helpful?

### 7.1.1

What are $F_{d,v}^{0.95}$? Is that the $F$-statistic? $s_e$? Perhaps further discussion on how the $F$-statistic became relevant would be illuminating.


### 7.2.2

This reminds a lot of Loken and Gelman 2017. 

"Thus, as $\sigma^2_{\eta}$ increases, $\hat{\beta}_{x^*}$ becomes less than $." 
- Perhaps a missing symbol?

Figure 7.5 -- perhaps flip order of False and True in the legend? Maybe also differentiate by whether the lines solid or dashed?

### 7.2.3

Figure 7.6 is pretty interesting.

Figure 7.7 is very interesting. I think it would be nice if the lines for the marginal estimates were solid, black and quite large instead. Then the lines for the axes to be dashed and relatively small. 


# Chapter 8

## Section 8.1

- In my personal opinion, it'd be nice to denote $\sigma^2$ as $\sigma_\epsilon^2$. For a very long time, I was always very confused about the differences among $\sigma^2$, $\sigma_\epsilon^2$ and $\sigma_y^2$. A brief math-stat note about how all of these are related would not be un-appreciated.

> "or use ipsatized scores that sum to a constant..."

- Defining what ipsatized scores explicitly would be nice

> More generally, collinearity refers to the case when there are very high multiple correlations among the predictors, such as...

- When discussing 'multiple correlations', it would be nice to define and contrast it with simple correlation.

### Section 8.1.2

> Recall (Section #sec-data-beta) that the confidence ellipse for 

- Incomplete rendering?

## Section 8.2

I'm wondering if collinearity will be brought up in later chapters because it feels like we're getting quite deep into this topic. Maybe, too deep?

### 8.2.1

Given the recurring call-outs to the inverse of correlation/covariance matrices, it seems like some discussion of (partial) correlation networks seems awfully fitting as a way of visualizing the whole issue.

### Section 8.2.2

I can't help but intuit that there's a lot of ways to try to visualize collinearity, even outside of the network literature. This section seems very mathematical/analytical.


### Section 8.2.3

> the condition indices, using using squares whose background color is red

- Typo

- Is it really necessary to separate section 8.2.2 and 8.2.3? Or perhaps we could integrate them together? Or perhaps rename their section headings to better describe their interrelationship?

### Section 8.2.4

I don't know if it would be overkill, but it seems like discussing factor analysis and contrasting it with PCA would be nice. My recollection is that some multivariate statistics textbooks do discuss factor analysis, anyways. 
I suspect that some of the content in this section may be redundant with the chapter/sections on PCA in "Dimension Reduction". 

Figure 8.4
- Would it be helpful to useequal scales for the axes?

> An extreme outlier (case 20) appears in the lower left corner.

- Do you mean lower right?

## Section 8.3

> Replace highly correlated regressors with linear combination(s) of them.

- Perhaps it may be good to discuss the psychometric commitments that one may be making by choosing to do a linear combination of variables versus a factor model? I know this is a burgeoning literature, especially with composite structural equation modelling



Overall, I feel like there could have been more data-viz here and that there should probably be many options to choose from. I suspect that one could place this chapter before the one on dimension reduction. Perhaps we could start the book by introducing the central theme as "How to tackle large datasets with multiple response variables". Then, we could lead into collinearity as one of the common issues that one faces in large datasets.

# Chapter 9

## Section 9.1

> Hotelling’s $T^2$ (Hotelling, 1931) is an analog the square of a univariate 
 statistic, extended to the
 
 - Missing "analog to the square"?
 - Incomplete sentence
 
 
 > Consider the basic one-sample t-test, where we wish to test the hypothesis that the mean $\bar{x}$ of a set of $N$ measures on a test of basic math, with standard deviation  does not differ from an assumed mean $\mu_0 = 150$ for a population
 
- Would it be better to call them measurements instead of measures in this context? As measures seems to imply multiple different response variables to me.

I suspect the denominator of your equation for $t^2$ may be missing a square

$$
t^2 = \frac{N(\bar{x}-\mu_0)^2}{s}
$$

to

$$
t^2 = \frac{N(\bar{x}-\mu_0)^2}{s^2}
$$

> Then, a hypothesis test for the means on basic math (BM) and word problems (WP) is the test of the means of these two variables jointly equal their separate values

- Do you mean 'separate values', as in the null population values that we assumed *a priori*?

Figure 9.1 is missing the author source in the figure description

- My immediate reaction upon reading $T^2$ is to wonder whether Stein's paradox may be relevant.

- If I understand you correctly, $D^2_M$ is the squared Mahalonobis distances, right? Perhaps a note regarding this equivalence would be helpful.
    - Perhaps even a separate note regarding why the squared Mahalonobis distance is equal to $(\bar{x}-\mu_0)^T S^{-1}(\bar{x} - \mu_0)$ might be nice


## Section 9.2

- Just to clarify, would the matrix $X$ refer to different response variables or different predictor variables? I think you mean different response variables based on the context. But $X$ and especially $x_p$ are so often used to refer to predictor variables, that I think this is causing some sort of cognitive dissonance within me.

- Is there a $T^2$ analogue for paired t-tests and Welch's t-test?

My understanding/recollection is that the dis-advantage of the ANOVA $F$ test is that it can be insignificant even when one of the predictors have significant differences; and that it can be significant when one of the predictors does not have significant differences (?). Does this also apply to $T^2$?

> You can carry out the test that the means for both variables are jointly equal using either

- Do you mean that the means for both variables are jointly equal *between groups*?

> To see the differences between the groups on both variables together, we draw their data (68%) ellipses, using `heplots::covEllpses()`

- Typo `heplots::covEllipses()`?

> We can see that:
>   - Group 1 > Group 2 on Basic Math, but worse on Word Problems
>   - Group 2 > Group 1 on Word Problems, but worse on Basic Math
>   - Within each group, those who do better on Basic Math also do better on Word Problems

I guess what confuses me is, can you assign a p-value or effect size to how much "Group 1 > Group 2 on Basic Math, but worse on Word Problems"? And are these p-values/effect-sizes separate from "Group 2 > Group 1 on Word Problems, but worse on Basic Math" and "Within each group, those who do better on Basic Math also do better on Word Problems"? 

Because my impression is that the $T^2$'s significance motivates the data visualization, but then what of the parameter estimates (If any?). For meta-analyses, effect-size interpretation and decision-making, it seems like the parameter estimates are quite important. 


>  Hotelling’s $T^2$, the “size” of the difference between the means (labeled “1” and “2”) is assessed relative to the pooled within-group covariance matrix $S_p$, which is just a size-weighted average of the two within-sample matrices, $S_1$ and $S_2$.

What would the more general formula for multiple response variables be?

## Section 9.3

Would there be a way to more clearly show the relevance of the $H$ and $E$ R objects to Figure 9.4? 

```{r, eval = FALSE}
dot <- function(x, y) sum(x*y)
project_on <- function(a, p1, p2) {
    a <- as.numeric(a)
    p1 <- as.numeric(p1)
    p2 <- as.numeric(p2)
    dot <- function(x,y) sum( x * y)    
    t <- dot(p2-p1, a-p1) / dot(p2-p1, p2-p1)
    C <- p1 + t*(p2-p1)
    C
}
```

Why is the `dot` function written twice here?

> Then, we run the same code as before to plot the data ellipses, and follow this with a call to heplot() using the option add=TRUE which adds to an existing plot.

If I understand you correctly, is there a difference between a data ellipse and a 'model-based' ellipse? If so, I feel like this could given extra emphasis.

```{r, eval = FALSE}
covEllipses(mathscore[,c("BM", "WP")], mathscore$group,
            pooled=FALSE, # ?
            col = colors,
            cex=2, cex.lab=1.5,
            asp=1, 
            xlab="Basic math", ylab="Word problems"
            )
```

Why is pooled set to false here? Intuitively, I would've thought that they would be pooled because Hotelling's $T^2$ uses the pooled variance?


## Section 9.4

> The coefficients give $w = -0.84BM + 0.75WP$. 

- Do you mean $-0.084BM + 0.075WP$?

> To round this out, we can calculate the discriminant scores by multiplying the matrix $X$ by the vector $a = ...$ of the discriminant weights.

- I'm confused about why we would want to round this out and what this has to do with the vector of discriminant weights?

- I do appreciate that it's related to $T^2$, but I guess I'm confused about what to take away from Fisher's Discriminant Analysis and how/when I should be using it. 

## Section 9.5

### Section 9.5.1

- Considering how we spent some time on t-SNE, perhaps we could explore whether t-SNE might also be a helpful complement to PCA biplots?

```{r, eval = FALSE}
banknote.pca <- reflect(banknote.pca)
```


I don't know how much work this would be, but I get the impression that explicitly referring what libraries these functions come from, could help in replicating the textbook code quickly, as opposed to copying all the code and running from the top to see what happens. So for example, having `ggbiplot::reflect()` as opposed to just `reflect()` would help in knowing what is needed to replicate this exact code block.

This package seems promising: https://cran.rstudio.com/web/packages/origin/origin.pdf

> While all of the individual tests are highly significant, the average of the univariate $F$s is only 236. The multivariate test gains power by taking the correlations of the size measures into account.

- Do we ever have to worry about conditioning on a mediator/confounder when running a multivariate test?

## Section 9.6

> and there are different different calculations for a single measure corresponding to the various test statistics

- Double 'different'