{"title":"Getting Started","markdown":{"headingText":"Getting Started","headingAttr":{"id":"sec-getting_started","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"```{r include=FALSE}\nsource(\"R/common.R\")\n```\n\n\n```{r include=FALSE}\nlibrary(ggplot2) \nlibrary(tidyverse)\nlibrary(broom)\nggplot2::theme_set(theme_bw(base_size = 16))\n```\n\n\n## Why plot your data? {#sec-why_plot}\n\n> Getting information from a table is like extracting sunlight from a cucumber. @FarquharFarquhar:91\n\nAt the time the Farhquhar brothers wrote this pithy aphorism, graphical methods for understanding data had\nadvanced considerably, \n\n### Anscombe's Quartet\n\nIn 1973, Francis Anscombe [@Anscombe:73] famously constructed a set of four data sets\nillustrate the importance of plotting the graphs before analyzing and model building, and the effect of unusual observations on fitted models.\nNow known as _Anscombe's Quartet_, these data sets had identical statistical properties: the same\nmeans, standard devitions, correlations and regression lines. \n\nThe data set `datasets::anscombe` has 11 observation, recorded in wide format,\nwith variables `x1:x4` and `y1:y4`.\n```{r}\ndata(anscombe) \nhead(anscombe)\n```\n\nThe following code transforms this data to long format and calculates some summary statistics\nfor each `dataset`.\n\n\n```{r}\nanscombe_long <- anscombe |> \n  pivot_longer(everything(), \n               names_to = c(\".value\", \"dataset\"), \n               names_pattern = \"(.)(.)\"\n  ) |>\n  arrange(dataset)\n\nanscombe_long |>\n  group_by(dataset) |>\n  summarise(xbar      = mean(x),\n            ybar      = mean(y),\n            r         = cor(x, y),\n            intercept = coef(lm(y ~ x))[1],\n            slope     = coef(lm(y ~ x))[2]\n         )\n```\nAs we can see, all four data sets have nearly identical univariate and bivariate statistical\nmeasures. You can only see how they differ in graphs, which show their true natures to be vastly\ndifferent.\n\n@fig-ch02-anscombe1 is an enhanced version of Anscombe's plot of these data, adding\nhelpful annotations to show visually the underlying statistical summaries.\n\n```{r}\n#| label: fig-ch02-anscombe1\n#| echo: false\n#| fig-align: center\n#| out-width: \"90%\"\n#| fig-cap: \"Scatterplots of Anscombe's Quartet. Each plot shows the fitted regression line\n#|     and a 68% data ellipse representing the correlation between $x$ and $y$. \"\nknitr::include_graphics(\"figs/ch02-anscombe1.png\")\n```\n\nThis figure is produced as follows, using a single call to `ggplot`, faceted by `dataset`.\nAs we will see later (@sec-data-ellipse), the data ellipse (produced by `stat_ellipse()`)\nreflects the correlation between the variables.\n\n```{r}\n#| eval: false\ndesc <- tibble(\n  dataset = 1:4,\n  label = c(\"Pure error\", \"Lack of fit\", \"Outlier\", \"Influence\")\n)\n\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 4) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"red\", linewidth = 1.5) +\n  scale_x_continuous(breaks = seq(0,20,2)) +\n  scale_y_continuous(breaks = seq(0,12,2)) +\n  stat_ellipse(level = 0.5, color=col, type=\"norm\") +\n  geom_label(data=desc, aes(label = label), x=6, y=12) +\n  facet_wrap(~dataset, labeller = label_both) \n```\n\nThe subplots are labeled with the statistical idea they reflect:\n\n* dataset 1: **Pure error**. This is the typical case with well-behaved data. Variation of the points around the line reflect only measurement error or unreliability in the response, $y$.\n\n* dataset 2: **Lack of fit**. The data is clearly curvilinear, and would be very well described by a quadratic, `y ~ poly(x, 2)`. This violates the assumption of linear regression that the fitted model has the correct form.\n\n* dataset 3: **Outlier**. One point, second from the right, has a very large residual. Because this point is near the extreme of $x$, it pulls the regression line towards it, as you can see by imagining a line through the remaining points.\n\n* dataset 4: **Influence**. All but one of the points have the same $x$ value. The one unusual point has sufficient influence to force the regression line to fit it **exactly**.\n\nOne moral from this example:\n\n> **Linear regression only \"sees\" a line. It does its' best when the data are really curvilinear. Because the line is fit by least squares, it pulls the line toward discrepant points to minimize the sum of squared residuals.**\n\n::: {.callout-note}\n## Quartets\n\nThe essential idea of a statistical \"quartet\" is to illustrate four quite different data sets\nor circumstances that seem superficially the same, but yet are paradoxically very different\nwhen you look behind the scenes.\nFor example, in the context of causal analysis @Gelman-etal:2023, illustrated\nsets of four graphs, within each of which \nall four represent the same average (latent) causal effect but with\nmuch different patterns of individual effects.\nAs an example of machine learning models, @Biecek-etal:2023, introduced the \"Rashamon Quartet\",\na synthetic dataset for which four models from different classes \n(linear model, regression tree, random forest, neural network)\nhave practically identical predictive performance.\nIn all cases, the paradox is solved when\ntheir visualization reveals the distinct ways\nof understanding structure in the data.\n:::\n\n### A real example\n\n\nIn the mid 1980s, a consulting client had a strange problem.\nShe was conducting a study of the relation between body image and weight preoccupation in\nexercising and non-exercising people [@Davis:1990]. As part of the design, the researcher\nwanted to know if self-reported weight could be taken as a reliable indicator of\ntrue weight measured on a scale. It was expected that the correlations between reported\nand measured weight should be close to 1.0, and the slope of the regression lines\nfor men and women should also be close to 1.0. The data set is `car::Davis`.\n\nShe was therefore very surprise to see the following numerical results: For men,\nthe correlation was nearly perfect, but not so for women.\n\n```{r davis-cor}\ndata(Davis, package=\"carData\")\nDavis <- Davis |>\n  drop_na()          # drop missing cases\nDavis |>\n  group_by(sex) |>\n  select(sex, weight, repwt) |>\n  summarise(r = cor(weight, repwt))\n```\n\nSimilarly, the regression lines showed the expected slope for men, but that for women was only 0.26.\n\n```{r davis-lm}\nDavis |>\n  nest(data = -sex) |>\n  mutate(model = map(data, ~ lm(repwt ~ weight, data = .)),\n         tidied = map(model, tidy)) |>\n  unnest(tidied) |>\n  filter(term == \"weight\") |>\n  select(sex, term, estimate, std.error)\n\n```\n\nWhat could be wrong here?, the client asked. The consultant replied with the obvious question:\n\n> _Did you plot your data?_\n\nThe answer turned out to be one discrepant point, a female, whose measured weight was 166 kg\n(366 lbs!). This single point exerted so much influence that it pulled the fitted regression line\ndown to a slope of only 0.26.\n\n```{r}\n#| label: fig-ch02-davis-reg1\n#| fig-cap: \"Regression for Davis' data on reported weight and measures weight for men and women.\n#|    Separate regression lines, predicting reported weight from measured weight are shown for males and females.\n#|    One highly unusual point is highlighted.\"\nDavis |>\n  ggplot(aes(x = weight, y = repwt, color = sex, shape=sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  labs(y = \"Measured weight (kg)\", x = \"Reported weight (kg)\") +\n  geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  theme(legend.position = c(.8, .8))\n```\n\nIn this example, it was arguable that $x$ and $y$ axes should be reversed, \nto determine how well measured weight can be predicted from reported weight.\nIn `ggplot` this can easily be done by reversing the `x` and `y` aesthetics.\n\n```{r}\n#| label: fig-ch02-davis-reg2\n#| fig-cap: \"Regression for Davis' data on reported weight and measures weight for men and women.\n#|    Separate regression lines, predicting measured weight from re[ported] weight are shown for males and females.\n#|    The highly unusual point no longer has an effect on the fitted lines.\"\nDavis |>\n  ggplot(aes(y = weight, x = repwt, color = sex, shape=sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  labs(y = \"Measured weight (kg)\", x = \"Reported weight (kg)\") +\n  geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  theme(legend.position = c(.8, .8))\n```\n\nIn @fig-ch02-davis-reg2, this discrepant observation again stands out like a sore thumb, but\nit makes very little difference in the fitted line for females.\nThe reason is that this point is well within the range of the $x$ variable (`repwt`).\nTo impact the slope of the regression line, an observation must be unusual in_both_ $x$ and $y$.\nWe take up the topic of how to detect influential observations and what to do about them in \nChapter XX.\n\nThe value of such plots is not only that they can reveal possible problems with an analysis,\nbut also help identify their reasons and suggest corrective action.\nWhat went wrong here? Examination of the original data showed that this person switched\nthe values, recording her reported weight in the box for measured weight and vice versa.\n\n## Plots for data analysis\n\n\nVisualization methods take an enormous variety of forms,  but it is useful to distinguish several broad categories according to their use in data analysis:\n\n* __data plots__ : primarily plot the raw data, often with annotations to aid interpretation (regression lines and smooths, data ellipses, marginal distributions)\n\n* __reconnaissance plots__ : with more than a few variables, reconnaissance plots provide a high-level,\nbird's-eye overview of the data, allowing you to see patterns that might not be visible in a set of separate plots. Some examples are scatterplot matrices, showing all bivariate plots of variables\nin a data set; correlation diagrams, using visual glyphs to represent the correlations between\nall pairs of variables and \"trellis\" or faceted plots that show how a focal relation of\none or more variables differs across values of other variables.\n\n* __model plots__ :  plot the results of a fitted model, such as a regression line or curve\nto show uncertainty, or a regression surface in 3D, or a plot of coefficients in model\ntogether with confidence intervals.\nOther model plots try to take into account that \na fitted model may involve more variables than can be shown in a static 2D plot.\nSome examples of these are added variable plots, and marginal effect plots, both of which\nattempt to show the net relation of two focal variables, controling or adjusting for other variables\nin a model.\n\n* __diagnostic plots__ : indicating potential problems with the fitted model. These include residual plots, influence plots, plots for testing homogeneity of variance and so forth.\n\n* __dimension reduction plots__ : plot representations of the data into a space of fewer dimensions than the number of variables in the data set. Simple examples include principal components analysis (PCA) and the related biplots, and multidimensional scaling (MDS) methods.\n\nWe give some more details and a few examples in the sections that follow.\n\n\n## Data plots\n\nData plots portray the data in a space where the coordinate axes are the observed variables.\n\n*   1D plots include line plots, histograms and density estimates.\n*   2D plots are most often scatterplots, but contour plots or hex-binned plots are also useful when the sample size is large.\n<!-- *   For higher dimensions, biplots, showing the data in principal components space, together with vectors representing the correlations among variables, are often the most useful. -->\n\n\n## Model plots\n\n## Diagnostic plots\n\n## References {.unnumbered}\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","html-math-method":"mathjax","output-file":"getting_started.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.189","bibliography":["bib/references.bib","bib/R-refs.bib","bib/packages.bib","C:/Dropbox/localtexmf/bibtex/bib/graphics.bib","C:/Dropbox/localtexmf/bibtex/bib/statistics.bib","C:/Dropbox/localtexmf/bibtex/bib/timeref.bib"],"biblio-style":"apalike","link-citations":"no","crossref":{"chapters":true},"page-footer":"Copyright 2022, Michael Friendly","knitr":{"opts_chunk":{"fig.path":"figs/"}},"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}}}