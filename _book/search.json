[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "",
    "text": "Preface\nThis is an early draft of material that may or may not appear in the preface."
  },
  {
    "objectID": "index.html#one-two-many",
    "href": "index.html#one-two-many",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "ONE, TWO, MANY",
    "text": "ONE, TWO, MANY\nThere is an old and helpful idea I learned from John Hartigan in my graduate days at Princeton:\n\nIn statistics and data visualization all methods can be classified by the number of dimensions contemplated, on a scale of ONE, TWO, MANY.\n\nBy this, he meant that, at a global level, all data, statistical summaries, and graphical displays could be classified as:\n\n\nunivariate: a single variable, considered in isolation (age, COVID cases, pizzas ordered). Univariate numerical summaries are means, medians, measures of variablilty, and so forth. Univariate displays include dot plots, boxplots, histograms and density estimates.\n\nbivariate: two variables, considered jointly. Numerical summaries include correlations, covariances and two-way tables of frequencies or measures of association for categorical variables. Bivariate displays include scatterplots and mosaic plots.\n\nmultivariate: three or more variables, considered jointly. Numerical summaries include correlation and covariance matrices, consisting of all pairwise values, but also derived measures from the analysis of these matrices (eigenvalues, eigenvectors). Graphical displays of multivariate data can sometimes be shown in 3D, but often involve multiple views of the data projected into 2D plots.\n\nAs a quasi-numerical scale, I refer to these as 1D, 2D and nD. This admits the possibility of half-integer cases, such as 1.5D, where the main focus is on a single variable, but it is classified by a simple factor (gender). His point in this classification was that once you reached three variables, all higher dimensions involved similar summaries and data displays.\nUnivariate and bivariate methods and displays are well-known. This book is about how these ideas can be extended to an \\(n\\)-dimensional world. Three-dimensional data displays are now fairly easy to produce, even if they are sometimes difficult to understand. But how can we even think about four or more dimensions? The difficulty can be appreciated by considering the tale of Flatland."
  },
  {
    "objectID": "index.html#flatland",
    "href": "index.html#flatland",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Flatland",
    "text": "Flatland\n\nTo comport oneself with perfect propriety in Polygonal society, one ought to be a Polygon oneself. — Edwin A. Abbott, Flatland\n\nIn 1884, an English schoolmaster, Edwin Abbott Abbott, shook the world of Victorian culture with a slim volume, Flatland: A Romance of Many Dimensions (Abbott 1884). He described a two-dimensional world, Flatland, inhabited entirely by geometric figures in the plane. His purpose was satirical, to poke fun at the social and gender class system at the time: Women were mere line segments, while men were represented as polygons with varying numbers of sides— a triangle was a working man; gentlemen and professionals had more sides. Abbot published this under the pseudonym, “A Square”, suggesting his place in the hierarchy.\n\nTrue, said the Sphere; it appears to you a Plane, because you are not accustomed to light and shade and perspective; just as in Flatland a Hexagon would appear a Straight Line to one who has not the Art of Sight Recognition. But in reality it is a Solid, as you shall learn by the sense of Feeling. — Edwin A. Abbott, Flatland\n\nBut how did it feel to be a member of a flatland society? How could a point (a child?) understand a line (a woman)? How does a Triangle “see” a Hexagon or even a infinitely-sided Circle? Abbott introduces these ideas through dreams and visions:\n\nA Square dreams of visiting a one-dimensional Lineland where men appear as lines, and women are merely “illustrious points”, but the inhabitants can only see the Square as lines.\nIn another vision, the Square is visited by a Sphere, to illustrate what a 2D flatlander could understand from a 3D sphere.\n\n\n\n\n\nFigure 1: A 2D flatlander seeing a sphere pass through Flatland. Source: Abbott (1884)\n\n\n\nAbbott goes on to state what could be considered as a demonstration (or proof) by induction of the difficulties of seeing in 1, 2, 3 dimensions, and how the idea motion over time (one more dimension) could allow citizens of any 1D, 2D, 3D world to contemplate one more dimension.\n\nIn One Dimensions, did not a moving Point produce a Line with two terminal points? In two Dimensions, did not a moving Line produce a Square with four terminal points? In Three Dimensions, did not a moving Square produce - did not the eyes of mine behold it - that blessed being, a Cube, with eight terminal points? And in Four Dimensions, shall not a moving Cube - alas, for Analogy, and alas for the Progress of Truth if it be not so - shall not, I say the motion of a divine Cube result in a still more divine organization with sixteen terminal points? — Edwin A. Abbott\n\nFor Abbot, the way for a citizen of any world to image one more dimension was to consider how a higher-dimensional object would change over time. However, as far back as 1754 (Cajori 1926), the idea of adding a fourth dimension appears in Jean le Rond d’Alembert’s “Dimensions”, and one realization of a four-dimensional object is a tesseract, shown in the figure below.\n\n\n\n\nFigure 2: Geometrical object in 1 to 4 dimensions\n\n\n\nBut to really see a tesseract you have to view it in an animation over time: ::: {#fig-tesseract}\n\n\n\n\nAnimation of a tesseract. :::\nYet the deep mathematics of more than three dimensions only emerged in the 19th century. In Newtonian mechanics, space and time were always considered independent of each other. Our familiar three-dimensional space, of length, width, and height had formed the backbone of Euclidean geometry for millenea. However, the idea that space and time are indeed interwoven was first proposed by German mathematician Hermann Minkowski (1864–1909) in 1908. This was a powerful idea. It bore fruit when Albert Einstein revolutionized the Newtonian conceptions of gravity in 1915 when he presented a theory of general relativity which was based primarily on the fact that mass and energy warp the fabric of four-dimensional spacetime.\nThe parable of Flatland can provide inspiration for statistical thinking and data visualization. Once we go beyond bivariate statistics and 2D plots, we are in a multivariate world of possibly MANY dimensions. It takes only some imagination and suitable methods to get there."
  },
  {
    "objectID": "index.html#eureka",
    "href": "index.html#eureka",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "EUREKA!",
    "text": "EUREKA!\nEven modest sized multivariate data can have secrets that can be revealed in the right view. As an example, David Coleman at RCA Laboratories in Princeton, N.J. generated a data set of five (fictitious) measurements of grains of pollen for the 1986 Data Exposition at the Joint statistical Meetings. The first three variables are the lengths of geometric features 3848 observed sampled pollen grains – in the x, y, and z dimensions: a ridge along x, a nub in the y direction, and a crack in along the z dimension. The fourth variable is pollen grain weight, and the fifth is density. The challenge was to “find something interesting” in this data set.\nThose who solved the puzzle were able to find an orientation of this 5-dimensional data set, such that zooming in revealed a magic word, “EUREKA” spelled in points, as in the following figure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Four views of the pollen data, zooming in, clockwise from the upper left to discover the word “EUREKA”.\n\n\nThis can be seen better in a 3D animation. rgl (Adler and Murdoch 2023) is used to create a 3D scatterplot of the first three variables. Then the animation package (Xie 2021) is use to record a sequence of images, adjusting the rgl::par3d(zoom) value.\n\nlibrary(animation)\nlibrary(rgl)\ndata(pollen, package = \"animation\")\noopt = ani.options(interval = 0.05)\n## adjust the viewpoint\nuM =\n  matrix(c(-0.370919227600098, -0.513357102870941,\n           -0.773877620697021, 0, -0.73050606250763, 0.675815105438232,\n           -0.0981751680374146, 0, 0.573396027088165, 0.528906404972076,\n           -0.625681936740875, 0, 0, 0, 0, 1), 4, 4)\nopen3d(userMatrix = uM, \n       windowRect = c(10, 10, 510, 510))\n\nplot3d(pollen[, 1:3])\n\n# zoom in\nzm = seq(1, 0.045, length = 200)\npar3d(zoom = 1)\nfor (i in 1:length(zm)) {\n  par3d(zoom = zm[i])\n  ani.pause()\n}\nani.options(oopt)\n\n\n\n\n\n\nFigure 4: Animation of zooming in on the pollen data."
  },
  {
    "objectID": "index.html#what-i-assume",
    "href": "index.html#what-i-assume",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "What I assume",
    "text": "What I assume\nI assume the reader to have at least a basic familiarity with R. While R fundamentals are outside the scope of the current paper, I believe that this language provides a rich set of resources, far beyond that offered by other statistical software packages, and is well worth learning.\nFor those not familiar with R, I recommend Matloff (2011), Wickham (2014), and Cotton (2013) for introductions to programming in the language, and Fox and Weisberg (2018) and Teetor (2011) for learning about how to conduct basic statistical analyses."
  },
  {
    "objectID": "index.html#conventions-used-in-this-book",
    "href": "index.html#conventions-used-in-this-book",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Conventions used in this book",
    "text": "Conventions used in this book\nThe following typographic conventions are used in this book:\n\nitalic : indicates terms to be emphasized or defined in the text, …\nbold : is used for names of R packages (well, not so far)\nfixed-width : is used in program listings as well as in text to refer to variable and function names, R statement elements and keywords.\nfixed-width italic : isn’t used yet, but probably should be.\n\nFor R functions in packages, we use the notation package::function(), for example: car::Anova() to identify where those functions are defined"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "References",
    "text": "References\n\n\n\n\nAbbott, Edwin A. 1884. Flatland: A Romance of Many Dimensions. Cutchogue, NY: Buccaneer Books.\n\n\nAdler, Daniel, and Duncan Murdoch. 2023. Rgl: 3D Visualization Using OpenGL. https://CRAN.R-project.org/package=rgl.\n\n\nCajori, Florian. 1926. “Origins of Fourth Dimension Concepts.” The American Mathematical Monthly 33 (8): 397–406. https://doi.org/10.1080/00029890.1926.11986607.\n\n\nCotton, R. 2013. Learning R. Sebastopol, CA: O’Reilly Media.\n\n\nFox, John, and Sandford Weisberg. 2018. An r Companion to Applied Regression. Third. Thousand Oaks CA: SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ.\n\n\nMatloff, Norman. 2011. The Art of R Programming: A Tour of Statistical Software Design. San Francisco, CA: No Starch Press.\n\n\nTeetor, Paul. 2011. R cookbook. Sebastopol, CA: O’Reilly Media.\n\n\nWickham, Hadley. 2014. Advanced R. Boca Raton, FL: Chapman and Hall/CRC.\n\n\nXie, Yihui. 2021. Animation: A Gallery of Animations in Statistics and Utilities to Create Animations. https://yihui.org/animation/."
  },
  {
    "objectID": "intro.html#why-use-a-multivariate-design",
    "href": "intro.html#why-use-a-multivariate-design",
    "title": "1  Introduction",
    "section": "\n1.1 Why use a multivariate design",
    "text": "1.1 Why use a multivariate design\nA particular research outcome (e.g., depression, neuro-cognitive functioning, academic achievement, self-concept, attention deficit hyperactivity disorders) might take on a multivariate form if it has several observed measurement scales or related aspects by which it is quantified, or if there are multiple theoretically distinct outcomes that should be assessed in conjunction with each other (e.g., using depression, generalized anxiety, and stress inventories to model overall happiness). In this situation, the primary concern of the researcher is to ascertain the impact of potential predictors on two or more response variables simultaneously.\nFor example, if academic achievement is measured for adolescents by their reading, mathematics, science, and history scores, the following questions are of interest:\n\nDo predictors such as parent encouragement, socioeconomic status and school environmental variables affect all of these outcomes?\nDo they affect them in the same or different ways?\nHow many different aspects of academic achievement can be distinguished in the predictors? Equivalently, is academic achievement unidimensional or multidimensional in relation to the predictors?\n\nSimilarly, if psychiatric patients in various diagnostic categories are measured on a battery of tests related to social skills and cognitive functioning, we might want to know:\n\nWhich measures best discriminate among the diagnostic groups?\nWhich measures are most predictive of positive outcomes?\nFurther, how are the relationships between the outcomes affected by the predictors?\n\nSuch questions obviously concern more than just the separate univariate relations of each response to the predictors. Equally, or perhaps more importantly, are questions of how the response variables are predicted jointly.\n\n\n\n\n\n\nNote\n\n\n\nStructural equation modeling (SEM) offers another route to explore and analyze the relationships among multiple predictors and multiple responses. They have the advantage of being able to test potentially complex systems of linear equations in very flexible ways; however, these methods are often far removed from data analysis per se and except for path diagrams offer little in the way of visualization methods to aid in understanding and communicating the results. The graphical methods we describe here can also be useful in a SEM context."
  },
  {
    "objectID": "intro.html#linear-models-univariate-to-multivariate",
    "href": "intro.html#linear-models-univariate-to-multivariate",
    "title": "1  Introduction",
    "section": "\n1.2 Linear models: Univariate to multivariate",
    "text": "1.2 Linear models: Univariate to multivariate\nFor classical linear models for ANOVA and regression, the step from a univariate model for a single response, \\(y\\), to a multivariate one for a collection of \\(p\\) responses, \\(\\mathbf{y}\\) is conceptually very easy. That’s because the univariate model,\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_q x_q + u_i , \\]\nor, in matrix terms,\n\\[\\mathbf{y} = \\mathbf{X} \\; \\mathbf{\\beta} + \\mathbf{u}, \\quad\\mbox{   with   }\\quad \\mathbf{u} \\sim \\mathcal{N} (0, \\sigma^2 \\mathbf{I}) ,\\]\ngeneralizes directly to an analogous multivariate linear model (MLM),\n\\[\\mathbf{Y} = [\\mathbf{y_1}, \\mathbf{y_2}, \\dots, \\mathbf{y_p}] = \\mathbf{X} \\; \\mathbf{B} + \\mathbf{U} \\quad\\mbox{   with   }\\quad \\mathbf{U} \\sim \\mathcal{N} (\\mathbf{0}, \\mathbf{\\Sigma})\\]\nfor multiple responses (as will be discussed in detail). The design matrix, \\(\\mathbf{X}\\) remains the same, and the vector \\(\\beta\\) of coefficients becomes a matrix \\(\\mathbf{B}\\), with one column for each of the \\(p\\) outcome variables.\nHappily as well, hypothesis tests for the MLM are also straight-forward generalizations of the familiar \\(F\\) and \\(t\\)-tests for univariate response models. Moreover, there is a rich geometry underlying these generalizations  which we can exploit for understanding and visualization."
  },
  {
    "objectID": "intro.html#visualization-is-harder",
    "href": "intro.html#visualization-is-harder",
    "title": "1  Introduction",
    "section": "\n1.3 Visualization is harder",
    "text": "1.3 Visualization is harder\nHowever, with two or more response variables, visualizations for multivariate models are not as simple as they are for their univariate counterparts for understanding the effects of predictors, model parameters, or model diagnostics. Consequently, the results of such studies are often explored and discussed solely in terms of coefficients and significance, and visualizations of the relationships are only provided for one response variable at a time, if at all. This tradition can mask important nuances, and lead researchers to draw erroneous conclusions.\nThe aim of this book is to describe and illustrate some central methods that we have developed over the last ten years that aid in the understanding and communication of the results of multivariate linear models (Friendly 2007; Friendly and Meyer 2016). These methods rely on data ellipsoids as simple, minimally sufficient visualizations of variance that can be shown in 2D and 3D plots. As will be demonstrated, the Hypothesis-Error (HE) plot framework applies this idea to the results of multivariate tests of linear hypotheses.\nFurther, in the case where there are more than just a few outcome variables, the important nectar of their relationships to predictors can often be distilled in a multivariate juicer— a projection of the multivariate relationships to the predictors in the low-D space that captures most of the flavor. This idea can be applied using canonical correlation plots and with canonical discriminant HE plots.\n\n\n\n\nProjection: The cover image from Hofstadter’s Goedel, Bach and Escher illustrates projection of 3D solids onto each 2D plane."
  },
  {
    "objectID": "intro.html#sec-problems",
    "href": "intro.html#sec-problems",
    "title": "1  Introduction",
    "section": "\n1.4 Problems in understanding and communicating MLM results",
    "text": "1.4 Problems in understanding and communicating MLM results\nIn my consulting practice within the Statistical Consulting Service at York University, I see hundreds of clients each year ranging from advanced undergraduate thesis students, to graduate students and faculty from a variety of fields. Over the last two decades, and across each of these groups, I have noticed an increasing desire to utilize multivariate methods. As researchers are exposed to the utility and power of multivariate tests, they see them as an appealing alternative to running many univariate ANOVAs or multiple regressions for each response variable separately.\nHowever, multivariate analyses are more complicated than such approaches, especially when it comes to understanding and communicating results. Output is typically voluminous, and researchers will often get lost in the numbers. While software (SPSS, SAS and R) make tabular summary displays easy, these often obscure the findings that researchers are most interested in. The most common analytic oversights that we have observed are:\n\nAtomistic data screening: Researchers have mostly learned the assumptions (the Holy Trinity of normality, constant variance and independence) of univariate linear models, but then apply univariate tests (e.g., Shapiro-Wilk) and diagnostic plots (normal QQ plots) to every predictor and every response.\nBonferroni everywhere: Faced with the task of reporting the results for multiple response measures and a collection of predictors for each, a common tendency is to run (and sometimes report) each of the separate univariate response models and then apply a correction for multiple testing. Not only is this confusing and awkward to report, but it is largely unnecessary because the multivariate tests provide protection for multiple testing.\nReverting to univariate visualizations: To display results, SPSS and SAS make some visualization methods available through menu choices or syntax, but usually these are the wrong (or at least unhelpful) choices, in that they generate separate univariate graphs for the individual responses.\n\nThis book to discusses a few essential procedures for multivariate linear models, how their interpretation can be aided through the use of well-crafted (though novel) visualizations, and provides replicable sample code in R to showcase their use in applied behaviorial research. A later section [ref?] provides some practical guidelines for analyzing, visualizing and reporting such models to help avoid these and other problems.\n\n#cat(\"Packages used here:\\n\")\nwrite_pkgs(file = .pkg_file)\n#&gt; 7  packages used here:\n#&gt;  base, datasets, graphics, grDevices, methods, stats, utils"
  },
  {
    "objectID": "intro.html#references",
    "href": "intro.html#references",
    "title": "1  Introduction",
    "section": "References",
    "text": "References\n\n\n\n\nFriendly, Michael. 2007. “HE Plots for Multivariate General Linear Models.” Journal of Computational and Graphical Statistics 16 (2): 421–44. https://doi.org/10.1198/106186007X208407.\n\n\nFriendly, Michael, and David Meyer. 2016. Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count Data. Boca Raton, FL: Chapman & Hall/CRC."
  },
  {
    "objectID": "getting_started.html#sec-why_plot",
    "href": "getting_started.html#sec-why_plot",
    "title": "2  Getting Started",
    "section": "\n2.1 Why plot your data?",
    "text": "2.1 Why plot your data?\n\nGetting information from a table is like extracting sunlight from a cucumber. Farquhar and Farquhar (1891)\n\nAt the time the Farhquhar brothers wrote this pithy aphorism, graphical methods for understanding data had advanced considerably, but were not universally practiced, prompting their complaint.\nThe main graphic forms we use today—the pie chart, line graphs and bar—were invented by William Playfair around 1800 (Playfair 1786, 1801). The scatterplot arrived shortly after (Herschel 1833) and thematic maps showing the spatial distributions of social variables (crime, suicides, literacy) were used for the first time to reason about important societal questions (Guerry 1833) such as “is increased education associated with lower rates of crime?”\nIn the last half of the 18th Century, the idea of correlation was developed (Galton 1886; Pearson 1896) and the period, roughly 1860–1890, dubbed the “Golden Age of Graphics (Funkhouser 1937) became the richest period of innovation and beauty in the entire history of data visualization. During this time there was an incredible development of visual thinking, represented by the work of Charles Joseph Minard, advances in the role of visualization within scientific discovery, as illustrated through Francis Galton, and graphical excellence, embodied in state statistical atlases produced in France and elsewhere. See Friendly (2008); Friendly and Wainer (2021) for this history.\n\n2.1.1 Anscombe’s Quartet\nIn 1973, Francis Anscombe (Anscombe 1973) famously constructed a set of four data sets illustrate the importance of plotting the graphs before analyzing and model building, and the effect of unusual observations on fitted models. Now known as Anscombe’s Quartet, these data sets had identical statistical properties: the same means, standard devitions, correlations and regression lines.\nHis purpose was to debunk three notions that had been prevalent at the time:\n\nNumerical calculations are exact, but graphs are rough;\nFor any particular kind of statistical data there is just one set of calculations constituting a correct statistical analysis;\nPerforming intricate calculations is virtuous, whereas actually looking at the data is cheating.\n\nThe data set datasets::anscombe has 11 observations, recorded in wide format, with variables x1:x4 and y1:y4. ::: {.cell layout-align=“center”}\ndata(anscombe) \nhead(anscombe)\n#&gt;   x1 x2 x3 x4   y1   y2    y3   y4\n#&gt; 1 10 10 10  8 8.04 9.14  7.46 6.58\n#&gt; 2  8  8  8  8 6.95 8.14  6.77 5.76\n#&gt; 3 13 13 13  8 7.58 8.74 12.74 7.71\n#&gt; 4  9  9  9  8 8.81 8.77  7.11 8.84\n#&gt; 5 11 11 11  8 8.33 9.26  7.81 8.47\n#&gt; 6 14 14 14  8 9.96 8.10  8.84 7.04\n:::\nThe following code transforms this data to long format and calculates some summary statistics for each dataset.\n\nanscombe_long &lt;- anscombe |&gt; \n  pivot_longer(everything(), \n               names_to = c(\".value\", \"dataset\"), \n               names_pattern = \"(.)(.)\"\n  ) |&gt;\n  arrange(dataset)\n\nanscombe_long |&gt;\n  group_by(dataset) |&gt;\n  summarise(xbar      = mean(x),\n            ybar      = mean(y),\n            r         = cor(x, y),\n            intercept = coef(lm(y ~ x))[1],\n            slope     = coef(lm(y ~ x))[2]\n         )\n#&gt; # A tibble: 4 × 6\n#&gt;   dataset  xbar  ybar     r intercept slope\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 1           9  7.50 0.816      3.00 0.500\n#&gt; 2 2           9  7.50 0.816      3.00 0.5  \n#&gt; 3 3           9  7.5  0.816      3.00 0.500\n#&gt; 4 4           9  7.50 0.817      3.00 0.500\n\nAs we can see, all four data sets have nearly identical univariate and bivariate statistical measures. You can only see how they differ in graphs, which show their true natures to be vastly different.\nFigure 2.1 is an enhanced version of Anscombe’s plot of these data, adding helpful annotations to show visually the underlying statistical summaries.\n\n\n\n\nFigure 2.1: Scatterplots of Anscombe’s Quartet. Each plot shows the fitted regression line and a 68% data ellipse representing the correlation between \\(x\\) and \\(y\\).\n\n\n\nThis figure is produced as follows, using a single call to ggplot(), faceted by dataset. As we will see later (Section 3.1.1), the data ellipse (produced by stat_ellipse()) reflects the correlation between the variables.\n\ndesc &lt;- tibble(\n  dataset = 1:4,\n  label = c(\"Pure error\", \"Lack of fit\", \"Outlier\", \"Influence\")\n)\n\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 4) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"red\", linewidth = 1.5) +\n  scale_x_continuous(breaks = seq(0,20,2)) +\n  scale_y_continuous(breaks = seq(0,12,2)) +\n  stat_ellipse(level = 0.5, color=col, type=\"norm\") +\n  geom_label(data=desc, aes(label = label), x=6, y=12) +\n  facet_wrap(~dataset, labeller = label_both) \n\nThe subplots are labeled with the statistical idea they reflect:\n\ndataset 1: Pure error. This is the typical case with well-behaved data. Variation of the points around the line reflect only measurement error or unreliability in the response, \\(y\\).\ndataset 2: Lack of fit. The data is clearly curvilinear, and would be very well described by a quadratic, y ~ poly(x, 2). This violates the assumption of linear regression that the fitted model has the correct form.\ndataset 3: Outlier. One point, second from the right, has a very large residual. Because this point is near the extreme of \\(x\\), it pulls the regression line towards it, as you can see by imagining a line through the remaining points.\ndataset 4: Influence. All but one of the points have the same \\(x\\) value. The one unusual point has sufficient influence to force the regression line to fit it exactly.\n\nOne moral from this example:\n\nLinear regression only “sees” a line. It does its’ best when the data are really curvilinear. Because the line is fit by least squares, it pulls the line toward discrepant points to minimize the sum of squared residuals.\n\n\n\n\n\n\n\nDatasaurus Dozen\n\n\n\nThe method Anscombe used to compose his quartet is unknown, but it turns out that that there is a method to construct a wider collection of data sets with identical statistical properties. After all, in a bivariate data set with \\(n\\) observations, the correlation has \\((n-2)\\) degrees of freedom, so it is possible to choose this number of \\((x, y)\\) pairs to yield any given value. As it happens, it is possible to create any number of data sets with the same means, standard deviations and correlations with nearly any shape you like — even a dinosaur!\nThe Datasaurus Dozen was first publicized by Alberto Cairo in a blog post and are available in the datasauRus package Davies, Locke, and D’Agostino McGowan (2022). As shown in Figure 2.2, the sets include a star, cross, circle, bullseye, horizontal and vertical lines, and, of course the “dino”. The method (Matejka and Fitzmaurice 2017) uses simulated annealing, an iterative process that perturbs the points in a scatterplot, moving them towards a given shape while keeping the statistical summaries close to the fixed target value.\nThe datasauRus package just contains the data sets, but a general method, called statistical metamers, for producing such data sets has been described by Elio Campitelli and implemented in the metamer package.\n\n\n\n\n\n\nFigure 2.2: Animation of the Dinosaur Dozen data sets. Source: https://youtu.be/It4UA75z_KQ\n\n\n\n\n\n\n\n\n\nQuartets\n\n\n\nThe essential idea of a statistical “quartet” is to illustrate four quite different data sets or circumstances that seem superficially the same, but yet are paradoxically very different when you look behind the scenes. For example, in the context of causal analysis Gelman, Hullman, and Kennedy (2023), illustrated sets of four graphs, within each of which all four represent the same average (latent) causal effect but with much different patterns of individual effects. As an example of machine learning models, Biecek et al. (2023), introduced the “Rashamon Quartet”, a synthetic dataset for which four models from different classes (linear model, regression tree, random forest, neural network) have practically identical predictive performance. In all cases, the paradox is solved when their visualization reveals the distinct ways of understanding structure in the data.\n\n\n\n2.1.2 A real example\nIn the mid 1980s, a consulting client had a strange problem. She was conducting a study of the relation between body image and weight preoccupation in exercising and non-exercising people (Davis 1990). As part of the design, the researcher wanted to know if self-reported weight could be taken as a reliable indicator of true weight measured on a scale. It was expected that the correlations between reported and measured weight should be close to 1.0, and the slope of the regression lines for men and women should also be close to 1.0. The data set is car::Davis.\nShe was therefore very surprise to see the following numerical results: For men, the correlation was nearly perfect, but not so for women.\n\ndata(Davis, package=\"carData\")\nDavis &lt;- Davis |&gt;\n  drop_na()          # drop missing cases\nDavis |&gt;\n  group_by(sex) |&gt;\n  select(sex, weight, repwt) |&gt;\n  summarise(r = cor(weight, repwt))\n#&gt; # A tibble: 2 × 2\n#&gt;   sex       r\n#&gt;   &lt;fct&gt; &lt;dbl&gt;\n#&gt; 1 F     0.501\n#&gt; 2 M     0.979\n\nSimilarly, the regression lines showed the expected slope for men, but that for women was only 0.26.\n\nDavis |&gt;\n  nest(data = -sex) |&gt;\n  mutate(model = map(data, ~ lm(repwt ~ weight, data = .)),\n         tidied = map(model, tidy)) |&gt;\n  unnest(tidied) |&gt;\n  filter(term == \"weight\") |&gt;\n  select(sex, term, estimate, std.error)\n#&gt; # A tibble: 2 × 4\n#&gt;   sex   term   estimate std.error\n#&gt;   &lt;fct&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 M     weight    0.990    0.0229\n#&gt; 2 F     weight    0.262    0.0459\n\nWhat could be wrong here?, the client asked. The consultant replied with the obvious question:\n\nDid you plot your data?\n\nThe answer turned out to be one discrepant point, a female, whose measured weight was 166 kg (366 lbs!). This single point exerted so much influence that it pulled the fitted regression line down to a slope of only 0.26.\n\nDavis |&gt;\n  ggplot(aes(x = weight, y = repwt, color = sex, shape=sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  labs(y = \"Measured weight (kg)\", x = \"Reported weight (kg)\") +\n  geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  theme(legend.position = c(.8, .8))\n\n\n\nFigure 2.3: Regression for Davis’ data on reported weight and measures weight for men and women. Separate regression lines, predicting reported weight from measured weight are shown for males and females. One highly unusual point is highlighted.\n\n\n\nIn this example, it was arguable that \\(x\\) and \\(y\\) axes should be reversed, to determine how well measured weight can be predicted from reported weight. In ggplot this can easily be done by reversing the x and y aesthetics.\n\nDavis |&gt;\n  ggplot(aes(y = weight, x = repwt, color = sex, shape=sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  labs(y = \"Measured weight (kg)\", x = \"Reported weight (kg)\") +\n  geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  theme(legend.position = c(.8, .8))\n\n\n\nFigure 2.4: Regression for Davis’ data on reported weight and measures weight for men and women. Separate regression lines, predicting measured weight from re[ported] weight are shown for males and females. The highly unusual point no longer has an effect on the fitted lines.\n\n\n\nIn Figure 2.4, this discrepant observation again stands out like a sore thumb, but it makes very little difference in the fitted line for females. The reason is that this point is well within the range of the \\(x\\) variable (repwt). To impact the slope of the regression line, an observation must be unusual in_both_ \\(x\\) and \\(y\\). We take up the topic of how to detect influential observations and what to do about them in Chapter XX.\nThe value of such plots is not only that they can reveal possible problems with an analysis, but also help identify their reasons and suggest corrective action. What went wrong here? Examination of the original data showed that this person switched the values, recording her reported weight in the box for measured weight and vice versa."
  },
  {
    "objectID": "getting_started.html#plots-for-data-analysis",
    "href": "getting_started.html#plots-for-data-analysis",
    "title": "2  Getting Started",
    "section": "\n2.2 Plots for data analysis",
    "text": "2.2 Plots for data analysis\nVisualization methods take an enormous variety of forms, but it is useful to distinguish several broad categories according to their use in data analysis:\n\ndata plots : primarily plot the raw data, often with annotations to aid interpretation (regression lines and smooths, data ellipses, marginal distributions)\nreconnaissance plots : with more than a few variables, reconnaissance plots provide a high-level, bird’s-eye overview of the data, allowing you to see patterns that might not be visible in a set of separate plots. Some examples are scatterplot matrices, showing all bivariate plots of variables in a data set; correlation diagrams, using visual glyphs to represent the correlations between all pairs of variables and “trellis” or faceted plots that show how a focal relation of one or more variables differs across values of other variables.\nmodel plots : plot the results of a fitted model, such as a regression line or curve to show uncertainty, or a regression surface in 3D, or a plot of coefficients in model together with confidence intervals. Other model plots try to take into account that a fitted model may involve more variables than can be shown in a static 2D plot. Some examples of these are added variable plots, and marginal effect plots, both of which attempt to show the net relation of two focal variables, controling or adjusting for other variables in a model.\ndiagnostic plots : indicating potential problems with the fitted model. These include residual plots, influence plots, plots for testing homogeneity of variance and so forth.\ndimension reduction plots : plot representations of the data into a space of fewer dimensions than the number of variables in the data set. Simple examples include principal components analysis (PCA) and the related biplots, and multidimensional scaling (MDS) methods.\n\nWe give some more details and a few examples in the sections that follow."
  },
  {
    "objectID": "getting_started.html#data-plots",
    "href": "getting_started.html#data-plots",
    "title": "2  Getting Started",
    "section": "\n2.3 Data plots",
    "text": "2.3 Data plots\nData plots portray the data in a space where the coordinate axes are the observed variables.\n\n1D plots include line plots, histograms and density estimates.\n2D plots are most often scatterplots, but contour plots or hex-binned plots are also useful when the sample size is large."
  },
  {
    "objectID": "getting_started.html#model-plots",
    "href": "getting_started.html#model-plots",
    "title": "2  Getting Started",
    "section": "\n2.4 Model plots",
    "text": "2.4 Model plots"
  },
  {
    "objectID": "getting_started.html#diagnostic-plots",
    "href": "getting_started.html#diagnostic-plots",
    "title": "2  Getting Started",
    "section": "\n2.5 Diagnostic plots",
    "text": "2.5 Diagnostic plots\n\ncat(\"Writing packages to \", .pkg_file, \"\\n\")\n#&gt; Writing packages to  C:/R/Projects/Vis-MLM-quarto/bib/pkgs.txt\nwrite_pkgs(file = .pkg_file)\n#&gt; 18  packages used here:\n#&gt;  base, broom, datasets, dplyr, forcats, ggplot2, graphics, grDevices, lubridate, methods, purrr, readr, stats, stringr, tibble, tidyr, tidyverse, utils"
  },
  {
    "objectID": "getting_started.html#references",
    "href": "getting_started.html#references",
    "title": "2  Getting Started",
    "section": "References",
    "text": "References\n\n\n\n\nAnscombe, F. J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27: 17–21.\n\n\nBiecek, Przemyslaw, Hubert Baniecki, Mateusz Krzyzinski, and Dianne Cook. 2023. “Performance Is Not Enough: A Story of the Rashomon’s Quartet,” February. https://arxiv.org/abs/2302.13356.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022. datasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nDavis, C. 1990. “Body Image and Weight Preoccupation: A Comparison Between Exercising and Non-Exercising Women.” Appetite 16 (1): 84. https://doi.org/10.1016/0195-6663(91)90115-9.\n\n\nFarquhar, A. B., and H. Farquhar. 1891. Economic and Industrial Delusions: A Discourse of the Case for Protection. New York: Putnam.\n\n\nFriendly, Michael. 2008. “The Golden Age of Statistical Graphics.” Statistical Science 23 (4): 502–35. https://doi.org/10.1214/08-STS268.\n\n\nFriendly, Michael, and Howard Wainer. 2021. A History of Data Visualization and Graphic Communication. Cambridge, MA: Harvard University Press. https://doi.org/10.4159/9780674259034.\n\n\nFunkhouser, H. Gray. 1937. “Historical Development of the Graphical Representation of Statistical Data.” Osiris 3 (1): 269–405. http://tinyurl.com/32ema9.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” Journal of the Anthropological Institute 15: 246–63. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0.\n\n\nGelman, Andrew, Jessica Hullman, and Lauren Kennedy. 2023. “Causal Quartets: Different Ways to Attain the Same Average Treatment Effect.” http://www.stat.columbia.edu/~gelman/research/unpublished/causal_quartets.pdf.\n\n\nGuerry, André-Michel. 1833. Essai Sur La Statistique Morale de La France. Paris: Crochard.\n\n\nHerschel, John F. W. 1833. “On the Investigation of the Orbits of Revolving Double Stars: Being a Supplement to a Paper Entitled \"Micrometrical Measures of 364 Double Stars\".” Memoirs of the Royal Astronomical Society 5: 171–222.\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs.” In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM. https://doi.org/10.1145/3025453.3025912.\n\n\nPearson, Karl. 1896. “Contributions to the Mathematical Theory of Evolution—III, Regression, Heredity and Panmixia.” Philosophical Transactions of the Royal Society of London, A, 187: 253–318.\n\n\nPlayfair, William. 1786. Commercial and Political Atlas: Representing, by Copper-Plate Charts, the Progress of the Commerce, Revenues, Expenditure, and Debts of England, During the Whole of the Eighteenth Century. London: Debrett; Robinson;; Sewell. http://ucpj.uchicago.edu/Isis/journal/demo/v000n000/000000/000000.fg4.html.\n\n\n———. 1801. Statistical Breviary; Shewing, on a Principle Entirely New, the Resources of Every State and Kingdom in Europe. London: Wallis."
  },
  {
    "objectID": "multivariate_plots.html#sec-bivariate_summaries",
    "href": "multivariate_plots.html#sec-bivariate_summaries",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.1 Bivariate summaries",
    "text": "3.1 Bivariate summaries\n+ smoothers\n+ data ellipses\n    \n\n3.1.1 The Data Ellipse\nThe data ellipse (Monette 1990), or concentration ellipse (Dempster 1969) is a remarkably simple and effective display for viewing and understanding bivariate relationships in multivariate data. The data ellipse is typically used to add a visual summary to a scatterplot, that shows all together the means, standard deviations, correlation, and slope of the regression line for two variables. Under the classical assumption that the data are bivariate normally distributed, the data ellipse is also a sufficient visual summary, in the sense that it captures all relevant features of the data. See Friendly, Monette, and Fox (2013) for a complete discussion of the role of ellipsoids in statistical data visualization.\nIt is based on the idea that in a bivariate normal distribution, the contours of equal probability form a series of concentric ellipses. If the variables were uncorrelated and had the same variances, these would be circles, and Euclidean distance would measure the distance of each observation from the mean. When the variables are correlated, a different measure, Mahalanobis distance is the proper measure of how far a point is from the mean.\n\n\n\n\nFigure 3.1: 2D data with curves of constant distance from the centroid. Which of the points A and B is further from the mean (X)? Source: Ou Zhang\n\n\n\nTo illustrate, Figure 3.1 shows a scatterplot with labels for two points, “A” and “B”. Which is further from the mean, “X”? A contour of constant Euclidean distance, shown by the red dashed circle, ignores the apparent negative correlation, so point “A” is further. The blue ellipse for Mahalanobis distance takes the correlation into account, so point “B” has a greater distance from the mean.\nMathematically, Euclidean (squared) distance for \\(p\\) variables, \\(j = 1, 2, \\dots , p\\), is just a generalization of the square of a univariate standardized (\\(z\\)) score, \\(z^2 = [(y - \\bar{y}) / s]^2\\),\n\\[\nD_E^2 (\\mathbf{y}) = \\sum_j^p z_j^2 = \\mathbf{z}^T  \\mathbf{z} = (\\mathbf{y} - \\bar{\\mathbf{y}})^T \\operatorname{diag}(\\mathbf{S})^{-1} (\\mathbf{y} - \\bar{\\mathbf{y}}) \\; ,\n\\] where \\(\\mathbf{S}\\) is the sample variance-covariance matrix, \\(\\mathbf{S} = ({n-1})^{-1} \\sum_{i=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})^T (\\mathbf{y}_i - \\bar{\\mathbf{y}})\\).\nMahalanobis’ distance takes the correlations into account simply by using the covariances as well as the variances, \\[\nD_M^2 (\\mathbf{y}) = (\\mathbf{y} - \\bar{\\mathbf{y}})^T S^{-1} (\\mathbf{y} - \\bar{\\mathbf{y}}) \\; .\n\\] For \\(p\\) variables, the data ellipsoid \\(\\mathcal{E}_c\\) of size \\(c\\) is a \\(p\\)-dimensional ellipse, defined as the set of points \\(\\mathbf{y} = (y_1, y_2, \\dots y_p)\\) whose squared Mahalanobis distance, \\(D_M^2 ( \\mathbf{y} )\\) is less than or equal to \\(c^2\\).\nWhen \\(\\mathbf{y}\\) is (at least approximately) bivariate normal, \\(D_M^2(\\mathbf{y})\\) has a large-sample \\(\\chi^2_2\\) distribution (\\(\\chi^2\\) with 2 df), so taking \\(c^2 = \\chi^2_2 (0.68) = 2.28\\) gives a “1 standard deviation bivariate ellipse,” an analog of the standard interval \\(\\bar{y} \\pm 1 s\\), while \\(c^2 = \\chi^2_2 (0.95) = 5.99 \\approx 6\\) gives a data ellipse of 95% coverage.\nProperties\nThe essential ideas of correlation and regression and their relation to ellipses go back to Galton (1886). Galton’s goal was to to predict (or explain) how a heritable trait, \\(Y\\), (e.g., height) of children was related to that of their parents, \\(X\\). He made a semi-graphic table of the frequencies of 928 observations of the average height of father and mother versus the height of their child, shown in Figure 3.2. He then drew smoothed contour lines of equal frequencies and had the wonderful visual insight that these formed concentric shapes that were tolerably close to ellipses. He then calculated summaries, \\(\\mathrm{Ave}(Y | X)\\), and, for symmetry, \\(\\mathrm{Ave}(X | Y)\\), and plotted these as lines of means on his diagram. Lo and behold, he had a second visual insight: the lines of means of (\\(Y | X\\)) and (\\(X | Y\\)) corresponded approximately to the loci of horizontal and vertical tangents to the concentric ellipses. To complete the picture, he added lines showing the major and minor axes of the family of ellipses, with the result shown in Figure 3.2.\n\n\n\n\nFigure 3.2: Galton’s 1886 diagram, showing the relationship of height of children to the average of their parents’ height. The diagram is essentially an overlay of a geometrical interpretation on a bivariate grouped frequency distribution, shown as numbers.\n\n\n\nFor two variables, \\(x\\) and \\(y\\), the remarkable properties of the data ellipse are illustrated in Figure 3.3, a modern reconstruction of Galton’s diagram.\n\n\n\n\nFigure 3.3: Sunflower plot of Galton’s data on heights of parents and their children (in.), with 40%, 68% and 95% data ellipses and the regression lines of \\(y\\) on \\(x\\) (black) and \\(x\\) on \\(y\\) (grey).\n\n\n\n\nThe ellipses have the mean vector \\((\\bar{x}, \\bar{y})\\) as their center.\nThe lengths of arms of the central cross show the standard deviations of the variables, which correspond to the shadows of the ellipse covering 40% of the data. These are the bivariate analogs of the standard intervals \\(\\bar{x} \\pm 1 s_x\\) and \\(\\bar{y} \\pm 1 s_y\\).\nMore generally, shadows (projections) on the coordinate axes, or any linear combination of them, give any standard interval, \\(\\bar{x} \\pm k s_x\\) and \\(\\bar{y} \\pm k s_y\\). Those with \\(k=1, 1.5, 2.45\\), have bivariate coverage 40%, 68% and 95%, corresponding to these quantiles of the \\(\\chi^2\\) distribution with 2 degrees of freedom, i.e., \\(\\chi^2_2 (.40) \\approx 1^2\\), \\(\\chi^2_2 (.68) \\approx 1.5^2\\), and \\(\\chi^2_2 (.95) \\approx 2.45\\). \nThe regression line predicting \\(y\\) from \\(x\\) goes through the points where the ellipses have vertical tangents. The other regression line, predicting \\(x\\) from \\(y\\) goes through the points of horizontal tangency.\nThe correlation \\(r(x, y)\\) is the ratio of the vertical segment from the mean of \\(y\\) to the regression line to the vertical segment going to the top of the ellipse as shown at the right of the figure. It is \\(r = 0.46\\) in this example.\nThe residual standard deviation, \\(s_e = \\sqrt{MSE} = \\sqrt{\\Sigma (y - \\bar{y})^2 / n-2}\\), is the half-length of the ellipse at the mean \\(\\bar{x}\\)\n\nBecause Galton’s values of parent and child height were recorded in class intervals of 1 in., they are shown as sunflower symbols in Figure 3.3, with multiple ‘petals’ reflecting the number of observations at each location. This plot is constructed using sunflowerplot() and car::dataEllipse() for the ellipses.\n\ndata(Galton, package = \"HistData\")\n\nsunflowerplot(parent ~ child, data=Galton, \n      xlim=c(61,75), \n      ylim=c(61,75), \n      seg.col=\"black\", \n        xlab=\"Child height\", \n      ylab=\"Mid Parent height\")\n\ny.x &lt;- lm(parent ~ child, data=Galton)     # regression of y on x\nabline(y.x, lwd=2)\nx.y &lt;- lm(child ~ parent, data=Galton)     # regression of x on y\ncc &lt;- coef(x.y)\nabline(-cc[1]/cc[2], 1/cc[2], lwd=2, col=\"gray\")\n\nwith(Galton, \n     car::dataEllipse(child, parent, \n         plot.points=FALSE, \n         levels=c(0.40, 0.68, 0.95), \n         lty=1:3)\n    )"
  },
  {
    "objectID": "multivariate_plots.html#r-functions-for-data-ellipses",
    "href": "multivariate_plots.html#r-functions-for-data-ellipses",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.2 R functions for data ellipses",
    "text": "3.2 R functions for data ellipses\nA number of packages provide functions for drawing data ellipses of data, with various features.\n\ncar::scatterplot()\ncar::dataEllipse()\nheplots::covEllipses()\nggplot2::stat_ellipse()"
  },
  {
    "objectID": "multivariate_plots.html#sec-quantitative_data",
    "href": "multivariate_plots.html#sec-quantitative_data",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.3 Quantitative data:",
    "text": "3.3 Quantitative data:\n+ scatterplot matrices\n+ parallel coordinate plots"
  },
  {
    "objectID": "multivariate_plots.html#categorical-data",
    "href": "multivariate_plots.html#categorical-data",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.4 Categorical data:",
    "text": "3.4 Categorical data:\n+ mosaic plots"
  },
  {
    "objectID": "multivariate_plots.html#generalized-pair-plots",
    "href": "multivariate_plots.html#generalized-pair-plots",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.5 Generalized pair plots",
    "text": "3.5 Generalized pair plots\n\n#&gt; 7  packages used here:\n#&gt;  base, datasets, graphics, grDevices, methods, stats, utils"
  },
  {
    "objectID": "multivariate_plots.html#references",
    "href": "multivariate_plots.html#references",
    "title": "3  Plots of Multivariate Data",
    "section": "References",
    "text": "References\n\n\n\n\nDempster, A. P. 1969. Elements of Continuous Multivariate Analysis. Reading, MA: Addison-Wesley.\n\n\nFriendly, Michael, Georges Monette, and John Fox. 2013. “Elliptical Insights: Understanding Statistical Methods Through Elliptical Geometry.” Statistical Science 28 (1): 1–39. https://doi.org/10.1214/12-STS402.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” Journal of the Anthropological Institute 15: 246–63. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0.\n\n\nMonette, Georges. 1990. “Geometry of Multiple Regression and Interactive 3-D Graphics.” In Modern Methods of Data Analysis, edited by J. Fox and S. Long, 209–56. Beverly Hills, CA: SAGE Publications."
  },
  {
    "objectID": "linear_models.html#regression",
    "href": "linear_models.html#regression",
    "title": "4  Overview of Linear models",
    "section": "\n4.1 Regression",
    "text": "4.1 Regression"
  },
  {
    "objectID": "linear_models.html#anova",
    "href": "linear_models.html#anova",
    "title": "4  Overview of Linear models",
    "section": "\n4.2 ANOVA",
    "text": "4.2 ANOVA"
  },
  {
    "objectID": "linear_models.html#ancova",
    "href": "linear_models.html#ancova",
    "title": "4  Overview of Linear models",
    "section": "\n4.3 ANCOVA",
    "text": "4.3 ANCOVA"
  },
  {
    "objectID": "linear_models.html#references",
    "href": "linear_models.html#references",
    "title": "4  Overview of Linear models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "linear_models-plots.html#the-regression-quartet",
    "href": "linear_models-plots.html#the-regression-quartet",
    "title": "5  Plots for univariate response models",
    "section": "\n5.1 The “regression quartet”",
    "text": "5.1 The “regression quartet”\nFor a fitted model, plotting the model object with plot(model) provides for any of six basic plots, of which four are produced by default, giving rise to the term regression quartet for this collection. These are:\n\nResiduals vs. Fitted: For well-behaved data, the points should hover around a horizontal line at residual = 0, with no obvious pattern or trend.\nNormal Q-Q plot: A plot of sorted standardized residuals \\(e_i\\) (obtained fromrstudent(model)) against the theoretical values those values would have in a standard normal \\(\\mathcal{N}(0, 1)\\) distribution.\nScale-Location: Plots the square-root of the absolute values of the standardized residuals $ as a measure of “scale” against the fitted values \\(\\hat{y}_i\\) as a measure of “location”. This provides an assessment of homogeneity of variance, which appears as a tendency for scale to vary with location.\nResiduals vs. Leverage: Plots standardized residuals against leverage to help identify possibly influential observations. Leverage, or “hat” values (given by hat(model)) are proportional to the squared Mahalanobis distances of the predictor values \\(\\mathbf{x}_i\\) from the means, and measure the potential of an observation to change the fitted coefficients if that observation was deleted. Actual influence is measured by Cooks’s distance (cooks.distance(model)) and is proportional to the product of residual times leverage. Contours of constant Cook’s \\(D\\) are added to the plot.\n\nOne key feature of these plots is providing reference lines or smoothed curves for ease of judging the extent to which a plot conforms to the expected pattern; another is the labeling of observations which deviate from an assumption.\nThe base-R plot(model) plots are done much better in a variety of packages. I illustrate some versions from the car and performance packages.\nExample: Occupational prestige\nThese examples use the data on the prestige of 102 occupational categories and other measures from the 1971 Canadian Census, recorded in carData::Prestige. Our interest is in understanding how prestige (the Pineo-Ported prestige score, from a social survey) is related to census measures of the average education, income, percent women of incumbents in those occupations. Occupation type is a factor with levels \"bc\" (blue collar), \"wc\" (white collar) and \"prof\" (professional). TODO: These data should be introduced earlier with descriptive plots, scatterplots, …\n\ndata(Prestige, package=\"carData\")\n# `type` is really an ordered factor. Make it so.\nPrestige$type &lt;- ordered(Prestige$type,\n                         levels=c(\"bc\", \"wc\", \"prof\"))\nstr(Prestige)\n#&gt; 'data.frame':    102 obs. of  6 variables:\n#&gt;  $ education: num  13.1 12.3 12.8 11.4 14.6 ...\n#&gt;  $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...\n#&gt;  $ women    : num  11.16 4.02 15.7 9.11 11.68 ...\n#&gt;  $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...\n#&gt;  $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...\n#&gt;  $ type     : Ord.factor w/ 3 levels \"bc\"&lt;\"wc\"&lt;\"prof\": 3 3 3 3 3 3 3 3 3 3 ...\n\nWe fit a main-effects model using all predictors (ignoring census, the Canadian Census occupational code):\n\nprestige.mod &lt;- lm(prestige ~ education + income + women + type,\n                   data=Prestige)\n\nplot(model) produces four separate plots. For a quick look, I like to arrange them in a single 2x2 figure.\n\nop &lt;- par(mfrow = c(2,2), \n          mar=c(4,4,3,1)+.1)\nplot(prestige.mod, lwd=2, cex.lab=1.4)\npar(op)\n\n\n\nFigure 5.1: Regression quartet of diagnostic plots for the Prestige data. Several possibly unusual observations are labeled."
  },
  {
    "objectID": "linear_models-plots.html#other-diagnostic-plots",
    "href": "linear_models-plots.html#other-diagnostic-plots",
    "title": "5  Plots for univariate response models",
    "section": "\n5.2 Other Diagnostic plots",
    "text": "5.2 Other Diagnostic plots"
  },
  {
    "objectID": "linear_models-plots.html#coefficient-plots",
    "href": "linear_models-plots.html#coefficient-plots",
    "title": "5  Plots for univariate response models",
    "section": "\n5.3 Coefficient plots",
    "text": "5.3 Coefficient plots"
  },
  {
    "objectID": "linear_models-plots.html#added-variable-plots",
    "href": "linear_models-plots.html#added-variable-plots",
    "title": "5  Plots for univariate response models",
    "section": "\n5.4 Added-variable plots",
    "text": "5.4 Added-variable plots"
  },
  {
    "objectID": "linear_models-plots.html#marginal-plots",
    "href": "linear_models-plots.html#marginal-plots",
    "title": "5  Plots for univariate response models",
    "section": "\n5.5 Marginal plots",
    "text": "5.5 Marginal plots\n\n#&gt; Writing packages to  C:/R/Projects/Vis-MLM-quarto/bib/pkgs.txt\n#&gt; 7  packages used here:\n#&gt;  base, datasets, graphics, grDevices, methods, stats, utils"
  },
  {
    "objectID": "linear_models-plots.html#references",
    "href": "linear_models-plots.html#references",
    "title": "5  Plots for univariate response models",
    "section": "References",
    "text": "References\n\n\n\n\nBelsley, D. A., E. Kuh, and R. E. Welsch. 1980. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. New York: John Wiley; Sons.\n\n\nFox, John. 2020. Regression Diagnostics. 2nd ed. SAGE Publications, Inc. https://doi.org/10.4135/9781071878651.\n\n\nFox, John, and Sandford Weisberg. 2018. An r Companion to Applied Regression. Third. Thousand Oaks CA: SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2023. Car: Companion to Applied Regression. https://CRAN.R-project.org/package=car."
  },
  {
    "objectID": "collinearity-ridge.html#what-is-collinearity",
    "href": "collinearity-ridge.html#what-is-collinearity",
    "title": "6  Collinearity & Ridge Regression",
    "section": "\n6.1 What is collinearity?",
    "text": "6.1 What is collinearity?\nRecall the standard classical linear model for a response variable \\(y\\) with a collection of predictors in \\(\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_p)\\)\n\\[\n\\begin{eqnarray*}\n\\mathbf{y} & =&  \\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\cdots + \\beta_p \\mathbf{x}_p + \\mathbf{\\epsilon} \\\\\n         & = & \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\; ,\n\\end{eqnarray*}\n\\] for which the ordinary least squares solution is:\n\\[\n\\widehat{\\mathbf{b}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\; \\mathbf{X}^T \\mathbf{y} \\; ,\n\\] with sampling variances and covariances \\(\\text{Var} (\\widehat{\\mathbf{b}}) = \\sigma^2 \\times (\\mathbf{X}^T \\mathbf{X})^{-1}\\) and \\(\\sigma^2\\) is the variance of the residuals \\(\\mathbf{\\epsilon}\\), estimated by the mean squared error (MSE).\nIn the limiting case, when one \\(x_i\\) is perfectly predictable from the other \\(x\\)s, i.e., \\(R^2 (x_i | \\text{other }x) = 1\\),\n\nthere is no unique solution for the regression coefficients \\(\\mathbf{b} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X} \\mathbf{y}\\);\nthe standard errors \\(s (b_i)\\) of the estimated coefficients are infinite and t statistics \\(t_i = b_i / s (b_i)\\) are 0.\n\nThis extreme case reflects a situation when one or more predictors are effectively redundant, for example when you include two variables \\(x\\) and \\(y\\) and their sum \\(z = x + y\\) in a model, or use ipsatized scores that sum to a constant. More generally, collinearity refers to the case when there are very high multiple correlations among the predictors, such as \\(R^2 (x_i | \\text{other }x) \\ge 0.9\\). Note that you can’t tell simply by looking at the simple correlations. A large correlation \\(r_{ij}\\) is sufficient for collinearity, but not necessary — you can have variables \\(x_1, x_2, x_3\\) for which the pairwise correlation are low, but the multiple correlation is high.\nThe consequences are:\n\nThe estimated coefficients have large standard errors, \\(s(\\hat{b_j})\\). They are multiplied by the square root of the variance inflation factor, \\(\\sqrt{\\text{VIF}}\\), discussed below.\nThis deflates the \\(t\\)-statistics, \\(t = \\hat{b_j} / s(\\hat{b_j})\\) by the same factor.\nThus you may find a situation where an overall model is highly significant (large \\(F\\)-statistic), while no (or few) of the individual predictors are. This is a puzzlement!\nBeyond this, the least squares solution may have poor numerical accurracy (Longley 1967), because the solution depends on the determinant \\(|\\,\\mathbf{X}^T \\mathbf{X}\\,|\\), which approaches 0 as multiple correlations increase.\nAs well, recall that the coefficients \\(\\hat{b}\\) are partial coefficients, meaning the estimated change \\(\\Delta y\\) in \\(y\\) when \\(x\\) changes by one unit \\(\\Delta x\\), but holding all other variables constant. Then, the model may be trying to estimate something that does not occur in the data.\n\n\n6.1.1 Visualizing collinearity\nCollinearity can be illustrated in data space for two predictors in terms of the stability of the regression plane for a linear model Y = X1 + X2. In ?fig-collin-demo (adapted from Fox (2016), Fig. 13.2):\n\nshows a case where \\(X_1\\) and \\(X_2\\) are uncorrelated as can be seen in their scatter in the horizontal plane (+ symbols). The regression plane is well-supported; a small change in Y for one observation won’t make much difference.\nIn panel (b), \\(X_1\\) and \\(X_2\\) have a perfect correlation, \\(r (x_1, x_2) = 1.0\\). The regression plane is not unique; in fact there are an infinite number of planes that fit the data equally well. Note that, if all we care about is prediction (not the coefficients), we could use \\(X_1\\) or \\(X_2\\), or both, or any weighted sum of them in a model and get the same predicted values.\nShows a typical case where there is a strong correlation between \\(X_1\\) and \\(X_2\\). The regression plane here is unique, but is not well determined. A small change in Y can make quite a difference in the fitted value or coefficients, depending on the values of \\(X_1\\) and \\(X_2\\). Where \\(X_1\\) and \\(X_2\\) are far from their near linear relation in the botom plane, you can imagine that it is easy to tilt the plane substantially by a small change in \\(Y\\).\n\n\nknitr::include_graphics(\"images/collin-demo.png\")\n\n{#fig-collin-demo, fig-align=‘center’ width=100%}\n\n\nIt is also useful to visualize collinearity by comparing the representation in data space with the anologous view of the confidence ellipses for coefficients in beta space. To do so, we generate data from a known model \\(y = 2 x_1 + 2 x_2 + \\epsilon\\) with \\(\\epsilon \\tilde \\mathcal{N} (0, 100)\\) and various correlations between \\(x_1\\) and \\(x_2\\).\nWorking file: R/collin-data-beta.R"
  },
  {
    "objectID": "collinearity-ridge.html#measuring-collinearity-variance-inflation-factors",
    "href": "collinearity-ridge.html#measuring-collinearity-variance-inflation-factors",
    "title": "6  Collinearity & Ridge Regression",
    "section": "\n6.2 Measuring collinearity: Variance inflation factors",
    "text": "6.2 Measuring collinearity: Variance inflation factors\nHow can we measure the effect of collinearity? The essential idea is to compare, for each predictor the variance \\(s^2 (\\widehat{b_j})\\) that the coefficient that \\(x_j\\) would have if it was totally unrelated to the other predictors to the actual variance it has in the given model.\nFor two predictors such as shown in ?fig-collin-demo the sampling variance of \\(x_1\\) can be expressed as\n\\[\ns^2 (\\widehat{b_1}) = \\frac{MSE}{(n-1) \\; s^2(x_1)} \\; \\times \\; \\left[ \\frac{1}{1-r^2_{12}} \\right]\n\\] The first term here is the variance of \\(b_1\\) when the two predictors are uncorrelated. The term in brackets represents the variance inflation factor (Marquandt:70?), the amount by which the variance of the coefficent is multiplied as a consequence of the correlation \\(r_{12}\\) of the predictors. As \\(r_{12} \\rightarrow 1\\), the variances approaches infinity.\nMore generally, with any number of predictors, this relation has a similar form, replacing the simple correlation \\(r_{12}\\) with the multiple correlation predicting \\(x_j\\) from all others,\n\\[\ns^2 (\\widehat{b_j}) = \\frac{MSE}{(n-1) \\; s^2(x_j)} \\; \\times \\; \\left[ \\frac{1}{1-R^2_{j | \\text{others}}} \\right]\n\\] So, we have that the variance inflation factors are:\n\\[\n\\text{VIF}_j = \\frac{1}{1-R^2_{j \\,|\\, \\text{others}}}\n\\] In practice, it is often easier to think in terms of the square root, \\(\\sqrt{\\text{VIF}_j}\\) as the multiplier of the standard errors. The denominator, \\(1-R^2_{j | \\text{others}}\\) is sometimes called tolerance, a term I don’t find particularly useful.\nNote that when there are terms in the model with more than one df, such as education with four levels (and hence 3 df) or a polynomial term spaecified as poly(x, 3), the standard VIF calculation gives results that vary with how those terms are coded in the model. (FoxMonette:92?) define generalized, GVIFs as the inflation in the squared area of the confidence ellipse for the coefficients of such terms, relative to what would be obtained with uncorrelated data.\nExample: This example uses the cars data set in the VisCollin package containing various measures of size and performance on 406 models of automobiles from 1982. Interest is focused on predicting gas mileage, mpg.\n\ndata(cars, package = \"VisCollin\")\nstr(cars)\n#&gt; 'data.frame':    406 obs. of  10 variables:\n#&gt;  $ make    : Factor w/ 30 levels \"amc\",\"audi\",\"bmw\",..: 6 4 22 1 12 12 6 22 23 1 ...\n#&gt;  $ model   : chr  \"chevelle\" \"skylark\" \"satellite\" \"rebel\" ...\n#&gt;  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...\n#&gt;  $ cylinder: int  8 8 8 8 8 8 8 8 8 8 ...\n#&gt;  $ engine  : num  307 350 318 304 302 429 454 440 455 390 ...\n#&gt;  $ horse   : int  130 165 150 150 140 198 220 215 225 190 ...\n#&gt;  $ weight  : int  3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ...\n#&gt;  $ accel   : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n#&gt;  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...\n#&gt;  $ origin  : Factor w/ 3 levels \"Amer\",\"Eur\",\"Japan\": 1 1 1 1 1 1 1 1 1 1 ...\n\nWe fit a model predicting gas mileage (mpg) from the number of cylinders, engine displacement, horsepower, weight, time to accelerate from 0 – 60 mph and model year (1970–1982). Perhaps surprisingly, only weight and year appear to significantly predict gas mileage. What’s going on here?\n\ncars.mod &lt;- lm (mpg ~ cylinder + engine + horse + weight + accel + year, \n                data=cars)\nAnova(cars.mod)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: mpg\n#&gt;           Sum Sq  Df F value Pr(&gt;F)    \n#&gt; cylinder      12   1    0.99   0.32    \n#&gt; engine        13   1    1.09   0.30    \n#&gt; horse          0   1    0.00   0.98    \n#&gt; weight      1214   1  102.84 &lt;2e-16 ***\n#&gt; accel          8   1    0.70   0.40    \n#&gt; year        2419   1  204.99 &lt;2e-16 ***\n#&gt; Residuals   4543 385                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe check the variance inflation factors, using car::vif(). We see that most predictors have very high VIFs, indicating moderately severe multicollinearity.\n\nvif(cars.mod)\n#&gt; cylinder   engine    horse   weight    accel     year \n#&gt;    10.63    19.64     9.40    10.73     2.63     1.24\n\nsqrt(vif(cars.mod))\n#&gt; cylinder   engine    horse   weight    accel     year \n#&gt;     3.26     4.43     3.07     3.28     1.62     1.12\n\nAccording to \\(\\sqrt{\\text{VIF}}\\), the standard error of cylinder has been multiplied by 3.26 and it’s \\(t\\)-value divided by this number, compared with the case when all predictors are uncorrelated. engine, horse and weight suffer a similar fate.\n\n\n\n\n\n\nTip\n\n\n\nConnection with inverse of correlation matrix\nIn the linear regression model with standardized predictors, the covariance matrix of the estimated intercept-excluding parameter vector \\(\\mathbf{b}^\\star\\) has the simpler form, \\[\n\\mathcal{V} (\\mathbf{b}^\\star) = \\frac{\\sigma^2}{n-1} \\mathbf{R}^{-1}_{X} \\; .\n\\] where \\(\\mathbf{R}_{X}\\) is the correlation matrix among the predictors. It can then be seen that the VIF\\(_j\\) are just the diagonal entries of \\(\\mathbf{R}^{-1}_{X}\\).\nMore generally, the matrix \\(\\mathbf{R}^{-1}_{X} = (r^{ij})\\), when standardized to a correlation matrix as \\(-r^{ij} / \\sqrt{r^{ii} \\; r^{jj}}\\) gives the matrix of all partial correlations, \\(r_{ij \\,|\\, \\textrm{others}}\\). }\n\n\n\n6.2.1 Collinearity diagnostics\nOK, large VIF\\(_j\\) indicate predictor coefficients whose estimation is degraded due to large \\(R^2_{j \\,|\\, \\text{others}}\\). But To go further, we need to determine:\n\nhow many dimensions in the space of the predictors are associated with nearly collinear relations?\nwhich predictors are most strongly implicated in each of these?\n\nAnswsers to these questions are provided using measures developed by Belsley and colleagues (Belsley, Kuh, and Welsch 1980; Belsley:91a?). These measures are based on the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots \\lambda_p\\) of the correlation matrix \\(R_{X}\\) of the predictors (preferably centered and scaled, and not including the constant term for the intercept), and the corresponding eigenvectors in the columns of \\(\\mathbf{V}_{p \\times p}\\), given by the the eigen decomposition \\[\n\\mathbf{R}_{X} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T\n\\] By elementary matrix algebra, the eigen decomposition of \\(\\mathbf{R}_{XX}^{-1}\\) is then \\[\n\\mathbf{R}_{X}^{-1} = \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^T \\; ,\n\\tag{6.1}\\] so, \\(\\mathbf{R}_{X}\\) and \\(\\mathbf{R}_{XX}^{-1}\\) have the same eigenvectors, and the eigenvalues of \\(\\mathbf{R}_{X}^{-1}\\) are just \\(\\lambda_i^{-1}\\). Using Equation 6.1, the variance inflation factors may be expressed as \\[\n\\text{VIF}_j = \\sum_{k=1}^p \\frac{V^2_{jk}}{\\lambda_k} \\; ,\n\\] which shows that only the small eigenvalues contribute to variance inflation, but only for those predictors that have large eigenvector coefficients on those small components. These facts lead to the following diagnostic statistics for collinearity:\n\n\nCondition indices: The smallest of the eigenvalues, those for which \\(\\lambda_j \\approx 0\\), indicate collinearity and the number of small values indicates the number of near collinear relations. Because the sum of the eigenvalues, \\(\\Sigma \\lambda_i = p\\) increases with the number of predictors \\(p\\), it is useful to scale them all in relation to the largest. This leads to condition indices, defined as \\(\\kappa_j = \\sqrt{ \\lambda_1 / \\lambda_j}\\). These have the property that the resulting numbers have common interpretations regardless of the number of predictors.\n\nFor completely uncorrelated predictors, all \\(\\kappa_j = 1\\).\n\n\\(\\kappa_j \\rightarrow \\infty\\) as any \\(\\lambda_k \\rightarrow 0\\).\n\n\nVariance decomposition proportions: Large VIFs indicate variables that are involved in some nearly collinear relations, but they don’t indicate which other variable(s) each is involved with. For this purpose, Belsley et. al. (1980) and Belsley (1991) proposed calculation of the proportions of variance of each variable associated with each principal component as a decomposition of the coefficient variance for each dimension.\n\nThese measures can be calculated using VisCollin::colldiag(). For the current model, the usual display contains both the condition indices and variance proportions. However, even for a small example, it is often difficult to know what numbers to pay attention to.\n\n(cd &lt;- colldiag(cars.mod, center=TRUE))\n#&gt; Condition\n#&gt; Index    Variance Decomposition Proportions\n#&gt;           cylinder engine horse weight accel year \n#&gt; 1   1.000 0.005    0.003  0.005 0.004  0.009 0.010\n#&gt; 2   2.252 0.004    0.002  0.000 0.007  0.022 0.787\n#&gt; 3   2.515 0.004    0.001  0.002 0.010  0.423 0.142\n#&gt; 4   5.660 0.309    0.014  0.306 0.087  0.063 0.005\n#&gt; 5   8.342 0.115    0.000  0.654 0.715  0.469 0.052\n#&gt; 6  10.818 0.563    0.981  0.032 0.176  0.013 0.004\n\n(Belsley:91a?) recommends that the sources of collinearity be diagnosed (a) only for those components with large \\(\\kappa_j\\), and (b) for those components for which the variance proportion is large (say, \\(\\ge 0.5\\)) on two or more predictors. The print method for \"colldiag\" objects has a fuzz argument controlling this.\n\nprint(cd, fuzz = 0.5)\n#&gt; Condition\n#&gt; Index    Variance Decomposition Proportions\n#&gt;           cylinder engine horse weight accel year \n#&gt; 1   1.000  .        .      .     .      .     .   \n#&gt; 2   2.252  .        .      .     .      .    0.787\n#&gt; 3   2.515  .        .      .     .      .     .   \n#&gt; 4   5.660  .        .      .     .      .     .   \n#&gt; 5   8.342  .        .     0.654 0.715   .     .   \n#&gt; 6  10.818 0.563    0.981   .     .      .     .\n\nThe mystery is solved: There are two nearly collinear relations among the predictors, corresponding to the two smallest dimensions.\n\nDimension 5 reflects the high correlation between horsepower and weight,\nDimension 6 reflects the high correlation between number of cylinders and engine displacement.\n\nNote that the high variance proportion for year (0.787) on the second component creates no problem and should be ignored because (a) the condition index is low and (b) it shares nothing with other predictors.\n\n6.2.2 Tableplots\n\n6.2.3 Collinearity biplots"
  },
  {
    "objectID": "collinearity-ridge.html#remedies-for-collinearity",
    "href": "collinearity-ridge.html#remedies-for-collinearity",
    "title": "6  Collinearity & Ridge Regression",
    "section": "\n6.3 Remedies for collinearity",
    "text": "6.3 Remedies for collinearity"
  },
  {
    "objectID": "collinearity-ridge.html#ridge-regression",
    "href": "collinearity-ridge.html#ridge-regression",
    "title": "6  Collinearity & Ridge Regression",
    "section": "\n6.4 Ridge regression",
    "text": "6.4 Ridge regression\n\n6.4.1 What is ridge regression?\n\n6.4.2 Univariate ridge trace plots\n\n6.4.3 Bivariate ridge trace plots\n\n#&gt; Writing packages to  C:/R/Projects/Vis-MLM-quarto/bib/pkgs.txt\n#&gt; 11  packages used here:\n#&gt;  base, car, carData, datasets, genridge, graphics, grDevices, methods, stats, utils, VisCollin"
  },
  {
    "objectID": "collinearity-ridge.html#references",
    "href": "collinearity-ridge.html#references",
    "title": "6  Collinearity & Ridge Regression",
    "section": "References",
    "text": "References\n\n\n\n\nBelsley, D. A., E. Kuh, and R. E. Welsch. 1980. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. New York: John Wiley; Sons.\n\n\nFox, John. 2016. Applied Regression Analysis and Generalized Linear Models. Third edition. Los Angeles: SAGE.\n\n\nLongley, James W. 1967. “An Appraisal of Least Squares Programs for the Electronic Computer from the Point of View of the User.” Journal of the American Statistical Association 62: 819–41. https://doi.org/https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896."
  },
  {
    "objectID": "hotelling.html#t2-as-a-generalized-t-test",
    "href": "hotelling.html#t2-as-a-generalized-t-test",
    "title": "7  Hotelling’s \\(T^2\\)",
    "section": "\n7.1 \\(T^2\\) as a generalized \\(t\\)-test",
    "text": "7.1 \\(T^2\\) as a generalized \\(t\\)-test\nHotelling’s \\(T^2\\) (Hotelling 1931) is an analog the square of a univariate \\(t\\) statistic, extended to the Consider the basic one-sample \\(t\\)-test, where we wish to test the hypothesis that the mean \\(\\bar{x}\\) of a set of \\(N\\) measures on a test of basic math, with standard deviation \\(s\\) does not differ from an assumed mean \\(\\mu_0 = 150\\) for a population. The \\(t\\) statistic for testing \\(H_0 : \\mu = \\mu_0\\) against the two-sided alternative, \\(H_0 : \\mu \\ne \\mu_0\\) is \\[\nt = \\frac{(\\bar{x} - \\mu_0)}{s / \\sqrt{N}} = \\frac{(\\bar{x} - \\mu_0)\\sqrt{N}}{s}\n\\]\nSquaring this gives\n\\[\nt^2 = \\frac{N (\\bar{x} - \\mu_0)^2}{s} = N (\\bar{x} - \\mu_0)(s^2)^{-1} (\\bar{x} - \\mu_0)\n\\]\nNow consider we also have measures on a test of solving word problems for the same sample. Then, a hypothesis test for the means on basic math (BM) and word problems (WP) is the test of the means of these two variables jointly equal their separate values, say, \\((150, 100)\\).\n\\[\nH_0 : \\mathbf{\\mu} = \\mathbf{\\mu_0} =\n  \\begin{pmatrix}\n    \\mu_{0,BM} \\\\ \\mu_{0,WP}\n  \\end{pmatrix}\n  =\n  \\begin{pmatrix}\n    150 \\\\ 100\n  \\end{pmatrix}\n\\]\n\nHotelling’s \\(T^2\\) is then the analog of \\(t^2\\), with the variance-covariance matrix \\(\\mathbf{S}\\) of the scores on (BM, WP) replacing the variance of a single score. This is nothing more than the squared Mahalanobis distance between the sample mean vector \\((\\bar{x}_{BM}, \\bar{x}_{WP})^T\\) and the hypothesized means \\(\\mathbf{\\mu}_0\\), in the metric of \\(\\mathbf{S}\\), as shown in Figure 7.1.\n\\[\\begin{align*}\nT^2 &= N (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)^T \\; \\mathbf{S}^{-1} \\; (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0) \\\\\n    &= N D^2_M (\\bar{\\mathbf{x}}, \\mathbf{\\mu}_0)\n\\end{align*}\\]\n\n\n\n\nFigure 7.1: Hotelling’s T^2 statistic as the squared distance between the sample means and hypothesized means relative to the variance-covariance matrix. Source: Author"
  },
  {
    "objectID": "hotelling.html#t2-properties",
    "href": "hotelling.html#t2-properties",
    "title": "7  Hotelling’s \\(T^2\\)",
    "section": "\n7.2 \\(T^2\\) properties",
    "text": "7.2 \\(T^2\\) properties\nAside from it’s elegant geometric interpretation Hotelling’s \\(T^2\\) has simple properties that aid in understanding the extension to more complex multivariate tests.\n\nMaximum \\(t^2\\) : Consider constructing a new variable \\(w\\) as a linear combination of the scores in a matrix \\(\\mathbf{X} = [ \\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}]\\) with weights \\(\\mathbf{a}\\), \\[\nw = a_1 \\mathbf{x_1} + a_2 \\mathbf{x_2} + \\dots + a_p \\mathbf{x_p} = \\mathbf{X} \\mathbf{a}\n\\] Hotelling’s \\(T^2\\) is then the maximum value of a univariate \\(t^2 (\\mathbf{a})\\) over all possible choices of the weights in \\(\\mathbf{a}\\). In this way, Hotellings test reduces a multivariate problem to a univariate one.\nEigenvalue : Hotelling showed that \\(T^2\\) is the one non-zero eigenvalue (latent root) \\(\\lambda\\) of the matrix \\(\\mathbf{Q}_H = N (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)^T (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)\\) relative to \\(\\mathbf{Q}_E = \\mathbf{S}\\) that solves the equation \\[\n(\\mathbf{Q}_H - \\lambda \\mathbf{Q}_E) \\mathbf{a} = 0\n\\tag{7.1}\\] In more complex MANOVA problems, there are more than one non-zero latent roots, \\(\\lambda_1, \\lambda_2, \\dots \\lambda_s\\), and test statistics are functions of these.\nEigenvector : The corresponding eigenvector is \\(\\mathbf{a} = \\mathbf{S}^{-1} (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)\\). These are the (raw) discriminant coefficients, giving the relative contribution of each variable to \\(T^2\\).\nCritical values : For a single response, the square of a \\(t\\) statistic with \\(N-1\\) degrees of freedom is an \\(F (1, N-1)\\) statistic. But we chose \\(\\mathbf{a}\\) to give the maximum \\(t^2 (\\mathbf{a})\\); this can be taken into account with a transformation of \\(T^2\\) to give an exact \\(F\\) test with the correct sampling distribution: \\[\nF^* = \\frac{N - p}{p (N-1)} T^2 \\; \\sim \\; F (p, N - p)\n\\tag{7.2}\\]\nInvariance under linear transformation : Just as a univariate \\(t\\)-test is unchanged if we apply a linear transformation to the variable, \\(x \\rightarrow a x + b\\), \\(T^2\\) is invariant under all linear (affine) transformations, \\[\n\\mathbf{x}_{p \\times 1} \\rightarrow \\mathbf{C}_{p \\times p} \\mathbf{x} + \\mathbf{b}\n\\] So, you get the same results if you convert penguins flipper lengths from millimeters to centimeters or inches. The same is true for all MANOVA tests.\nTwo-sample tests : With minor variations in notation, everything above applies to the more usual test of equality of multivariate means in a two sample test of \\(H_0 : \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2\\). \\[\nT^2 = N (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)^T \\; \\mathbf{S}_p^{-1} \\; (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)\n\\] where \\(\\mathbf{S}_p\\) is the pooled within-sample variance covariance matrix.\n\nExample\nThe data set heplots::mathscore gives (fictitious) scores on a test of basic math skills (BM) and solving word problems (WP) for two groups of \\(N=6\\) students in an algebra course, each taught by different instructors.\n\ndata(mathscore, package = \"heplots\")\nstr(mathscore)\n#&gt; 'data.frame':    12 obs. of  3 variables:\n#&gt;  $ group: Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 2 2 2 2 ...\n#&gt;  $ BM   : int  190 170 180 200 150 180 160 190 150 160 ...\n#&gt;  $ WP   : int  90 80 80 120 60 70 120 150 90 130 ...\n\nYou can carry out the test that the means for both variables are jointly equal using either Hotelling::hotelling.test() (Curran and Hersh 2021) or car::Anova(),\n\nhotelling.test(cbind(BM, WP) ~ group, data=mathscore) |&gt; print()\n#&gt; Test stat:  64.174 \n#&gt; Numerator df:  2 \n#&gt; Denominator df:  9 \n#&gt; P-value:  0.0001213\n\nmath.mod &lt;- lm(cbind(BM, WP) ~ group, data=mathscore)\nAnova(math.mod)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;       Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; group  1     0.865     28.9      2      9 0.00012 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWhat’s wrong with just doing the two \\(t\\)-tests (or equivalent \\(F\\)-test with lm())?\n\nAnova(mod1 &lt;- lm(BM ~ group, data=mathscore))\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: BM\n#&gt;           Sum Sq Df F value Pr(&gt;F)  \n#&gt; group       1302  1    4.24  0.066 .\n#&gt; Residuals   3071 10                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnova(mod2 &lt;- lm(WP ~ group, data=mathscore))\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: WP\n#&gt;           Sum Sq Df F value Pr(&gt;F)   \n#&gt; group       4408  1    10.4  0.009 **\n#&gt; Residuals   4217 10                  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFrom this, we might conclude that the two groups do not differ significantly on Basic Math but strongly differ on Word problems. But the two univariate tests do not take the correlation among the mean differences into account.\nTo see the differences between the groups on both variables together, we draw their data (68%) ellipses, using heplots::covEllpses()\n\ncolors &lt;- c(\"darkgreen\", \"blue\")\ncovEllipses(cbind(BM, WP) ~ group, data = mathscore,\n            pooled=FALSE, \n            col = colors,\n            fill = TRUE, \n            fill.alpha = 0.05,\n            cex = 2, cex.lab = 1.5,\n            asp = 1,\n            xlab=\"Basic math\", ylab=\"Word problems\")\n# plot points\npch &lt;- ifelse(mathscore$group==1, 15, 16)\ncol &lt;- ifelse(mathscore$group==1, colors[1], colors[2])\npoints(mathscore[,2:3], pch=pch, col=col, cex=1.25)\n\n\n\nFigure 7.2: Data ellipses for the mathscore data, enclosing approximately 68% of the observations in each group\n\n\n\nWe can see that:\n\nGroup 1 &gt; Group 2 on Basic Math, but worse on Word Problems\nGroup 2 &gt; Group 1 on Word Problems, but worse on Basic Math\nWithin each group, those who do better on Basic Math also do better on Word Problems\n\nWe can also see why the univariate test, at least for Basic math is non-significant: the scores for the two groups overlap considerably on the horizontal axis. They are slightly better separated along the vertical axis for word problems. The plot also reveals why Hotelling’s \\(T^2\\) reveals such a strongly significant result: the two groups are very widely separated along an approximately 45\\(^o\\) line between them.\nA relatively simple interpretation is that the groups don’t really differ in overall math ability, but perhaps the instructor in Group 1 put more focus on basic math skills, while the instructor for Group 2 placed greater emphasis on solving word problems.\nIn Hotelling’s \\(T^2\\), the “size” of the difference between the means (labeled “1” and “2”) is assessed relative to the pooled within-group covariance matrix \\(\\mathbf{S}_p\\), which is just a size-weighted average of the two within-sample matrices, \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\),\n\\[\n\\mathbf{S}_p = [ (n_1 - 1) \\mathbf{S}_1 + (n_2 - 1) \\mathbf{S}_2 ] / (n_1 + n_2 - 2)\n\\]\nVisually, imagine sliding the the separate data ellipses to the grand mean, \\((\\bar{x}_{\\text{BM}}, \\bar{x}_{\\text{WP}})\\) and finding their combined data ellipse. This is just the data ellipse of the sample of deviations of the scores from their group means, or that of the residuals from the model lm(cbind(BM, WP) ~ group, data=mathscore)\nTo see this, we plot \\(\\mathbf{S}_1\\), \\(\\mathbf{S}_2\\) and \\(\\mathbf{S}_p\\) together,\n\ncovEllipses(cbind(BM, WP) ~ group, data = mathscore,\n            col = c(colors, \"red\"),\n            fill = c(FALSE, FALSE, TRUE), \n            fill.alpha = 0.3,\n            cex = 2, cex.lab = 1.5,\n            asp = 1,\n            xlab=\"Basic math\", ylab=\"Word problems\")\n\n\n\nFigure 7.3: Data ellipses and the pooled covariance matrix mathscore data.\n\n\n\nOne of the assumptions of the \\(T^2\\) test (and of MANOVA) is that the within-group variance covariance matrices, \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\), are the same. In Figure 7.3, you can see how the shapes of \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\) are very similar, differing in that the variance of word Problems is slightly greater for group 2. In Chapter XX we take of the topic of visualizing tests of this assumption, based on Box’s \\(M\\)-test."
  },
  {
    "objectID": "hotelling.html#he-plot-and-discriminant-axis",
    "href": "hotelling.html#he-plot-and-discriminant-axis",
    "title": "7  Hotelling’s \\(T^2\\)",
    "section": "\n7.3 HE plot and discriminant axis",
    "text": "7.3 HE plot and discriminant axis\nAs we describe in detail in Chapter XX, all the information relevant to the \\(T^2\\) test and MANOVA can be captured in the remarkably simple Hypothesis Error plot, which shows the relative size of two data ellipses,\n\n\n\\(\\mathbf{H}\\): the data ellipse of the fitted values, which are just the group means on the two variables, \\(\\bar{\\mathbf{x}}\\), corresponding to \\(\\mathbf{Q}_H\\) in Equation 7.1. In case of \\(T^2\\), the \\(\\mathbf{H}\\) matrix is of rank 1, so the “ellipse” plots as a line.\n\n\n# calculate H directly\nfit &lt;- fitted(math.mod)\nxbar &lt;- colMeans(mathscore[,2:3])\nN &lt;- nrow(mathscore)\ncrossprod(fit) - N * outer(xbar, xbar)\n#&gt;       BM    WP\n#&gt; BM  1302 -2396\n#&gt; WP -2396  4408\n\n# same as: SSP for group effect from Anova\nmath.aov &lt;- Anova(math.mod)\n(H &lt;- math.aov$SSP)\n#&gt; $group\n#&gt;       BM    WP\n#&gt; BM  1302 -2396\n#&gt; WP -2396  4408\n\n\n\n\\(\\mathbf{E}\\): the data ellipse of the residuals, the deviations of the scores from the group means, \\(\\mathbf{x} - \\bar{\\mathbf{x}}\\), corresponding to \\(\\mathbf{Q}_E\\).\n\n\n# calculate E directly\nresids &lt;- residuals(math.mod)\ncrossprod(resids)\n#&gt;      BM   WP\n#&gt; BM 3071 2808\n#&gt; WP 2808 4217\n\n# same as: SSPE from Anova\n(E &lt;- math.aov$SSPE)\n#&gt;      BM   WP\n#&gt; BM 3071 2808\n#&gt; WP 2808 4217\n\n\n7.3.1 heplot()\n\nheplots::heplot() takes the model object, extracts the \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) matrices (from summary(Anova(math.mod))) and plots them. There are many options to control the details.\n\nheplot(math.mod, \n       fill=TRUE, lwd = 3,\n       asp = 1,\n       cex=2, cex.lab=1.8,\n       xlab=\"Basic math\", ylab=\"Word problems\")\n\n\n\nFigure 7.4: Hypothesis error plot of the mathscore data. The line through the group means is the H ellipse, which plots as a line here. The red ellipse labeled ‘Error’ represents the pooled within-group covariance matrix.\n\n\n\nBut the HE plot offers more:\n\nA visual test of significance: the \\(\\mathbf{H}\\) ellipse is scaled so that it projects anywhere outside the \\(\\mathbf{E}\\) ellipse, if and only if the test is significant at a given \\(\\alpha\\) level (\\(\\alpha = 0.05\\) by default)\nThe \\(\\mathbf{H}\\) ellipse, which appears as a line, goes through the means of the two groups. This is also the discriminant axis, the direction in the space of the variables which maximally discriminates between the groups. That is, if we project the data points onto this line, we get the linear combination \\(w\\) which has the maximum possible univariate \\(t^2\\).\n\nYou can see how the HE plot relates to the plots of the separate data ellipses by overlaying them in a single figure. We also plot the scores on the discriminant axis, by using this small function to find the orthogonal projection of a point \\(\\mathbf{a}\\) on the line joining two points, \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\), which in math is \\(\\mathbf{p}_1 + \\frac{\\mathbf{d}^T (\\mathbf{a} - \\mathbf{p}_1)} {\\mathbf{d}^T \\mathbf{d}}\\), letting \\(\\mathbf{d} = \\mathbf{p}_1 - \\mathbf{p}_2\\).\n\ndot &lt;- function(x, y) sum(x*y)\nproject_on &lt;- function(a, p1, p2) {\n    a &lt;- as.numeric(a)\n    p1 &lt;- as.numeric(p1)\n    p2 &lt;- as.numeric(p2)\n    dot &lt;- function(x,y) sum( x * y)    \n    t &lt;- dot(p2-p1, a-p1) / dot(p2-p1, p2-p1)\n    C &lt;- p1 + t*(p2-p1)\n    C\n}\n\nThen, we run the same code as before to plot the data ellipses, and follow this with a call to heplot() using the option add=TRUE which adds to an existing plot. Following this, we find the group means and draw lines projecting the points on the line between them.\n\ncovEllipses(cbind(BM, WP) ~ group, data = mathscore,\n            pooled=FALSE, \n            col = colors,\n            cex=2, cex.lab=1.5,\n            asp=1, \n            xlab=\"Basic math\", ylab=\"Word problems\"\n            )\npch &lt;- ifelse(mathscore$group==1, 15, 16)\ncol &lt;- ifelse(mathscore$group==1, \"red\", \"blue\")\npoints(mathscore[,2:3], pch=pch, col=col, cex=1.25)\n\n# overlay with HEplot (add = TRUE)\nheplot(math.mod, \n       fill=TRUE, \n       cex=2, cex.lab=1.8, \n       fill.alpha=0.2, lwd=c(1,3),\n       add = TRUE, \n       error.ellipse=TRUE)\n\n# find group means\nmeans &lt;- mathscore |&gt;\n  group_by(group) |&gt;\n  summarize(BM = mean(BM), WP = mean(WP))\n\nfor(i in 1:nrow(mathscore)) {\n    gp &lt;- mathscore$group[i]\n    pt &lt;- project_on( mathscore[i, 2:3], means[1, 2:3], means[2, 2:3]) \n    segments(mathscore[i, \"BM\"], mathscore[i, \"WP\"], pt[1], pt[2], lwd = 1.2)\n}\n\n\n\nFigure 7.5: HE plot overlaid on top of the within-group data ellipses, with lines showing the projection of each point on the discriminant axis."
  },
  {
    "objectID": "hotelling.html#discriminant-analysis",
    "href": "hotelling.html#discriminant-analysis",
    "title": "7  Hotelling’s \\(T^2\\)",
    "section": "\n7.4 Discriminant analysis",
    "text": "7.4 Discriminant analysis\nDiscriminant analysis for two-group designs or for one-way MANOVA essentially turns the problem around: Instead of asking whether the mean vectors for two or more groups are equal, discriminant analysis tries to find the linear combination \\(w\\) of the response variables that has the greatest separation among the groups, allowing cases to be best classified. It was developed by Fisher (1936) as a solution to the biological taxonomy problem of developing a rule to classify instances of flowers—in his famous case, Iris flowers—into known species (I. setosa, I. versicolor, I. virginica) on the basis of multiple measurements (length and width of their sepals and petals).\n\n(math.lda &lt;- MASS::lda(group ~ ., data=mathscore))\n#&gt; Call:\n#&gt; lda(group ~ ., data = mathscore)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;   1   2 \n#&gt; 0.5 0.5 \n#&gt; \n#&gt; Group means:\n#&gt;    BM    WP\n#&gt; 1 178  83.3\n#&gt; 2 158 121.7\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;        LD1\n#&gt; BM -0.0835\n#&gt; WP  0.0753\n\nThe coefficients give \\(w = -0.84 \\text{BM} + 0.75 \\text{WP}\\). This is exactly the direction given by the line for the \\(\\mathbf{H}\\) ellipse in Figure 7.5.\nTo round this out, we can calculate the discriminant scores by multiplying the matrix \\(\\mathbf{X}\\) by the vector \\(\\mathbf{a} = \\mathbf{S}^{-1} (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)\\) of the discriminant weights.\n\nmath.lda$scaling\n#&gt;        LD1\n#&gt; BM -0.0835\n#&gt; WP  0.0753\n\nscores &lt;- cbind(group = mathscore$group,\n                as.matrix(mathscore[, 2:3]) %*% math.lda$scaling) |&gt;\n  as.data.frame()\nscores |&gt;\n  group_by(group) |&gt;\n  slice(1:3)\n#&gt; # A tibble: 6 × 2\n#&gt; # Groups:   group [2]\n#&gt;   group   LD1\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1 -9.09\n#&gt; 2     1 -8.17\n#&gt; 3     1 -9.01\n#&gt; 4     2 -4.33\n#&gt; 5     2 -4.58\n#&gt; 6     2 -5.75\n\nThen a \\(t\\)-test on these scores gives Hotelling’s \\(T\\), accessed via the statistic component of t.test()\n\nt &lt;- t.test(LD1 ~ group, data=scores)$statistic\nc(t, T2 =t^2)\n#&gt;     t  T2.t \n#&gt; -8.01 64.17\n\nFinally, it is instructive to compare violin plots for the three measures, BM, WP and LD1. To do this with ggplot2 requires reshaping the data from wide to long format so the plots can be faceted.\n\nscores &lt;- mathscore |&gt;\n  bind_cols(LD1 = scores[, \"LD1\"]) \n\nscores |&gt;\n  tidyr::gather(key = \"measure\", value =\"Score\", BM:LD1) |&gt;\n  mutate(measure = factor(measure, levels = c(\"BM\", \"WP\", \"LD1\"))) |&gt;\n  ggplot(aes(x = group, y = Score, color = group, fill = group)) +\n    geom_violin(alpha = 0.2) +\n    geom_jitter(width = .2, size = 2) +\n    facet_wrap( ~ measure, scales = \"free\", labeller = label_both) +\n    scale_fill_manual(values = c(\"darkgreen\", \"blue\")) +\n    scale_color_manual(values = c(\"darkgreen\", \"blue\")) +\n    theme_bw(base_size = 14) +\n    theme(legend.position = \"none\")\n\n\n\nFigure 7.6: Violin plots comparing group 1 and 2 for the two observed measures and the linear discriminant score.\n\n\n\nYou can readily see how well the groups are separated on the discriminant axes, relative to the two individual variables."
  },
  {
    "objectID": "hotelling.html#exercises",
    "href": "hotelling.html#exercises",
    "title": "7  Hotelling’s \\(T^2\\)",
    "section": "\n7.5 Exercises",
    "text": "7.5 Exercises\n\nThe value of Hotelling’s \\(T^2\\) found by hotelling.test() is 64.17. The value of the equivalent \\(F\\) statistic found by Anova() is 28.9. Verify that Equation 7.2 gives this result.\n\n\n#&gt; Writing packages to  C:/R/Projects/Vis-MLM-quarto/bib/pkgs.txt\n#&gt; 16  packages used here:\n#&gt;  base, broom, car, carData, corpcor, datasets, dplyr, ggplot2, graphics, grDevices, heplots, Hotelling, methods, stats, tidyr, utils"
  },
  {
    "objectID": "hotelling.html#references",
    "href": "hotelling.html#references",
    "title": "7  Hotelling’s \\(T^2\\)",
    "section": "References",
    "text": "References\n\n\n\n\nCurran, James, and Taylor Hersh. 2021. Hotelling: Hotelling’s t^2 Test and Variants. https://CRAN.R-project.org/package=Hotelling.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nHotelling, Harold. 1931. “The Generalization of Student’s Ratio.” The Annals of Mathematical Statistics 2 (3): 360–78. https://doi.org/10.1214/aoms/1177732979."
  },
  {
    "objectID": "mlm-viz.html#he-plot-framework",
    "href": "mlm-viz.html#he-plot-framework",
    "title": "\n8  Visualizing Multivariate Models\n",
    "section": "\n8.1 HE plot framework",
    "text": "8.1 HE plot framework\nChapter Chapter 7 illustrated the basic ideas of the framework for visualizing multivariate linear models in the context of a simple two group design, using Hotelling’s \\(T^2\\). These are illustrated in Figure 8.1.\n\nIn data space, each group is summarized by its data ellipse, representing the means and covariances.\nVariation against the hypothesis of equal means can be seen by the \\(\\mathbf{H}\\) ellipse in the HE plot, representing the data ellipse of the fitted values. Error variance is shown in the \\(\\mathbf{E}\\) ellipse, representing the pooled within-group covariance matrix, \\(\\mathbf{S}_p\\) and the data ellipse of the residuals from the model.\nThe MANOVA (or Hotelling’s $T^2) is formally equivalent to a discriminant analysis, predicting group membership from the response variables. This effectively projects the \\(p\\)-dimensional space of the predictors into the smaller canonical space that shows the greatest differences among the groups.\n\n\n\n\n\nFigure 8.1: The Hypothesis Error plot framework. Source: author\n\n\n\nFor more complex models such as MANOVA with multiple factors or multivariate multivariate regression, there is one \\(\\mathbf{H}\\) ellipse for each term in the model. …\n\n8.1.1 HE plot details"
  },
  {
    "objectID": "mlm-viz.html#canonical-discriminant-analysis",
    "href": "mlm-viz.html#canonical-discriminant-analysis",
    "title": "\n8  Visualizing Multivariate Models\n",
    "section": "\n8.2 Canonical discriminant analysis",
    "text": "8.2 Canonical discriminant analysis\n\n#&gt; Writing packages to  C:/R/Projects/Vis-MLM-quarto/bib/pkgs.txt\n#&gt; 14  packages used here:\n#&gt;  base, broom, car, carData, datasets, dplyr, ggplot2, graphics, grDevices, heplots, methods, stats, tidyr, utils"
  },
  {
    "objectID": "mlm-review.html#anova---manova",
    "href": "mlm-review.html#anova---manova",
    "title": "9  Brief review of the multivariate linear model",
    "section": "\n9.1 ANOVA -> MANOVA",
    "text": "9.1 ANOVA -&gt; MANOVA"
  },
  {
    "objectID": "mlm-review.html#mra---mmra",
    "href": "mlm-review.html#mra---mmra",
    "title": "9  Brief review of the multivariate linear model",
    "section": "\n9.2 MRA -> MMRA",
    "text": "9.2 MRA -&gt; MMRA"
  },
  {
    "objectID": "mlm-review.html#ancova---mancova",
    "href": "mlm-review.html#ancova---mancova",
    "title": "9  Brief review of the multivariate linear model",
    "section": "\n9.3 ANCOVA -> MANCOVA",
    "text": "9.3 ANCOVA -&gt; MANCOVA"
  },
  {
    "objectID": "mlm-review.html#repeated-measures-designs",
    "href": "mlm-review.html#repeated-measures-designs",
    "title": "9  Brief review of the multivariate linear model",
    "section": "\n9.4 Repeated measures designs",
    "text": "9.4 Repeated measures designs"
  },
  {
    "objectID": "case-studies.html#neuro--and-social-cognitive-measures-in-psychiatric-groups",
    "href": "case-studies.html#neuro--and-social-cognitive-measures-in-psychiatric-groups",
    "title": "\n10  Case studies\n",
    "section": "\n10.1 Neuro- and Social-cognitive measures in psychiatric groups",
    "text": "10.1 Neuro- and Social-cognitive measures in psychiatric groups\nA Ph.D. dissertation by Laura Hartman (2016) at York University was designed to evaluate whether and how clinical patients diagnosed (on the DSM-IV) as schizophrenic or with schizoaffective disorder could be distinguished from each other and from a normal, control sample on collections of standardized tests in the following domains:\n\nNeuro-cognitive: processing speed, attention, verbal learning, visual learning and problem solving;\nSocial-cognitive: managing emotions, theory of mind, externalizing, personalizing bias.\n\nThe study is an important contribution to clinical research because the two diagnostic categories are subtly different and their symptoms often overlap. Yet, they’re very different and often require different treatments. A key difference between schizoaffective disorder and schizophrenia is the prominence of mood disorder involving bipolar, manic and depressive moods. With schizoaffective disorder, mood disorders are front and center. With schizophrenia, that is not a dominant part of the disorder, but psychotic ideation (hearing voices, seeing imaginary people) is.\n\n10.1.1 Research questions\nThis example is concerned with the following substantitive questions:\n\nTo what extent can patients diagnosed as schizophrenic or with schizoaffective disorder be distinguished from each other and from a normal control sample using a well-validated, comprehensive neurocognitive battery specifically designed for individuals with psychosis (Heinrichs et al. 2015) ?\nIf the groups differ, do any of the cognitive domains show particularly larger or smaller differences among these groups?\nDo the neurocognitive measures discriminate among the in the same or different ways? If different, how many separate aspects or dimensions are distinguished?\n\nApart from the research interest, it could aid diagnosis and treatment if these similar mental disorders could be distinguished by tests in the cognitive domain.\n\n10.1.2 Data\nThe clinical sample comprised 116 male and female patients who met the following criteria: 1) a diagnosis of schizophrenia (\\(n\\) = 70) or schizoaffective disorder (\\(n\\) = 46) confirmed by the Structured Clinical Interview for DSM-IV-TR Axis I Disorders; 2) were outpatients; 3) a history free of developmental or learning disability; 4) age 18-65; 5) a history free of neurological or endocrine disorder; and 6) no concurrent diagnosis of substance use disorder. Non-psychiatric control participants (\\(n\\) = 146) were screened for medical and psychiatric illness and history of substance abuse and were recruited from three outpatient clinics.\n\ndata(NeuroCog, package=\"heplots\")\nglimpse(NeuroCog)\n#&gt; Rows: 242\n#&gt; Columns: 10\n#&gt; $ Dx        &lt;fct&gt; Schizophrenia, Schizophrenia, Schizophrenia, Sch…\n#&gt; $ Speed     &lt;int&gt; 19, 8, 14, 7, 21, 31, -1, 17, 7, 37, 30, 26, 32,…\n#&gt; $ Attention &lt;int&gt; 9, 25, 23, 18, 9, 10, 8, 20, 30, 15, 27, 20, 23,…\n#&gt; $ Memory    &lt;int&gt; 19, 15, 15, 14, 35, 26, 3, 27, 26, 17, 28, 22, 2…\n#&gt; $ Verbal    &lt;int&gt; 33, 28, 20, 34, 28, 29, 20, 30, 26, 33, 34, 33, …\n#&gt; $ Visual    &lt;int&gt; 24, 24, 13, 16, 29, 21, 12, 32, 27, 21, 19, 18, …\n#&gt; $ ProbSolv  &lt;int&gt; 39, 40, 32, 31, 45, 33, 29, 29, 30, 33, 30, 39, …\n#&gt; $ SocialCog &lt;int&gt; 28, 37, 24, 36, 28, 28, 28, 44, 39, 24, 32, 36, …\n#&gt; $ Age       &lt;int&gt; 44, 26, 55, 53, 51, 21, 53, 56, 48, 46, 48, 31, …\n#&gt; $ Sex       &lt;fct&gt; Female, Male, Female, Male, Male, Male, Male, Fe…\n\nThe diagnostic classification variable is called Dx in the data set. To facilitate answering questions regarding group differences, the following contrasts were applied: the first column compares the control group to the average of the diagnosed groups, the second compares the schizophrenia group against the schizoaffective group.\n\ncontrasts(NeuroCog$Dx)\n#&gt;                 [,1] [,2]\n#&gt; Schizophrenia   -0.5    1\n#&gt; Schizoaffective -0.5   -1\n#&gt; Control          1.0    0\n\nIn this analysis, we ignore the SocialCog variable. The primary focus is on the variables Attention : ProbSolv.\n\n10.1.3 A first look\nAs always, plot the data first! We want an overview of the distributions of the variables to see the centers, spread, shape and possible outliers for each group on each variable.\nThe plot below combines the use of boxplots and violin plots to give an informative display. As we saw earlier (Chapter XXX), doing this with ggplot2 requires reshaping the data to long format.\n\n# Reshape from wide to long\nNC_long &lt;- NeuroCog |&gt;\n  dplyr::select(-SocialCog, -Age, -Sex) |&gt;\n  tidyr::gather(key = response, value = \"value\", Speed:ProbSolv)\n# view 3 observations per group and measure\nNC_long |&gt;\n  group_by(Dx) |&gt;\n  sample_n(3) |&gt; ungroup()\n#&gt; # A tibble: 9 × 3\n#&gt;   Dx              response  value\n#&gt;   &lt;fct&gt;           &lt;chr&gt;     &lt;int&gt;\n#&gt; 1 Schizophrenia   Speed        39\n#&gt; 2 Schizophrenia   Visual       21\n#&gt; 3 Schizophrenia   Memory       40\n#&gt; 4 Schizoaffective ProbSolv     40\n#&gt; 5 Schizoaffective Speed        25\n#&gt; 6 Schizoaffective Verbal       48\n#&gt; 7 Control         Speed        33\n#&gt; 8 Control         ProbSolv     43\n#&gt; 9 Control         Attention    37\n\nIn the plot, we take care to adjust the transparency (alpha) values for the points, violin plots and boxplots so that all can be seen. Options for geom_boxplot() are used to give these greater visual prominence.\n\nCodeggplot(NC_long, aes(x=Dx, y=value, fill=Dx)) +\n  geom_jitter(shape=16, alpha=0.8, size=1, width=0.2) +\n  geom_violin(alpha = 0.1) +\n  geom_boxplot(width=0.5, alpha=0.4, \n               outlier.alpha=1, outlier.size = 3, outlier.color = \"red\") +\n  scale_x_discrete(labels = c(\"Schizo\", \"SchizAff\", \"Control\")) +\n  facet_wrap(~response, scales = \"free_y\", as.table = FALSE) +\n  theme_bw() +\n  theme(legend.position=\"bottom\",\n        axis.title = element_text(size = rel(1.2)),\n        axis.text  = element_text(face = \"bold\"),\n        strip.text = element_text(size = rel(1.2)))\n\n\n\nFigure 10.1: Boxplots and violin plots of the NeuroCog data.\n\n\n\nWe can see that the control participants score higher on all measures, but there is no consistent pattern of medians for the two patient groups. But these univariate summaries do not inform about the relations among variables.\n\n10.1.4 Bivariate views\nCorrgram\nA corrgram (Friendly 2002) provides a useful reconnaisance plot of the bivariate correlations in the data set. It suppresses details, and allows focus on the overall pattern. The corrgram::corrgram() function has the ability to enhance perception by permuting the variables in the order of their variable vectors in a biplot, so more highly correlated variables are adjacent in the plot, and example of effect ordering for data displays (Friendly and Kwan 2003).\nThe plot below includes all variables except for Dx group. There are a number of panel.* functions for choosing how the correlation for each pair is rendered.\n\nNeuroCog |&gt;\n  select(-Dx) |&gt;\n  corrgram(order = TRUE,\n           diag.panel = panel.density,\n           upper.panel = panel.pie)\n\n\n\nFigure 10.2: corrgram of the NeuroCog data. The upper and lower triangles use two different ways of encoding the value of the correlation for each pair of variables.\n\n\n\nIn this plot you can see that adjacent variables are more highly correlated than those more widely separated. The diagonal panels show that most variables are reasonably symmetric in their distributions. Age, not included in this analysis is negatively correlated with the others: older participants tend to do less well on these tests.\nScatterplot matrix\nA scatterplot matrix gives a more detailed overview of all pairwise relations. The plot below suppresses the data points and summarizes the relation using data ellipses and regression lines. The model syntax, ~ Speed + ... |Dx, treats Dx as a conditioning variable (similar to the use of the color aestheic in ggplot2) giving a separate data ellipse and regression line for each group. (The legend is suppressed here. The groups are Schizophrenic, SchizoAffective, Normal.)\n\nscatterplotMatrix(~ Speed + Attention + Memory + Verbal + Visual + ProbSolv | Dx,\n  data=NeuroCog,\n  plot.points = FALSE,\n  smooth = FALSE,\n  legend = FALSE,\n  col = scales::hue_pal()(3),\n  ellipse=list(levels=0.68))\n\n\n\nFigure 10.3: Scatterplot matrix of the NeuroCog data. Points are suppressed here, focusing on the data ellipses and regression lines. Colors for the groups: Schizophrenic (red), SchizoAffective (green), Normal (blue)\n\n\n\nIn this figure, we can see that the regression lines have similar slopes and similar data ellipses for the groups, though with a few exceptions.\nTODO: Should we add biplot here?"
  },
  {
    "objectID": "case-studies.html#fitting-the-mlm",
    "href": "case-studies.html#fitting-the-mlm",
    "title": "\n10  Case studies\n",
    "section": "\n10.2 Fitting the MLM",
    "text": "10.2 Fitting the MLM\nWe proceed to fit the one-way MANOVA model.\n\nNC.mlm &lt;- lm(cbind(Speed, Attention, Memory, Verbal, Visual, ProbSolv) ~ Dx,\n             data=NeuroCog)\nAnova(NC.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Dx  2     0.299     6.89     12    470 1.6e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe first research question is captured by the contrasts for the Dx factor shown above. We can test these with car::linearHypothesis(). The contrast Dx1 for control vs. the diagnosed groups is highly significant,\n\n# control vs. patients\nprint(linearHypothesis(NC.mlm, \"Dx1\"), SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            1     0.289     15.9      6    234 2.8e-15 ***\n#&gt; Wilks             1     0.711     15.9      6    234 2.8e-15 ***\n#&gt; Hotelling-Lawley  1     0.407     15.9      6    234 2.8e-15 ***\n#&gt; Roy               1     0.407     15.9      6    234 2.8e-15 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nbut the second contrast, Dx2, comparing the schizophrenic and schizoaffective group, is not.\n\n# Schizo vs SchizAff\nprint(linearHypothesis(NC.mlm, \"Dx2\"), SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)\n#&gt; Pillai            1     0.006    0.249      6    234   0.96\n#&gt; Wilks             1     0.994    0.249      6    234   0.96\n#&gt; Hotelling-Lawley  1     0.006    0.249      6    234   0.96\n#&gt; Roy               1     0.006    0.249      6    234   0.96\n\n\n10.2.1 HE plot\nSo the question becomes: how to understand these results.heplot() shows the visualization of the multivariate model in the space of two response variables (the first two by default). The result (Figure 10.4) tells a very simple story: The control group performs higher on higher measures than the diagnosed groups, which do not differ between themselves.\n(For technical reasons, to abbreviate the group labels in the plot, we need to update() the MLM model after the labels are reassigned.)\n\n# abbreviate levels for plots\nNeuroCog$Dx &lt;- factor(NeuroCog$Dx, \n                      labels = c(\"Schiz\", \"SchAff\", \"Contr\"))\nNC.mlm &lt;- update(NC.mlm)\n\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\nheplot(NC.mlm, \n       fill=TRUE, fill.alpha=0.1,\n       cex.lab=1.3, cex=1.25)\npar(op)\n\n\n\nFigure 10.4: HE plot of Speed and Attention in the MLM for the NeuroCog data. The labeled points show the means of the groups on the two variables. The blue H ellipse for groups indicates the strong positive correlation of the group means.\n\n\n\nThis pattern is consistent across all of the response variables, as we see from a plot of pairs(NC.mlm):\n\npairs(NC.mlm, \n      fill=TRUE, fill.alpha=0.1,\n      var.cex=2)\n\n\n\nFigure 10.5: HE plot matrix of the MLM for NeuroCog data.\n\n\n\n\n10.2.2 Canonical space\nWe can gain further insight, and a simplified plot showing all the response variables by projecting the MANOVA into the canonical space, which is entirely 2-dimensional (because \\(df_h=2\\)). However, the output from shows that 98.5% of the mean differences among groups can be accounted for in one canonical dimension. ::: {.cell layout-align=“center”}\nNC.can &lt;- candisc(NC.mlm)\nNC.can\n#&gt; \n#&gt; Canonical Discriminant Analysis for Dx:\n#&gt; \n#&gt;    CanRsq Eigenvalue Difference Percent Cumulative\n#&gt; 1 0.29295    0.41433      0.408    98.5       98.5\n#&gt; 2 0.00625    0.00629      0.408     1.5      100.0\n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;   LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1        0.703     7.53    12   468   9e-13 ***\n#&gt; 2        0.994     0.30     5   235    0.91    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n:::\nFigure 10.6 is the result of the plot() method for class \"candisc\" objects, that is, the result of calling plot(NC.can, ...). It plots the two canonical scores, \\(\\mathbf{Z}_{n \\times 2}\\) for the subjects, together with data ellipses for each of the three groups.\n\npos &lt;- c(4, 1, 4, 4, 1, 3)\ncol &lt;- c(\"red\", \"darkgreen\", \"blue\")\nop &lt;- par(mar=c(5,4,1,1)+.1)\nplot(NC.can, \n     ellipse=TRUE, \n     rev.axes=c(TRUE,FALSE), \n     pch=c(7,9,10),\n     var.cex=1.2, cex.lab=1.5, var.lwd=2,  scale=4.5, \n     col=col,\n       var.col=\"black\", var.pos=pos,\n     prefix=\"Canonical dimension \")\npar(op)\n\n\n\nFigure 10.6: Canonical discriminant plot for the NeuroCog data MANOVA. Scores on the two canonical dimensions are plotted, together with 68% data ellipses for each group.\n\n\n\nThe interpretation of Figure 10.6 is again fairly straightforward. As noted earlier (REF???), the projections of the variable vectors in this plot on the coordinate axes are proportional to the correlations of the responses with the canonical scores. From this, we see that the normal group differs from the two patient groups, having higher scores on all the neurocognitive variables, most of which are highyl correlated. The problem solving measure is slightly different, and this, compared to the cluster of memory, verbal and attention, is what distinguishes the schizophrenic group from the schizoaffectives.\nThe separation of the groups is essentially one-dimensional, with the control group higher on all measures. Moreover, the variables processing speed and visual memory are the purest measures of this dimension, but all variables contribute positively. The second canonical dimension accounts for only 1.5% of group mean differences and is non-significant (by a likelihood ratio test). Yet, if we were to interpret it, we would note that the schizophrenia group is slightly higher on this dimension, scoring better in problem solving and slightly worse on working memory, attention, and verbal learning tasks.\nSummary\nThis analysis gives a very simple description of the data, in relation to the research questions posed earlier:\n\nOn the basis of these neurocognitive tests, the schizophrenic and schizoaffective groups do not differ significantly overall, but these groups differ greatly from the normal controls.\nAll cognitive domains distinguish the groups in the same direction, with the greatest differences shown for the variables most closely aligned with the horizontal axis in Figure 10.6."
  },
  {
    "objectID": "case-studies.html#social-cognitive-measures",
    "href": "case-studies.html#social-cognitive-measures",
    "title": "\n10  Case studies\n",
    "section": "\n10.3 Social cognitive measures",
    "text": "10.3 Social cognitive measures\nThe social cognitive measures were designed to tap various aspects of the perception and cognitive processing of emotions of others. Emotion perception was assessed using a Managing Emotions score from the MCCB. A “theory of mind” (ToM) score assessed ability to read the emotions of others from photographs of the eye region of male and female faces. Two other measures, externalizing bias (ExtBias) and personalizing bias (PersBias) were calculated from a scale measuring the degree to which individuals attribute internal, personal or situational causal attributions to positive and negative social events.\nThe analysis of the SocialCog data proceeds in a similar way: first we fit the MANOVA model, then test the overall differences among groups using Anova(). We find that the overall multivariate test is again significant,\n\ndata(SocialCog, package=\"heplots\")\nSC.mlm &lt;-  lm(cbind(MgeEmotions,ToM, ExtBias, PersBias) ~ Dx,\n               data=SocialCog)\nAnova(SC.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Dx  2     0.212     3.97      8    268 0.00018 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTesting the same two contrasts using linearHypothesis() (results not shown), w e find that the overall multivariate test is again significant, but now both contrasts are significant (Dx1: \\(F(4, 133)=5.21, p &lt; 0.001\\); Dx2: \\(F(4, 133)=2.49, p = 0.0461\\)), the test for Dx2 just barely.\n\n# control vs. patients\nprint(linearHypothesis(SC.mlm, \"Dx1\"), SSP=FALSE)\n# Schizo vs. SchizAff\nprint(linearHypothesis(SC.mlm, \"Dx2\"), SSP=FALSE)\n\nThese results are important, because, if they are reliable and make sense substantively, they imply that patients with schizophrenia and schizoaffective diagnoses can be distinguished by their performance on tasks assessing social perception and cognition. This was potentially a new finding in the literature on schizophrenia.\nAs we did above, it is useful to visualize the nature of these differences among groups with HE plots for the SC.mlm model. Each contrast has a corresponding \\(\\mathbf{H}\\) ellipse, which we can show in the plot using the hypotheses argument. With a single degree of freedom, these degenerate ellipses plot as lines.\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\nheplot(SC.mlm, \n       hypotheses=list(\"Dx1\"=\"Dx1\", \"Dx2\"=\"Dx2\"),\n       fill=TRUE, fill.alpha=.1,\n       cex.lab=1.5, cex=1.2)\npar(op)\n\n\n\nFigure 10.7: HE plot of Speed and Attention in the MLM for the SocialCog data. The labeled points show the means of the groups on the two variables. The lines for Dx1 and Dx2 show the tests of the contrasts among groups.\n\n\n\nIt can be seen that the three group means are approximately equally spaced on the ToM measure, whereas for MgeEmotions, the control and schizoaffective groups are quite similar, and both are higher than the schizophrenic group. This ordering of the three groups was somewhat similar for the other responses, as we could see in a pairs(SC.mlm) plot.\n\n10.3.1 Model checking\nNormally, we would continue this analysis, and consider other HE and canonical discriminant plots to further interpret the results, in particular the relations of the cognitive measures to group differences, or perhaps an analysis of the relationships between the neuro- and social-cognitive measures. We don’t pursue this here for reasons of length, but this example actually has a more important lesson to demonstrate.\nBefore beginning the MANOVA analyses, extensive data screening was done by the client using SPSS, in which all the response and predictor variables were checked for univariate normality and multivariate normality (MVN) for both sets. This traditional approach yielded a huge amount of tabular output and no graphs, and did not indicate any major violation of assumptions.1\nA simple visual test of MVN and the possible presence of multivariate outliers is related to the theory of the data ellipse: Under MVN, the squared Mahalanobis distances \\(D^2_M (\\mathbf{y}) = (\\mathbf{y} - \\bar{\\mathbf{y}})' \\, \\mathbf{S}^{-1} \\, (\\mathbf{y} - \\bar{\\mathbf{y}})\\) should follow a \\(\\chi^2_p\\) distribution. Thus, a quantile-quantile plot of the ordered \\(D^2_M\\) values vs. corresponding quantiles of the \\(\\chi^2\\) distribution should approximate a straight line (Cox 1968; Healy 1968). Note that this should be applied to the residuals from the model – residuals(SC.mlm) – and not to the response variables directly.\nheplots::cqplot() implements this for \"mlm\" objects Calling this function for the model SC.mlm produces . It is immediately apparent that there is one extreme multivariate outlier; three other points are identified, but the remaining observations are nearly within the 95% confidence envelope (using a robust MVE estimate of \\(\\mathbf{S}\\)).\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\ncqplot(SC.mlm, method=\"mve\", \n       id.n=4, \n       main=\"\", \n       cex.lab=1.25)\npar(op)\n\n\n\nFigure 10.8: Chi-square quantile-quantile plot for residuals from the model SC.mlm. The confidence band gives a point-wise 95% envelope, providing information about uncertainty. One extreme multivariate outlier is highlighted.\n\n\n\nFurther checking revealed that this was a data entry error where one case (15) in the schizophrenia group had a score of -33 recorded on the ExtBias measure, whose valid range was (-10, +10). In R, it is very easy to re-fit a model to a subset of observations (rather than modifying the data set itself) using update(). The result of the overall Anova and the test of Dx1 were unchanged; however, the multivariate test for the most interesting contrast Dx2 comparing the schizophrenia and schizoaffective groups became non-significant at the \\(\\alpha=0.05\\) level (\\(F(4, 133)=2.18, p = 0.0742\\)).\n\nSC.mlm1 &lt;- update(SC.mlm, \n                  subset=rownames(SocialCog)!=\"15\")\n\nAnova(SC.mlm1)\nprint(linearHypothesis(SC.mlm1, \"Dx1\"), SSP=FALSE)\nprint(linearHypothesis(SC.mlm1, \"Dx2\"), SSP=FALSE)\n\n\n10.3.2 Canonical HE plot\nThis outcome creates a bit of a quandry for further analysis (do univariate follow-up tests? try a robust model?) and reporting (what to claim about the Dx2 contrast?) that we don’t explore here. Rather, we proceed to attempt to interpret the MLM with the aid of canonical analysis and a canonical HE plot. The canonical analysis of the model SC.mlm1 now shows that both canonical dimensions are significant, and account for 83.9% and 16.1% of between group mean differences respectively.\n\nSC.can1 &lt;- candisc(SC.mlm1)\nSC.can1\n#&gt; \n#&gt; Canonical Discriminant Analysis for Dx:\n#&gt; \n#&gt;   CanRsq Eigenvalue Difference Percent Cumulative\n#&gt; 1 0.1645     0.1969      0.159    83.9       83.9\n#&gt; 2 0.0364     0.0378      0.159    16.1      100.0\n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;   LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1        0.805     3.78     8   264 0.00032 ***\n#&gt; 2        0.964     1.68     3   133 0.17537    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\nheplot(SC.can1, \n    fill=TRUE, fill.alpha=.1,\n    hypotheses=list(\"Dx1\"=\"Dx1\", \"Dx2\"=\"Dx2\"),\n    lwd = c(1, 2, 3, 3),\n    col=c(\"red\", \"blue\", \"darkgreen\", \"darkgreen\"),\n    var.lwd=2, \n    var.col=\"black\", \n    label.pos=c(3,1), \n    var.cex=1.2, \n    cex=1.25, cex.lab=1.2, \n    scale=2.8,\n    prefix=\"Canonical dimension \")\npar(op)\n\n\n\nFigure 10.9: Canonical HE plot for the corrected SocialCog MANOVA. The variable vectors show the correlations of the responses with the canonical variables. The embedded green lines show the projections of the H ellipses for the contrasts Dx1 and Dx2 in canonical space.\n\n\n\nThe HE plot version of this canonical plot is shown in Figure 10.9. Because the heplot() method for a \"candisc\" object refits the original model to the \\(\\mathbf{Z}\\) canonical scores, it is easy to also project other linear hypotheses into this space. Note that in this view, both the Dx1 and Dx2 contrasts project outside \\(\\mathbf{E}\\) ellipse.2.\nThis canonical HE plot has a very simple description:\n\nDimension 1 orders the groups from control to schizoaffective to schizophrenia, while dimension 2 separates the schizoaffective group from the others;\nExternalizing bias and theory of mind contributes most to the first dimension, while personal bias and managing emotions are more aligned with the second; and,\nThe relations of the two contrasts to group differences and to the response variables can be easily read from this plot.\n\n\n#cat(\"Packages used here:\\n\")\nwrite_pkgs(file = .pkg_file)\n#&gt; 16  packages used here:\n#&gt;  base, broom, candisc, car, carData, corrgram, datasets, dplyr, ggplot2, graphics, grDevices, heplots, methods, stats, tidyr, utils"
  },
  {
    "objectID": "case-studies.html#references",
    "href": "case-studies.html#references",
    "title": "\n10  Case studies\n",
    "section": "References",
    "text": "References\n\n\n\n\nCox, D. R. 1968. “Notes on Some Aspects of Regression Analysis.” Journal of the Royal Statistical Society Series A 131: 265–79.\n\n\nFriendly, Michael. 2002. “Corrgrams: Exploratory Displays for Correlation Matrices.” The American Statistician 56 (4): 316–24. http://datavis.ca/papers/corrgram.pdf.\n\n\nFriendly, Michael, and E. Kwan. 2003. “Effect Ordering for Data Displays.” Computational Statistics and Data Analysis 43 (4): 509–39. http://authors.elsevier.com/sd/article/S0167947302002906.\n\n\nHartman, L. I. 2016. “Schizophrenia and Schizoaffective Disorder: One Condition or Two?” PhD dissertation, York University.\n\n\nHealy, M. J. R. 1968. “Multivariate Normal Plotting.” Journal of the Royal Statistical Society Series C 17 (2): 157–61.\n\n\nHeinrichs, R. Walter, Farena Pinnock, Eva Muharib, Leah Hartman, Joel Goldberg, and Stephanie McDermid Vaz. 2015. “Neurocognitive Normality in Schizophrenia Revisited.” Schizophrenia Research: Cognition 2 (4): 227–32. https://doi.org/10.1016/j.scog.2015.09.001.\n\n\nMardia, K. V. 1970. “Measures of Multivariate Skewness and Kurtosis with Applications.” Biometrika 57 (3): 519–30. https://doi.org/http://dx.doi.org/10.2307/2334770.\n\n\n———. 1974. “Applications of Some Measures of Multivariate Skewness and Kurtosis in Testing Normality and Robustness Studies.” Sankhya: The Indian Journal of Statistics, Series B 36 (2): 115–28. http://www.jstor.org/stable/25051892."
  },
  {
    "objectID": "case-studies.html#footnotes",
    "href": "case-studies.html#footnotes",
    "title": "\n10  Case studies\n",
    "section": "",
    "text": "Actually, multivariate normality of the predictors in \\(\\mathbf{X}\\) is not required in the MLM. This assumption applies only to the conditional values \\(\\mathbf{Y} \\;|\\; \\mathbf{X}\\), i.e., that the errors \\(\\mathbf{u}_{i}' \\sim \\mathcal{N}_{p}(\\mathbf{0},\\boldsymbol{\\Sigma})\\) with constant covariance matrix. Moreover, the widely used MVN test statistics, such as Mardia’s (1970) test based on multivariate skewness and kurtosis are known to be quite sensitive to mild departures in kurtosis (Mardia 1974) which do not threaten the validity of the multivariate tests.↩︎\nThe direct application of significance tests to canonical scores probably requires some adjustment because these are computed to have the optimal between-group discrimination.↩︎"
  },
  {
    "objectID": "eqcov.html#homogeneity-of-variance-in-univariate-anova",
    "href": "eqcov.html#homogeneity-of-variance-in-univariate-anova",
    "title": "\n11  Visualizing Tests for Equality of Covariance Matrices\n",
    "section": "\n11.1 Homogeneity of Variance in Univariate ANOVA",
    "text": "11.1 Homogeneity of Variance in Univariate ANOVA\nIn classical (Gaussian) univariate ANOVA models, the main interest is typically on tests of mean differences in a response \\(y\\) according to one or more factors. The validity of the typical \\(F\\) test, however, relies on the assumption of homogeneity of variance: all groups have the same (or similar) variance, \\[\n\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_g^2 \\; .\n\\]\nIt turns out that the \\(F\\) test for differences in means is relatively robust to violation of this assumption (Harwell et al. 1992), as long as the group sizes are roughly equal.1\nA variety of classical test statistics for homogeneity of variance are available, including Hartley’s \\(F_{max}\\) (Hartley 1950), Cochran’s C (Cochran 1941),and Bartlett’s test (Bartlett 1937), but these have been found to have terrible statistical properties (Rogan and Keselman 1977), which prompted Box’s famous quote.\nLevene (1960) introduced a different form of test, based on the simple idea that when variances are equal across groups, the average absolute values of differences between the observations and group means will also be equal, i.e., substituting an \\(L_1\\) norm for the \\(L_2\\) norm of variance. In a one-way design, this is equivalent to a test of group differences in the means of the auxilliary variable \\(z_{ij} = | y_{ij} - \\bar{y}_i |\\).\nMore robust versions of this test were proposed by Brown and Forsythe (1974). These tests substitute the group mean by either the group median or a trimmed mean in the ANOVA of the absolute deviations, and should be almost always preferred to Levene’s version. See Conover, Johnson, and Johnson (1981) for an early review and Gastwirth, Gel, and Miao (2009) for a general discussion of these tests. In what follows, we refer to this class of tests as “Levene-type” tests and suggest a multivariate extension described below (?sec-mlevene)."
  },
  {
    "objectID": "eqcov.html#homogeneity-of-variance-in-manova",
    "href": "eqcov.html#homogeneity-of-variance-in-manova",
    "title": "\n11  Visualizing Tests for Equality of Covariance Matrices\n",
    "section": "\n11.2 Homogeneity of variance in MANOVA",
    "text": "11.2 Homogeneity of variance in MANOVA\nIn the MANOVA context, the main emphasis, of course, is on differences among mean vectors, testing \\[\nH_0 : \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2 = \\cdots = \\mathbf{\\mu}_g \\; .\n\\] However, the standard test statistics (Wilks’ Lambda, Hotelling-Lawley trace, Pillai-Bartlett trace, Roy’s maximum root) rely upon the analogous assumption that the within-group covariance matrices for all groups are equal, \\[\n\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\cdots = \\mathbf{\\Sigma}_g \\; .\n\\]\nInsert pairs covEllipses for penguins data\nTo preview the main example, Figure 11.1 shows data ellipses for the main size variables in the palmerpenguins::penguins data.\n\n\n\n\nFigure 11.1: Data ellipses for the penguins data.\n\n\n\nThey covariance ellipses look pretty similar in size, shape and orientation. But what does Box’s M test (described below) say? As you can see, it concludes strongly against the null hypothesis.\n\nboxM(cbind(bill_length, bill_depth, flipper_length, body_mass) ~ species, data=peng)\n#&gt; \n#&gt;  Box's M-test for Homogeneity of Covariance Matrices\n#&gt; \n#&gt; data:  Y\n#&gt; Chi-Sq (approx.) = 75, df = 20, p-value = 3e-08"
  },
  {
    "objectID": "eqcov.html#assessing-heterogeneity-of-covariance-matrices-boxs-m-test",
    "href": "eqcov.html#assessing-heterogeneity-of-covariance-matrices-boxs-m-test",
    "title": "\n11  Visualizing Tests for Equality of Covariance Matrices\n",
    "section": "\n11.3 Assessing heterogeneity of covariance matrices: Box’s M test",
    "text": "11.3 Assessing heterogeneity of covariance matrices: Box’s M test\nBox (1949) proposed the following likelihood-ratio test (LRT) statistic for testing the hypothesis of equal covariance matrices, \\[\nM = (N -g) \\ln \\;|\\; \\mathbf{S}_p \\;|\\; - \\sum_{i=1}^g (n_i -1) \\ln \\;|\\; \\mathbf{S}_i \\;|\\; \\; ,\n\\] {eq-boxm}\nwhere \\(N = \\sum n_i\\) is the total sample size and \\(\\mathbf{S}_p = (N-g)^{-1} \\sum_{i=1}^g (n_i - 1) \\mathbf{S}_i\\) is the pooled covariance matrix. \\(M\\) can thus be thought of as a ratio of the determinant of the pooled \\(\\mathbf{S}_p\\) to the geometric mean of the determinants of the separate \\(\\mathbf{S}_i\\).\nIn practice, there are various transformations of the value of \\(M\\) to yield a test statistic with an approximately known distribution (Timm 1975). Roughly speaking, when each \\(n_i &gt; 20\\), a \\(\\chi^2\\) approximation is often used; otherwise an \\(F\\) approximation is known to be more accurate.\nAsymptotically, \\(-2 \\ln (M)\\) has a \\(\\chi^2\\) distribution. The \\(\\chi^2\\) approximation due to Box (1949, 1950) is that \\[\nX^2 = -2 (1-c_1) \\ln (M) \\quad \\sim \\quad \\chi^2_{df}\n\\] with \\(df = (g-1) p (p+1)/2\\) degrees of freedom, and a bias correction constant: \\[\nc_1 = \\left(\n\\sum_i \\frac{1}{n_i -1}\n- \\frac{1}{N-g}\n\\right)\n\\frac{2p^2 +3p -1}{6 (p+1)(g-1)} \\; .\n\\]\nIn this form, Bartlett’s test for equality of variances in the univariate case is the special case of Box’s M when there is only one response variable, so Bartlett’s test is sometimes used as univariate follow-up to determine which response variables show heterogeneity of variance.\nYet, like its univariate counterpart, Box’s test is well-known to be highly sensitive to violation of (multivariate) normality and the presence of outliers. For example, Tiku and Balakrishnan (1984) concluded from simulation studies that the normal-theory LRT provides poor control of Type I error under even modest departures from normality. O’Brien (1992) proposed some robust alternatives, and showed that Box’s normal theory approximation suffered both in controlling the null size of the test and in power. Zhang and Boos (1992) also carried out simulation studies with similar conclusions and used bootstrap methods to obtain corrected critical values."
  },
  {
    "objectID": "eqcov.html#visualizing-heterogeneity",
    "href": "eqcov.html#visualizing-heterogeneity",
    "title": "\n11  Visualizing Tests for Equality of Covariance Matrices\n",
    "section": "\n11.4 Visualizing heterogeneity",
    "text": "11.4 Visualizing heterogeneity\nThe goal of this chapter is to use the above background as a platform for discussing approaches to visualizing and testing the heterogeneity of covariance matrices in multivariate designs. While researchers often rely on a single number to determine if their data have met a particular threshold, such compression will often obscure interesting information, particularly when a test concludes that differences exist, and one is left to wonder ``why?’’. It is within this context where, again, visualizations often reign supreme. In fact, we find it somewhat surprising that this issue has not been addressed before graphically in any systematic way. TODO: cut this down\nIn what follows, we propose three visualization-based approaches to questions of heterogeneity of covariance in MANOVA designs:\n\ndirect visualization of the information in the \\(\\mathbf{S}_i\\) and \\(\\mathbf{S}_p\\) using data ellipsoids to show size and shape as minimal schematic summaries;\na simple dotplot of the components of Box’s M test: the log determinants of the \\(\\mathbf{S}_i\\) together with that of the pooled \\(\\mathbf{S}_p\\). Extensions of these simple plots raise the question of whether measures of heterogeneity other than that captured in Box’s test might also be useful; and,\nthe connection between Levene-type tests and an ANOVA (of centered absolute differences) suggests a parallel with a multivariate extension of Levene-type tests and a MANOVA. We explore this with a version of Hypothesis-Error (HE) plots we have found useful for visualizing mean differences in MANOVA designs.\n\n\n#&gt; Writing packages to  C:/R/Projects/Vis-MLM-quarto/bib/pkgs.txt\n#&gt; 15  packages used here:\n#&gt;  base, broom, candisc, car, carData, datasets, dplyr, ggplot2, graphics, grDevices, heplots, methods, stats, tidyr, utils"
  },
  {
    "objectID": "eqcov.html#references",
    "href": "eqcov.html#references",
    "title": "\n11  Visualizing Tests for Equality of Covariance Matrices\n",
    "section": "\n11.5 References",
    "text": "11.5 References\n\n\n\n\nBartlett, M. S. 1937. “Properties of Sufficiency and Statistical Tests.” Proceedings of the Royal Society of London. Series A 160 (901): 268–82. https://doi.org/10.2307/96803.\n\n\nBox, G. E. P. 1949. “A General Distribution Theory for a Class of Likelihood Criteria.” Biometrika 36 (3-4): 317–46. https://doi.org/10.1093/biomet/36.3-4.317.\n\n\n———. 1950. “Problems in the Analysis of Growth and Wear Curves.” Biometrics 6: 362–89.\n\n\n———. 1953. “Non-Normality and Tests on Variances.” Biometrika 40 (3/4): 318–35. https://doi.org/10.2307/2333350.\n\n\nBrown, Morton B., and Alan B. Forsythe. 1974. “Robust Tests for Equality of Variances.” Journal of the American Statistical Association 69 (346): 364–67. https://doi.org/10.1080/01621459.1974.10482955.\n\n\nCochran, W. G. 1941. “The Distribution of the Largest of a Set of Estimated Variances as a Fraction of Their Total.” Annals of Eugenics 11 (1): 47–52. https://doi.org/10.1111/j.1469-1809.1941.tb02271.x.\n\n\nConover, W. J., Mark E. Johnson, and Myrle M. Johnson. 1981. “A Comparative Study of Tests for Homogeneity of Variances, with Applications to the Outer Continental Shelf Bidding Data.” Technometrics 23 (4): 351–61. https://doi.org/10.1080/00401706.1981.10487680.\n\n\nGastwirth, Joseph L., Yulia R. Gel, and Weiwen Miao. 2009. “The Impact of Levene’s Test of Equality of Variances on Statistical Theory and Practice.” Statistical Science 24 (3): 343–60. https://doi.org/10.1214/09-STS301.\n\n\nHartley, H. O. 1950. “The Use of Range in Analysis of Variance.” Biometrika 37 (3–4): 271–80. https://doi.org/10.1093/biomet/37.3-4.271.\n\n\nHarwell, M. R., E. N. Rubinstein, W. S. Hayes, and C. C. Olds. 1992. “Summarizing Monte Carlo Results in Methodological Research: The One- and Two-Factor Fixed Effects ANOVA Cases.” Journal of Educational and Behavioral Statistics 17 (4): 315–39. https://doi.org/10.3102/10769986017004315.\n\n\nLevene, Howard. 1960. “Robust Tests for Equality of Variances.” In Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling, edited by Ingram Olkin, S. G. Ghurye, W. Hoeffding, W. G. Madow, and H. B. Mann, 278–92. Stanford, Calif: Stanford University Press.\n\n\nLix, J. M., L. M. Keselman, and H. J. Keselman. 1996. “Consequences of Assumption Violations Revisited: A Quantitative Review of Alternatives to the One-Way Analysis of Variance F Test.” Review of Educational Research 66 (4): 579–619. https://doi.org/10.3102/00346543066004579.\n\n\nO’Brien, Peter C. 1992. “Robust Procedures for Testing Equality of Covariance Matrices.” Biometrics 48 (3): 819–27. http://www.jstor.org/stable/2532347.\n\n\nRogan, J. C., and H. J. Keselman. 1977. “Is the ANOVA f-Test Robust to Variance Heterogeneity When Sample Sizes Are Equal?: An Investigation via a Coefficient of Variation.” American Educational Research Journal 14 (4): 493–98. https://doi.org/10.3102/00028312014004493.\n\n\nTiku, M. L., and N. Balakrishnan. 1984. “Testing Equality of Population Variances the Robust Way.” Communications in Statistics - Theory and Methods 13 (17): 2143–59. https://doi.org/10.1080/03610928408828818.\n\n\nTimm, N. H. 1975. Multivariate Analysis with Applications in Education and Psychology. Belmont, CA: Wadsworth (Brooks/Cole).\n\n\nZhang, Ji, and Dennis D. Boos. 1992. “Bootstrap Critical Values for Testing Homogeneity of Covariance Matrices.” Journal of the American Statistical Association 87 (418): 425–29. http://www.jstor.org/stable/2290273."
  },
  {
    "objectID": "eqcov.html#footnotes",
    "href": "eqcov.html#footnotes",
    "title": "\n11  Visualizing Tests for Equality of Covariance Matrices\n",
    "section": "",
    "text": "If group sizes are greatly unequal and homogeneity of variance is violated, then the \\(F\\) statistic is too liberal (\\(p\\) values too large) when large sample variances are associated with small group sizes. Conversely, the \\(F\\) statistic is too conservative if large variances are associated with large group sizes.↩︎"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "\n12  Summary\n",
    "section": "",
    "text": "This will grow into a summary of the book. But for now, I leave it as the most beautiful of identities, Euler’s \\(e^{\\pi i} + 1 = 0\\)\n\nexp(pi * 0i) \n\n[1] 1+0i"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abbott, Edwin A. 1884. Flatland: A Romance of Many Dimensions.\nCutchogue, NY: Buccaneer Books.\n\n\nAdler, Daniel, and Duncan Murdoch. 2023. Rgl: 3D Visualization Using\nOpenGL. https://CRAN.R-project.org/package=rgl.\n\n\nAnscombe, F. J. 1973. “Graphs in Statistical Analysis.”\nThe American Statistician 27: 17–21.\n\n\nBartlett, M. S. 1937. “Properties of Sufficiency and Statistical\nTests.” Proceedings of the Royal Society of London. Series\nA 160 (901): 268–82. https://doi.org/10.2307/96803.\n\n\nBelsley, D. A., E. Kuh, and R. E. Welsch. 1980. Regression\nDiagnostics: Identifying Influential Data and Sources of\nCollinearity. New York: John Wiley; Sons.\n\n\nBiecek, Przemyslaw, Hubert Baniecki, Mateusz Krzyzinski, and Dianne\nCook. 2023. “Performance Is Not Enough: A Story of the Rashomon’s\nQuartet,” February. https://arxiv.org/abs/2302.13356.\n\n\nBox, G. E. P. 1949. “A General Distribution Theory for a Class of\nLikelihood Criteria.” Biometrika 36 (3-4): 317–46. https://doi.org/10.1093/biomet/36.3-4.317.\n\n\n———. 1950. “Problems in the Analysis of Growth and Wear\nCurves.” Biometrics 6: 362–89.\n\n\n———. 1953. “Non-Normality and Tests on Variances.”\nBiometrika 40 (3/4): 318–35. https://doi.org/10.2307/2333350.\n\n\nBrown, Morton B., and Alan B. Forsythe. 1974. “Robust Tests for\nEquality of Variances.” Journal of the American Statistical\nAssociation 69 (346): 364–67. https://doi.org/10.1080/01621459.1974.10482955.\n\n\nCajori, Florian. 1926. “Origins of Fourth Dimension\nConcepts.” The American Mathematical Monthly 33 (8):\n397–406. https://doi.org/10.1080/00029890.1926.11986607.\n\n\nCochran, W. G. 1941. “The Distribution of the Largest of a Set of\nEstimated Variances as a Fraction of Their Total.” Annals of\nEugenics 11 (1): 47–52. https://doi.org/10.1111/j.1469-1809.1941.tb02271.x.\n\n\nConover, W. J., Mark E. Johnson, and Myrle M. Johnson. 1981. “A\nComparative Study of Tests for Homogeneity of Variances, with\nApplications to the Outer Continental Shelf Bidding Data.”\nTechnometrics 23 (4): 351–61. https://doi.org/10.1080/00401706.1981.10487680.\n\n\nCotton, R. 2013. Learning R. Sebastopol, CA:\nO’Reilly Media.\n\n\nCox, D. R. 1968. “Notes on Some Aspects of Regression\nAnalysis.” Journal of the Royal Statistical Society Series\nA 131: 265–79.\n\n\nCurran, James, and Taylor Hersh. 2021. Hotelling: Hotelling’s t^2\nTest and Variants. https://CRAN.R-project.org/package=Hotelling.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022.\ndatasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nDavis, C. 1990. “Body Image and Weight Preoccupation: A Comparison\nBetween Exercising and Non-Exercising Women.” Appetite\n16 (1): 84. https://doi.org/10.1016/0195-6663(91)90115-9.\n\n\nDempster, A. P. 1969. Elements of Continuous Multivariate\nAnalysis. Reading, MA: Addison-Wesley.\n\n\nFarquhar, A. B., and H. Farquhar. 1891. Economic and Industrial\nDelusions: A Discourse of the Case for Protection. New York:\nPutnam.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic\nProblems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nFox, John. 2016. Applied Regression Analysis and Generalized Linear\nModels. Third edition. Los Angeles: SAGE.\n\n\n———. 2020. Regression Diagnostics. 2nd ed. SAGE\nPublications, Inc. https://doi.org/10.4135/9781071878651.\n\n\nFox, John, and Sandford Weisberg. 2018. An r Companion to Applied\nRegression. Third. Thousand Oaks CA: SAGE\nPublications. https://books.google.ca/books?id=uPNrDwAAQBAJ.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2023. Car: Companion to\nApplied Regression. https://CRAN.R-project.org/package=car.\n\n\nFriendly, Michael. 2002. “Corrgrams: Exploratory Displays for\nCorrelation Matrices.” The American Statistician 56 (4):\n316–24. http://datavis.ca/papers/corrgram.pdf.\n\n\n———. 2007. “HE Plots for Multivariate General Linear\nModels.” Journal of Computational and Graphical\nStatistics 16 (2): 421–44. https://doi.org/10.1198/106186007X208407.\n\n\n———. 2008. “The Golden Age of Statistical\nGraphics.” Statistical Science 23 (4): 502–35. https://doi.org/10.1214/08-STS268.\n\n\nFriendly, Michael, and E. Kwan. 2003. “Effect Ordering for Data\nDisplays.” Computational Statistics and Data Analysis 43\n(4): 509–39. http://authors.elsevier.com/sd/article/S0167947302002906.\n\n\nFriendly, Michael, and David Meyer. 2016. Discrete Data Analysis\nwith R: Visualization and Modeling Techniques for\nCategorical and Count Data. Boca Raton, FL: Chapman & Hall/CRC.\n\n\nFriendly, Michael, Georges Monette, and John Fox. 2013.\n“Elliptical Insights: Understanding Statistical Methods Through\nElliptical Geometry.” Statistical Science 28 (1): 1–39.\nhttps://doi.org/10.1214/12-STS402.\n\n\nFriendly, Michael, and Howard Wainer. 2021. A History of Data\nVisualization and Graphic Communication. Cambridge, MA: Harvard\nUniversity Press. https://doi.org/10.4159/9780674259034.\n\n\nFunkhouser, H. Gray. 1937. “Historical Development of the\nGraphical Representation of Statistical Data.” Osiris 3\n(1): 269–405. http://tinyurl.com/32ema9.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in\nHereditary Stature.” Journal of the Anthropological\nInstitute 15: 246–63. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0.\n\n\nGastwirth, Joseph L., Yulia R. Gel, and Weiwen Miao. 2009. “The\nImpact of Levene’s Test of Equality of\nVariances on Statistical Theory and Practice.” Statistical\nScience 24 (3): 343–60. https://doi.org/10.1214/09-STS301.\n\n\nGelman, Andrew, Jessica Hullman, and Lauren Kennedy. 2023. “Causal\nQuartets: Different Ways to Attain the Same Average Treatment\nEffect.” http://www.stat.columbia.edu/~gelman/research/unpublished/causal_quartets.pdf.\n\n\nGuerry, André-Michel. 1833. Essai Sur La Statistique Morale de La\nFrance. Paris: Crochard.\n\n\nHartley, H. O. 1950. “The Use of Range in Analysis of\nVariance.” Biometrika 37 (3–4): 271–80. https://doi.org/10.1093/biomet/37.3-4.271.\n\n\nHartman, L. I. 2016. “Schizophrenia and Schizoaffective Disorder:\nOne Condition or Two?” PhD dissertation, York University.\n\n\nHarwell, M. R., E. N. Rubinstein, W. S. Hayes, and C. C. Olds. 1992.\n“Summarizing Monte Carlo Results in Methodological Research: The\nOne- and Two-Factor Fixed Effects ANOVA Cases.”\nJournal of Educational and Behavioral Statistics 17 (4):\n315–39. https://doi.org/10.3102/10769986017004315.\n\n\nHealy, M. J. R. 1968. “Multivariate Normal Plotting.”\nJournal of the Royal Statistical Society Series C 17 (2):\n157–61.\n\n\nHeinrichs, R. Walter, Farena Pinnock, Eva Muharib, Leah Hartman, Joel\nGoldberg, and Stephanie McDermid Vaz. 2015. “Neurocognitive\nNormality in Schizophrenia Revisited.” Schizophrenia\nResearch: Cognition 2 (4): 227–32. https://doi.org/10.1016/j.scog.2015.09.001.\n\n\nHerschel, John F. W. 1833. “On the Investigation of the Orbits of\nRevolving Double Stars: Being a Supplement to a Paper Entitled\n\"Micrometrical Measures of 364 Double Stars\".” Memoirs of the\nRoyal Astronomical Society 5: 171–222.\n\n\nHotelling, Harold. 1931. “The Generalization of Student’s Ratio.” The Annals of\nMathematical Statistics 2 (3): 360–78. https://doi.org/10.1214/aoms/1177732979.\n\n\nLevene, Howard. 1960. “Robust Tests for Equality of\nVariances.” In Contributions to Probability and Statistics:\nEssays in Honor of Harold Hotelling, edited by Ingram\nOlkin, S. G. Ghurye, W. Hoeffding, W. G. Madow, and H. B. Mann, 278–92.\nStanford, Calif: Stanford University Press.\n\n\nLix, J. M., L. M. Keselman, and H. J. Keselman. 1996.\n“Consequences of Assumption Violations Revisited: A Quantitative\nReview of Alternatives to the One-Way Analysis of Variance\nF Test.” Review of Educational Research 66\n(4): 579–619. https://doi.org/10.3102/00346543066004579.\n\n\nLongley, James W. 1967. “An Appraisal of Least Squares Programs\nfor the Electronic Computer from the Point of View of the User.”\nJournal of the American Statistical Association 62: 819–41.\nhttps://doi.org/https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896.\n\n\nMardia, K. V. 1970. “Measures of Multivariate Skewness and\nKurtosis with Applications.” Biometrika 57 (3): 519–30.\nhttps://doi.org/http://dx.doi.org/10.2307/2334770.\n\n\n———. 1974. “Applications of Some Measures of Multivariate Skewness\nand Kurtosis in Testing Normality and Robustness Studies.”\nSankhya: The Indian Journal of Statistics, Series B 36 (2):\n115–28. http://www.jstor.org/stable/25051892.\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats,\nDifferent Graphs.” In Proceedings of the 2017\nCHI Conference on Human Factors in Computing Systems.\nACM. https://doi.org/10.1145/3025453.3025912.\n\n\nMatloff, Norman. 2011. The Art of R Programming:\nA Tour of Statistical Software Design. San Francisco,\nCA: No Starch Press.\n\n\nMonette, Georges. 1990. “Geometry of Multiple Regression and\nInteractive 3-D Graphics.” In Modern Methods of\nData Analysis, edited by J. Fox and S. Long, 209–56. Beverly Hills,\nCA: SAGE Publications.\n\n\nO’Brien, Peter C. 1992. “Robust Procedures for Testing Equality of\nCovariance Matrices.” Biometrics 48 (3): 819–27. http://www.jstor.org/stable/2532347.\n\n\nPearson, Karl. 1896. “Contributions to the Mathematical Theory of\nEvolution—III, Regression, Heredity and Panmixia.”\nPhilosophical Transactions of the Royal Society of London, A,\n187: 253–318.\n\n\nPlayfair, William. 1786. Commercial and Political Atlas:\nRepresenting, by Copper-Plate Charts, the Progress of the Commerce,\nRevenues, Expenditure, and Debts of England, During the Whole of the\nEighteenth Century. London: Debrett; Robinson;; Sewell. http://ucpj.uchicago.edu/Isis/journal/demo/v000n000/000000/000000.fg4.html.\n\n\n———. 1801. Statistical Breviary; Shewing, on a Principle Entirely\nNew, the Resources of Every State and Kingdom in\nEurope. London: Wallis.\n\n\nRogan, J. C., and H. J. Keselman. 1977. “Is the ANOVA\nf-Test Robust to Variance Heterogeneity When Sample Sizes Are Equal?: An\nInvestigation via a Coefficient of Variation.” American\nEducational Research Journal 14 (4): 493–98. https://doi.org/10.3102/00028312014004493.\n\n\nTeetor, Paul. 2011. R cookbook.\nSebastopol, CA: O’Reilly Media.\n\n\nTiku, M. L., and N. Balakrishnan. 1984. “Testing Equality of\nPopulation Variances the Robust Way.” Communications in\nStatistics - Theory and Methods 13 (17): 2143–59. https://doi.org/10.1080/03610928408828818.\n\n\nTimm, N. H. 1975. Multivariate Analysis with Applications in\nEducation and Psychology. Belmont, CA: Wadsworth (Brooks/Cole).\n\n\nWickham, Hadley. 2014. Advanced R. Boca\nRaton, FL: Chapman and Hall/CRC.\n\n\nXie, Yihui. 2021. Animation: A Gallery of Animations in Statistics\nand Utilities to Create Animations. https://yihui.org/animation/.\n\n\nZhang, Ji, and Dennis D. Boos. 1992. “Bootstrap Critical Values\nfor Testing Homogeneity of Covariance Matrices.” Journal of\nthe American Statistical Association 87 (418): 425–29. http://www.jstor.org/stable/2290273.\n\n\nPackage used"
  }
]