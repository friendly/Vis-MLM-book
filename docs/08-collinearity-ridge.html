<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>8&nbsp; Collinearity &amp; Ridge Regression – Visualizing Multivariate Data and Models in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./09-hotelling.html" rel="next">
<link href="./07-lin-mod-topics.html" rel="prev">
<link href="./images/favicon/favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-98431dec32babc1722de8bb78f6a2c4f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05-linear_models.html">Univariate Linear Models</a></li><li class="breadcrumb-item"><a href="./08-collinearity-ridge.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Collinearity &amp; Ridge Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05-linear_models.html">Univariate Linear Models</a></li><li class="breadcrumb-item"><a href="./08-collinearity-ridge.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Collinearity &amp; Ridge Regression</span></a></li></ol></nav>
      <h1 class="title"><span id="sec-collin" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Collinearity &amp; Ridge Regression</span></span></h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Visualizing Multivariate Data and Models in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/friendly/vis-MLM-book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Orienting Ideas</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Getting Started</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Exploratory Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-multivariate_plots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Plots of Multivariate Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-pca-biplot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Dimension Reduction</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Univariate Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Overview of Linear models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-linear_models-plots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Plots for univariate response models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-lin-mod-topics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Topics in Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-collinearity-ridge.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Collinearity &amp; Ridge Regression</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Multivariate Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-hotelling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hotelling’s <span class="math inline">\(T^2\)</span></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-mlm-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Multivariate Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-mlm-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Visualizing Multivariate Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-eqcov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Visualizing Equality of Covariance Matrices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-infl-robust.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Multiviate Influence and Robust Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-case-studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">End matter</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./91-colophon.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Colophon</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./95-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-what-is-collin" id="toc-sec-what-is-collin" class="nav-link active" data-scroll-target="#sec-what-is-collin"><span class="header-section-number">8.1</span> What is collinearity?</a>
  <ul class="collapse">
<li><a href="#sec-vis-collin" id="toc-sec-vis-collin" class="nav-link" data-scroll-target="#sec-vis-collin"><span class="header-section-number">8.1.1</span> Visualizing collinearity</a></li>
  <li><a href="#sec-data-beta-space" id="toc-sec-data-beta-space" class="nav-link" data-scroll-target="#sec-data-beta-space"><span class="header-section-number">8.1.2</span> Data space and <span class="math inline">\(\beta\)</span> space</a></li>
  </ul>
</li>
  <li>
<a href="#sec-measure-collin" id="toc-sec-measure-collin" class="nav-link" data-scroll-target="#sec-measure-collin"><span class="header-section-number">8.2</span> Measuring collinearity</a>
  <ul class="collapse">
<li><a href="#sec-vif" id="toc-sec-vif" class="nav-link" data-scroll-target="#sec-vif"><span class="header-section-number">8.2.1</span> Variance inflation factors</a></li>
  <li><a href="#vif-displays" id="toc-vif-displays" class="nav-link" data-scroll-target="#vif-displays"><span class="header-section-number">8.2.2</span> VIF displays</a></li>
  <li><a href="#sec-colldiag" id="toc-sec-colldiag" class="nav-link" data-scroll-target="#sec-colldiag"><span class="header-section-number">8.2.3</span> Collinearity diagnostics</a></li>
  </ul>
</li>
  <li><a href="#sec-tableplot" id="toc-sec-tableplot" class="nav-link" data-scroll-target="#sec-tableplot"><span class="header-section-number">8.3</span> Tableplots</a></li>
  <li><a href="#sec-collin-biplots" id="toc-sec-collin-biplots" class="nav-link" data-scroll-target="#sec-collin-biplots"><span class="header-section-number">8.4</span> Collinearity biplots</a></li>
  <li><a href="#sec-remedies" id="toc-sec-remedies" class="nav-link" data-scroll-target="#sec-remedies"><span class="header-section-number">8.5</span> Remedies for collinearity: What can I do?</a></li>
  <li>
<a href="#sec-ridge" id="toc-sec-ridge" class="nav-link" data-scroll-target="#sec-ridge"><span class="header-section-number">8.6</span> Ridge regression</a>
  <ul class="collapse">
<li><a href="#properties-of-ridge-regression" id="toc-properties-of-ridge-regression" class="nav-link" data-scroll-target="#properties-of-ridge-regression"><span class="header-section-number">8.6.1</span> Properties of ridge regression</a></li>
  <li><a href="#the-genridge-package" id="toc-the-genridge-package" class="nav-link" data-scroll-target="#the-genridge-package"><span class="header-section-number">8.6.2</span> The <code>genridge</code> package</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ridge-univar" id="toc-sec-ridge-univar" class="nav-link" data-scroll-target="#sec-ridge-univar"><span class="header-section-number">8.7</span> Univariate ridge trace plots</a>
  <ul class="collapse">
<li><a href="#whats-not-to-like" id="toc-whats-not-to-like" class="nav-link" data-scroll-target="#whats-not-to-like"><span class="header-section-number">8.7.1</span> What’s not to like?</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ridge-bivar" id="toc-sec-ridge-bivar" class="nav-link" data-scroll-target="#sec-ridge-bivar"><span class="header-section-number">8.8</span> Bivariate ridge trace plots</a>
  <ul class="collapse">
<li><a href="#visualizing-the-bias-variance-tradeoff" id="toc-visualizing-the-bias-variance-tradeoff" class="nav-link" data-scroll-target="#visualizing-the-bias-variance-tradeoff"><span class="header-section-number">8.8.1</span> Visualizing the bias-variance tradeoff</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ridge-low-rank" id="toc-sec-ridge-low-rank" class="nav-link" data-scroll-target="#sec-ridge-low-rank"><span class="header-section-number">8.9</span> Low-rank views</a>
  <ul class="collapse">
<li><a href="#biplot-view" id="toc-biplot-view" class="nav-link" data-scroll-target="#biplot-view"><span class="header-section-number">8.9.1</span> Biplot view</a></li>
  </ul>
</li>
  <li><a href="#what-have-we-learned" id="toc-what-have-we-learned" class="nav-link" data-scroll-target="#what-have-we-learned"><span class="header-section-number">8.10</span> What have we learned?</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/friendly/vis-MLM-book/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content"><!--- For HTML Only ---><!-- \require{newcommand} --><!-- %\renewcommand*{\det}[1]{\mathrm{det} (#1)} --><!-- %\renewcommand*{\det}[1]{|#1|} --><!-- \providecommand{\sizedmat}[2]{\mathord{\mathop{\mat{#1}}\limits_{(#2)}}} --><!-- \newcommand*{\E}{\mathcal{E}} --><!-- Index generation --><!-- % R packages:  indexed under both package name and packages! --><!-- % data sets:  --><!-- % R stuff --><p>In univariate multiple regression models, we usually hope to have high correlations between the outcome <span class="math inline">\(y\)</span> and each of the predictors, <span class="math inline">\(\mathbf{X} = [\mathbf{x}_1, \mathbf{x_2}, \dots]\)</span>. But high correlations <em>among</em> the predictors can cause problems in estimating and testing their effects. Exactly the same problems can exist in multivariate response models, because they involve only the relations among the predictor variables, so the problems and solutions discussed here apply equally to MLMs.</p>
<p>The problem of high correlations among the predictors in a model is called <strong>collinearity</strong> (or multicollinearity), referring to the situation when two or more predictors are very nearly linearly related to each other (collinear). This chapter illustrates the nature of collinearity geometrically, using data and confidence ellipsoids (<a href="#sec-what-is-collin" class="quarto-xref"><span>Section 8.1</span></a>) It describes diagnostic measures to asses these effects (<a href="#sec-measure-collin" class="quarto-xref"><span>Section 8.2</span></a>) and presents some novel visual tools for these purposes using the <span style="color: brown;"><strong>VisCollin</strong></span> package. These include <em>tableplots</em> (<a href="#sec-tableplot" class="quarto-xref"><span>Section 8.3</span></a>) and collinearity biplots (<a href="#sec-collin-biplots" class="quarto-xref"><span>Section 8.4</span></a>).</p>
<p>One class of solutions for collinearity involves <em>regularization methods</em> such as ridge regression (<a href="#sec-ridge" class="quarto-xref"><span>Section 8.6</span></a>). Another collection of graphical methods, <em>generalized ridge trace plots</em>, implemented in the <span style="color: brown;"><strong>genridge</strong></span> package, sheds further light on what is accomplished by this technique. In <a href="#sec-ridge-low-rank" class="quarto-xref"><span>Section 8.9</span></a> we see that, once again, PCA-related techniques like the biplot can be insightful—here, to understand the nature of collinearity and ridge regression. More generally, the methods of this chapter are further examples of how data and confidence ellipsoids can be used to visualize bias <strong>and</strong> precision of regression estimates.</p>
<p><strong>Packages</strong></p>
<p>In this chapter I use the following packages. Load them now.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/friendly/VisCollin">VisCollin</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/friendly/genridge">genridge</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sthda.com/english/rpkgs/factoextra">factoextra</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggrepel.slowkow.com/">ggrepel</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://patchwork.data-imaginist.com">patchwork</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://easystats.github.io/easystats/">easystats</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="sec-what-is-collin" class="level2 page-columns page-full" data-number="8.1"><h2 data-number="8.1" class="anchored" data-anchor-id="sec-what-is-collin">
<span class="header-section-number">8.1</span> What is collinearity?</h2>
<p>Researchers who have studies standard treatments of linear models (e.g, <span class="citation" data-cites="Graybill1961">Graybill (<a href="95-references.html#ref-Graybill1961" role="doc-biblioref">1961</a>)</span>; <span class="citation" data-cites="Hocking2013">Hocking (<a href="95-references.html#ref-Hocking2013" role="doc-biblioref">2013</a>)</span>) are often less than clear about what collinearity is, how to find its sources and how to take steps to resolve them. There are a number of important diagnostic measures that can help, but these are usually presented in a tabular display like <a href="#fig-collinearity-diagnostics-SPSS" class="quarto-xref">Figure&nbsp;<span>8.1</span></a>, which prompted this query on an online forum:</p>
<blockquote class="blockquote">
<p>Some of my collinearity diagnostics have large values, or small values, or whatever they are <em>not</em> supposed to be</p>
<ul>
<li>What is bad?</li>
<li>If bad, what can I do about it?</li>
</ul>
</blockquote>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-collinearity-diagnostics-SPSS" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-collinearity-diagnostics-SPSS-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/collinearity-diagnostics-SPSS.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-collinearity-diagnostics-SPSS-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: Collinearity diagnostics for a multiple regression model from SPSS. <em>Source</em>: Arndt Regorz, How to interpret a Collinearity Diagnostics table in SPSS, https://bit.ly/3YRB82b
</figcaption></figure>
</div>
</div>
</div>
<p>The trouble with displays like <a href="#fig-collinearity-diagnostics-SPSS" class="quarto-xref">Figure&nbsp;<span>8.1</span></a> is that the important information is hidden in a sea of numbers, some of which are bad when <em>large</em>, others bad when they are <em>small</em> and a large bunch which are irrelevant to interpretation.</p>
<p>In <span class="citation" data-cites="FriendlyKwan:2009">Friendly &amp; Kwan (<a href="95-references.html#ref-FriendlyKwan:2009" role="doc-biblioref">2009</a>)</span>, we liken this problem to that of the reader of Martin Hansford’s successful series of books, <em>Where’s Waldo</em>. These consist of a series of full-page illustrations of hundreds of people and things and a few Waldos— a character wearing a red and white striped shirt and hat, glasses, and carrying a walking stick or other paraphernalia. Waldo was never disguised, yet the complex arrangement of misleading visual cues in the pictures made him very hard to find. Collinearity diagnostics often provide a similar puzzle: where should you look in traditional tabular displays?<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<!-- This image based on: https://x.com/ErrorJustin/status/830205933598879744 -->
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-wheres-waldo" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-wheres-waldo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/wheres-waldo.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wheres-waldo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: A scene from one of the <em>Where’s Waldo</em> books. Waldo wears a red-striped shirt, but far too many of the other figures in the scene have horizontal red stripes, making it very difficult to find him among all the distractors. This is often the problem with collinearity diagnostics. <em>Source</em>: Modified from https://bit.ly/48KPcOo
</figcaption></figure>
</div>
</div>
</div>
<p>Recall the standard classical linear model for a response variable <span class="math inline">\(y\)</span> with a collection of predictors in <span class="math inline">\(\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_p)\)</span></p>
<p><span class="math display">\[\begin{aligned}
\mathbf{y}  &amp; =  \beta_0 + \beta_1 \mathbf{x}_1 + \beta_2 \mathbf{x}_2 + \cdots + \beta_p \mathbf{x}_p + \boldsymbol{\epsilon} \\
            &amp; =  \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon} \; ,
\end{aligned}\]</span></p>
<p>for which the ordinary least squares solution is:</p>
<p><span class="math display">\[
\widehat{\mathbf{b}} = (\mathbf{X}^\mathsf{T} \mathbf{X})^{-1} \; \mathbf{X}^\mathsf{T} \mathbf{y} \; .
\]</span> The sampling variances and covariances of the estimated coefficients is <span class="math inline">\(\text{Var} (\widehat{\mathbf{b}}) = \sigma_\epsilon^2 \times (\mathbf{X}^\mathsf{T} \mathbf{X})^{-1}\)</span> and <span class="math inline">\(\sigma_\epsilon^2\)</span> is the variance of the residuals <span class="math inline">\(\mathbf{\epsilon}\)</span>, estimated by the mean squared error (MSE).</p>
<p>In the limiting case, collinearity becomes particularly problematic when one <span class="math inline">\(x_i\)</span> is <em>perfectly</em> predictable from the other <span class="math inline">\(x\)</span>s, i.e., <span class="math inline">\(R^2 (x_i | \text{other }x) = 1\)</span>. This is problematic because:</p>
<ul>
<li>there is no <em>unique</em> solution for the regression coefficients <span class="math inline">\(\mathbf{b} = (\mathbf{X}^\mathsf{T} \mathbf{X})^{-1} \mathbf{X} \mathbf{y}\)</span>;</li>
<li>the standard errors <span class="math inline">\(s (b_i)\)</span> of the estimated coefficients are infinite and <em>t</em> statistics <span class="math inline">\(t_i = b_i / s (b_i)\)</span> are 0.</li>
</ul>
<p>This extreme case reflects a situation when one or more predictors are effectively redundant, for example when you include two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and their sum <span class="math inline">\(z = x + y\)</span> in a model. For instance, a dataset may include variables for income, expenses, and savings. But income is the sum of expenses and savings, so not all three should be used as predictors.</p>
<p>A more subtle case is the use <em>ipsatized</em>, defined as scores that sum to a constant, such as proportions of a total. You might have scores on tests of reading, math, spelling and geography. With ipsatized scores, any one of these is necessarily 1 <span class="math inline">\(-\)</span> sum of the others, i.e., if reading is 0.5, math and geography are both 0.15, then geography must be 0.2. Once thre of the four scores are known, the last provides no new information. </p>
<p>More generally, collinearity refers to the case when there are very high <strong>multiple correlations</strong> among the predictors, such as <span class="math inline">\(R^2 (x_i | \text{other }x) \ge 0.9\)</span>. Note that you can’t tell simply by looking at the simple correlations. A large correlation <span class="math inline">\(r_{ij}\)</span> is <em>sufficient</em> for collinearity, but not <em>necessary</em>—you can have variables <span class="math inline">\(x_1, x_2, x_3\)</span> for which the pairwise correlation are low, but the multiple correlation is high.</p>
<p>The consequences are:</p>
<ul>
<li>The estimated coefficients have large standard errors, <span class="math inline">\(s(\hat{b}_j)\)</span>. They are multiplied by the square root of the variance inflation factor, <span class="math inline">\(\sqrt{\text{VIF}}\)</span>, discussed below.</li>
<li>The large standard errors deflate the <span class="math inline">\(t\)</span>-statistics, <span class="math inline">\(t = \hat{b}_j / s(\hat{b}_j)\)</span>, by the same factor, so a coefficient that would significant if the predictors were uncorrelated becomes insignificant when collinearity is present.</li>
<li>Thus you may find a situation where an overall model is highly significant (large <span class="math inline">\(F\)</span>-statistic), while no (or few) of the individual predictors are. This is a puzzlement!</li>
<li>Beyond this, the least squares solution may have poor numerical accuracy <span class="citation" data-cites="Longley:1967">(<a href="95-references.html#ref-Longley:1967" role="doc-biblioref">Longley, 1967</a>)</span>, because the solution depends inversely on the determinant <span class="math inline">\(|\,\mathbf{X}^\mathsf{T} \mathbf{X}\,|\)</span>, which approaches 0 as multiple correlations increase.</li>
<li>There is an interpretive problem as well. Recall that the coefficients <span class="math inline">\(\hat{b}\)</span> are <em>partial coefficients</em>, meaning that they estimate change <span class="math inline">\(\Delta y\)</span> in <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> changes by one unit <span class="math inline">\(\Delta x\)</span>, but holding <strong>all other variables constant</strong>. Then, the model may be trying to estimate something that does not occur in the data. (For example: predicting strength from the highly correlated height and weight)</li>
</ul>
<section id="sec-vis-collin" class="level3" data-number="8.1.1"><h3 data-number="8.1.1" class="anchored" data-anchor-id="sec-vis-collin">
<span class="header-section-number">8.1.1</span> Visualizing collinearity</h3>
<p>Collinearity can be illustrated in data space for two predictors in terms of the stability of the regression plane for a linear model <code>Y = X1 + X2</code>. <a href="#fig-collin-demo" class="quarto-xref">Figure&nbsp;<span>8.3</span></a> shows three cases as 3D plots of <span class="math inline">\((X_1, X_2, Y)\)</span>, where the correlation of predictors can be observed in the <span class="math inline">\((X_1, X_2)\)</span> plane.</p>
<ol type="a">
<li><p>shows a case where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are uncorrelated as can be seen in their scatter in the horizontal plane (<code>+</code> symbols). The gray regression plane is well-supported; a small change in Y for one observation won’t make much difference.</p></li>
<li><p>In panel (b), <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have a perfect correlation, <span class="math inline">\(r (x_1, x_2) = 1.0\)</span>. The regression plane is not unique; in fact there are an infinite number of planes that fit the data equally well. Note that, if all we care about is prediction (not the coefficients), we could use <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span>, or both, or any weighted sum of them in a model and get the same predicted values.</p></li>
<li><p>Shows a typical case where there is a strong correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. The regression plane here is unique, but is not well determined. A small change in Y <strong>can</strong> make quite a difference in the fitted value or coefficients, depending on the values of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are far from their near linear relation in the botom plane, you can imagine that it is easy to tilt the plane substantially by a small change in <span class="math inline">\(Y\)</span>.</p></li>
</ol>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-collin-demo" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-collin-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/collin-demo.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-collin-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: Effect of collinearity on the least squares regression plane. (a) Small correlation between predictors; (b) Perfect correlation ; (c) Very strong correlation. The black points show the data Y values, white points are the fitted values in the regression plane, and + signs represent the values of X1 and X2. <em>Source</em>: Adapted from <span class="citation" data-cites="Fox:2016:ARA">Fox (<a href="95-references.html#ref-Fox:2016:ARA" role="doc-biblioref">2016</a>)</span>, Fig. 13.2
</figcaption></figure>
</div>
</div>
</div>
</section><section id="sec-data-beta-space" class="level3 page-columns page-full" data-number="8.1.2"><h3 data-number="8.1.2" class="anchored" data-anchor-id="sec-data-beta-space">
<span class="header-section-number">8.1.2</span> Data space and <span class="math inline">\(\beta\)</span> space</h3>
<p>It is also useful to visualize collinearity by comparing the representation in <strong>data space</strong> with the analogous view of the confidence ellipses for coefficients in <strong>beta space</strong>. To do so in this example, I generate data from a known model <span class="math inline">\(y = 3 x_1 + 3 x_2 + \epsilon\)</span> with <span class="math inline">\(\epsilon \sim \mathcal{N} (0, 100)\)</span> and various true correlations between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, <span class="math inline">\(\rho_{12} = (0, 0.8, 0.97)\)</span> <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>R file: <code>R/collin-data-beta.R</code></p>
</div></div><p>First, I use <code>MASS:mvrnorm()</code> to construct a list of three data frames <code>XY</code> with the same means and standard deviations, but with different correlations. In each case, the variable <span class="math inline">\(y\)</span> is generated with true coefficients <code>beta</code> <span class="math inline">\(=(3, 3)\)</span>, and the fitted model for that value of <code>rho</code> is added to a corresponding list of models, <code>mods</code>.</p>
<div class="cell" data-layout-align="center">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode" id="cb2" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">421</span><span class="op">)</span>            <span class="co"># reproducibility</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">200</span>                 <span class="co"># sample size</span></span>
<span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>            <span class="co"># means</span></span>
<span><span class="va">s</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>             <span class="co"># standard deviations</span></span>
<span><span class="va">rho</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.8</span>, <span class="fl">0.97</span><span class="op">)</span>   <span class="co"># correlations</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">)</span>          <span class="co"># true coefficients</span></span>
<span></span>
<span><span class="co"># Specify a covariance matrix, with standard deviations</span></span>
<span><span class="co">#   s[1], s[2] and correlation r</span></span>
<span><span class="va">Cov</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">s</span>, <span class="va">r</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">s</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,        <span class="va">r</span> <span class="op">*</span> <span class="va">s</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">*</span><span class="va">s</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,</span>
<span>         <span class="va">r</span> <span class="op">*</span> <span class="va">s</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">*</span><span class="va">s</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="va">s</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">2</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Generate a dataframe of X, y for each rho</span></span>
<span><span class="co"># Fit the model for each</span></span>
<span><span class="va">XY</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">vector</a></span><span class="op">(</span>mode <span class="op">=</span><span class="st">"list"</span>, length <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">rho</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">mods</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">vector</a></span><span class="op">(</span>mode <span class="op">=</span><span class="st">"list"</span>, length <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">rho</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">rho</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">r</span> <span class="op">&lt;-</span> <span class="va">rho</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span><span class="va">N</span>, <span class="va">mu</span>, Sigma <span class="op">=</span> <span class="fu">Cov</span><span class="op">(</span><span class="va">s</span>, <span class="va">r</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"x1"</span>, <span class="st">"x2"</span><span class="op">)</span></span>
<span>  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">beta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">X</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">beta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">X</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span>  <span class="va">XY</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">X</span>, y<span class="op">=</span><span class="va">y</span><span class="op">)</span></span>
<span>  <span class="va">mods</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span>, data<span class="op">=</span><span class="va">XY</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The estimated coefficients can then be extracted using <code><a href="https://rdrr.io/r/stats/coef.html">coef()</a></code> applied to each model:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">coefs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">mods</span>, <span class="va">coef</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">coefs</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"mod"</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, <span class="st">" (rho="</span>, <span class="va">rho</span>, <span class="st">")"</span><span class="op">)</span></span>
<span><span class="va">coefs</span></span>
<span><span class="co">#&gt;             mod1 (rho=0) mod2 (rho=0.8) mod3 (rho=0.97)</span></span>
<span><span class="co">#&gt; (Intercept)         1.01        -0.0535           0.141</span></span>
<span><span class="co">#&gt; x1                  3.18         3.4719           3.053</span></span>
<span><span class="co">#&gt; x2                  1.68         2.9734           2.059</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, I define a function to plot the data ellipse (<code><a href="https://rdrr.io/pkg/car/man/Ellipses.html">car::dataEllipse()</a></code>) for each data frame and confidence ellipse (<code><a href="https://rdrr.io/pkg/car/man/Ellipses.html">car::confidenceEllipse()</a></code>) for the coefficients in the corresponding fitted model. In the plots in <a href="#fig-collin-data-beta" class="quarto-xref">Figure&nbsp;<span>8.4</span></a>, I specify the x, y limits for each plot so that the relative sizes of these ellipses are comparable, so that variance inflation can be assessed visually.</p>
<div class="cell" data-layout-align="center">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode" id="cb4" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">do_plots</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">XY</span>, <span class="va">mod</span>, <span class="va">r</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">XY</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/car/man/Ellipses.html">dataEllipse</a></span><span class="op">(</span><span class="va">X</span>,</span>
<span>    levels<span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"darkgreen"</span>,</span>
<span>    fill <span class="op">=</span> <span class="cn">TRUE</span>, fill.alpha <span class="op">=</span> <span class="fl">0.05</span>,</span>
<span>    xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>    ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">)</span>, asp <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span>, <span class="fu"><a href="https://rdrr.io/r/base/bquote.html">bquote</a></span><span class="op">(</span><span class="va">rho</span> <span class="op">==</span> <span class="fu">.</span><span class="op">(</span><span class="va">r</span><span class="op">)</span><span class="op">)</span>, cex <span class="op">=</span> <span class="fl">2</span>, pos <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span>
<span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/car/man/Ellipses.html">confidenceEllipse</a></span><span class="op">(</span><span class="va">mod</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>    fill <span class="op">=</span> <span class="cn">TRUE</span>, fill.alpha <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>    xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"x1 coefficient, "</span>, <span class="va">beta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"x2 coefficient, "</span>, <span class="va">beta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">10</span><span class="op">)</span>,</span>
<span>    ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">10</span><span class="op">)</span>,</span>
<span>    asp <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">beta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="va">beta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, pch <span class="op">=</span> <span class="st">"+"</span>, cex<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v<span class="op">=</span><span class="fl">0</span>, h<span class="op">=</span><span class="fl">0</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">op</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">4</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">+</span><span class="fl">0.1</span>,</span>
<span>          mfcol <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>          cex.lab <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">rho</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu">do_plots</span><span class="op">(</span><span class="va">XY</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span>, <span class="va">mods</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span>, <span class="va">rho</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span><span class="va">op</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-collin-data-beta" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-collin-data-beta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-collin-data-beta-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-collin-data-beta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.4: 95% <span style="color: darkgreen;">Data ellipses</span> for x1, x2 and the corresponding 95% <span style="color: red;">confidence ellipses</span> for their coefficients in the model predicting y. In the confidence ellipse plots, reference lines show the value (0,0) for the null hypothesis and “+” marks the true values for the coefficients. This figure adapts an example by John Fox (2022).
</figcaption></figure>
</div>
</div>
</div>
<p>Recall (<a href="07-lin-mod-topics.html#sec-betaspace" class="quarto-xref"><span>Section 7.1</span></a>) that the confidence ellipse for <span class="math inline">\((\beta_1, \beta_2)\)</span> is just a 90 degree rotation (and rescaling) of the data ellipse for <span class="math inline">\((x_1, x_2)\)</span>: it is wide (more variance) in any direction where the data ellipse is narrow.</p>
<p>The shadows of the confidence ellipses on the coordinate axes in <a href="#fig-collin-data-beta" class="quarto-xref">Figure&nbsp;<span>8.4</span></a> represent the standard errors of the coefficients, and get larger with increasing <span class="math inline">\(\rho\)</span>. This is the effect of variance inflation, described in the following section.</p>
</section></section><section id="sec-measure-collin" class="level2" data-number="8.2"><h2 data-number="8.2" class="anchored" data-anchor-id="sec-measure-collin">
<span class="header-section-number">8.2</span> Measuring collinearity</h2>
<p>This section first describes the <em>variance inflation factor</em> (VIF) used to measure the effect of possible collinearity on each predictor and a collection of diagnostic measures designed to help interpret these. Then I describe some novel graphical methods to make these effects more readily understandable, to answer the “Where’s Waldo” question posed at the outset.</p>
<section id="sec-vif" class="level3" data-number="8.2.1"><h3 data-number="8.2.1" class="anchored" data-anchor-id="sec-vif">
<span class="header-section-number">8.2.1</span> Variance inflation factors</h3>
<p>How can we measure the effect of collinearity? The essential idea is to compare, for each predictor the variance <span class="math inline">\(s^2 (\widehat{b_j})\)</span> that the coefficient that <span class="math inline">\(x_j\)</span> would have if it was totally unrelated to the other predictors to the actual variance it has in the given model.</p>
<p>For two predictors such as shown in <a href="#fig-collin-data-beta" class="quarto-xref">Figure&nbsp;<span>8.4</span></a> the sampling variance of <span class="math inline">\(x_1\)</span> can be expressed as</p>
<p><span class="math display">\[
s^2 (\widehat{b_1}) = \frac{MSE}{(n-1) \; s^2(x_1)} \; \times \; \left[ \frac{1}{1-r^2_{12}} \right]
\]</span> The first term here is the variance of <span class="math inline">\(b_1\)</span> when the two predictors are uncorrelated. The term in brackets represents the <strong>variance inflation factor</strong> <span class="citation" data-cites="Marquardt:1970">(<a href="95-references.html#ref-Marquardt:1970" role="doc-biblioref">Marquardt, 1970</a>)</span>, the amount by which the variance of the coefficient is multiplied as a consequence of the correlation <span class="math inline">\(r_{12}\)</span> of the predictors. As <span class="math inline">\(r_{12} \rightarrow 1\)</span>, the variances approaches infinity.</p>
<p>More generally, with any number of predictors, this relation has a similar form, replacing the simple correlation <span class="math inline">\(r_{12}\)</span> with the multiple correlation predicting <span class="math inline">\(x_j\)</span> from all others,</p>
<p><span class="math display">\[
s^2 (\widehat{b_j}) = \frac{MSE}{(n-1) \; s^2(x_j)} \; \times \; \left[ \frac{1}{1-R^2_{j | \text{others}}} \right]
\]</span> So, we have that the variance inflation factors are:</p>
<p><span class="math display">\[
\text{VIF}_j = \frac{1}{1-R^2_{j \,|\, \text{others}}}
\]</span> In practice, it is often easier to think in terms of the square root, <span class="math inline">\(\sqrt{\text{VIF}_j}\)</span> as the multiplier of the standard errors. The denominator, <span class="math inline">\(1-R^2_{j | \text{others}}\)</span> is sometimes called <strong>tolerance</strong>, a term I don’t find particularly useful, but it is just the proportion of the variance of <span class="math inline">\(x_j\)</span> that is <em>not</em> explainable from the others.</p>
<p>For the cases shown in <a href="#fig-collin-data-beta" class="quarto-xref">Figure&nbsp;<span>8.4</span></a> the VIFs and their square roots are:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">vifs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">mods</span>, <span class="fu">car</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">vifs</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"rho:"</span>, <span class="va">rho</span><span class="op">)</span></span>
<span><span class="va">vifs</span></span>
<span><span class="co">#&gt;    rho: 0 rho: 0.8 rho: 0.97</span></span>
<span><span class="co">#&gt; x1      1     3.09      18.6</span></span>
<span><span class="co">#&gt; x2      1     3.09      18.6</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">vifs</span><span class="op">)</span></span>
<span><span class="co">#&gt;    rho: 0 rho: 0.8 rho: 0.97</span></span>
<span><span class="co">#&gt; x1      1     1.76      4.31</span></span>
<span><span class="co">#&gt; x2      1     1.76      4.31</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that when there are terms in the model with more than one degree of freedom, such as education with four levels (and hence 3 df) or a polynomial term specified as <code>poly(age, 3)</code>, that variable, education or age is represented by three separate <span class="math inline">\(x\)</span>s in the model matrix, and the standard VIF calculation gives results that vary with how those terms are coded in the model.</p>
<p>To allow for these cases, <span class="citation" data-cites="FoxMonette:92">Fox &amp; Monette (<a href="95-references.html#ref-FoxMonette:92" role="doc-biblioref">1992</a>)</span> define <em>generalized</em>, GVIFs as the inflation in the squared area of the confidence ellipse for the coefficients of such terms, relative to what would be obtained with uncorrelated data. Visually, this can be seen by comparing the areas of the ellipses in the bottom row of <a href="#fig-collin-data-beta" class="quarto-xref">Figure&nbsp;<span>8.4</span></a>. Because the magnitude of the GVIF increases with the number of degrees of freedom for the set of parameters, Fox &amp; Monette suggest the analog <span class="math inline">\(\sqrt{\text{GVIF}^{1/2 \text{df}}}\)</span> as the measure of impact on standard errors. This is what <code><a href="https://rdrr.io/pkg/car/man/vif.html">car::vif()</a></code> calculates for a factor or other term with more than 1 df.</p>
<div id="exm-cars-collin" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.1</strong></span> <strong>Cars data</strong></p>
<!-- **TODO**: Use `performance::check_collinearity` and `plot()` method here -->
<p>This example uses the <code>cars</code> dataset in the <code>VisCollin</code> package containing various measures of size and performance on 406 models of automobiles from 1982. Interest is focused on predicting gas mileage, <code>mpg</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">cars</span>, package <span class="op">=</span> <span class="st">"VisCollin"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">cars</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'data.frame':  406 obs. of  10 variables:</span></span>
<span><span class="co">#&gt;  $ make    : Factor w/ 30 levels "amc","audi","bmw",..: 6 4 22 1 12 12 6 22 23 1 ...</span></span>
<span><span class="co">#&gt;  $ model   : chr  "chevelle" "skylark" "satellite" "rebel" ...</span></span>
<span><span class="co">#&gt;  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...</span></span>
<span><span class="co">#&gt;  $ cylinder: int  8 8 8 8 8 8 8 8 8 8 ...</span></span>
<span><span class="co">#&gt;  $ engine  : num  307 350 318 304 302 429 454 440 455 390 ...</span></span>
<span><span class="co">#&gt;  $ horse   : int  130 165 150 150 140 198 220 215 225 190 ...</span></span>
<span><span class="co">#&gt;  $ weight  : int  3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ...</span></span>
<span><span class="co">#&gt;  $ accel   : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...</span></span>
<span><span class="co">#&gt;  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...</span></span>
<span><span class="co">#&gt;  $ origin  : Factor w/ 3 levels "Amer","Eur","Japan": 1 1 1 1 1 1 1 1 1 1 ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We fit a model predicting gas mileage (<code>mpg</code>) from the number of cylinders, engine displacement, horsepower, weight, time to accelerate from 0 – 60 mph and model year (1970–1982). Perhaps surprisingly, only <code>weight</code> and <code>year</code> appear to significantly predict gas mileage. What’s going on here?</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cars.mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span> <span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">cylinder</span> <span class="op">+</span> <span class="va">engine</span> <span class="op">+</span> <span class="va">horse</span> <span class="op">+</span> </span>
<span>                      <span class="va">weight</span> <span class="op">+</span> <span class="va">accel</span> <span class="op">+</span> <span class="va">year</span>, </span>
<span>                data<span class="op">=</span><span class="va">cars</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html">Anova</a></span><span class="op">(</span><span class="va">cars.mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; Anova Table (Type II tests)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: mpg</span></span>
<span><span class="co">#&gt;           Sum Sq  Df F value Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; cylinder      12   1    0.99   0.32    </span></span>
<span><span class="co">#&gt; engine        13   1    1.09   0.30    </span></span>
<span><span class="co">#&gt; horse          0   1    0.00   0.98    </span></span>
<span><span class="co">#&gt; weight      1214   1  102.84 &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; accel          8   1    0.70   0.40    </span></span>
<span><span class="co">#&gt; year        2419   1  204.99 &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Residuals   4543 385                   </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We check the variance inflation factors, using <code><a href="https://rdrr.io/pkg/car/man/vif.html">car::vif()</a></code>. We see that most predictors have very high VIFs, indicating moderately severe multicollinearity.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">cars.mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; cylinder   engine    horse   weight    accel     year </span></span>
<span><span class="co">#&gt;    10.63    19.64     9.40    10.73     2.63     1.24</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">cars.mod</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; cylinder   engine    horse   weight    accel     year </span></span>
<span><span class="co">#&gt;     3.26     4.43     3.07     3.28     1.62     1.12</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>According to <span class="math inline">\(\sqrt{\text{VIF}}\)</span>, the standard error of <code>cylinder</code> has been multiplied by <span class="math inline">\(\sqrt{10.63} = 3.26\)</span> and it’s <span class="math inline">\(t\)</span>-value is divided by this number, compared with the case when all predictors are uncorrelated. <code>engine</code>, <code>horse</code> and <code>weight</code> suffer a similar fate.</p>
<p>If we also included the factor <code>origin</code> in the models, we would get the generalized GVIF:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cars.mod2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span> <span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">cylinder</span> <span class="op">+</span> <span class="va">engine</span> <span class="op">+</span> <span class="va">horse</span> <span class="op">+</span> </span>
<span>                       <span class="va">weight</span> <span class="op">+</span> <span class="va">accel</span> <span class="op">+</span> <span class="va">year</span> <span class="op">+</span> <span class="va">origin</span>, </span>
<span>                 data<span class="op">=</span><span class="va">cars</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">cars.mod2</span><span class="op">)</span></span>
<span><span class="co">#&gt;           GVIF Df GVIF^(1/(2*Df))</span></span>
<span><span class="co">#&gt; cylinder 10.74  1            3.28</span></span>
<span><span class="co">#&gt; engine   22.94  1            4.79</span></span>
<span><span class="co">#&gt; horse     9.96  1            3.16</span></span>
<span><span class="co">#&gt; weight   11.07  1            3.33</span></span>
<span><span class="co">#&gt; accel     2.63  1            1.62</span></span>
<span><span class="co">#&gt; year      1.30  1            1.14</span></span>
<span><span class="co">#&gt; origin    2.10  2            1.20</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Connection with inverse of correlation matrix">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Connection with inverse of correlation matrix
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the linear regression model with standardized predictors, the covariance matrix of the estimated intercept-excluded parameter vector <span class="math inline">\(\mathbf{b}^\star\)</span> has the simpler form, <span class="math display">\[
\mathcal{V} (\mathbf{b}^\star) = \frac{\sigma^2}{n-1} \mathbf{R}^{-1}_{X} \; .
\]</span> where <span class="math inline">\(\mathbf{R}_{X}\)</span> is the correlation matrix among the predictors. It can then be seen that the VIF<span class="math inline">\(_j\)</span> are just the diagonal entries of <span class="math inline">\(\mathbf{R}^{-1}_{X}\)</span>.</p>
<p>More generally, the matrix <span class="math inline">\(\mathbf{R}^{-1}_{X} = (r^{ij})\)</span>, when standardized to a correlation matrix as <span class="math inline">\(-r^{ij} / \sqrt{r^{ii} \; r^{jj}}\)</span> gives the matrix of all partial correlations, <span class="math inline">\(r_{ij} \,|\, \text{others}\)</span>.</p>
<p>This inverse connection is analogous to the dual relationship (<a href="#sec-data-beta-space" class="quarto-xref"><span>Section 8.1.2</span></a>) between ellipses in data space, based on <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> and in <span class="math inline">\(\beta\)</span>-space, based on <span class="math inline">\((\mathbf{X}^\top \mathbf{X})^{-1}\)</span>.</p>
</div>
</div>
</section><section id="vif-displays" class="level3" data-number="8.2.2"><h3 data-number="8.2.2" class="anchored" data-anchor-id="vif-displays">
<span class="header-section-number">8.2.2</span> VIF displays</h3>
<p>Beyond <code><a href="https://rdrr.io/pkg/car/man/vif.html">car::vif()</a></code>, the <span style="color: brown;"><strong>easystats</strong></span> suite of packages has some useful functions for displaying VIFs in helpful tables and plots. <code><a href="https://easystats.github.io/performance/reference/check_collinearity.html">performance::check_collinearity()</a></code> calculates VIFs and their standard errors and returns a <code>"check_collinearity"</code> data frame. The plot method for this plots these, using a log scale because it is multiples of variance that matter here. It uses intervals of 1-5, 5-10, 10+ to highlight low, medium and high variance inflation with colored bands.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cars.collin</span> <span class="op">&lt;-</span> <span class="fu">check_collinearity</span><span class="op">(</span><span class="va">cars.mod</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cars.collin</span>, </span>
<span>     linewidth <span class="op">=</span> <span class="fl">1.1</span>,</span>
<span>     size_point <span class="op">=</span> <span class="fl">5</span>, size_title <span class="op">=</span> <span class="fl">16</span>, base_size <span class="op">=</span> <span class="fl">14</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-cars-check-collin" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-cars-check-collin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-cars-check-collin-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cars-check-collin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.5: Variance inflation plot. VIF is plotted on a log scale. Colored bands show regions of <span style="color: green;">low</span>, <span style="color: blue;">medium</span>and <span style="color: red;">high</span> variance inflation.
</figcaption></figure>
</div>
</div>
</div>
<p>The graphic properties here help to make the problematic variables more apparent than in a table of numbers, though the underlying message is the same. Number of cylinders, engine displacement and weight are the collinearity bad boys. Knowing this helps to pin Waldo down a bit, but to really find him, we need a few more diagnostics and better graphical methods.</p>
</section><section id="sec-colldiag" class="level3" data-number="8.2.3"><h3 data-number="8.2.3" class="anchored" data-anchor-id="sec-colldiag">
<span class="header-section-number">8.2.3</span> Collinearity diagnostics</h3>
<p>OK, we now know that large VIF<span class="math inline">\(_j\)</span> indicate predictor coefficients whose estimation is degraded due to large <span class="math inline">\(R^2_{j \,|\, \text{others}}\)</span>. But for this to be useful, we need to determine:</p>
<ul>
<li>how many dimensions in the space of the predictors are associated with nearly collinear relations?</li>
<li>which predictors are most strongly implicated in each of these?</li>
</ul>
<p>Answers to these questions are provided using measures developed by Belsley and colleagues <span class="citation" data-cites="Belsley-etal:80 Belsley:91a">(<a href="95-references.html#ref-Belsley-etal:80" role="doc-biblioref">Belsley et al., 1980</a>; <a href="95-references.html#ref-Belsley:91a" role="doc-biblioref">Belsley, 1991</a>)</span>. These measures are based on the eigenvalues <span class="math inline">\(\lambda_1, \lambda_2, \dots \lambda_p\)</span> of the correlation matrix <span class="math inline">\(R_{X}\)</span> of the predictors (preferably centered and scaled, and not including the constant term for the intercept), and the corresponding eigenvectors in the columns of <span class="math inline">\(\mathbf{V}_{p \times p}\)</span>, given by the the eigen decomposition</p>
<p><span class="math display">\[
\mathbf{R}_{X} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^\mathsf{T}
\]</span></p>
<p>By elementary matrix algebra, the eigen decomposition of <span class="math inline">\(\mathbf{R}_{XX}^{-1}\)</span> is then</p>
<p><span id="eq-rxinv-eigen"><span class="math display">\[
\mathbf{R}_{X}^{-1} = \mathbf{V} \mathbf{\Lambda}^{-1} \mathbf{V}^\mathsf{T} \; ,
\tag{8.1}\]</span></span></p>
<p>so, <span class="math inline">\(\mathbf{R}_{X}\)</span> and <span class="math inline">\(\mathbf{R}_{XX}^{-1}\)</span> have the same eigenvectors, and the eigenvalues of <span class="math inline">\(\mathbf{R}_{X}^{-1}\)</span> are just <span class="math inline">\(\lambda_i^{-1}\)</span>. Using <a href="#eq-rxinv-eigen" class="quarto-xref">Equation&nbsp;<span>8.1</span></a>, the variance inflation factors may be expressed as</p>
<p><span class="math display">\[
\text{VIF}_j = \sum_{k=1}^p \frac{V^2_{jk}}{\lambda_k} \; ,
\]</span></p>
<p>which shows that (a) only the <em>small</em> eigenvalues contribute to variance inflation, but (b) only for those predictors that have large eigenvector coefficients on those small components. These facts lead to the following diagnostic statistics for collinearity:</p>
<ul>
<li>
<p><strong>Condition indices</strong>: The smallest of the eigenvalues, those for which <span class="math inline">\(\lambda_j \approx 0\)</span>, indicate collinearity and the number of small values indicates the number of near collinear relations. Because the sum of the eigenvalues, <span class="math inline">\(\Sigma \lambda_i = p\)</span> increases with the number of predictors <span class="math inline">\(p\)</span>, it is useful to scale them all in relation to the largest. This leads to <em>condition indices</em>, defined as <span class="math inline">\(\kappa_j = \sqrt{ \lambda_1 / \lambda_j}\)</span>. These have the property that the resulting numbers have common interpretations regardless of the number of predictors.</p>
<ul>
<li>For completely uncorrelated predictors, all <span class="math inline">\(\kappa_j = 1\)</span>.</li>
<li>
<span class="math inline">\(\kappa_j \rightarrow \infty\)</span> as any <span class="math inline">\(\lambda_k \rightarrow 0\)</span>.</li>
<li>As a rule of thumb, <span class="citation" data-cites="Belsley:91a">Belsley (<a href="95-references.html#ref-Belsley:91a" role="doc-biblioref">1991</a>)</span> suggests that <span class="math inline">\(\kappa_j &gt; 30\)</span> indicates severe collinearity, while values <span class="math inline">\(\kappa_j &gt; 10\)</span> reflect a moderate problem.</li>
</ul>
</li>
<li><p><strong>Variance decomposition proportions</strong>: Large VIFs indicate variables that are involved in <em>some</em> nearly collinear relations, but they don’t indicate <em>which</em> other variable(s) each is involved with. For this purpose, <span class="citation" data-cites="Belsley-etal:80">Belsley et al. (<a href="95-references.html#ref-Belsley-etal:80" role="doc-biblioref">1980</a>)</span> and <span class="citation" data-cites="Belsley:91a">Belsley (<a href="95-references.html#ref-Belsley:91a" role="doc-biblioref">1991</a>)</span> proposed calculation of the <em>proportions of variance</em> of each variable associated with each principal component as a decomposition of the coefficient variance for each dimension.</p></li>
</ul>
<p>These measures can be calculated using <code><a href="https://friendly.github.io/VisCollin/reference/colldiag.html">VisCollin::colldiag()</a></code>. For the current model, the usual display contains both the condition indices and variance proportions. However, even for a small example, it is often difficult to know what numbers to pay attention to.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="op">(</span><span class="va">cd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://friendly.github.io/VisCollin/reference/colldiag.html">colldiag</a></span><span class="op">(</span><span class="va">cars.mod</span>, center<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Condition</span></span>
<span><span class="co">#&gt; Index    -- Variance Decomposition Proportions --</span></span>
<span><span class="co">#&gt;           cylinder engine horse weight accel year </span></span>
<span><span class="co">#&gt; 1   1.000 0.005    0.003  0.005 0.004  0.009 0.010</span></span>
<span><span class="co">#&gt; 2   2.252 0.004    0.002  0.000 0.007  0.022 0.787</span></span>
<span><span class="co">#&gt; 3   2.515 0.004    0.001  0.002 0.010  0.423 0.142</span></span>
<span><span class="co">#&gt; 4   5.660 0.309    0.014  0.306 0.087  0.063 0.005</span></span>
<span><span class="co">#&gt; 5   8.342 0.115    0.000  0.654 0.715  0.469 0.052</span></span>
<span><span class="co">#&gt; 6  10.818 0.563    0.981  0.032 0.176  0.013 0.004</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><span class="citation" data-cites="Belsley:91a">Belsley (<a href="95-references.html#ref-Belsley:91a" role="doc-biblioref">1991</a>)</span> recommends that the sources of collinearity be diagnosed (a) only for those components with large <span class="math inline">\(\kappa_j\)</span>, and (b) for those components for which the variance proportion is large (say, <span class="math inline">\(\ge 0.5\)</span>) on <em>two</em> or more predictors. The print method for <code>"colldiag"</code> objects has a <code>fuzz</code> argument controlling this. The <code>descending = TRUE</code> argument puts the rows with the largest condition indices at the top.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">cd</span>, fuzz <span class="op">=</span> <span class="fl">0.5</span>, descending <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Condition</span></span>
<span><span class="co">#&gt; Index    -- Variance Decomposition Proportions --</span></span>
<span><span class="co">#&gt;           cylinder engine horse weight accel year </span></span>
<span><span class="co">#&gt; 6  10.818 0.563    0.981   .     .      .     .   </span></span>
<span><span class="co">#&gt; 5   8.342  .        .     0.654 0.715   .     .   </span></span>
<span><span class="co">#&gt; 4   5.660  .        .      .     .      .     .   </span></span>
<span><span class="co">#&gt; 3   2.515  .        .      .     .      .     .   </span></span>
<span><span class="co">#&gt; 2   2.252  .        .      .     .      .    0.787</span></span>
<span><span class="co">#&gt; 1   1.000  .        .      .     .      .     .</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The mystery is solved, if you can read that table with these recommendations in mind. There are two nearly collinear relations among the predictors, corresponding to the two smallest dimensions with largest condition indices.</p>
<ul>
<li>Dimension 6 reflects the high correlation between number of cylinders and engine displacement.</li>
<li>Dimension 5 reflects the high correlation between horsepower and weight,</li>
</ul>
<p>Note that the high variance proportion for <code>year</code> (0.787) on the second component creates no problem and should be ignored because (a) the condition index is low and (b) it shares nothing with other predictors.</p>
</section></section><section id="sec-tableplot" class="level2 page-columns page-full" data-number="8.3"><h2 data-number="8.3" class="anchored" data-anchor-id="sec-tableplot">
<span class="header-section-number">8.3</span> Tableplots</h2>
<p>The default tabular display of condition indices and variance proportions from <code><a href="https://friendly.github.io/VisCollin/reference/colldiag.html">colldiag()</a></code> is what triggered the comparison to “Where’s Waldo”. It suffers from the fact that the important information — (a) how many Waldos? (b) where are they hiding — is disguised by being embedded in a sea of mostly irrelevant numbers, just as Waldo is hiding in <a href="#fig-wheres-waldo" class="quarto-xref">Figure&nbsp;<span>8.2</span></a> in a field of stripy things. The simple option of using a principled <code>fuzz</code> factor helps considerably, but not entirely.</p>
<p>The simplified tabular display above can be improved to make the patterns of collinearity more visually apparent and to signify warnings directly to the eyes. A <strong>tableplot</strong> <span class="citation" data-cites="Kwan-etal:2009">(<a href="95-references.html#ref-Kwan-etal:2009" role="doc-biblioref">Kwan et al., 2009</a>)</span> is a semi-graphic display that presents numerical information in a table using shapes proportional to the value in a cell and other visual attributes (shape type, color fill, and so forth) to encode other information.</p>
<p>For collinearity diagnostics, these show:</p>
<ul>
<li><p>the condition indices, using <em>squares</em> whose background color is <span style="color: red;">red</span> for condition indices &gt; 10, <span style="color: brown;">brown</span> for values &gt; 5 and <span style="color: green;">green</span> otherwise, reflecting danger, warning and OK respectively. The value of the condition index is encoded within this using a white square whose side is proportional to the value (up to some maximum value, <code>cond.max</code> that fills the cell).</p></li>
<li><p>Variance decomposition proportions are shown by filled <em>circles</em> whose radius is proportional to those values and are filled (by default) with shades ranging from white through pink to red. Rounded values of those diagnostics are printed in the cells.</p></li>
</ul>
<p>The tableplot below (<a href="#fig-cars-tableplot" class="quarto-xref">Figure&nbsp;<span>8.6</span></a>) encodes all the information from the values of <code><a href="https://friendly.github.io/VisCollin/reference/colldiag.html">colldiag()</a></code> printed above. To aid perception, it uses <code>prop.col</code> color breaks such that variance proportions &lt; 0.3 are shaded white. The visual message is that one should attend to collinearities with large condition indices <strong>and</strong> large variance proportions implicating two or more predictors.</p>

<!-- fig.code: R/cars-colldiag.R -->
<div class="no-row-height column-margin column-container"><div class="">
<p>R file: <code>R/cars-colldiag.R</code></p>
</div></div><div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://friendly.github.io/VisCollin/reference/tableplot.html">tableplot</a></span><span class="op">(</span><span class="va">cd</span>, title <span class="op">=</span> <span class="st">"Tableplot of cars data"</span>, </span>
<span>          cond.max <span class="op">=</span> <span class="fl">30</span> <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-cars-tableplot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-cars-tableplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-cars-tableplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cars-tableplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.6: <strong>Tableplot</strong> of condition indices and variance proportions for the <code>cars</code> data. In column 1, the square symbols are scaled relative to a maximum condition index of 30. In the remaining columns, variance proportions (times 100) are shown as circles scaled relative to a maximum of 100.
</figcaption></figure>
</div>
</div>
</div>
</section><section id="sec-collin-biplots" class="level2" data-number="8.4"><h2 data-number="8.4" class="anchored" data-anchor-id="sec-collin-biplots">
<span class="header-section-number">8.4</span> Collinearity biplots</h2>
<p>As we have seen, the collinearity diagnostics are all functions of the eigenvalues and eigenvectors of the correlation matrix of the predictors in the regression model, or alternatively, the SVD of the <span class="math inline">\(\mathbf{X}\)</span> matrix in the linear model (excluding the constant). The standard biplot <span class="citation" data-cites="Gabriel:71 GowerHand:96">(<a href="95-references.html#ref-Gabriel:71" role="doc-biblioref">Gabriel, 1971</a>; <a href="95-references.html#ref-GowerHand:96" role="doc-biblioref">Gower &amp; Hand, 1996</a>)</span> (see: <a href="04-pca-biplot.html#sec-biplot" class="quarto-xref"><span>Section 4.3</span></a>) can be regarded as a multivariate analog of a scatterplot, obtained by projecting a multivariate sample into a low-dimensional space (typically of 2 or 3 dimensions) accounting for the greatest variance in the data.</p>
<p>However the standard biplot is less useful for visualizing the relations among the predictors that lead to nearly collinear relations. Instead, biplots of the <strong>smallest dimensions</strong> show these relations directly, and can show other features of the data as well, such as outliers and leverage points. We use <code>prcomp(X, scale.=TRUE)</code> to obtain the PCA of the correlation matrix of the predictors:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cars.X</span> <span class="op">&lt;-</span> <span class="va">cars</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/where.html">where</a></span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">mpg</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">tidyr</span><span class="fu">::</span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/drop_na.html">drop_na</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">cars.pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">cars.X</span>, scale. <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">cars.pca</span></span>
<span><span class="co">#&gt; Standard deviations (1, .., p=6):</span></span>
<span><span class="co">#&gt; [1] 2.070 0.911 0.809 0.367 0.245 0.189</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Rotation (n x k) = (6 x 6):</span></span>
<span><span class="co">#&gt;             PC1     PC2    PC3    PC4     PC5     PC6</span></span>
<span><span class="co">#&gt; cylinder -0.454 -0.1869  0.168 -0.659 -0.2711 -0.4725</span></span>
<span><span class="co">#&gt; engine   -0.467 -0.1628  0.134 -0.193 -0.0109  0.8364</span></span>
<span><span class="co">#&gt; horse    -0.462 -0.0177 -0.123  0.620 -0.6123 -0.1067</span></span>
<span><span class="co">#&gt; weight   -0.444 -0.2598  0.278  0.350  0.6860 -0.2539</span></span>
<span><span class="co">#&gt; accel     0.330 -0.2098  0.865  0.143 -0.2774  0.0337</span></span>
<span><span class="co">#&gt; year      0.237 -0.9092 -0.335  0.025 -0.0624  0.0142</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The standard deviations above are the square roots <span class="math inline">\(\sqrt{\lambda_j}\)</span> of the eigenvalues of the correlation matrix, and are returned in the <code>sdev</code> component of the <code>"prcomp"</code> object. The eigenvectors are returned in the <code>rotation</code> component, whose directions are arbitrary. Because we are interested in seeing the relative magnitude of variable vectors, we are free to multiply them by any constant to make them more visible in relation to the scores for the cars.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cars.pca</span><span class="op">$</span><span class="va">rotation</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">2.5</span> <span class="op">*</span> <span class="va">cars.pca</span><span class="op">$</span><span class="va">rotation</span>    <span class="co"># reflect &amp; scale variable vectors</span></span>
<span></span>
<span><span class="va">ggp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_pca.html">fviz_pca_biplot</a></span><span class="op">(</span></span>
<span>  <span class="va">cars.pca</span>,</span>
<span>  axes <span class="op">=</span> <span class="fl">6</span><span class="op">:</span><span class="fl">5</span>,</span>
<span>  geom <span class="op">=</span> <span class="st">"point"</span>,</span>
<span>  col.var <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>  labelsize <span class="op">=</span> <span class="fl">5</span>,</span>
<span>  pointsize <span class="op">=</span> <span class="fl">1.5</span>,</span>
<span>  arrowsize <span class="op">=</span> <span class="fl">1.5</span>,</span>
<span>  addEllipses <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  ggtheme <span class="op">=</span> <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span>base_size <span class="op">=</span> <span class="fl">14</span><span class="op">)</span>,</span>
<span>  title <span class="op">=</span> <span class="st">"Collinearity biplot for cars data"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># add point labels for outlying points</span></span>
<span><span class="va">dsq</span> <span class="op">&lt;-</span> <span class="fu">heplots</span><span class="fu">::</span><span class="fu"><a href="https://friendly.github.io/heplots/reference/Mahalanobis.html">Mahalanobis</a></span><span class="op">(</span><span class="va">cars.pca</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>, <span class="fl">6</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">scores</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">cars.pca</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>, <span class="fl">6</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">scores</span><span class="op">$</span><span class="va">name</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">scores</span><span class="op">)</span></span>
<span></span>
<span><span class="va">ggp</span> <span class="op">+</span> <span class="fu"><a href="https://ggrepel.slowkow.com/reference/geom_text_repel.html">geom_text_repel</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">scores</span><span class="op">[</span><span class="va">dsq</span> <span class="op">&gt;</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">qchisq</a></span><span class="op">(</span><span class="fl">0.95</span>, df <span class="op">=</span> <span class="fl">6</span><span class="op">)</span>,<span class="op">]</span>,</span>
<span>                <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">PC6</span>,</span>
<span>                    y <span class="op">=</span> <span class="va">PC5</span>,</span>
<span>                    label <span class="op">=</span> <span class="va">name</span><span class="op">)</span>,</span>
<span>                vjust <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>,</span>
<span>                size <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-cars-collin-biplot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-cars-collin-biplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-cars-collin-biplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cars-collin-biplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.7: <strong>Collinearity biplot</strong> of the Cars data, showing the last two dimensions. The projections of the variable vectors on the coordinate axes are proportional to their variance proportions. To reduce graphic clutter, only the most outlying observations in predictor space are identified by case labels. An extreme outlier (case 20) appears in the lower right corner.
</figcaption></figure>
</div>
</div>
</div>
<p>As with the tabular display of variance proportions, Waldo is hiding in the dimensions associated with the smallest eigenvalues (largest condition indices). As well, it turns out that outliers in the predictor space (also high leverage observations) can often be seen as observations far from the centroid in the space of the smallest principal components.</p>
<p>The projections of the variable vectors in <a href="#fig-cars-collin-biplot" class="quarto-xref">Figure&nbsp;<span>8.7</span></a> on the Dimension 5 and Dimension 6 axes are proportional to their variance proportions shown above. The relative lengths of these variable vectors can be considered to indicate the extent to which each variable contributes to collinearity for these two near-singular dimensions.</p>
<p>Thus, we see again that Dimension 6 is largely determined by <code>engine</code> size, with a substantial (negative) relation to <code>cylinder</code>. Dimension 5 has its’ strongest relations to <code>weight</code> and <code>horse</code>.</p>
<p>Moreover, there is one observation, #20, that stands out as an outlier in predictor space, far from the centroid. It turns out that this vehicle, a Buick Estate wagon, is an early-year (1970) American behemoth. It had an 8-cylinder, 455 cu. in, 225 horse-power engine, and able to go from 0 to 60 mph in 10 sec. (Its MPG is only slightly under-predicted from the regression model, however.) </p>
<p>With PCA and the biplot, we are used to looking at the dimensions that account for the most variation, but the answer to <em>Where’s Waldo?</em> is that he is hiding in the <em>smallest</em> data dimensions, just as he does in <a href="#fig-wheres-waldo" class="quarto-xref">Figure&nbsp;<span>8.2</span></a> where the weak signals of his stripped shirt, hat and glasses are embedded in a visual field of noise. As we just saw, outliers hide there also, hoping to escape detection. These small dimensions are also implicated in ridge regression as we will see shortly (<a href="#sec-ridge" class="quarto-xref"><span>Section 8.6</span></a>).</p>
</section><section id="sec-remedies" class="level2" data-number="8.5"><h2 data-number="8.5" class="anchored" data-anchor-id="sec-remedies">
<span class="header-section-number">8.5</span> Remedies for collinearity: What can I do?</h2>
<p>Collinearity is often a <strong>data</strong> problem, for which there is no magic cure. Nevertheless there are some general guidelines and useful techniques to address this problem.</p>
<ul>
<li><p><strong>Pure prediction</strong>: If we are only interested in predicting / explaining an outcome, and not the model coefficients or which are “significant”, collinearity can be largely ignored. The fitted values are unaffected by collinearity, even in the case of perfect collinearity as shown in <a href="#fig-collin-demo" class="quarto-xref">Figure&nbsp;<span>8.3</span></a> (b).</p></li>
<li>
<p><strong>Structural collinearity</strong>: Sometimes collinearity results from structural relations among the variables that relate to how they have been defined.</p>
<ul>
<li><p>For example, polynomial terms, like <span class="math inline">\(x, x^2, x^3\)</span> or interaction terms like <span class="math inline">\(x_1, x_2, x_1 * x_2\)</span> are necessarily correlated. A simple cure is to <em>center</em> the predictors at their means, using <span class="math inline">\(x - \bar{x}, (x - \bar{x})^2, (x - \bar{x})^3\)</span> or <span class="math inline">\((x_1 - \bar{x}_1), (x_2 - \bar{x}_2), (x_1 - \bar{x}_1) * (x_2 - \bar{x}_2)\)</span>. Centering removes the spurious ill-conditioning, thus reducing the VIFs. Note that in polynomial models, using <code>y ~ poly(x, 3)</code> to specify a cubic model generates <em>orthogonal</em> (uncorrelated) regressors, whereas in <code>y ~ x + I(x^2) + I(x^3)</code> the terms have built-in correlations.</p></li>
<li><p>When some predictors share a common cause, as in GNP or population in time-series or cross-national data, you can reduce collinearity by re-defining predictors to reflect <em>per capita measures</em>. In a related example with sports data, when you have cumulative totals (e.g., runs, hits, homeruns in baseball) for players over years, expressing these measures as <em>per year</em> will reduce the common effect of longevity on these measures.</p></li>
</ul>
</li>
<li>
<p><strong>Model re-specification</strong>:</p>
<ul>
<li><p>Drop one or more regressors that have a high VIF, if they are not deemed to be essential to understanding the model. Care must be taken here to not omit variables which should be controlled or accounted for in interpretation.</p></li>
<li><p>Replace highly correlated regressors with less correlated linear combination(s) of them. For example, two related variables, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> can be replaced without any loss of information by replacing them with their sum and difference, <span class="math inline">\(z_1 = x_1 + x_2\)</span> and <span class="math inline">\(z_2 = x_1 - x_2\)</span>. For instance, in a dataset on fitness, we may have correlated predictors of resting pulse rate and pulse rate while running. Transforming these to average pulse rate and their difference gives new variables which are interpretable and less correlated.</p></li>
</ul>
</li>
<li>
<p><strong>Statistical remedies</strong>:</p>
<ul>
<li><p>Transform the predictors <span class="math inline">\(\mathbf{X}\)</span> to uncorrelated principal component scores <span class="math inline">\(\mathbf{Z} = \mathbf{X} \mathbf{V}\)</span>, and regress <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{Z}\)</span>. These will have the identical overall model fit without loss of information. A related technique is <em>incomplete</em> principal components regression, where some of the smallest dimensions (those causing collinearity) are omitted from the model. The trade-off is that it may be more difficult to interpret what the model means, but this can be countered with a biplot, showing the projections of the original variables into the reduced space of the principal components.</p></li>
<li><p>Use <strong>regularization methods</strong> such as ridge regression and lasso, which correct for collinearity by introducing shrinking coefficients towards 0, introducing a small amount of bias, . See the <a href="https://CRAN.R-project.org/package=genridge">genridge</a> package and its <a href="https://friendly.github.io/genridge/"><code>pkgdown</code> documentation</a> for visualization methods.</p></li>
<li><p>use Bayesian regression; if multicollinearity prevents a regression coefficient from being estimated precisely, then a prior on that coefficient will help to reduce its posterior variance.</p></li>
</ul>
</li>
</ul>
<!-- **Example**: Centering --><div id="exm-centering" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.2</strong></span> <strong>Centering</strong></p>
<p>To illustrate the effect of centering a predictor in a polynomial model, we generate a perfect quadratic relationship, <span class="math inline">\(y = x^2\)</span> and consider the correlations of <span class="math inline">\(y\)</span> with <span class="math inline">\(x\)</span> and with <span class="math inline">\((x - \bar{x})^2\)</span>. The correlation of <span class="math inline">\(y\)</span> with <span class="math inline">\(x\)</span> is 0.97, while the correlation of <span class="math inline">\(y\)</span> with <span class="math inline">\((x - \bar{x})^2\)</span> is zero.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">20</span></span>
<span><span class="va">y1</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">y2</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">XY</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y1</span>, <span class="va">y2</span><span class="op">)</span></span>
<span></span>
<span><span class="op">(</span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">XY</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;        x    y1    y2</span></span>
<span><span class="co">#&gt; x  1.000 0.971 0.000</span></span>
<span><span class="co">#&gt; y1 0.971 1.000 0.238</span></span>
<span><span class="co">#&gt; y2 0.000 0.238 1.000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The effect of centering here is remove the linear association in what is a purely quadratic relationship, as can be seen by plotting <code>y1</code> and <code>y2</code> against <code>x</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">r1</span> <span class="op">&lt;-</span> <span class="va">R</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">r2</span> <span class="op">&lt;-</span> <span class="va">R</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">]</span></span>
<span></span>
<span><span class="va">gg1</span> <span class="op">&lt;-</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">XY</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, formula <span class="op">=</span> <span class="va">y</span><span class="op">~</span><span class="va">x</span>, linewidth <span class="op">=</span> <span class="fl">2</span>, se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"X"</span>, y <span class="op">=</span> <span class="st">"Y"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span>base_size <span class="op">=</span> <span class="fl">16</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span><span class="st">"text"</span>, x <span class="op">=</span> <span class="fl">5</span>, y <span class="op">=</span> <span class="fl">350</span>, size <span class="op">=</span> <span class="fl">6</span>,</span>
<span>           label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"X Uncentered\nr ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">r1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">gg2</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">XY</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y2</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, formula <span class="op">=</span> <span class="va">y</span><span class="op">~</span><span class="va">x</span>, linewidth <span class="op">=</span> <span class="fl">2</span>, se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"X"</span>, y <span class="op">=</span> <span class="st">"Y"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span>base_size <span class="op">=</span> <span class="fl">16</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span><span class="st">"text"</span>, x <span class="op">=</span> <span class="fl">5</span>, y <span class="op">=</span> <span class="fl">80</span>, size <span class="op">=</span> <span class="fl">6</span>,</span>
<span>           label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"X Centered\nr ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">r2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">gg1</span> <span class="op">+</span> <span class="va">gg2</span>         <span class="co"># show plots side-by-side</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-collin-centering" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-collin-centering-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-collin-centering-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-collin-centering-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.8: Centering a predictor removes the nessessary correlation in a quadratic regression
</figcaption></figure>
</div>
</div>
</div>
</div>
<!-- **Example**: Interactions -->
<div id="exm-interactions" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.3</strong></span> <strong>Interactions</strong></p>
<p>manufacturing process to produce acetylene in relation to reactor temperature (<code>temp</code>), the <code>ratio</code> of two components and the contact <code>time</code> in the reactor. A naive response surface model might suggest that yield is quadratic in time and there are potential interactions among all pairs of predictors.</p>
<!-- fig.code: R/acetlyne-colldiag.R -->
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Acetylene</span>, package <span class="op">=</span> <span class="st">"genridge"</span><span class="op">)</span></span>
<span><span class="va">acetyl.mod0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span></span>
<span>  <span class="va">yield</span> <span class="op">~</span> <span class="va">temp</span> <span class="op">+</span> <span class="va">ratio</span> <span class="op">+</span> <span class="va">time</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">time</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> </span>
<span>          <span class="va">temp</span><span class="op">:</span><span class="va">time</span> <span class="op">+</span> <span class="va">temp</span><span class="op">:</span><span class="va">ratio</span> <span class="op">+</span> <span class="va">time</span><span class="op">:</span><span class="va">ratio</span>,</span>
<span>  data<span class="op">=</span><span class="va">Acetylene</span><span class="op">)</span></span>
<span></span>
<span><span class="op">(</span><span class="va">acetyl.vif0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">acetyl.mod0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;       temp      ratio       time  I(time^2)  temp:time temp:ratio </span></span>
<span><span class="co">#&gt;        383      10555      18080        564       9719       9693 </span></span>
<span><span class="co">#&gt; ratio:time </span></span>
<span><span class="co">#&gt;        225</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>These results are horrible! How much does centering help? I first center all three predictors and then use <code><a href="https://rdrr.io/r/stats/update.html">update()</a></code> to re-fit the same model using the centered data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Acetylene.centered</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">Acetylene</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>temp <span class="op">=</span> <span class="va">temp</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">temp</span><span class="op">)</span>,</span>
<span>         time <span class="op">=</span> <span class="va">time</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">time</span><span class="op">)</span>,</span>
<span>         ratio <span class="op">=</span> <span class="va">ratio</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">ratio</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">acetyl.mod1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span><span class="op">(</span><span class="va">acetyl.mod0</span>, data<span class="op">=</span><span class="va">Acetylene.centered</span><span class="op">)</span></span>
<span><span class="op">(</span><span class="va">acetyl.vif1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">acetyl.mod1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;       temp      ratio       time  I(time^2)  temp:time temp:ratio </span></span>
<span><span class="co">#&gt;      57.09       1.09      81.57      51.49      44.67      30.69 </span></span>
<span><span class="co">#&gt; ratio:time </span></span>
<span><span class="co">#&gt;      33.33</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This is far better, although still not great in terms of VIF. But, how much have we improved the situation by the simple act of centering the predictors? The square roots of the ratios of VIFs tell us the impact of centering on the standard errors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">acetyl.vif0</span> <span class="op">/</span> <span class="va">acetyl.vif1</span><span class="op">)</span></span>
<span><span class="co">#&gt;       temp      ratio       time  I(time^2)  temp:time temp:ratio </span></span>
<span><span class="co">#&gt;       2.59      98.24      14.89       3.31      14.75      17.77 </span></span>
<span><span class="co">#&gt; ratio:time </span></span>
<span><span class="co">#&gt;       2.60</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we use <code>poly(time, 2)</code> in the model for the centered data. Because there are multiple degree of freedom terms in the model, <code><a href="https://rdrr.io/pkg/car/man/vif.html">car::vif()</a></code> calculates GVIFs here. The final column gives <span class="math inline">\(\sqrt{\text{GVIF}^{1/2 \text{df}}}\)</span>, the remaining effect of collinearity on the standard errors of terms in this model.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acetyl.mod2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">yield</span> <span class="op">~</span> <span class="va">temp</span> <span class="op">+</span> <span class="va">ratio</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">time</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> </span>
<span>                          <span class="va">temp</span><span class="op">:</span><span class="va">time</span> <span class="op">+</span> <span class="va">temp</span><span class="op">:</span><span class="va">ratio</span> <span class="op">+</span> <span class="va">time</span><span class="op">:</span><span class="va">ratio</span>,</span>
<span>                  data<span class="op">=</span><span class="va">Acetylene.centered</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">acetyl.mod2</span>, type <span class="op">=</span> <span class="st">"term"</span><span class="op">)</span></span>
<span><span class="co">#&gt;                  GVIF Df GVIF^(1/(2*Df))</span></span>
<span><span class="co">#&gt; temp            57.09  1            7.56</span></span>
<span><span class="co">#&gt; ratio            1.09  1            1.05</span></span>
<span><span class="co">#&gt; poly(time, 2) 1733.56  2            6.45</span></span>
<span><span class="co">#&gt; temp:time       44.67  1            6.68</span></span>
<span><span class="co">#&gt; temp:ratio      30.69  1            5.54</span></span>
<span><span class="co">#&gt; ratio:time      33.33  1            5.77</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section><section id="sec-ridge" class="level2" data-number="8.6"><h2 data-number="8.6" class="anchored" data-anchor-id="sec-ridge">
<span class="header-section-number">8.6</span> Ridge regression</h2>
<p>Ridge regression is an instance of a class of techniques designed to obtain more favorable predictions at the expense of some increase in bias, compared to ordinary least squares (OLS) estimation. These methods began as a way of solving collinearity problems in OLS regression with highly correlated predictors <span class="citation" data-cites="HoerlKennard:1970a">(<a href="95-references.html#ref-HoerlKennard:1970a" role="doc-biblioref">Hoerl &amp; Kennard, 1970</a>)</span>.</p>
<p>More recently ridge regression developed to a larger class of model selection methods, of which the LASSO method of <span class="citation" data-cites="Tibshriani:regr:1996">Tibshirani (<a href="95-references.html#ref-Tibshriani:regr:1996" role="doc-biblioref">1996</a>)</span> and LAR method of <span class="citation" data-cites="Efron-etal:leas:2004">Efron et al. (<a href="95-references.html#ref-Efron-etal:leas:2004" role="doc-biblioref">2004</a>)</span> are well-known instances. See, for example, the reviews in <span class="citation" data-cites="Vinod:1978">Vinod (<a href="95-references.html#ref-Vinod:1978" role="doc-biblioref">1978</a>)</span> and <span class="citation" data-cites="McDonald:2009">McDonald (<a href="95-references.html#ref-McDonald:2009" role="doc-biblioref">2009</a>)</span> for details and context omitted here. The case of ridge regression has also been extended to the case of two or more response variables <span class="citation" data-cites="Brown-Zidek-1980 Haitovsky1987">(<a href="95-references.html#ref-Brown-Zidek-1980" role="doc-biblioref">Brown &amp; Zidek, 1980</a>; <a href="95-references.html#ref-Haitovsky1987" role="doc-biblioref">Haitovsky, 1987</a>)</span>.</p>
<p>An essential idea behind these methods is that the OLS estimates are constrained in some way, shrinking them, on average, toward zero, to achieve increased predictive accuracy at the expense of some increase in bias. Another common characteristic is that they involve some tuning parameter (<span class="math inline">\(k\)</span>) or criterion to quantify the tradeoff between bias and variance. In many cases, analytical or computationally intensive methods have been developed to choose an optimal value of the tuning parameter, for example using generalized cross validation, bootstrap methods.</p>
<p>A common means to visualize the effects of shrinkage in these problems is to make what are called <em>univariate ridge trace plots</em> (<a href="#sec-ridge-univar" class="quarto-xref"><span>Section 8.7</span></a>) showing how the estimated coefficients <span class="math inline">\(\widehat{\boldsymbol{\beta}}_k\)</span> change as the shrinkage criterion <span class="math inline">\(k\)</span> increases. (An example is shown in <a href="#fig-longley-traceplot1" class="quarto-xref">Figure&nbsp;<span>8.10</span></a> below.) But this only provides a view of bias. It is the wrong graphic form for a multivariate problem where we want to visualize bias in the coefficients <span class="math inline">\(\widehat{\boldsymbol{\beta}}_k\)</span> vs.&nbsp;their precision, as reflected in their estimated variances, <span class="math inline">\(\widehat{\textsf{Var}} (\widehat{\boldsymbol{\beta}}_k)\)</span>. A more useful graphic plots the confidence ellipses for the coefficients, showing both bias and precision (<a href="#sec-ridge-bivar" class="quarto-xref"><span>Section 8.8</span></a>). Some of the material below borrows from <span class="citation" data-cites="Friendly-2011-gentalk">Friendly (<a href="95-references.html#ref-Friendly-2011-gentalk" role="doc-biblioref">2011</a>)</span> and <span class="citation" data-cites="Friendly:genridge:2013">Friendly (<a href="95-references.html#ref-Friendly:genridge:2013" role="doc-biblioref">2013</a>)</span>.</p>
<section id="properties-of-ridge-regression" class="level3" data-number="8.6.1"><h3 data-number="8.6.1" class="anchored" data-anchor-id="properties-of-ridge-regression">
<span class="header-section-number">8.6.1</span> Properties of ridge regression</h3>
<p>To provide some context, I summarize the properties of ridge regression below, comparing the OLS estimates with their ridge counterparts. To avoid unnecessary details related to the intercept, assume the predictors have been centered at their means and the unit vector is omitted from <span class="math inline">\(\mathbf{X}\)</span>. Further, to avoid scaling issues, we standardize the columns of <span class="math inline">\(\mathbf{X}\)</span> to unit length, so that <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X}\)</span> is a also correlation matrix.</p>
<p>The ordinary least squares estimates of coefficients and their estimated variance covariance matrix take the (hopefully now) familiar form</p>
<p><span id="eq-OLS-beta-var"><span class="math display">\[\begin{aligned}
\widehat{\boldsymbol{\beta}}^{\mathrm{OLS}} = &amp;
    (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1} \mathbf{X}^\mathsf{T}\mathbf{y} \:\: ,\\
\widehat{\mathsf{Var}} (\widehat{\boldsymbol{\beta}}^{\mathrm{OLS}}) = &amp;
    \widehat{\sigma}_{\epsilon}^2 (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}.
\end{aligned} \tag{8.2}\]</span></span></p>
<p>As we saw ealier, one signal of the problem of collinearity is that the determinant <span class="math inline">\(\mathrm{det}(\mathbf{X}^\mathsf{T}\mathbf{X})\)</span> approaches zero as the predictors become more collinear. The inverse <span class="math inline">\((\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\)</span> becomes numerically unstable, or does not exist if the determinant becomes zero in the case of exact dependency of one variable on the others.</p>
<p>Ridge regression uses a trick to avoid this. It adds a constant, <span class="math inline">\(k\)</span> to the diagonal elements, replacing <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X}\)</span> with <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X} + k \mathbf{I}\)</span> in <a href="#eq-OLS-beta-var" class="quarto-xref">Equation&nbsp;<span>8.2</span></a>. This drives the determinant away from zero as <span class="math inline">\(k\)</span> increases. The ridge regression estimates then become,</p>
<p><span id="eq-ridge-beta-var"><span class="math display">\[\begin{aligned}
\widehat{\boldsymbol{\beta}}^{\mathrm{RR}}_k = &amp;
    (\mathbf{X}^\mathsf{T}\mathbf{X} + k \mathbf{I})^{-1} \mathbf{X}^\mathsf{T}\mathbf{y}  \\
                                    = &amp; \mathbf{G}_k \, \widehat{\boldsymbol{\beta}}^{\mathrm{OLS}} \:\: ,\\
\widehat{\mathsf{Var}} (\widehat{\boldsymbol{\beta}}^{\mathrm{RR}}_k) = &amp;
     \widehat{\sigma}^2  \mathbf{G}_k (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1} \mathbf{G}_k^\mathsf{T}\:\: ,
\end{aligned} \tag{8.3}\]</span></span></p>
<p>where <span class="math inline">\(\mathbf{G}_k = \left[\mathbf{I} + k (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1} \right] ^{-1}\)</span> is the <span class="math inline">\((p \times p)\)</span> <em>shrinkage</em> matrix. Thus, as <span class="math inline">\(k\)</span> increases, <span class="math inline">\(\mathbf{G}_k\)</span> decreases, and drives <span class="math inline">\(\widehat{\mathbf{\beta}}^{\mathrm{RR}}_k\)</span> toward <span class="math inline">\(\mathbf{0}\)</span> <span class="citation" data-cites="HoerlKennard:1970a">(<a href="95-references.html#ref-HoerlKennard:1970a" role="doc-biblioref">Hoerl &amp; Kennard, 1970</a>)</span>.</p>
<p>Another insight, from the shrinkage literature, is that ridge regression can be formulated as least squares regression, minimizing a residual sum of squares, <span class="math inline">\(\text{RSS}(k)\)</span>, which adds a penalty for large coefficients,</p>
<p><span id="eq-ridgeRSS"><span class="math display">\[
\text{RSS}(k) = (\mathbf{y}-\mathbf{X} \mathbf{\beta}) ^\mathsf{T}(\mathbf{y}-\mathbf{X} \boldsymbol{\beta}) + k \boldsymbol{\beta}^\mathsf{T}\boldsymbol{\beta} \quad\quad (k \ge 0)
\:\: ,
\tag{8.4}\]</span></span> where the penalty restrict the coefficients to some squared length <span class="math inline">\(\boldsymbol{\beta}^\mathsf{T}\boldsymbol{\beta} = \Sigma \beta_i \le t(k)\)</span>.</p>
<p>The geometry of ridge regession is illustrated in <a href="#fig-ridge-demo" class="quarto-xref">Figure&nbsp;<span>8.9</span></a> for two coefficients <span class="math inline">\(\boldsymbol{\beta} = (\beta_1, \beta_2)\)</span>. The <span style="color: blue;">blue circles</span> at the origin, having radii <span class="math inline">\(\sqrt{t_k}\)</span>, show the constraint that the sum of squares of coefficients, <span class="math inline">\(\boldsymbol{\beta}^\mathsf{T}\boldsymbol{\beta} = \beta_1^2 + \beta_2^2\)</span> be less than <span class="math inline">\(k\)</span>. The <span style="color: red;">red ellipses</span> show contours of the covariance ellipse of <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{\mathrm{OLS}}\)</span>. As the shrinkage constant <span class="math inline">\(k\)</span> increases, the center of these ellipses travel along the path illustrated toward <span class="math inline">\(\boldsymbol{\beta} = \mathbf{0}\)</span> This path is called the <em>locus of osculation</em>, the path along which circles or ellipses first kiss as they expand, like the pattern of ripples from rocks dropped into a pond <span class="citation" data-cites="Friendly-etal:ellipses:2013">(<a href="95-references.html#ref-Friendly-etal:ellipses:2013" role="doc-biblioref">Friendly et al., 2013</a>)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ridge-demo" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ridge-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ridge-demo.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ridge-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.9: Geometric interpretation of ridge regression, using elliptical contours of the <span class="math inline">\(\text{RSS}(k)\)</span> function. The blue circles at the origin show the constraint that the sum of squares of coefficients, <span class="math inline">\(\boldsymbol{\beta}^\mathsf{T}\boldsymbol{\beta}\)</span> be less than <span class="math inline">\(k\)</span>. The red ellipses show the covariance ellipse of two coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span>. Ridge regression finds the point <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{\mathrm{RR}}_k\)</span> where the OLS contours just kiss the constraint region. <em>Source</em>: <span class="citation" data-cites="Friendly-etal:ellipses:2013">Friendly et al. (<a href="95-references.html#ref-Friendly-etal:ellipses:2013" role="doc-biblioref">2013</a>)</span>.
</figcaption></figure>
</div>
</div>
</div>
<!-- The blue circles at the origin show the constraint that the sum of squares of coefficients, $\\boldsymbol{\\beta}\\trans \\boldsymbol{\\beta}$ be less than $k$. The red ellipses show the covariance ellipse of two coefficients $\\boldsymbol{\\beta}$
-->
<p><a href="#eq-ridge-beta-var" class="quarto-xref">Equation&nbsp;<span>8.3</span></a> is computationally expensive, potentially numerically unstable for small <span class="math inline">\(k\)</span>, and it is conceptually opaque, in that it sheds little light on the underlying geometry of the data in the column space of <span class="math inline">\(\mathbf{X}\)</span>. An alternative formulation can be given in terms of the singular value decomposition (SVD) of <span class="math inline">\(\mathbf{X}\)</span>,</p>
<p><span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{D} \mathbf{V}^\mathsf{T}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are respectively <span class="math inline">\(n\times p\)</span> and <span class="math inline">\(p\times p\)</span> orthonormal matrices, so that <span class="math inline">\(\mathbf{U}^\mathsf{T}\mathbf{U} = \mathbf{V}^\mathsf{T}\mathbf{V} = \mathbf{I}\)</span>, and <span class="math inline">\(\mathbf{D} = \mathrm{diag}\, (d_1, d_2, \dots d_p)\)</span> is the diagonal matrix of ordered singular values, with entries <span class="math inline">\(d_1 \ge d_2 \ge \cdots \ge d_p \ge 0\)</span>.</p>
<p>Because <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X} = \mathbf{V} \mathbf{D}^2 \mathbf{V}^\mathsf{T}\)</span>, the eigenvalues of <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X}\)</span> are given by <span class="math inline">\(\mathbf{D}^2\)</span> and therefore the eigenvalues of <span class="math inline">\(\mathbf{G}_k\)</span> can be shown <span class="citation" data-cites="HoerlKennard:1970a">(<a href="95-references.html#ref-HoerlKennard:1970a" role="doc-biblioref">Hoerl &amp; Kennard, 1970</a>)</span> to be the diagonal elements of</p>
<p><span class="math display">\[
\mathbf{D}(\mathbf{D}^2 + k \mathbf{I} )^{-1} \mathbf{D} = \mathrm{diag}\,  \left(\frac{d_i^2}{d_i^2 + k}\right) \:\: .
\]</span></p>
<p>Noting that the eigenvectors, <span class="math inline">\(\mathbf{V}\)</span> are the principal component vectors, and that <span class="math inline">\(\mathbf{X} \mathbf{V} = \mathbf{U} \mathbf{D}\)</span>, the ridge estimates can be calculated more simply in terms of <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{D}\)</span> as</p>
<p><span class="math display">\[
\widehat{\boldsymbol{\beta}}^{\mathrm{RR}}_k = (\mathbf{D}^2 + k \mathbf{I})^{-1} \mathbf{D} \mathbf{U}^\mathsf{T}\mathbf{y} = \left( \frac{d_i}{d_i^2 + k}\right) \: \mathbf{u}_i^\mathsf{T}\mathbf{y}, \quad i=1, \dots p \:\: .
\]</span></p>
<p>The terms <span class="math inline">\(d^2_i / (d_i^2 + k) \le 1\)</span> are thus the factors by which the coordinates of <span class="math inline">\(\mathbf{u}_i^\mathsf{T}\mathbf{y}\)</span> are shrunk with respect to the orthonormal basis for the column space of <span class="math inline">\(\mathbf{X}\)</span>. The small singular values <span class="math inline">\(d_i\)</span> correspond to the directions which ridge regression shrinks the most. These are the directions which contribute most to collinearity, discussed earlier.</p>
<p>This analysis also provides an alternative and more intuitive characterization of the ridge tuning constant. By analogy with OLS, where the hat matrix, <span class="math inline">\(\mathbf{H} = \mathbf{X} (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1} \mathbf{X}^\mathsf{T}\)</span> reflects degrees of freedom <span class="math inline">\(\text{df} = \mathrm{tr} (\mathbf{H}) = p\)</span> corresponding to the <span class="math inline">\(p\)</span> parameters, the effective degrees of freedom for ridge regression <span class="citation" data-cites="Hastie-etal-2009">(<a href="95-references.html#ref-Hastie-etal-2009" role="doc-biblioref">Hastie et al., 2009</a>)</span> is</p>
<p><span id="eq-dfk"><span class="math display">\[\begin{aligned}
\text{df}_k
    = &amp; \text{tr}[\mathbf{X} (\mathbf{X}^\mathsf{T}\mathbf{X} + k \mathbf{I})^{-1} \mathbf{X}^\mathsf{T}] \\
    = &amp; \sum_i^p \text{df}_k(i) = \sum_i^p \left( \frac{d_i^2}{d_i^2 + k} \right) \:\: .
\end{aligned} \tag{8.5}\]</span></span></p>
<p><span class="math inline">\(\text{df}_k\)</span> is a monotone decreasing function of <span class="math inline">\(k\)</span>, and hence any set of ridge constants can be specified in terms of equivalent <span class="math inline">\(\text{df}_k\)</span>. Greater shrinkage corresponds to fewer coefficients being estimated.</p>
<p>There is a close connection with principal components regression mentioned in <a href="#sec-remedies" class="quarto-xref"><span>Section 8.5</span></a>. Ridge regression shrinks <em>all</em> dimensions in proportion to <span class="math inline">\(\text{df}_k(i)\)</span>, so the low variance dimensions are shrunk more. Principal components regression discards the low variance dimensions and leaves the high variance dimensions unchanged.</p>
</section><section id="the-genridge-package" class="level3" data-number="8.6.2"><h3 data-number="8.6.2" class="anchored" data-anchor-id="the-genridge-package">
<span class="header-section-number">8.6.2</span> The <code>genridge</code> package</h3>
<p>Ridge regression and other shrinkage methods are available in several packages including <span style="color: brown;"><strong>MASS</strong></span> (the <code><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">lm.ridge()</a></code> function), <span style="color: brown;"><strong>glmnet</strong></span> <span class="citation" data-cites="R-glmnet">(<a href="95-references.html#ref-R-glmnet" role="doc-biblioref">Friedman et al., 2025</a>)</span>, and <span style="color: brown;"><strong>penalized</strong></span> <span class="citation" data-cites="R-penalized">(<a href="95-references.html#ref-R-penalized" role="doc-biblioref">Goeman et al., 2022</a>)</span>, but none of these provides insightful graphical displays. <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet::glmnet()</a></code> also implements a method for multivariate responses with a `family=“mgaussian”.</p>
<p>Here, I focus in the <span style="color: brown;"><strong>genridge</strong></span> package package <span class="citation" data-cites="R-genridge">(<a href="95-references.html#ref-R-genridge" role="doc-biblioref">Friendly, 2024</a>)</span>, where the <code><a href="https://friendly.github.io/genridge/reference/ridge.html">ridge()</a></code> function is the workhorse and <code>pca.ridge()</code> transforms these results to PCA/SVD space. <code>vif.ridge()</code> calculates VIFs for class <code>"ridge"</code> objects and <code><a href="https://friendly.github.io/genridge/reference/precision.html">precision()</a></code> calculates precision and shrinkage measures.</p>
<p>A variety of plotting functions is available for univariate, bivariate and 3D plots:</p>
<ul>
<li>
<code><a href="https://friendly.github.io/genridge/reference/traceplot.html">traceplot()</a></code> Traditional univariate ridge trace plots</li>
<li>
<code>plot.ridge()</code> Bivariate 2D ridge trace plots, showing the covariance ellipse of the estimated coefficients</li>
<li>
<code>pairs.ridge()</code> All pairwise bivariate ridge trace plots</li>
<li>
<code>plot3d.ridge()</code> 3D ridge trace plots with ellipsoids</li>
<li>
<code>biplot.ridge()</code> ridge trace plots in PCA/SVD space</li>
</ul>
<p>In addition, the <code><a href="https://friendly.github.io/genridge/reference/pca.html">pca()</a></code> method for <code>"ridge"</code> objects transforms the coefficients and covariance matrices of a ridge object from predictor space to the equivalent, but more interesting space of the PCA of <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X}\)</span> or the SVD of <span class="math inline">\(\mathbf{X}\)</span>. <code>biplot.pcaridge()</code> adds variable vectors to the bivariate plots of coefficients in PCA space</p>
</section></section><section id="sec-ridge-univar" class="level2" data-number="8.7"><h2 data-number="8.7" class="anchored" data-anchor-id="sec-ridge-univar">
<span class="header-section-number">8.7</span> Univariate ridge trace plots</h2>
<p>A classic example for ridge regression is Longley’s <span class="citation" data-cites="Longley:1967">(<a href="95-references.html#ref-Longley:1967" role="doc-biblioref">1967</a>)</span> data, consisting of 7 economic variables, observed yearly from 1947 to 1962 (n=16), in the dataset <code>longley</code>. The goal is to predict Employed from <code>GNP</code>, <code>Unemployed</code>, <code>Armed.Forces</code>, <code>Population</code>, <code>Year</code>, and <code>GNP.deflator</code>.</p>
<!-- fig.code: taken from genridge  README.Rmd -->
<!-- fig.code: R/genridge-longley-figs1.R -->
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">longley</span>, package<span class="op">=</span><span class="st">"datasets"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">longley</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'data.frame':  16 obs. of  7 variables:</span></span>
<span><span class="co">#&gt;  $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...</span></span>
<span><span class="co">#&gt;  $ GNP         : num  234 259 258 285 329 ...</span></span>
<span><span class="co">#&gt;  $ Unemployed  : num  236 232 368 335 210 ...</span></span>
<span><span class="co">#&gt;  $ Armed.Forces: num  159 146 162 165 310 ...</span></span>
<span><span class="co">#&gt;  $ Population  : num  108 109 110 111 112 ...</span></span>
<span><span class="co">#&gt;  $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...</span></span>
<span><span class="co">#&gt;  $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>These data were constructed to illustrate numerical problems in least squares software at the time, and they are (purposely) perverse, in that:</p>
<ul>
<li>Each variable is a time series so that there is clearly a lack of independence among predictors. <code>Year</code> is at least implicitly correlated with most of the others.</li>
<li>Worse, there is also some structural collinearity among the variables <code>GNP</code>, <code>Year</code>, <code>GNP.deflator</code>, and <code>Population</code>; for example, <code>GNP.deflator</code> is a multiplicative factor to account for inflation.</li>
</ul>
<p>We fit the regression model, and sure enough, there are some extremely large VIFs. The largest, for <code>GNP</code> represents a multiplier of <span class="math inline">\(\sqrt{1788.5} = 42.3\)</span> on the standard errors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb23" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">longley.lm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Employed</span> <span class="op">~</span> <span class="va">GNP</span> <span class="op">+</span> <span class="va">Unemployed</span> <span class="op">+</span> <span class="va">Armed.Forces</span> <span class="op">+</span> </span>
<span>                            <span class="va">Population</span> <span class="op">+</span> <span class="va">Year</span> <span class="op">+</span> <span class="va">GNP.deflator</span>, </span>
<span>                 data<span class="op">=</span><span class="va">longley</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">longley.lm</span><span class="op">)</span></span>
<span><span class="co">#&gt;          GNP   Unemployed Armed.Forces   Population         Year </span></span>
<span><span class="co">#&gt;      1788.51        33.62         3.59       399.15       758.98 </span></span>
<span><span class="co">#&gt; GNP.deflator </span></span>
<span><span class="co">#&gt;       135.53</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Shrinkage values can be specified using <span class="math inline">\(k\)</span> (where <span class="math inline">\(k = 0\)</span> corresponds to OLS) or the equivalent degrees of freedom <span class="math inline">\(\text{df}_k\)</span> (<a href="#eq-dfk" class="quarto-xref">Equation&nbsp;<span>8.5</span></a>). (The function uses argument <code>lambda</code>, <span class="math inline">\(\lambda \equiv k\)</span> for the shrinkage constant.) Among other quantities, <code><a href="https://friendly.github.io/genridge/reference/ridge.html">ridge()</a></code> returns a matrix containing the coefficients for each predictor for each shrinkage value and other quantities.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.002</span>, <span class="fl">0.005</span>, <span class="fl">0.01</span>, <span class="fl">0.02</span>, <span class="fl">0.04</span>, <span class="fl">0.08</span><span class="op">)</span></span>
<span><span class="va">lridge</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://friendly.github.io/genridge/reference/ridge.html">ridge</a></span><span class="op">(</span><span class="va">Employed</span> <span class="op">~</span> <span class="va">GNP</span> <span class="op">+</span> <span class="va">Unemployed</span> <span class="op">+</span> <span class="va">Armed.Forces</span> <span class="op">+</span> </span>
<span>                           <span class="va">Population</span> <span class="op">+</span> <span class="va">Year</span> <span class="op">+</span> <span class="va">GNP.deflator</span>, </span>
<span>    data<span class="op">=</span><span class="va">longley</span>, lambda<span class="op">=</span><span class="va">lambda</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">lridge</span>, digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; Ridge Coefficients:</span></span>
<span><span class="co">#&gt;        GNP     Unemployed  Armed.Forces  Population  Year  </span></span>
<span><span class="co">#&gt; 0.000  -3.447  -1.828      -0.696        -0.344       8.432</span></span>
<span><span class="co">#&gt; 0.002  -2.114  -1.644      -0.658        -0.713       7.466</span></span>
<span><span class="co">#&gt; 0.005  -1.042  -1.491      -0.623        -0.936       6.567</span></span>
<span><span class="co">#&gt; 0.010  -0.180  -1.361      -0.588        -1.003       5.656</span></span>
<span><span class="co">#&gt; 0.020   0.499  -1.245      -0.548        -0.868       4.626</span></span>
<span><span class="co">#&gt; 0.040   0.906  -1.155      -0.504        -0.523       3.577</span></span>
<span><span class="co">#&gt; 0.080   1.091  -1.086      -0.458        -0.086       2.642</span></span>
<span><span class="co">#&gt;        GNP.deflator</span></span>
<span><span class="co">#&gt; 0.000   0.157      </span></span>
<span><span class="co">#&gt; 0.002   0.022      </span></span>
<span><span class="co">#&gt; 0.005  -0.042      </span></span>
<span><span class="co">#&gt; 0.010  -0.026      </span></span>
<span><span class="co">#&gt; 0.020   0.098      </span></span>
<span><span class="co">#&gt; 0.040   0.321      </span></span>
<span><span class="co">#&gt; 0.080   0.570</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The standard univariate plot, given by <code><a href="https://friendly.github.io/genridge/reference/traceplot.html">traceplot()</a></code>, simply plots the estimated coefficients for each predictor against the shrinkage factor <span class="math inline">\(k\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://friendly.github.io/genridge/reference/traceplot.html">traceplot</a></span><span class="op">(</span><span class="va">lridge</span>, </span>
<span>          X <span class="op">=</span> <span class="st">"lambda"</span>,</span>
<span>          xlab <span class="op">=</span> <span class="st">"Ridge constant (k)"</span>,</span>
<span>          xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.02</span>, <span class="fl">0.08</span><span class="op">)</span>, cex.lab<span class="op">=</span><span class="fl">1.25</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-longley-traceplot1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-longley-traceplot1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-longley-traceplot1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2100">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longley-traceplot1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.10: Univariate ridge trace plot for the coefficients of predictors of Employment in Longley’s data via ridge regression, with ridge constants k = (0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08). The dotted lines show optimal values for shrinkage by two criteria (HKB, LW).
</figcaption></figure>
</div>
</div>
</div>
<p>You can see that the coefficients for Year and GNP are shrunk considerably. Differences from the <span class="math inline">\(\beta\)</span> value at <span class="math inline">\(k =0\)</span> represent the bias (smaller <span class="math inline">\(\mid \beta \mid\)</span>) needed to achieve more stable estimates.</p>
<p>The dotted lines in <a href="#fig-longley-traceplot1" class="quarto-xref">Figure&nbsp;<span>8.10</span></a> show choices for the ridge constant by two commonly used criteria to balance bias against precision due to <span class="citation" data-cites="Hoerl-etal-1975">Hoerl et al. (<a href="95-references.html#ref-Hoerl-etal-1975" role="doc-biblioref">1975</a>)</span> (<code>HKB</code>) and <span class="citation" data-cites="LawlessWang:1976">Lawless &amp; Wang (<a href="95-references.html#ref-LawlessWang:1976" role="doc-biblioref">1976</a>)</span> (<code>LW</code>). These values (along with a generalized cross-validation value <code>GCV</code>) are also stored in the “ridge” object as a vector <code>criteria</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lridge</span><span class="op">$</span><span class="va">criteria</span></span>
<span><span class="co">#&gt;    kHKB     kLW    kGCV </span></span>
<span><span class="co">#&gt; 0.00428 0.03230 0.00200</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<!-- These values seem rather small, but note that the coefficients for Year and GNP are shrunk considerably. -->
<p>The shrinkage constant <span class="math inline">\(k\)</span> doesn’t have much intrinsic meaning, so it is often easier to interpret the plot when coefficients are plotted against the equivalent degrees of freedom, <span class="math inline">\(\text{df}_k\)</span>. OLS corresponds to <span class="math inline">\(\text{df}_k = 6\)</span> degrees of freedom in the space of six parameters, and the effect of shrinkage is to decrease the degrees of freedom, as if estimating fewer parameters.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> This more natural scale also makes the changes in coefficient with shrinkage more nearly linear.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb27" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://friendly.github.io/genridge/reference/traceplot.html">traceplot</a></span><span class="op">(</span><span class="va">lridge</span>, </span>
<span>          X <span class="op">=</span> <span class="st">"df"</span>,</span>
<span>          xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">6.2</span><span class="op">)</span>, cex.lab<span class="op">=</span><span class="fl">1.25</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-longley-traceplot2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-longley-traceplot2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-longley-traceplot2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2100">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longley-traceplot2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.11: Univariate ridge trace plot using equivalent degrees of freedom, <span class="math inline">\(\text{df}_k\)</span> to specify shrinkage. This scale is easier to understand and makes the traces of prarameters more nearly linear.
</figcaption></figure>
</div>
</div>
</div>
<section id="whats-not-to-like" class="level3" data-number="8.7.1"><h3 data-number="8.7.1" class="anchored" data-anchor-id="whats-not-to-like">
<span class="header-section-number">8.7.1</span> What’s not to like?</h3>
<p>A bigger problem is that these univariate plots are the <strong>wrong kind</strong> of plot! They show the trends in increased bias (toward smaller <span class="math inline">\(\mid \beta \mid\)</span>) associated with larger <span class="math inline">\(k\)</span>, but they do not show the accompanying increase in <em>precision</em> (decrease in variance) achieved by allowing a bit of bias.</p>
<p>For that, we need to consider the variances and covariances of the estimated coefficients. The univariate trace plot is the wrong graphic form for what is essentially a multivariate problem, where we would like to visualize how <em>both</em> coefficients and their variances change with <span class="math inline">\(k\)</span>.</p>
</section></section><section id="sec-ridge-bivar" class="level2" data-number="8.8"><h2 data-number="8.8" class="anchored" data-anchor-id="sec-ridge-bivar">
<span class="header-section-number">8.8</span> Bivariate ridge trace plots</h2>
<p>The bivariate analog of the trace plot suggested by <span class="citation" data-cites="Friendly:genridge:2013">Friendly (<a href="95-references.html#ref-Friendly:genridge:2013" role="doc-biblioref">2013</a>)</span> plots bivariate confidence ellipses for <em>pairs</em> of coefficients. Their centers, <span class="math inline">\((\widehat{\beta}_i, \widehat{\beta}_j)\)</span> compared to the OLS values show the bias induced for each coefficient, and also how the change in the ridge estimate for one parameter is related to changes for other parameters.</p>
<p>The size and shapes of the covariance ellipses show directly the effect on precision of the estimates as a function of the ridge tuning constant. and their size and shape indicate sampling variance, <span class="math inline">\(\widehat{\text{Var}} (\mathbf{\widehat{\beta}}_{ij})\)</span>.</p>
<p>Here (<a href="#fig-longley-plot-ridge" class="quarto-xref">Figure&nbsp;<span>8.12</span></a>), I plot those for GNP against four of the other predictors. The <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> method for <code>"ridge"</code> objects plots these ellipses for a pair of variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb28" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">clr</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"brown"</span>, <span class="st">"darkgreen"</span>,<span class="st">"blue"</span>, <span class="st">"cyan4"</span>, <span class="st">"magenta"</span><span class="op">)</span></span>
<span><span class="va">pch</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">15</span><span class="op">:</span><span class="fl">18</span>, <span class="fl">7</span>, <span class="fl">9</span>, <span class="fl">12</span><span class="op">)</span></span>
<span><span class="va">lambdaf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="op">~</span><span class="fu">widehat</span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">^</span><span class="va">OLS</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/character.html">as.character</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="fl">5</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lridge</span>, variables<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">i</span><span class="op">)</span>, </span>
<span>       radius<span class="op">=</span><span class="fl">0.5</span>, cex.lab<span class="op">=</span><span class="fl">1.5</span>, col<span class="op">=</span><span class="va">clr</span>, </span>
<span>       labels<span class="op">=</span><span class="cn">NULL</span>, fill<span class="op">=</span><span class="cn">TRUE</span>, fill.alpha<span class="op">=</span><span class="fl">0.2</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="va">lridge</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">]</span>, <span class="va">lridge</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span>,<span class="va">i</span><span class="op">]</span>, </span>
<span>       <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="op">~</span><span class="fu">widehat</span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">^</span><span class="va">OLS</span><span class="op">)</span>, </span>
<span>       cex<span class="op">=</span><span class="fl">1.5</span>, pos<span class="op">=</span><span class="fl">4</span>, offset<span class="op">=</span><span class="fl">.1</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="va">lridge</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">i</span><span class="op">)</span><span class="op">]</span>, <span class="va">lambdaf</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span>, pos<span class="op">=</span><span class="fl">3</span>, cex<span class="op">=</span><span class="fl">1.3</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-longley-plot-ridge" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-longley-plot-ridge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-longley-plot-ridge-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longley-plot-ridge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.12: <strong>Bivariate ridge trace</strong> plots for the coefficients of four predictors against the coefficient for GNP in Longley’s data, with k = 0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08. In most cases, the coefficients are driven toward zero, but the bivariate plot also makes clear the reduction in variance, as well as the bivariate path of shrinkage.
</figcaption></figure>
</div>
</div>
</div>
<p>As can be seen, the coefficients for each pair of predictors trace a graceful path generally toward the origin (0,0), and the covariance ellipses get smaller, indicating increased precision. Most often these paths are rather direct, but it takes a peculiar curvilinear route in the case of population and GNP here.</p>
<p>The <code><a href="https://rdrr.io/r/graphics/pairs.html">pairs()</a></code> method for <code>"ridge"</code> objects shows all pairwise views in scatterplot matrix form. <code>radius</code> sets the base size of the ellipse-generating circle for the covariance ellipses.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb29" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/pairs.html">pairs</a></span><span class="op">(</span><span class="va">lridge</span>, radius<span class="op">=</span><span class="fl">0.5</span>, diag.cex <span class="op">=</span> <span class="fl">2</span>, </span>
<span>      fill <span class="op">=</span> <span class="cn">TRUE</span>, fill.alpha <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-longley-pairs" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-longley-pairs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-longley-pairs-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longley-pairs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.13: Scatterplot matrix of bivariate ridge trace plots. Each panel shows the effect of shrinkage on the covariance ellipse for a given pair of predictors.
</figcaption></figure>
</div>
</div>
</div>
<p>Most of the shrinkage paths in <a href="#fig-longley-pairs" class="quarto-xref">Figure&nbsp;<span>8.13</span></a> are regular, but those involving population are curvilinear, reflecting more complex behavior in the ridge method.</p>
<section id="visualizing-the-bias-variance-tradeoff" class="level3" data-number="8.8.1"><h3 data-number="8.8.1" class="anchored" data-anchor-id="visualizing-the-bias-variance-tradeoff">
<span class="header-section-number">8.8.1</span> Visualizing the bias-variance tradeoff</h3>
<p>The function <code><a href="https://friendly.github.io/genridge/reference/precision.html">precision()</a></code> calculates a number of measures of the effect of shrinkage of the coefficients in relation to the “size” of the covariance matrix <span class="math inline">\(\boldsymbol{\mathcal{V}}_k \equiv \widehat{\mathsf{Var}} (\widehat{\boldsymbol{\beta}}^{\mathrm{RR}}_k)\)</span>. Larger shrinkage <span class="math inline">\(k\)</span> should lead to a smaller ellipsoid for <span class="math inline">\(\boldsymbol{\mathcal{V}}_k\)</span>, indicating increased precision.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb30" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pdat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://friendly.github.io/genridge/reference/precision.html">precision</a></span><span class="op">(</span><span class="va">lridge</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt;       lambda   df   det  trace max.eig norm.beta norm.diff</span></span>
<span><span class="co">#&gt; 0.000  0.000 6.00 -12.9 18.119  15.419     1.000     0.000</span></span>
<span><span class="co">#&gt; 0.002  0.002 5.70 -13.6 11.179   8.693     0.857     0.695</span></span>
<span><span class="co">#&gt; 0.005  0.005 5.42 -14.4  6.821   4.606     0.741     1.276</span></span>
<span><span class="co">#&gt; 0.010  0.010 5.14 -15.4  4.042   2.181     0.637     1.783</span></span>
<span><span class="co">#&gt; 0.020  0.020 4.82 -16.8  2.218   1.025     0.528     2.262</span></span>
<span><span class="co">#&gt; 0.040  0.040 4.48 -18.7  1.165   0.581     0.423     2.679</span></span>
<span><span class="co">#&gt; 0.080  0.080 4.13 -21.1  0.587   0.260     0.337     3.027</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, the first three terms described below are (inverse) measures of precision; the last two quantify shrinkage:</p>
<ul>
<li><p><code>det</code> <span class="math inline">\(=\log{| \mathcal{V}_k |}\)</span> is an overall measure of variance of the coefficients. It is the (linearized) volume of the covariance ellipsoid and corresponds conceptually to Wilks’ Lambda criterion.</p></li>
<li><p><code>trace</code> <span class="math inline">\(=\text{trace} (\boldsymbol{\mathcal{V}}_k)\)</span> is the sum of the variances and also the sum of the eigenvalues of <span class="math inline">\(\boldsymbol{\mathcal{V}}_k\)</span>, conceptually similar to Pillai’s trace criterion.</p></li>
<li><p><code>max.eig</code> is the largest eigenvalue measure of size, an analog of Roy’s maximum root test.</p></li>
<li><p><code>norm.beta</code> <span class="math inline">\(= \left \Vert \boldsymbol{\beta}\right \Vert / \max{\left \Vert \boldsymbol{\beta}\right \Vert}\)</span> is a summary measure of shrinkage, the normalized root mean square of the estimated coefficients. It starts at 1.0 for <span class="math inline">\(k=0\)</span> and decreases with the penalty for large coefficients.</p></li>
<li><p><code>diff.beta</code> is the root mean square of the difference from the OLS estimate <span class="math inline">\(\lVert \mathbf{\beta}_{\text{OLS}} - \mathbf{\beta}_k \rVert\)</span>. This measure is inversely related to <code>norm.beta</code>.</p></li>
</ul>
<p>Plotting shrinkage against a measure of variance gives a <em>direct</em> view of the tradeoff between bias and precision. In <a href="#fig-longley-precision-plot" class="quarto-xref">Figure&nbsp;<span>8.14</span></a> I plot <code>norm.beta</code> against <code>det</code>, and join the points with a curve. (<span style="color: brown;"><strong>genridge</strong></span> contains a plot method for <code>"precision"</code> objects which produce similar plots.)</p>
<p><strong>TODO</strong>: Redo this fig using <code>plot.precsion()</code> method</p>
<!-- fig-code: R/genridge-longley-fig2.R -->
<div class="cell" data-layout-align="center">
<details class="code-fold"><summary>Show the code</summary><div class="sourceCode" id="cb31" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">splines</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">pdat</span>, <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">norm.beta</span>, <span class="va">det</span>, type<span class="op">=</span><span class="st">"b"</span>, </span>
<span>       cex.lab<span class="op">=</span><span class="fl">1.25</span>, pch<span class="op">=</span><span class="fl">16</span>, </span>
<span>       cex<span class="op">=</span><span class="fl">1.5</span>, col<span class="op">=</span><span class="va">clr</span>, lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>       xlab<span class="op">=</span><span class="st">'shrinkage: ||b|| / max(||b||)'</span>,</span>
<span>       ylab<span class="op">=</span><span class="st">'variance: log |Var(b)|'</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="va">norm.beta</span>, <span class="va">det</span>, </span>
<span>       labels <span class="op">=</span> <span class="va">lambdaf</span>, </span>
<span>       cex <span class="op">=</span> <span class="fl">1.25</span>, </span>
<span>       pos <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="fl">4</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">norm.beta</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">det</span><span class="op">)</span>, </span>
<span>       labels <span class="op">=</span> <span class="st">"log |Variance| vs. Shrinkage"</span>, </span>
<span>       cex<span class="op">=</span><span class="fl">1.5</span>, pos<span class="op">=</span><span class="fl">4</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span><span class="co"># find locations for optimal shrinkage criteria</span></span>
<span><span class="va">mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">det</span>, <span class="va">norm.beta</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/splines/bs.html">bs</a></span><span class="op">(</span><span class="va">lambda</span>, df<span class="op">=</span><span class="fl">5</span><span class="op">)</span>, </span>
<span>          data<span class="op">=</span><span class="va">pdat</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>lambda<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">lridge</span><span class="op">$</span><span class="va">kHKB</span>, </span>
<span>                         <span class="va">lridge</span><span class="op">$</span><span class="va">kLW</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod</span>, <span class="va">x</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">fit</span><span class="op">[</span>,<span class="fl">2</span><span class="op">:</span><span class="fl">1</span><span class="op">]</span>, pch<span class="op">=</span><span class="fl">15</span>, </span>
<span>       col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.82</span><span class="op">)</span>, cex<span class="op">=</span><span class="fl">1.6</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="va">fit</span><span class="op">[</span>,<span class="fl">2</span><span class="op">:</span><span class="fl">1</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"HKB"</span>, <span class="st">"LW"</span><span class="op">)</span>, </span>
<span>     pos<span class="op">=</span><span class="fl">3</span>, cex<span class="op">=</span><span class="fl">1.5</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.20</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-longley-precision-plot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-longley-precision-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-longley-precision-plot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longley-precision-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.14: <strong>Precision plot</strong> showing the tradeoff between bias and precision. Bias increases as we move away from the OLS solution, but precision increases.
</figcaption></figure>
</div>
</div>
</div>
<p>You can see that in this example the HKB criterion prefers a smaller degree of shrinkage, but achieves only a modest decrease in variance. But variance decreases more sharply thereafter and the LW choice achieves greater precision.</p>
</section></section><section id="sec-ridge-low-rank" class="level2" data-number="8.9"><h2 data-number="8.9" class="anchored" data-anchor-id="sec-ridge-low-rank">
<span class="header-section-number">8.9</span> Low-rank views</h2>
<p>Just as principal components analysis gives low-dimensional views of a data set, PCA can be useful to understand ridge regression, just as it did for the problem of collinearity.</p>
<p>The <code>pca</code> method transforms a <code>"ridge"</code> object from parameter space, where the estimated coefficients are <span class="math inline">\(\beta_k\)</span> with covariance matrices <span class="math inline">\(\boldsymbol{\mathcal{V}}_k\)</span>, to the principal component space defined by the right singular vectors, <span class="math inline">\(\mathbf{V}\)</span>, of the singular value decomposition <span class="math inline">\(\mathbf{U} \mathbf{D} \mathbf{V}^\mathsf{T}\)</span> of the scaled predictor matrix, <span class="math inline">\(\mathbf{X}\)</span>. In PCA space the total variance of the predictors remains the same, but it is distributed among the linear combinations that account for successively greatest variance.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb32" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">plridge</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://friendly.github.io/genridge/reference/pca.html">pca</a></span><span class="op">(</span><span class="va">lridge</span><span class="op">)</span></span>
<span><span class="va">plridge</span></span>
<span><span class="co">#&gt; Ridge Coefficients:</span></span>
<span><span class="co">#&gt;        dim1     dim2     dim3     dim4     dim5     dim6   </span></span>
<span><span class="co">#&gt; 0.000  1.51541  0.37939  1.80131  0.34595  5.97391  6.74225</span></span>
<span><span class="co">#&gt; 0.002  1.51537  0.37935  1.80021  0.34308  5.69497  5.06243</span></span>
<span><span class="co">#&gt; 0.005  1.51531  0.37928  1.79855  0.33886  5.32221  3.68519</span></span>
<span><span class="co">#&gt; 0.010  1.51521  0.37918  1.79579  0.33205  4.79871  2.53553</span></span>
<span><span class="co">#&gt; 0.020  1.51500  0.37898  1.79031  0.31922  4.00988  1.56135</span></span>
<span><span class="co">#&gt; 0.040  1.51459  0.37858  1.77944  0.29633  3.01774  0.88291</span></span>
<span><span class="co">#&gt; 0.080  1.51377  0.37778  1.75810  0.25915  2.01876  0.47238</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, a <code><a href="https://friendly.github.io/genridge/reference/traceplot.html">traceplot()</a></code> of the resulting <code>"pcaridge"</code> object shows how the dimensions are affected by shrinkage, shown on the scale of degrees of freedom in <a href="#fig-longley-pca-traceplot" class="quarto-xref">Figure&nbsp;<span>8.15</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb33" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://friendly.github.io/genridge/reference/traceplot.html">traceplot</a></span><span class="op">(</span><span class="va">plridge</span>, X<span class="op">=</span><span class="st">"df"</span>, </span>
<span>          cex.lab <span class="op">=</span> <span class="fl">1.2</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-longley-pca-traceplot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-longley-pca-traceplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-longley-pca-traceplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2100">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longley-pca-traceplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.15: Ridge traceplot for the longley regression viewed in PCA space. The dimensions are the linear combinations of the predictors which account for greatest variance.
</figcaption></figure>
</div>
</div>
</div>
<p>What may be surprising at first is that the coefficients for the first 4 components are not shrunk at all. These large dimensions are immune to ridge tuning. Rather, the effect of shrinkage is seen only on the <em>last two dimensions</em>. But those also are the directions that contribute most to collinearity as we saw earlier.</p>
<!-- **pairs()** -- Should I include this ? -->
<p>A <code><a href="https://rdrr.io/r/graphics/pairs.html">pairs()</a></code> plot gives a dramatic representation bivariate effects of shrinkage in PCA space: the principal components of X are uncorrelated, so the ellipses are all aligned with the coordinate axes. The ellipses largely coincide for dimensions 1 to 4 where there is little effect of shrinkage. You can see them shrink in one direction in the last two columns and rows, and in both for the combination of (<code>dim5</code>, <code>dim6</code>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/pairs.html">pairs</a></span><span class="op">(</span><span class="va">plridge</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-longley-pca-pairs" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-longley-pca-pairs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-longley-pca-pairs-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longley-pca-pairs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.16: <strong>pairs method</strong>: All pairwise bivariate ridge plots shown in PCA space.
</figcaption></figure>
</div>
</div>
</div>
<p>If we focus on the plot of dimensions <code>dim5:dim6</code>, we can see where all the shrinkage action is in this representation. Generally, the predictors that are related to the smallest dimension (6) are shrunk quickly at first.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">plridge</span>, variables<span class="op">=</span><span class="fl">5</span><span class="op">:</span><span class="fl">6</span>, </span>
<span>     fill <span class="op">=</span> <span class="cn">TRUE</span>, fill.alpha<span class="op">=</span><span class="fl">0.15</span>, cex.lab <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="va">plridge</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span>, <span class="fl">5</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span>, </span>
<span>     label <span class="op">=</span> <span class="va">lambdaf</span>, </span>
<span>     cex<span class="op">=</span><span class="fl">1.5</span>, pos<span class="op">=</span><span class="fl">4</span>, offset<span class="op">=</span><span class="fl">.1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-longley-pca-dim56" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-longley-pca-dim56-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-longley-pca-dim56-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2100">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longley-pca-dim56-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.17: Bivariate ridge trace plot for the smallest two dimensions. The coefficients for these two dimensions head smoothly toward zero and their variance also shrinks.
</figcaption></figure>
</div>
</div>
</div>
<section id="biplot-view" class="level3" data-number="8.9.1"><h3 data-number="8.9.1" class="anchored" data-anchor-id="biplot-view">
<span class="header-section-number">8.9.1</span> Biplot view</h3>
<p>The question arises how to relate this view of shrinkage in PCA space to the original predictors. The biplot is again your friend. In effect, it adds vectors showing the contributions of the predictors to a plot like <a href="#fig-longley-pca-dim56" class="quarto-xref">Figure&nbsp;<span>8.17</span></a>. You can project variable vectors for the predictor variables into the PCA space of the smallest dimensions, where the shrinkage action mostly occurs to see how the predictor variables relate to these dimensions.</p>
<p><code>biplot.pcaridge()</code> supplements the standard display of the covariance ellipsoids for a ridge regression problem in PCA/SVD space with labeled arrows showing the contributions of the original variables to the dimensions plotted. Recall from <a href="04-pca-biplot.html#sec-biplot" class="quarto-xref"><span>Section 4.3</span></a> that these reflect the correlations of the variables with the PCA dimensions. The lengths of the arrows reflect the proportion of variance that each predictors shares with the components.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb36" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/biplot.html">biplot</a></span><span class="op">(</span><span class="va">plridge</span>, radius<span class="op">=</span><span class="fl">0.5</span>, </span>
<span>       ref<span class="op">=</span><span class="cn">FALSE</span>, asp<span class="op">=</span><span class="fl">1</span>, </span>
<span>       var.cex<span class="op">=</span><span class="fl">1.15</span>, cex.lab<span class="op">=</span><span class="fl">1.3</span>, col<span class="op">=</span><span class="va">clr</span>,</span>
<span>       fill<span class="op">=</span><span class="cn">TRUE</span>, fill.alpha<span class="op">=</span><span class="fl">0.15</span>, </span>
<span>       prefix<span class="op">=</span><span class="st">"Dimension "</span><span class="op">)</span></span>
<span><span class="co">#&gt; Vector scale factor set to  5.25</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="va">plridge</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span>,<span class="fl">5</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span>, <span class="va">lambdaf</span>, pos<span class="op">=</span><span class="fl">2</span>, cex<span class="op">=</span><span class="fl">1.3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-longley-pca-biplot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-longley-pca-biplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/ch08/fig-longley-pca-biplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longley-pca-biplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.18: Biplot view of the ridge trace plot for the smallest two dimensions, where the effects of shrinkage are most apparent.
</figcaption></figure>
</div>
</div>
</div>
<p>The biplot view in <a href="#fig-longley-pca-biplot" class="quarto-xref">Figure&nbsp;<span>8.18</span></a> showing the two smallest dimensions is particularly useful for understanding how the predictors contribute to shrinkage in ridge regression. Here, Year and Population largely contribute to dimension 5; a contrast between (Year, Population) and GNP contributes to dimension 6.</p>
</section></section><section id="what-have-we-learned" class="level2" data-number="8.10"><h2 data-number="8.10" class="anchored" data-anchor-id="what-have-we-learned">
<span class="header-section-number">8.10</span> What have we learned?</h2>
<p><strong>TODO</strong>: Consider replacing this with bullet point take-aways.</p>
<p>This chapter has considered the problems in regression models which stem from high correlations among the predictors. We saw that collinearity results in unstable estimates of coefficients with larger uncertainty, often dramatically more so than would be the case if the predictors were uncorrelated.</p>
<p>Collinearity can be seen as merely a “data problem” which can safely be ignored if we are only interested in prediction. When we want to understand a model, ridge regression can tame the collinearity beast by shrinking the coefficients slightly to gain greater precision in the estimates.</p>
<p>Beyond these statistical considerations, the methods of this chapter highlight the roles of multivariate thinking and visualization in understanding these phenomena and the methods developed for solving them. Data ellipses and confidence ellipses for coefficients again provide tools for visualizing what is concealed in numerical summaries. A perhaps surprising feature of both collinearity and ridge regression is that the important information usually resides in the smallest PCA dimensions and biplots help again to understand these dimensions.</p>
<!--

::: {.cell layout-align="center"}
**Packages used here**:

23  packages used here:
 bayestestR, car, carData, correlation, datawizard, dplyr, easystats, effectsize, factoextra, genridge, ggplot2, ggrepel, insight, knitr, MASS, modelbased, parameters, patchwork, performance, report, see, splines, VisCollin
:::

-->
<!-- ## References {.unnumbered} -->


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-Belsley:91a" class="csl-entry" role="listitem">
Belsley, D. A. (1991). <em>Conditioning diagnostics: Collinearity and weak data in regression</em>. Wiley.
</div>
<div id="ref-Belsley-etal:80" class="csl-entry" role="listitem">
Belsley, D. A., Kuh, E., &amp; Welsch, R. E. (1980). <em>Regression diagnostics: Identifying influential data and sources of collinearity</em>. John Wiley; Sons.
</div>
<div id="ref-Brown-Zidek-1980" class="csl-entry" role="listitem">
Brown, P. J., &amp; Zidek, J. V. (1980). Adaptive multivariate ridge regression. <em>The Annals of Statistics</em>, <em>8</em>(1), 64–74. <a href="http://www.jstor.org/stable/2240743">http://www.jstor.org/stable/2240743</a>
</div>
<div id="ref-Efron-etal:leas:2004" class="csl-entry" role="listitem">
Efron, B., Hastie, T., Johnstone, I., &amp; Tibshirani, R. (2004). Least angle regression. <em>The Annals of Statistics</em>, <em>32</em>(2), 407–499.
</div>
<div id="ref-Fox:2016:ARA" class="csl-entry" role="listitem">
Fox, J. (2016). <em>Applied regression analysis and generalized linear models</em> (Third edition.). SAGE.
</div>
<div id="ref-FoxMonette:92" class="csl-entry" role="listitem">
Fox, J., &amp; Monette, G. (1992). Generalized collinearity diagnostics. <em>Journal of the American Statistical Association</em>, <em>87</em>(417), 178–183.
</div>
<div id="ref-R-glmnet" class="csl-entry" role="listitem">
Friedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K., Simon, N., &amp; Yang, J. (2025). <em>Glmnet: Lasso and elastic-net regularized generalized linear models</em>. <a href="https://glmnet.stanford.edu">https://glmnet.stanford.edu</a>
</div>
<div id="ref-Friendly-2011-gentalk" class="csl-entry" role="listitem">
Friendly, M. (2011). <em>Generalized ridge trace plots: Visualizing bias and precision with the genridge <span>R</span> package</em>. SCS Seminar.
</div>
<div id="ref-Friendly:genridge:2013" class="csl-entry" role="listitem">
Friendly, M. (2013). The generalized ridge trace plot: Visualizing bias <em>and</em> precision. <em>Journal of Computational and Graphical Statistics</em>, <em>22</em>(1), 50–68. <a href="https://doi.org/10.1080/10618600.2012.681237">https://doi.org/10.1080/10618600.2012.681237</a>
</div>
<div id="ref-R-genridge" class="csl-entry" role="listitem">
Friendly, M. (2024). <em>Genridge: Generalized ridge trace plots for ridge regression</em>. <a href="https://github.com/friendly/genridge">https://github.com/friendly/genridge</a>
</div>
<div id="ref-FriendlyKwan:2009" class="csl-entry" role="listitem">
Friendly, M., &amp; Kwan, E. (2009). Where’s <span>Waldo</span>: Visualizing collinearity diagnostics. <em>The American Statistician</em>, <em>63</em>(1), 56–65. <a href="https://doi.org/10.1198/tast.2009.0012">https://doi.org/10.1198/tast.2009.0012</a>
</div>
<div id="ref-Friendly-etal:ellipses:2013" class="csl-entry" role="listitem">
Friendly, M., Monette, G., &amp; Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. <em>Statistical Science</em>, <em>28</em>(1), 1–39. <a href="https://doi.org/10.1214/12-STS402">https://doi.org/10.1214/12-STS402</a>
</div>
<div id="ref-Gabriel:71" class="csl-entry" role="listitem">
Gabriel, K. R. (1971). The biplot graphic display of matrices with application to principal components analysis. <em>Biometrics</em>, <em>58</em>(3), 453–467. <a href="https://doi.org/10.2307/2334381">https://doi.org/10.2307/2334381</a>
</div>
<div id="ref-R-penalized" class="csl-entry" role="listitem">
Goeman, J., Meijer, R., Chaturvedi, N., &amp; Lueder, M. (2022). <em>Penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the cox model</em>. <a href="https://doi.org/10.32614/CRAN.package.penalized">https://doi.org/10.32614/CRAN.package.penalized</a>
</div>
<div id="ref-GowerHand:96" class="csl-entry" role="listitem">
Gower, J. C., &amp; Hand, D. J. (1996). <em>Biplots</em>. Chapman &amp; Hall.
</div>
<div id="ref-Graybill1961" class="csl-entry" role="listitem">
Graybill, F. A. (1961). <em>An introduction to linear statistical models</em>. McGraw-Hill.
</div>
<div id="ref-Haitovsky1987" class="csl-entry" role="listitem">
Haitovsky, Y. (1987). On multivariate ridge regression. <em>Biometrika</em>, <em>74</em>(3), 563–570. <a href="https://doi.org/10.1093/biomet/74.3.563">https://doi.org/10.1093/biomet/74.3.563</a>
</div>
<div id="ref-Hastie-etal-2009" class="csl-entry" role="listitem">
Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The elements of statistical learning: Data mining, inference and prediction</em> (2nd ed.). Springer. <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">http://www-stat.stanford.edu/~tibs/ElemStatLearn/</a>
</div>
<div id="ref-Hocking2013" class="csl-entry" role="listitem">
Hocking, R. R. (2013). <em>Methods and applications of linear models: Regression and the analysis of variance</em>. Wiley. <a href="https://books.google.ca/books?id=iq2J-1iS6HcC">https://books.google.ca/books?id=iq2J-1iS6HcC</a>
</div>
<div id="ref-HoerlKennard:1970a" class="csl-entry" role="listitem">
Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge regression: <span>B</span>iased estimation for nonorthogonal problems. <em>Technometrics</em>, <em>12</em>, 55–67.
</div>
<div id="ref-Hoerl-etal-1975" class="csl-entry" role="listitem">
Hoerl, A. E., Kennard, R. W., &amp; Baldwin, K. F. (1975). Ridge regression: Some simulations. <em>Communications in Statistics</em>, <em>4</em>(2), 105–123. <a href="https://doi.org/10.1080/03610927508827232">https://doi.org/10.1080/03610927508827232</a>
</div>
<div id="ref-Kwan-etal:2009" class="csl-entry" role="listitem">
Kwan, E., Lu, I. R. R., &amp; Friendly, M. (2009). Tableplot: A new tool for assessing precise predictions. <em>Zeitschrift f<span>ü</span>r Psychologie / Journal of Psychology</em>, <em>217</em>(1), 38–48. <a href="https://doi.org/10.1027/0044-3409.217.1.38">https://doi.org/10.1027/0044-3409.217.1.38</a>
</div>
<div id="ref-LawlessWang:1976" class="csl-entry" role="listitem">
Lawless, J. F., &amp; Wang, P. (1976). A simulation study of ridge and other regression estimators. <em>Communications in Statistics</em>, <em>5</em>, 307–323.
</div>
<div id="ref-Longley:1967" class="csl-entry" role="listitem">
Longley, J. W. (1967). An appraisal of least squares programs for the electronic computer from the point of view of the user. <em>Journal of the American Statistical Association</em>, <em>62</em>, 819–841. https://doi.org/<a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896">https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896</a>
</div>
<div id="ref-Marquardt:1970" class="csl-entry" role="listitem">
Marquardt, D. W. (1970). Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation. <em>Technometrics</em>, <em>12</em>, 591–612.
</div>
<div id="ref-McDonald:2009" class="csl-entry" role="listitem">
McDonald, G. C. (2009). Ridge regression. <em>Wiley Interdisciplinary Reviews: Computational Statistics</em>, <em>1</em>(1), 93–100. <a href="https://doi.org/10.1002/wics.14">https://doi.org/10.1002/wics.14</a>
</div>
<div id="ref-Tibshriani:regr:1996" class="csl-entry" role="listitem">
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. <em>Journal of the Royal Statistical Society, Series B: Methodological</em>, <em>58</em>, 267–288.
</div>
<div id="ref-Vinod:1978" class="csl-entry" role="listitem">
Vinod, H. D. (1978). A survey of ridge regression and related techniques for improvements over ordinary least squares. <em>The Review of Economics and Statistics</em>, <em>60</em>(1), 121–131. <a href="http://www.jstor.org/stable/1924340">http://www.jstor.org/stable/1924340</a>
</div>
</div>
</section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>The “Where’s Waldo” problem has attracted attention in machine learning, AI and computational image analysis circles. One approach uses convolutional neural networks. <a href="https://github.com/agnarbjoernstad/FindWaldo">FindWaldo</a> is one example implemented in Python.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This example is adapted from one by John Fox (2022), <a href="https://socialsciences.mcmaster.ca/jfox/Courses/SORA-TABA/slides-collinearity.pdf">Collinearity Diagnostics</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>A related shrinkage method, LASSO (Least Absolute Shrinkage and Selection Operator) <span class="citation" data-cites="Tibshriani:regr:1996">(<a href="95-references.html#ref-Tibshriani:regr:1996" role="doc-biblioref">Tibshirani, 1996</a>)</span> uses a penalty term of the sum of <em>absolute values</em> of the coefficients, <span class="math inline">\(\Sigma \lvert \beta_i \rvert \le t(k)\)</span> rather than the sum of squares in <a href="#eq-ridgeRSS" class="quarto-xref">Equation&nbsp;<span>8.4</span></a>. The effect of this change is to shrink some coefficients exactly to zero, effectively eliminating them from the model. This makes LASSO a <em>model selection method</em>, similar in aim to other best subset regression methods. This is widely used in machine learning methods, where interpretation less important than prediction accuracy. <a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/friendly\.github\.io\/Vis-MLM-book");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./07-lin-mod-topics.html" class="pagination-link" aria-label="Topics in Linear Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Topics in Linear Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./09-hotelling.html" class="pagination-link" aria-label="Hotelling's $T^2$">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hotelling’s <span class="math inline">\(T^2\)</span></span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/friendly/vis-MLM-book/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer><script src="site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>