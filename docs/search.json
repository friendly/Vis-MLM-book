[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "",
    "text": "Preface\nThis book is about graphical methods developed recently for multivariate data, and their uses in understanding relationships when there are several aspects to be considered together. Data visualization methods for statistical analysis are well-developed and widely available in R for simple linear models with a single outcome variable, as well as for more complex models with nonlinear effects, hierarchical data with observations grouped within larger units and so forth.\nHowever, with applied research in the sciences, social and behavioral in particular, it is often the case that the phenomena of interest (e.g., depression, job satisfaction, academic achievement, childhood ADHD disorders, etc.) can be measured in several different ways or related aspects. Understanding how these different aspects are related can be crucial to our knowledge of the general phenomenon.\nFor example, if academic achievement can be measured for adolescents by reading, mathematics, science and history scores, how do predictors such as parental encouragement, school environment and socioeconomic status affect all these outcomes? In a similar way? In different ways? In such cases, much more can be understood from a multivariate approach that considers the correlations among the outcomes. Yet, sadly, researchers typically examine the outcomes one by one which often only tells part of the data story.\nHowever, to convey the statistical and graphic methods to do these things, I begin with some warm-up exercises in multivariate thinking, with a grand scheme for statistics and data visualization, a parable, and an example of multivariate discovery.\nTODO: Include an overview of the parts and chapters somewhere here",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Features",
    "text": "Features\nSome key substantive and pedagogical features of the book are:\n\nThe writing style is purposely pedagogical (hopefully not too pedantic), in that it aims to teach how to think about analysis and graphics for multivariate data. That is, I try to convey how you can achieve understanding of statistical concepts and data visualization through ways of representing those ideas in diagrams and plots, and producing graphics using R functions and packages.\nTo help understand the how modern statistical and graphic methods became more powerful over time, the book takes a historical perspective, where it is useful to convey how the innovations we use today evolved.\nStatistical data visualization is cast in a general framework by their goal for communicating information, either to your self or others (such as see the data, visualize a model, diagnose problems), rather than a categorization by graphic types like bar charts and line graphs. This is best informed by the principles and goals of communication, for example making graphic comparison easy and ordering factors and variables according to what should be seen, called effect ordering for data display (Friendly & Kwan, 2003). \nData visualization is seen as a combination of exposure—plotting the raw data—and summarization— plotting statistical summaries—to highlight what should be noticed. For example, data ellipses and confidence ellipses are widely used as simple, effective summaries of data and fitted model parameters. When the data is complex, the idea of visual thinning can be used to balance the tradeoff. \nThe book exploits the rich connections among statistics, geometry and data visualization. Statistical ideas, particularly for multivariate data, can be more easily understood in terms of geometrical ones that can be seen in diagrams and data displays. More importantly, ideas from one domain can amplify what we can understand from another.\nThese graphical tools can be used to understand or explain a wide variety of statistical concepts, phenomena, and paradoxes such as Simpson’s paradox (Simpson’s paradox: marginal and conditional relationships), effects of measurement error (Measurement error), bias-variance tradeoff (Visualizing the bias-variance tradeoff), and so forth.\nThe HE (“hypothesis - error”) plot framework provides a simple way to understand the results of statistical tests and the relations among response outcomes in the multivariate linear model.\nDimension reduction techniques such as PCA and discriminant analysis are presented as “multivariate juicers”, able to squeeze the important information in high-dimensional data into informative two-dimensional views. But sometimes, the most important information for a problem lies in the smallest dimensions, as is the case in outlier detection (Multivariate normality and outliers), collinearity (Collinearity biplots) and ridge regression (Low-rank views).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-i-assume",
    "href": "index.html#what-i-assume",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "What I assume",
    "text": "What I assume\nIt is assumed that the reader has at least a basic background in applied, intermediate statistics. This would normally include material on simple and multiple regression as well as simple analysis of variance (ANOVA) designs. This also means that you should be familiar with the basic ideas of statistical inference including hypothesis tests and confidence intervals. \nThere will also be some mathematics in the book where words and diagrams are not enough. The mathematical level will be intermediate, mostly consisting of simple algebra. No derivations, proofs, theorems here!\nFor multivariate methods, it will be useful to express ideas using matrix notation to simplify presentation. It will be enough for you to recognize that a single symbol \\(\\mathbf{y}\\) can be a shorthand for \\(n\\) scores on a variable like weight, and the symbol \\(\\mathbf{X}\\) can represent an entire data table, with, say \\(n\\) observations on \\(p\\) variables like height, body mass index, diet components, and so forth. Then, the notation \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta}\\) represents an entire linear model to relate weight to these other variables. I’m using this math notation to express ideas, and all you will need is a reading-level of understanding.\nFor this, the first chapter of Fox (2021), A Mathematical Primer for Social Statistics, is excellent, and the rest is well worth reading. If you want to learn something of using matrix algebra for data analysis and statistics in R, I recommend our package matlib (Friendly et al., 2024).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-resources",
    "href": "index.html#r-resources",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "R Resources",
    "text": "R Resources\nI also assume the reader to have at least a basic familiarity with statistical analysis in R. While R fundamentals are outside the scope of the book, I believe that this language provides a rich set of resources, far beyond that offered by other statistical software packages, and is well worth learning. For those not familiar with R or wish to learn new skills, I recommend:\n\nCotton (2013), Learning R (online) provides a well-rounded basic introduction to R, covering data types, lists and data frames, functions, packages and workflow for data analysis and graphics;\nMatloff(2011), The Art of R Programming (online) is devoted to learning the programming features of R. It covers all the basics (data types, arrays, data frames), R functions, object-oriented programming, debugging, and so forth.\nWickham(2019), Advanced R (online) is aimed at intermediate R programmers who want to dive deeper into R and learn how things work,\nLong & Teetor (2019), R Cookbook 2\\(^{nd}\\) Ed (online) provides how-to recipies for basic tasks from working with RStudio, to input and output, general statistics, graphics, and regression / ANOVA;\nFox & Weisberg (2018), An R Companion to Applied Regression is a fantastic resource for learning how to perform statistical analyses in R and visualize results with insightful graphics. It is the companion book to Fox’s (2016), Applied Regression Analysis and Generalized Linear Models, which I consider the best intermediate-level modern treatment of these topics. I make heavy use of the accompanying car package (Fox & Weisberg, 2019) which provides important and convenient graphical methods.\n\nWhen you work with R, it may be useful to have this collection of R and RStudio cheatsheets I prepared for my graduate data visualization course.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-graphics-resources",
    "href": "index.html#r-graphics-resources",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "R graphics resources",
    "text": "R graphics resources\nIn this book, I create a large number of graphs in R, and have aimed to present and describe how I do them using R packages and code to manipulate the data or numerical output from analysis function, so that you can learn from these examples to apply these ideas to your own data.\nIn writing this, I’ve also tried to exemplify graphical principles that underlie effective graphic communication. You might find the lecture notes, extensive resources and R examples for my course, Psychology of Data Visualization useful.\nIn addition, there are a few books I recommend:\n\n            \n\n \n\nClaus Wilke (2019), Fundamentals of Data Visualization (online) A well thought out presentation of important ideas of graphic presentation; it covers a wide range of topics, with good practical advice and lots of examples. How to do these in R is covered in his course notes.\nKeiran Healy (2019), Data Visualization: A Practical Introduction (online). A highly accessible, hands-on primer on how to create effective graphics from data using ggplot2, with a focus on how to think about the information you want to show.\nAntony Unwin (2024), Getting (more out of) Graphics. This book offers a collection of 25 case studies of interesting datasets, exemplifying desirable features of graphs use to understand them and using ggplot2 graphics. A second part provides useful advice on graphical practice, drawing on the lessons of the examples from the first part. The R code for all chapters is available online.\nNicola Rennie (2025), The Art of Data Visualization with ggplot2 (online). Rennie offers a kind of master class in designing effective, attractive graphics using ggplot2. The examples chosen stem from the weeklyTidy Tuesday challenges that invite graphic programmers and designers to to work on a shared dataset to see what they can do.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-coding-style-used-here",
    "href": "index.html#r-coding-style-used-here",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "R coding style used here",
    "text": "R coding style used here\n\n\n\n\n\n\nNote to reviewers\n\n\n\nThe coding style for computing and graphics used in this book are expressed using both the traditional functional syntax, f(g(x)) and the newer approach using pipes (|&gt;) of the tidyverse. Similarly, I use both base R graphics and plots based on the “ggverse” of ggplot2 and it’s large family of add-on and extension packages.1\nHow and why it is this way should be explained to the reader. The material below is a start, but needs a bit of fleshing out or editing down.\n\n\n\nLike natural language and the graphic methods used in this book, R syntax and the programming style for graphics has evolved considerably since R was first introduced by Ross Ihaka and Robert Gentleman in 1992. It was originally based on the S programming language (Becker et al., 1988) and designed as a functional language. This means that programs are constructed by applying and composing functions, like log(x), exp(x), which return a value. …\n\npipes\ntidyverse\nR graphics: plot() -&gt; ggplot()\n\n\nRather than being dogmatic about using the newest, most politically correct style,2 in this book I have taken the view that what is most important about programming and graphics software is that they serve as a route—as short and direct as possible—between having an idea in your head of what you want to do, and seeing the result on your screen or in your report, as illustrated in Figure 1.\n\n\n\n\n\n\n\nFigure 1: The expressive power of graphics software can be considered as minimizing the path from an idea in your head to finished graphic.\n\n\n\n\nConsequently, for nearly every graph in this book, I used what I considered to be the most effective style to produce an admirable graphic, but—perhaps more importantly–to be able to describe how I coded that to the reader.\nFor example, the car package and my heplots and related packages use base-R graphics, but I could customize their use in examples by using the conventions of points(), lines(), text() and even par() when necessary. However, if I was starting this project anew, I would now use tinyplot (McDermott et al., 2025), which has removed many of the cringeworthy features of base-R graphics.\nOn the other hand, ggplot2 package was designed to be an elegant language, based on the grammar of graphics (Wilkinson, 1999). It allows you to think of building plots logically and coherently, layer by layer. Instead of memorizing specific function calls and their arguments for every type of chart, you learn a flexible, high-level language for describing what you want your graphic to look like. This promotes a more structured thinking about data visualization, making it easier for you to iterate3, as we always do, to create beautiful, publication-quality graphics.\nThis is great in theory, but as you will see here, in many code examples, beyond the basic geom_* elements, a good deal of the effort to produce them required multiple steps to (a) get my data into a tidy format, (b) assign proper scale_*s to data variables and (c) use theme() arguments to control the large and small aspects of graphic design that contribute to the elegance and potential beauty of finished product you see.\nConsequently, this book stands on the shoulders of several giants in R graphics software, but the goal of reducing the gap between the idea of a graph and the visual result can still be narrowed.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#typographic-conventions-used-in-this-book",
    "href": "index.html#typographic-conventions-used-in-this-book",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Typographic conventions used in this book",
    "text": "Typographic conventions used in this book\nThe following typographic conventions are used in this book:\n\nitalic : indicates terms or phrases to be emphasized or defined in the text; bold : is used for terms to be given strong emphasis, particularly for their first mention.\nPackage names are printed in bold and colored brown, for example ggplot2, car and the matlib package. These uses generate citations like ggplot2 (Wickham, 2016) on their first use. Package references in the text are automatically indexed, individually and under a “Packages” heading.\nDatasets are rendered as their name in monospaced font, like Prestige or indicating the package from which they come, as in carData::Prestige. These too are automatically indexed.\nA monospaced typewriter font is used in the text to refer to variable and function names, such as education and plot(). This font is also for R statement elements, keywords and code fragments as well.\nR code in program listings and output is presented in a monospaced (typewriter) font, fira mono\n\n\nFor R functions in packages, I use the notation package::function(), such as car::Anova(), to identify that the Anova() function is defined in the car package. This also means you can get help on a function by typing ?car::Anova in the console, or a list of its arguments and default values from args(car::Anova).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI am profoundly grateful to my friends and colleagues John Fox, Georges Monette and Phil Chalmers at York University who have inspired me with their insights into statistical thinking and visualization of statistical ideas over many years. They also contributed greatly to the R packages that help make the methods of this book accessible and easy to use.\nThere is also a host of graduate students I have taught, supervised and worked with over my 50+ year career. Among these, Ernest Kwan and Matthew Sigal were important contributors to the development of data visualization ideas and techniques reflected here. Agnieska Kopinska, Gigi Luk and Touraj Amiri were TAs and RAs who contributed to my teaching and research. Most recently, Udi Alter and Michael Shiuming Truong worked as research assistants and helped me in numerous ways with work on this book.\nWriting this book using Quarto within the RStudio (now Posit) development environment presented many technical challenges I had not encountered in previous books (Friendly & Meyer (2016)). I am grateful to Mickaël Canouil, Christophe Dervieux, Felix Benning and others in the quarto-dev community who graciously helped me solve many issues, and again to Michael Shiuming Truong who helped with this effort with incisive comments, suggestions and an eagle-eye to typographic and programming details.\nThe book also relies heavily on the graphic ideas and software of many R developers, including Cory Brunson, Vincent Arel-Bundock, Di Cook, John Fox, Duncan Murdoch, who replied to issues and feature requests on their packages. I am also indebted to Gina Reynolds, Teun van den Brand and other participants in the the ggplot2 extenders club for help with ggplot() methods I needed for things like labeling “noteworthy” observations in plots.\n\n\n\n\n\nBecker, R. A., Chambers, J. M., & Wilks, A. R. (1988). The new S language. Wadsworth & Brooks/Cole.\n\n\nCotton, R. (2013). Learning R. O’Reilly Media.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (Third edition.). SAGE.\n\n\nFox, J. (2021). A mathematical primer for social statistics (2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878835\n\n\nFox, J., & Weisberg, S. (2018). An R companion to applied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFox, J., & Weisberg, S. (2019). An R companion to applied regression (Third). Sage. https://www.john-fox.ca/Companion/\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix functions for teaching and learning linear algebra and multivariate statistics. https://github.com/friendly/matlib\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays. Computational Statistics and Data Analysis, 43(4), 509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nHealy, K. (2019). Data visualization: A practical introduction. Princeton University Press. http://www.socviz.co\n\n\nLong, J. D., & Teetor, P. (2019). R cookbook: Proven recipes for data analysis, statistics, and graphics. O’Reilly. https://rc2e.com/\n\n\nMatloff, N. (2011). The art of R programming: A tour of statistical software design. No Starch Press.\n\n\nMcDermott, G., Arel-Bundock, V., & Zeileis, A. (2025). tinyplot: Lightweight extension of the base r graphics system. https://doi.org/10.32614/CRAN.package.tinyplot\n\n\nRennie, N. (2025). The art of data visualization with ggplot2. CRC Press. https://nrennie.rbind.io/art-of-viz/\n\n\nUnwin, A. (2024). Getting (more out of) graphics: Practice and principles of data visualisation. Chapman; Hall/CRC. https://doi.org/10.1201/9781003131212\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWickham, H. (2019). Advanced r. Chapman; Hall/CRC. https://doi.org/10.1201/9781351201315\n\n\nWilke, C. O. (2019). Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. clauswilke.com/dataviz\n\n\nWilkinson, L. (1999). The grammar of graphics. Springer.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "",
    "text": "The official ggplot2 extensions gallery website, lists 151 registered extension packages available as of this writing. There is also the ggplot2 extenders club, an active group of developers organized by Gina Reynolds and Teun van den Brand, who aim to facilitate thinking about further growth of ggplot ideas and implementations.↩︎\nSee Norm Matloff’s essay Tidyverse Skeptic. He argues that the tidyverse is not a good vehicle for teaching novice, noncoders, and that using base-R as the vehicle of instruction brings students to a more skilled level, in shorter time.↩︎\nYou should think of the “80-20” graphics rule when you work. This says you can produce 80% of your finished graphic with 20% of your total effort. But the corollary is that it takes you 80% of your time to fix the limitations of the remaining 20%.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-Author.html",
    "href": "00-Author.html",
    "title": "Author",
    "section": "",
    "text": "Michael Friendly\n\n datavis.ca [Legacy web site]\n dataVisFriendly [BlueSky]\n friendly [Github]\n vis.social/@datavisFriendly [Mastodon]\n\n\nMichael Friendly is a Fellow of the American Statistical Association, Professor of Psychology and coordinator of the Statistical Consulting Service at York University, He is an associate editor of the Journal of Graphical and Computational Statistics and has served as an editorial collaborator for many other journals. He received his Ph.D. in psychometrics and cognitive psychology from Princeton University.\nHis current research work includes the development of graphical methods for data visualization, where he is a principal innovator of novel methods for relatively “un-vizzed” problems, including visualizing categorical (Friendly, 2000; Friendly & Meyer, 2016,) and those for multivariate data, the subject of this book. Along with this, he is the author and maintainer of many R packages, described on his GitHub R packages page.\nHis passion for the deep roots of modern graphical methods led him to become an world-known amateur historian of data visualization. In the latter, he directs The Milestones Project, a comprehensive catalog and database of the principal developments in the histories of thematic cartography, statistical graphics and data visualization. He is a founder of an international group, Les Chevaliers des Albums de Statistique Graphique, devoted to this history and is author of multiple books and research papers on these topics. His most recent book in this area is Friendly & Wainer (2021) A History of Data Visualization and Graphic Communication, Harvard University Press.\n\n\n\n\nFriendly, M. (2000). Visualizing categorical data. SAS Institute.\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034",
    "crumbs": [
      "Author"
    ]
  },
  {
    "objectID": "01-Prelude.html",
    "href": "01-Prelude.html",
    "title": "Warm-up Exercises",
    "section": "",
    "text": "The Magic of Graphs\nThe aim of this book is to blow your mind! Or at least to expand your imagination and take visual thinking to higher dimensions. But, just like kneading dough to make it more flexible before shaping and baking, or stretching your muscles before a race or starting your car before you drive, it is helpful to begin with some warm-up exercises, mental ones here.\nIn this book, I present a number of graphical methods, some novel, designed to help you understand multivariate data and models, particularly when the number of variables exceeds what can easily be shown on paper or on your screen. Visualizing these in higher dimensions relies on ideas from geometry like projection and other analogies, and some mathematical techniques to bridge the gap between our three-dimensional experience and higher-dimensional realities of data and statistical models.\nTo set the stage for this expedition and hopefully rouse your enthusiasm for this endeavor, I try to pave the way for multivariate thinking with a bit of history, a simple scheme (1-2-MANY) for statistics and data visualization, a parable of thinking in one more dimension, and a recent example of multivariate discovery.\nThese inspiring words were written by Henry David Hubbard, the first Secretary of the US National Bureau of Standards (also known for modernizing the Mendeleev’s periodic table). They reflect the time in the early 1900s when appreciation for graphical methods was widespread and applications of graphical methods in science and popular writings had finally become mainstream, along with textbooks (e.g., Peddle (1910); Haskell (1919)) and college courses (Costelloe (1915)).\nYet, perhaps paradoxically, this period, from 1900–1950 has been called the Modern Dark Ages of data visualization (Friendly et al., 2015; Friendly & Denis, 2001). There were few new graphical innovations and, by the mid-1930s, the enthusiasm for visualization among statisticians had been supplanted by the rise of quantification and formal, often statistical, models in the social and biological sciences. Numbers—parameter estimates, and, especially, standard errors—were precise. Pictures were—well, just pictures: pretty or evocative, perhaps, but incapable of stating a “fact” to three or more decimals. Or so it seemed to statisticians.\nThe re-birth of interest in data visualization among statisticians was heralded by John Tukey (1962) in “The Future of Data Analysis”, where he issued a call for the recognition of data analysis as a legitimate branch of statistics distinct from mathematical statistics. His ideas for Exploratory Data Analysis, or EDA (Tukey, 1977) introduced a host of new graphic methods (boxplots, suspended rootograms, two-way table displays, etc.), which were soon implemented in new software systems. You probably know the rest—an explosive growth of new ideas for high-dimensional data and statistical models, machine learning methods, and so forth.",
    "crumbs": [
      "Orienting Ideas",
      "Warm-up Exercises"
    ]
  },
  {
    "objectID": "01-Prelude.html#the-magic-of-graphs",
    "href": "01-Prelude.html#the-magic-of-graphs",
    "title": "Warm-up Exercises",
    "section": "",
    "text": "There is a magic in graphs. The profile of a curve reveals in a flash a whole situation—the life history of an epidemic, a panic, or an era of prosperity. The curve informs the mind, awakens the imagination, convinces. — Henry Hubbard, in Foreword to Brinton (1939), Graphic Presentation\n\n\nThe graphic art depicts magnitudes to the eye. It does more. It compels the seeing of relations. We may portray by simple graphic methods whole masses of intricate routine, the organization of an enterprise, or the plan of a campaign. Graphs serve as storm signals for the manager, statesman, engineer; as potent narratives for the actuary, statist, naturalist; and as forceful engines of research for science, technology and industry. They display results. They disclose new facts and laws. They reveal discoveries as the bud unfolds the flower. — Henry Hubbard, in Foreword to Brinton (1939), Graphic Presentation\n\n\n\n\nGraphic discoveries, 1900–1950\nAll the modern forms of data visualization—the bar and pie chart, line graphs and the scatterplot had been invented 100 years earlier. Now, in this period, roughly 1900–1950, graphical methods were used, perhaps for the first time, to provide new insights, discoveries, and theories in astronomy, physics, biology, and other sciences.\n\n        \n\n \nThree useful examples are:\n\nEdward W. Maunder (1904) plotted the occurrence of sunspots by their latitude over time. He observed a consistent, repeating pattern like the wings of a butterfies, with sunspots drifting toward the equator in 11 year cycles. This pattern proved crucial for understanding the properties of the Sun’s magnetic field.\nEjnar Hertzsprung (1911) and Henry Norris Russell (1914) constructed plots of the luminosity of stars in relation to their color temperature and discovered an intriguing coherent pattern, but with a cluster of stars (“Red Giants”) detached from that main sequence. They used this to explain the changes as a star evolves. It provided an entirely new way to look at stars, and laid the groundwork for modern stellar physics and evolution. See Spence & Garrison (1993) for the full story of this graph.\nHenry Gwyn Jeffreys Moseley (1913) discovered, largely on graphical analysis, that the concept of atomic number (number of protons in an atom) rather than atomic weight (protons + neutrons = weight) gave a very simple theory of chemical elements. His plot of serial numbers of the elements vs. square root of frequencies from X-ray spectra showed a striking pattern of a series of straight lines. He noted gaps in the points and predicted the existence of several yet-undiscovered elements.\n\nNone of these graphs are particularly brilliant by today’s standards. They were all hand-drawn and rather crude. Yet in each case, something previously opaque became remarkably apparent in the graphic representation of data: a repeating pattern, some outliers departing from the rest, or the beautiful coherence of a set of linear relations.",
    "crumbs": [
      "Orienting Ideas",
      "Warm-up Exercises"
    ]
  },
  {
    "objectID": "01-Prelude.html#one-two-many",
    "href": "01-Prelude.html#one-two-many",
    "title": "Warm-up Exercises",
    "section": "ONE, TWO, MANY",
    "text": "ONE, TWO, MANY\nTo start thinking about the idea of “dimensions” of data and visualization, there is an old and helpful idea I learned from John Hartigan in my graduate days at Princeton:\n\nIn statistics and data visualization all methods can be classified by the number of dimensions contemplated, on a scale of ONE, TWO, MANY.\n\nBy this, he meant that, at a global level, all data, statistical summaries, and graphical displays could be classified as:\n\n\nunivariate: a single variable, considered in isolation (age, COVID cases, pizzas ordered). Univariate numerical summaries are means, medians, measures of variablilty, and so forth. Univariate displays include dot plots, boxplots, histograms and density estimates.\n\nbivariate: two variables, considered jointly. Numerical summaries include correlations, covariances and two-way tables of frequencies or measures of association for categorical variables. Bivariate displays include scatterplots and mosaic plots.\n\nmultivariate: three or more variables, considered jointly. Numerical summaries include correlation and covariance matrices, consisting of all pairwise values, but also derived measures from the analysis of these matrices (eigenvalues, eigenvectors). Graphical displays of multivariate data can sometimes be shown in 3D, but often involve multiple views of the data projected into 2D plots.\n\nAs a quasi-numerical scale, I refer to these as 1D, 2D and nD. This admits the possibility of half-integer cases, such as 1.5D, where the main focus is on a single variable, but that is classified by a simple factor (e.g., gender), or 2.5D where a 2D scatterplot can show other variables using color, shape or other visual attributes. John’s point in this classification was that once you’ve reached three variables, all higher dimensions involve similar summaries and data displays.\nUnivariate and bivariate methods and displays are well-known. This book is about how these ideas can be extended to an \\(n\\)-dimensional world. Three-dimensional data displays are now fairly easy to produce, even if they are sometimes difficult to understand. But how can we even think about four or more dimensions? The difficulty can be appreciated by considering the tale of Flatland.",
    "crumbs": [
      "Orienting Ideas",
      "Warm-up Exercises"
    ]
  },
  {
    "objectID": "01-Prelude.html#flatland",
    "href": "01-Prelude.html#flatland",
    "title": "Warm-up Exercises",
    "section": "Flatland",
    "text": "Flatland\n\nTo comport oneself with perfect propriety in Polygonal society, one ought to be a Polygon oneself. — Edwin A. Abbott, Flatland\n\nIn 1884, an English schoolmaster, Edwin Abbott Abbott, shook the world of Victorian culture with a slim volume, Flatland: A Romance of Many Dimensions (Abbott, 1884). He described a two-dimensional world, Flatland, inhabited entirely by geometric figures in the plane. His purpose was satirical, to poke fun at the social and gender class system at the time: Women were mere line segments, while men were represented as polygons with varying numbers of sides— a triangle was a working man, but acute isosceles were soldiers or criminals of very small angle; gentlemen and professionals had more sides. Abbot published this under the pseudonym, “A Square”, suggesting his place in the hierarchy.\n\nTrue, said the Sphere; it appears to you a Plane, because you are not accustomed to light and shade and perspective; just as in Flatland a Hexagon would appear a Straight Line to one who has not the Art of Sight Recognition. But in reality it is a Solid, as you shall learn by the sense of Feeling. — Edwin A. Abbott, Flatland\n\nBut how did it feel to be a member of a flatland society? How could a point (a newborn child?) understand a line (a woman)? How does a Triangle “see” a Hexagon or even a infinitely-sided Circle? Abbott introduces the very idea of different dimensions of existence through dreams and visions:\n\nA Square dreams of visiting a one-dimensional Lineland where men appear as lines, and women are merely “illustrious points”, but the inhabitants can only see the Square as lines.\nIn a vision, the Square is visited by a Sphere, to illustrate what a 2D Flatlander could understand from a 3D sphere (Figure 1) that passes through the plane he inhabits. It is a large circle when seen at the moment of its’ greatest extent. As the Spehere rises, it becomes progressively smaller, until it becomes a point, and then vanishes.\n\n\n\n\n\n\n\n\nFigure 1: A 2D Flatlander seeing a sphere as it passes through Flatland. The line, labeled ‘My Eye’ indicates what the Flatlander would see. Source: Abbott (1884)\n\n\n\n\nAbbott goes on to state what could be considered as a demonstration (or proof) by induction of the difficulties of seeing in 1, 2, 3 dimensions, and how the idea motion over time (one more dimension) could allow citizens of any 1D, 2D, 3D world to contemplate one more dimension.\n\nIn One Dimensions, did not a moving Point produce a Line with two terminal points? In two Dimensions, did not a moving Line produce a Square with four terminal points? In Three Dimensions, did not a moving Square produce—did not the eyes of mine behold it—that blessed being, a Cube, with eight terminal points? And in Four Dimensions, shall not a moving Cube—alas, for Analogy, and alas for the Progress of Truth if it be not so—shall not, I say the motion of a divine Cube result in a still more divine organization with sixteen terminal points? — Edwin A. Abbott\n\nFor Abbot, the way for a citizen of any world to imagine one more dimension was to consider how a higher-dimensional object would change over time.1 A line moved over time could produce a rectangle as shown in Figure 2; that rectangle moving in another direction over time would produce a 3D figure, and so forth.\n\n\n\n\n\n\n\nFigure 2: Geometrical objects in 1 to 4 dimensions. One more dimension can be thought of as the trace of movement over time.\n\n\n\n\nBut wait! Where does that 4D thing (a tesseract) come from? To really see a tesseract it helps to view it in an animation over time (Figure 3). But like the Square, contemplating 3D from a 2D world, it takes some imagination.\n\n\n\n\n\nFigure 3: Animation of a tesseract, a cube changing over time.\n\n\nYet the deep mathematics of more than three dimensions only emerged in the 19th century. In Newtonian mechanics, space and time were always considered independent of each other. Our familiar three-dimensional space, of length, width, and height had formed the backbone of Euclidean geometry for millenea.\nHowever, the idea that space and time are indeed interwoven was first proposed by German mathematician Hermann Minkowski (1864–1909) in a 1908 lecture titled “Space and Time”.2 This was a powerful idea. It bore fruit when Albert Einstein revolutionized the Newtonian conceptions of gravity in 1915 when he presented a theory of general relativity which was based primarily on the fact that mass and energy warp the fabric of four-dimensional spacetime.\nThe parable of Flatland can provide inspiration for statistical thinking and data visualization. Once we go beyond bivariate statistics and 2D plots, we are in a multivariate world of possibly MANY dimensions. It takes only some imagination and suitable methods to get there.\nLike Abbott’s Flatland, this book is a romance, in many dimensions, of what we can learn from modern methods of data visualization.",
    "crumbs": [
      "Orienting Ideas",
      "Warm-up Exercises"
    ]
  },
  {
    "objectID": "01-Prelude.html#eureka",
    "href": "01-Prelude.html#eureka",
    "title": "Warm-up Exercises",
    "section": "EUREKA!",
    "text": "EUREKA!\nEven modest sized multivariate data can have secrets that can be revealed in the right view. As an example, David Coleman at RCA Laboratories in Princeton, N.J. generated a dataset of five (fictitious) measurements of grains of pollen for the 1986 Data Exposition at the Joint statistical Meetings, now available as HistData::Pollen.\nThe first three variables are the lengths of geometric features 3848 observed sampled pollen grains – in the x, y, and z dimensions: a ridge along x, a nub in the y direction, and a crack along the z dimension. The fourth variable is pollen grain weight, and the fifth is density. The challenge was to “find something interesting” in this dataset.  \nThis problem is a case of searching for an unspecified needle in a five dimensional haystack. Those who solved the puzzle were able to find an orientation of this 5-dimensional dataset, such that zooming in revealed a magic word, “EUREKA” spelled in points, as in the following figure which shows four successive magnifications of the data (clockwise, from the upper left).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Four views of the pollen data, zooming in, clockwise from the upper left to discover the word “EUREKA”.\n\n\nThe path to finding the hidden word can be seen better in a 3D animation. The rgl package (Murdoch & Adler, 2025) is used to create a 3D scatterplot of the first three variables. Then the animation package (Xie, 2021) package is used to record a sequence of images, adjusting the rgl::par3d(zoom) value.\n\nShow the codelibrary(animation)\nlibrary(rgl)\ndata(pollen, package = \"animation\")\noopt = ani.options(interval = 0.05)\n## adjust the viewpoint\nuM =\n  matrix(c(-0.3709192276, -0.5133571028, -0.7738776206, 0, \n           -0.7305060625,  0.6758151054, -0.0981751680, 0, \n            0.57339602708, 0.5289064049, -0.6256819367, 0, \n           0, 0, 0, 1), 4, 4)\nopen3d(userMatrix = uM, \n       windowRect = c(10, 10, 510, 510))\n\nplot3d(pollen[, 1:3])\n\n# zoom in\nzm = seq(1, 0.045, length = 200)\npar3d(zoom = 1)\nfor (i in 1:length(zm)) {\n  par3d(zoom = zm[i])\n  ani.pause()\n}\nani.options(oopt)\n\n\n\n\n\n\nAnimation of zooming in on the pollen data to reveal the word “EUREKA”. This figure only appears in the online version.\n\n\n\nFigure 5\n\n\nThe vague idea of finding “something interesting” in high-D data was first posed by John and Paul Tukey (1985) as an exploratory visualization method they called scagnostics, short for “scatterplot diagnostics”. Wilkinson et al. (2005) made this idea concrete by defining measures of features such as “clumpy”, “striated”, “convex”, “skinny”, and so forth. These methods are implemented in the scagnostics and cassowaryr packages.\nAnother idea is to search through high-D data space for views or projections with interesting features, initially called projection pursuit (Friedman, 1987; Friedman & Tukey, 1974). Some modern methods for this are illustrated in Animated tours.",
    "crumbs": [
      "Orienting Ideas",
      "Warm-up Exercises"
    ]
  },
  {
    "objectID": "01-Prelude.html#sec-discoveries",
    "href": "01-Prelude.html#sec-discoveries",
    "title": "Warm-up Exercises",
    "section": "Multivariate scientific discoveries",
    "text": "Multivariate scientific discoveries\nLest the EUREKA example seem contrived (which it admittedly is), truly multivariate visualization has played an important role in quite a few scientific discoveries. Among these, Francis Galton’s (1863) discovery of the anti-cyclonic pattern of wind direction in relation to barometric pressure from many weather measures recorded systematically across all weather stations, lighthouses and observatories in Europe in December 1861 stands out as the best example of a scientific discovery achieved almost entirely through graphical means–– something that was totally unexpected, and purely the product of his use of remarkably novel high-dimensional graphs (Friendly & Wainer, 2021, pp. 170–173).\nA more recent example is the discovery of two general classes in the development of Type 2 diabetes by Reaven & Miller (1979), using PRIM-9 (Fishkeller et al., 1974), the first computer system for high-dimensional visualization3. In an earlier study Reaven & Miller (1968) examined the relation between blood glucose levels and the production of insulin in normal subjects and in patients with varying degrees of hyperglycemia (elevated blood sugar level). They found a peculiar ‘’horse shoe’’ shape in this relation (shown in Figure 6), about which they could only speculate: perhaps individuals with the best glucose tolerance also had the lowest levels of insulin as a response to an oral dose of glucose; perhaps those with low glucose response could secrete higher levels of insulin; perhaps those who were low on both glucose and insulin responses followed some other mechanism. In 2D plots, this was a mystery. \n\ndata(Diabetes, package=\"heplots\")\nplot(instest ~ glutest, data=Diabetes, \n     pch=16,\n     cex.lab=1.25,\n     xlab=\"Glucose response\",\n     ylab=\"Insulin response\")\n\n\n\n\n\n\nFigure 6: Reproduction of a graph similar to that from Reaven & Miller (1968) on the relationship between glucose and insulin response to being given an oral dose of glucose.\n\n\n\n\nAn answer to their questions came ten years later, when they were able to visualize similar but new data in 3D using the PRIM-9 system. In a carefully controlled study, they also measured ‘’steady state plasma glucose’’ (SSPG), a measure of the efficiency of insulin use in the body, where large values mean insulin resistance, as well as other variables. PRIM-9 allowed them to explore various sets of three variables, and, more importantly, to rotate a given plot in three dimensions to search for interesting features. One plot that stood out concerned the relation between plasma glucose response, plasma insulin response and SSPG response, shown in Figure 7.\n\n\n\n\n\n\n\nFigure 7: Artist’s rendition of data from Reaven & Miller (1979) as seen in three dimensions using the PRIM-9 system. Labels for the clusters have been added, identifying the three groups of patients. Source: Reaven & Miller (1979).\n\n\n\n\nFrom this graphical insight, they were able to classify the participants into three groups, based on clinical levels of glucose and insulin. The people in the wing on the left in Figure 7 were considered to have overt diabetes, the most advanced form, characterized by elevated fasting blood glucose concentration and classical diabetic symptoms. Those in the right wing were classified as latent or chemical diabetics, with no symptoms of diabetes but demonstrable abnormality of oral or intravenous glucose tolerance. Those in the central blob were classified as normal.\nPrevious thinking was that Type 2 diabetes (when the body cannot make enough insulin, as opposed to Type I, an autoimmune condition where the pancreatic cells have been destroyed) progressed from the chemical stage to an overt one in a smooth transition. However, it was clear from Figure 7 that the only “path” from one to the other lead through the cluster of normal patients near the origin, so that explanation must be wrong. Instead, this suggested that the chemical and overt diabetics were distinct classes. Indeed, longitudinal studies showed that patients classified as chemical diabetics rarely developed the overt form. Our understanding of the etiology of Type 2 diabetes was altered dramatically by the power of high-D interactive graphics.\n\n\n\n\nAbbott, E. A. (1884). Flatland: A romance of many dimensions. Buccaneer Books.\n\n\nBrinton, W. C. (1939). Graphic presentation. Brinton Associates. https://archive.org/details/graphicpresentat00brinrich\n\n\nCajori, F. (1926). Origins of fourth dimension concepts. The American Mathematical Monthly, 33(8), 397–406. https://doi.org/10.1080/00029890.1926.11986607\n\n\nCostelloe, M. F. P. (1915). Graphic methods and the presentation of this subject to first year college students. Nebraska Blue Print.\n\n\nFishkeller, M. A., Friedman, J. H., & Tukey, J. W. (1974). PRIM-9, an interactive multidimensional data display and analysis system. Proceedings of the Pacific ACM Regional Conference.\n\n\nFriedman, J. H. (1987). Exploratory projection pursuit. Journal of the American Statistical Association, 82, 249–266.\n\n\nFriedman, J. H., & Tukey, J. W. (1974). A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on Computers, C-23(9), 881–890. https://doi.org/10.1109/T-C.1974.224051\n\n\nFriendly, M., & Denis, D. (2001). The roots and branches of statistical graphics. Journal de La Société Française de Statistique, 141(4), 51–60. http://www.numdam.org/item/JSFS_2000__141_4_51_0.pdf\n\n\nFriendly, M., Sigal, M., & Harnanansingh, D. (2015). The Milestones Project: A database for the history of data visualization. In M. Kimball & C. Kostelnick (Eds.), Visible numbers: The history of data visualization. Ashgate Press.\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034\n\n\nGalton, F. (1863). Meteorographica, or methods of mapping the weather. Macmillan. http://www.mugu.com/galton/books/meteorographica/index.htm\n\n\nHaskell, A. C. (1919). How to make and use graphic charts. Codex.\n\n\nHertzsprung, E. (1911). Publikationen des astrophysikalischen observatorium zu Potsdam.\n\n\nMaunder, E. W. (1904). Note on the distribution of sun-spots in heliographic latitude, 1874 to 1902. Royal Astronomical Society Monthly Notices, 64, 747–761.\n\n\nMoseley, H. (1913). The high frequency spectra of the elements. Philosophical Magazine, 26, 1024–1034.\n\n\nMurdoch, D., & Adler, D. (2025). Rgl: 3D visualization using OpenGL. https://doi.org/10.32614/CRAN.package.rgl\n\n\nPeddle, J. B. (1910). The construction of graphical charts. McGraw-Hill.\n\n\nReaven, G. M., & Miller, R. G. (1968). Study of the relationship between glucose and insulin responses to an oral glucose load in man. Diabetes, 17(9), 560–569. https://doi.org/10.2337/diab.17.9.560\n\n\nReaven, G. M., & Miller, R. G. (1979). An attempt to define the nature of chemical diabetes using a multidimensional analysis. Diabetologia, 16, 17–24.\n\n\nRussell, H. N. (1914). Relations between the spectra and other characteristics of the stars. Popular Astronomy, 22, 275–294.\n\n\nSpence, I., & Garrison, R. F. (1993). A remarkable scatterplot. The American Statistician, 47(1), 12–19.\n\n\nTukey, J. W. (1962). The future of data analysis. The Annals of Mathematical Statistics, 33(1), 1–67. https://doi.org/10.1214/aoms/1177704711\n\n\nTukey, J. W. (1977). Exploratory data analysis. Addison Wesley.\n\n\nTukey, J. W., & Tukey, P. A. (1985). Computer graphics and exploratory data analysis: An introduction. Proceedings of the Sixth Annual Conference and Exposition: Computer Graphics85, III, 773–785.\n\n\nWilkinson, L., Anand, A., & Grossman, R. L. (2005). Graph-theoretic scagnostics. In J. T. Stasko & M. O. Ward (Eds.), Proceedings of the IEEE information visualization 2005 (pp. 157–164). IEEE Computer Society. http://dblp.uni-trier.de/db/conf/infovis/infovis2005.html#WilkinsonAG05\n\n\nXie, Y. (2021). Animation: A gallery of animations in statistics and utilities to create animations. https://yihui.org/animation/",
    "crumbs": [
      "Orienting Ideas",
      "Warm-up Exercises"
    ]
  },
  {
    "objectID": "01-Prelude.html#footnotes",
    "href": "01-Prelude.html#footnotes",
    "title": "Warm-up Exercises",
    "section": "",
    "text": "In his famous TV series, Cosmos, Carl Sagan provides an intriguing video presentation Flatland and the 4th dimension. However, as far back as 1754 (Cajori, 1926), the idea of adding a fourth dimension appears in Jean le Rond d’Alembert’s “Dimensions”, and one realization of a four-dimensional object is a tesseract, shown in Figure 2.↩︎\nSee the translation by Lewertoff and Petkov, “Space and Time Minkowski’s Papers on Relativity”, https://bit.ly/45NvgZR↩︎\nPRIM-9 is an acronym for Picturing, Rotation, Isolation and Masking in up to 9 dimensions. These operations are fundamental to interactive and dynamic data visualization.↩︎",
    "crumbs": [
      "Orienting Ideas",
      "Warm-up Exercises"
    ]
  },
  {
    "objectID": "02-intro.html",
    "href": "02-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Why use a multivariate design?\nA major goal of this book is to introduce you to multivariate thinking in research design and data analysis. This is accomplished with a collection of powerful graphical methods designed to make it easier to understand and communicate results when there are multiple response variables to be considered together. However, these methods are somewhat more complex than the standard univariate ones, so it is worth considering these questions: Why should you to go to this trouble? What’s in it for me?\nThe first important point is that reality in many research domains is inherently multivariate, particularly in the social and behavioral sciences. Theoretical constructs such as “depression”, “academic achievement”, “self-concept”, “happiness” or “perfectionism” can often be measured by different scales, or have been identified to have more than one aspect or context worthy of study.\nConceptual advantages\nAnalyzing these together from a multivariate approach can reveal relationships completely missed or ignored by simpler univariate analyses. Theory is strengthened and becomes more nuanced from a multivariate perspective.\nYou can understand each of those terms, but actually all of these constructs are inherently multidimensional. The idea of “self-concept”, for instance is comprised of all the beliefs that an individual has about him/herself. These include physical, social, intellectual, family, emotional, and professional/academic domains, which reflect how individuals perceive their bodies, roles in society, abilities, as well as their familial connections, and competencies in work and school.\nEven something simpler, like academic achievements of adolescents, aged 10–16 is comprised of measures of their knowledge and performance in the domains of reading, mathematics, science, history, etc. Say we want to assess the influence of predictors such as parent encouragement, their socioeconomic status and school environmental variables on these outcomes. In a comprehensive study, the following questions are of interest:\nSimilarly, if psychiatric patients in various diagnostic categories are measured on a battery of tests related to social skills and cognitive functioning, we might want to know:\nSuch questions obviously concern more than just the separate univariate relations of each response to the predictors. Equally, perhaps more importantly, are questions of how the response variables are predicted jointly considering their correlations that a multivariate approach can reveal.\nStatistical advantages\nA second important point is that a multivariate modeling approach can offer statistical advantages:",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-intro.html#why-use-a-multivariate-design",
    "href": "02-intro.html#why-use-a-multivariate-design",
    "title": "1  Introduction",
    "section": "",
    "text": "Do predictors (parental encouragement, …) affect all of these outcomes? Or do some of these have weak or null effects?\nDo they affect them in the same or different ways? That is, do their effects tend to go in the same directions?\nHow many different aspects of academic achievement can be distinguished in the predictors? Equivalently, is academic achievement unidimensional or multidimensional in relation to the predictors?\n\n\n\nWhich measures best discriminate among the diagnostic groups?\nWhich measures are most predictive of positive outcomes?\nFurther, how are the relationships between the outcomes affected by the predictors?\n\n\n\n\n\n\n\n\nWhat about SEM?\n\n\n\nStructural equation modeling (SEM) offers another route to explore and analyze the relationships among multiple predictors and multiple responses. They have the advantage of being able to test potentially complex systems of linear equations in very flexible ways. However, these methods are often far removed from data analysis per se because they typically start with data summaries such as means and covariance matrices. The raw data is nicely bundled up in these summaries, but consequently there is little room to learn from features such as non-linear relations, interesting clusters, or anomalous observations that might threaten the validity of a proposed model. But see Prokofieva et al. (2023) for an approach (ESEM) using R that combines exploratory analysis with standard SEM, or Brandmaier et al. (2014) for an approach combining SEM and decision trees to allow data-driven refinement of models.\nMoreover, except for path diagrams, they usually offer little in the way of visualization methods to aid in understanding and communicating the results. The graphical methods we describe here can also be useful in a SEM context. For example, multivariate multiple regression models (Section 10.6) and canonical correlation (Section 11.10) can be used to explore relationships among the observed variables used in a SEM, but in a data-centric way. \n\n\n\n\n\nThey can have increased statistical power to detect a significant effect, because multivariate tests pool the strength of effects across a collection of positively related response variables.\nThis easily avoids inflated Type I error rates (false positives) from multiple separate tests on each of the outcomes.\nWe gain deeper insights into how outcome variables vary together.\nWith more than a few outcome variables dimension reduction methods offer views of the data and model in 2D plots that capture their essence.\n\n\n1.1.1 Multivariate vs. multivariable methods\n\nmultivariate \\(\\ne\\) multivariable\n\n“Multi-” terminology is important. In this era of multivitamins, multitools, multifactor authentication and even the multiverse, it is well to understand the distinction between multivariate and multivariable methods as these terms are generally used and as I use them here in relation to statistical methods and data visualization. The distinction is simple:\n\nMultivariable methods have a single dependent variable and more than one independent variables or covariates, such as in multiple regression. Multiple regression is the prime example.\nMultivariate methods for linear models, such as multivariate multiple regression or multivariate analysis of variance, have more than one dependent, response or outcome variable. Canonical correlation analysis considers the relation between two sets of variables, but treats them symmetrically. Other multivariate methods such as principal components analysis or factor analysis don’t distinguish between independent and dependent, but treat all variables on an equal footing in a single set.\n\n\n1.1.2 Ubiquity of multivariate research designs\nMultivariate response designs are increasingly common in applied behavioral and social science research, and are utilized to analyze a wide range of phenomena. For instance, as an introductory exercise, graduate students enrolled in a multivariate data analysis course1 in the Psychology Department at York University between 1990 and 2015 were asked to scan one or more journals in their area and to find at least one paper that utilized some form of multivariate analysis.\nWith an average of 15–20 students per year and over approximately 20 years, this yielded a bibliographic database containing 405 exemplars (Friendly & 6140 Members, 2012). These used a variety of multivariate and supplementary univariate methods, ranging from MANOVA to path analysis, from canonical correlation to multidimensional scaling, which were categorized by statistical method using keyword terms.\nOverall, MANOVA, factor analysis, multiple regression, correlation, and principal components analysis were utilized the most frequently. See Figure 1.1 for a summary of these results, using a heatmap tabular display of frequencies by time. An overall categorization pertaining to the type of multivariate model used (CDA = Categorical data analysis, FA = Factor analysis, GDA = Geometric data analysis, and LM = Linear models) is shown in the right margin. This is clearly not a representative sample of the literature, but it does illustrate an increase of articles deemed noteworthy over time.\n\n\n\n\n\n\n\nFigure 1.1: Heatmap of frequencies of method keyword by decade for research papers submitted by students in a graduate level multivariate data analysis course.\n\n\n\n\nFurther, a content analysis of the article titles reveals the variation in topics discussed. For instance, Figure 1.2 is a wordcloud showing the 50 most frequently-used words, with size proportional to occurrence. While a few of the terms are analytic (e.g., the most frequently used term is “analysis”, with 33 occurrences), others showcase the breadth of psychological research, as personality, factor, social, memory, children, cognitive, behavior, all appear in the top 15, with 14 or more appearances each.\n\n\n\n\n\n\n\nFigure 1.2: Wordcloud illustrating the topic content of the articles in the bibliographic database, showing the 50 most frequently-used, with font size proportional to frequency.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-intro.html#linear-models-univariate-to-multivariate",
    "href": "02-intro.html#linear-models-univariate-to-multivariate",
    "title": "1  Introduction",
    "section": "\n1.2 Linear models: Univariate to multivariate",
    "text": "1.2 Linear models: Univariate to multivariate\nThe path to multivariate thinking in this book is eased by the simplicity of extending univariate models to multivariate ones in notation and computation. For classical linear models for ANOVA and regression, the step from a univariate model for a single response, \\(y\\), to a multivariate one for a collection of \\(p\\) responses, \\(\\mathbf{y}\\) is conceptually very easy. That’s because the univariate model with \\(q\\) predictors,\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_q x_q + \\epsilon_i \\;, \\]\nwhen cast in matrix terms,\n\\[\n\\mathbf{y} = \\mathbf{X} \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\;, \\quad\\mbox{   with   }\\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N} (0, \\sigma^2 \\mathbf{I}) \\;,\n\\]\ngeneralizes directly to an analogous multivariate linear model (MLM),\n\\[\n\\mathbf{Y} = [\\mathbf{y_1}, \\mathbf{y_2}, \\dots, \\mathbf{y_p}] = \\mathbf{X} \\; \\mathbf{B} + \\boldsymbol{\\Large\\varepsilon}\\quad\\mbox{   with   }\\quad \\boldsymbol{\\Large\\varepsilon}\\sim \\mathcal{N} (\\mathbf{0}, \\boldsymbol{\\Sigma}) \\; ,\n\\]\nfor multiple responses (as will be discussed in detail in Chapter 10). The design matrix, \\(\\mathbf{X}\\) remains the same, and the vector \\(\\beta\\) of coefficients simply becomes a matrix \\(\\mathbf{B}\\), with one column for each of the \\(p\\) outcome variables.\nComputationally, all of these are handled by a single R function, lm(). Methods for the resulting \"lm\" objects (also of class mlm\" for the multivariate case) provide general means for analysis and graphics and have been developed widely in various R packages.\nHappily as well, hypothesis tests for the MLM are also straight-forward generalizations of the familiar \\(F\\) and \\(t\\)-tests for univariate response models. Moreover, there is a rich geometry underlying these generalizations (Friendly et al., 2013) which we can exploit for understanding and visualization in Chapter 11.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-intro.html#visualization-is-harder",
    "href": "02-intro.html#visualization-is-harder",
    "title": "1  Introduction",
    "section": "\n1.3 Visualization is harder",
    "text": "1.3 Visualization is harder\nHowever, with two or more response variables, visualizations for multivariate models are not as simple as they are for their univariate counterparts. These include plots for understanding the data at hand, the effects of predictors, model parameters, or model diagnostics.\nConsequently, the results of such studies are often explored and discussed solely in terms of coefficients and significance, and visualizations of the relationships are only provided for one response variable at a time, if at all. This tradition can mask important nuances, and lead researchers to draw erroneous conclusions.\nThe aim of this book is to describe and illustrate some central methods that we have developed over the last thirty years that aid in the understanding and communication of the results of multivariate models (Friendly, 1994, 2002, 2007; Friendly & Meyer, 2016). These methods for quantitative data rely on data ellipsoids as simple, minimally sufficient visualizations of variance that can be shown in 2D and 3D plots. As will be demonstrated, the Hypothesis-Error (HE) plot framework (Section 11.1) applies this idea to the results of multivariate tests of linear hypotheses. \nFurther, in the case where there are more than just a few outcome variables, the important nectar of their relationships to predictors can often be distilled in a multivariate juicer— a projection of the multivariate relationships to the predictors in the low-D space that captures most of the flavor. This idea can be applied using animated tours of high-D data (Section 3.11), principal components analysis and biplots (Section 4.3) canonical correlation plots and with canonical discriminant HE plots (Section 11.7). \nA glimpse of this idea can be seen in Figure 1.3, the cover image from Douglas Hofstadter’s Gödel, Bach and Escher (1979). The complete image in 3D is made up of two 3D solids, with various complicated cutouts and holes. Each 2D view of this is the shadow, or projection shown on the walls. These 2D views capture some salient aspects of the complete 3D scene, but as in the case of the Flatlander Square trying to understand the Sphere, it takes some imagination to get there. The methods of this book are designed to expand your understanding of relationships in higher-dimensional data.\n\n\n\n\n\n\n\n\nFigure 1.3: Projection: The cover image from Douglas Hofstadter’s Gödel, Bach and Escher (1979) illustrates projection of 3D solids onto each 2D plane. Each 2D view captures some salient aspect of the complete figure, but is incomplete.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-intro.html#sec-problems",
    "href": "02-intro.html#sec-problems",
    "title": "1  Introduction",
    "section": "\n1.4 Problems in understanding and communicating MLM results",
    "text": "1.4 Problems in understanding and communicating MLM results\nIn my consulting practice within the Statistical Consulting Service at York University, I see hundreds of clients each year ranging from advanced undergraduate thesis students, to graduate students and faculty from a variety of fields. Initially, most clients used SAS or SPSS, but now most use R or Stata for their analyses.\nOver the last three decades, and across these groups, I have noticed an increasing desire to utilize multivariate methods. As researchers are exposed to the utility and power of multivariate tests, they see them as an appealing alternative to running many univariate ANOVAs or multiple regressions for each response variable separately.\nHowever, multivariate analyses are more complicated than such approaches, especially when it comes to understanding and communicating results. Output is typically voluminous, and researchers will often get lost in the numbers. While software in SAS, SPSS and R make tabular summary displays relatively easy, these often obscure the findings that researchers are most interested in. The most common analytic oversights that I have observed are:\n\nAtomistic model checking: Researchers have mostly learned the assumptions (the Holy Trinity of normality, constant variance and independence) of univariate linear models, but then apply univariate tests (e.g., Shapiro-Wilk) and diagnostic plots (normal QQ plots) to every predictor and every response. Instead, I recommend more comprehensive graphical methods like scale-location and influence plots (Section 6.1.2) and plots like chi-square QQ plots (Section 10.7) for multivariate models.\nBonferroni everywhere: Faced with the task of reporting the results for multiple response measures and a collection of predictors for each, a common tendency is to run (and sometimes report) each of the separate univariate response models and then apply a correction for multiple testing. Not only is this confusing and awkward to report, but it is largely unnecessary because the multivariate tests provide protection for multiple testing. Instead, I emphasize thinking about substantive questions concerning differences among groups and testing these with contrasts (Section 5.1.3) and linear hypotheses (Section 10.3.1).\nReverting to univariate visualizations: To display results, some software makes some visualization methods available through menu choices or syntax, but usually these are the wrong (or at least unhelpful) choices, in that they generate separate univariate graphs for the individual responses.\n\nThis book discusses a few essential procedures for multivariate linear models, how their interpretation can be aided through the use of well-crafted (though novel) visualizations, and provides replicable sample code in R to showcase their use in applied behavioral research. \n\n\n\n\n\n\nBrandmaier, A., Von Oertzen, T., Mcardle, J., & Lindenberger, U. (2014). Exploratory data mining with structural equation model trees. In J. J. M. & G. Ritschard (Ed.), Contemporary issues in exploratory data mining in the behavioral sciences (pp. 96–127).\n\n\nFriendly, M. (1994). Mosaic displays for multi-way contingency tables. Journal of the American Statistical Association, 89, 190–200. http://www.jstor.org/stable/2291215\n\n\nFriendly, M. (2002). Corrgrams: Exploratory displays for correlation matrices. The American Statistician, 56(4), 316–324. https://doi.org/10.1198/000313002533\n\n\nFriendly, M. (2007). HE plots for multivariate general linear models. Journal of Computational and Graphical Statistics, 16(2), 421–444. https://doi.org/10.1198/106186007X208407\n\n\nFriendly, M., & 6140 Members. (2012). Psychology 6140 research applications papers. online document. http://www.psych.yorku.ca/lab/psy6140/py614bib.pdf\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nHofstadter, D. R. (1979). Gödel, escher, bach: An eternal golden braid. Basic Books.\n\n\nProkofieva, M., Zarate, D., Parker, A., Palikara, O., & Stavropoulos, V. (2023). Exploratory structural equation modeling: A streamlined step by step approach using the r project software. BMC Psychiatry, 23(1). https://doi.org/10.1186/s12888-023-05028-9",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-intro.html#footnotes",
    "href": "02-intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "The web pages for this course can be found at Psychology 6140: Multivariate Data Analysis. I last taught this in 2016–2017.↩︎",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "03-getting_started.html",
    "href": "03-getting_started.html",
    "title": "2  Getting Started",
    "section": "",
    "text": "2.1 Why plot your data?\nAt the time the Farhquhar brothers wrote this pithy aphorism, graphical methods for understanding data had advanced considerably, but most statisticians still relied on tables to their communicate results, prompting their complaint. Most data tables we see today are designed for looking something up like the number of measles cases in Ontario compared to other provinces. Tables are not usually designed to reveal patterns, trends or anomalies, although this can be accomplished with modern table generating software such as the tinytable or gt packages.1\nThe main graphic forms we use today—the pie chart, line graphs and bar—were invented by William Playfair around 1800 (Playfair, 1786, 1801). The scatterplot arrived shortly after (Herschel, 1833) and thematic maps showing the spatial distributions of social variables (crime, suicides, literacy) were used for the first time to reason about important societal questions (Guerry, 1833) such as “is increased education associated with lower rates of crime?”\nIn the last half of the 18th Century, the idea of correlation was developed (Galton, 1886; Pearson, 1896) and the period, roughly 1860–1890, dubbed the “Golden Age of Graphics” (Friendly, 2008; Funkhouser, 1937) became the richest period of innovation and beauty in the entire history of data visualization. During this time there was an incredible development of visual thinking, represented by the work of Charles Joseph Minard, advances in the role of visualization within scientific discovery, as illustrated through Francis Galton, and graphical excellence, embodied in state statistical atlases produced in France and elsewhere. See Friendly (2008); Friendly & Wainer (2021) for this history. .\nThis chapter introduces the importance of graphing data through three nearly classic stories with the following themes:",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "03-getting_started.html#sec-why_plot",
    "href": "03-getting_started.html#sec-why_plot",
    "title": "2  Getting Started",
    "section": "",
    "text": "summary statistics are not enough: Anscombe’s Quartet demonstrates datasets that are indistinguishable by numerical summary statistics (mean, standard deviation, correlation), but whose relationships are vastly different.\none lousy point can ruin your day: A researcher is mystified by a difference between a correlation for men and women until she plots the data.\nfinding the signal in noise: The story of the US 1970 Draft Lottery shows how a weak, but reliable signal, reflecting bias in a process can be revealed by graphical enhancement and summarization.\n\n\n2.1.1 Anscombe’s Quartet\nIn 1973, Francis Anscombe (Anscombe, 1973) famously constructed a set of four datasets illustrate the importance of plotting the graphs before analyzing and model building, and the effect of unusual observations on fitted models. Now known as Anscombe’s Quartet, these datasets had identical statistical properties: the same means, standard deviations, correlations and regression lines. \nHis purpose was to debunk three notions that had been prevalent at the time:\n\nNumerical calculations are exact, but graphs are coarse and limited by perception and resolution;\nFor any particular kind of statistical data there is just one set of calculations constituting a correct statistical analysis;\nPerforming intricate calculations is virtuous, whereas actually looking at the data is cheating.\n\nThe dataset datasets::anscombe has 11 observations, recorded in wide format, with variables x1:x4 and y1:y4.\n\ndata(anscombe) \nhead(anscombe)\n#   x1 x2 x3 x4   y1   y2    y3   y4\n# 1 10 10 10  8 8.04 9.14  7.46 6.58\n# 2  8  8  8  8 6.95 8.14  6.77 5.76\n# 3 13 13 13  8 7.58 8.74 12.74 7.71\n# 4  9  9  9  8 8.81 8.77  7.11 8.84\n# 5 11 11 11  8 8.33 9.26  7.81 8.47\n# 6 14 14 14  8 9.96 8.10  8.84 7.04\n\nThe following code transforms this data to long format and calculates some summary statistics for each dataset.\n\nanscombe_long &lt;- anscombe |&gt; \n  pivot_longer(everything(), \n               names_to = c(\".value\", \"dataset\"), \n               names_pattern = \"(.)(.)\"\n  ) |&gt;\n  arrange(dataset)\n\nanscombe_long |&gt;\n  group_by(dataset) |&gt;\n  summarise(xbar      = mean(x),\n            ybar      = mean(y),\n            r         = cor(x, y),\n            intercept = coef(lm(y ~ x))[1],\n            slope     = coef(lm(y ~ x))[2]\n         )\n# # A tibble: 4 × 6\n#   dataset  xbar  ybar     r intercept slope\n#   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n# 1 1           9  7.50 0.816      3.00 0.500\n# 2 2           9  7.50 0.816      3.00 0.5  \n# 3 3           9  7.5  0.816      3.00 0.500\n# 4 4           9  7.50 0.817      3.00 0.500\n\nAs we can see, all four datasets have nearly identical univariate and bivariate statistical measures. You can only see how they differ in graphs, which show their true natures to be vastly different.\nFigure 2.1 is an enhanced version of Anscombe’s plot of these data, adding helpful annotations to show visually the underlying statistical summaries.\n\n\n\n\n\n\n\nFigure 2.1: Scatterplots of Anscombe’s Quartet. Each plot shows the fitted regression line and a 68% data ellipse representing the correlation between \\(x\\) and \\(y\\).\n\n\n\n\nThis figure is produced as follows, using a single call to ggplot(), faceted by dataset. As we will see later (Section 3.2), the data ellipse  (produced by stat_ellipse()) reflects the correlation between the variables. \n\ndesc &lt;- tibble(\n  dataset = 1:4,\n  label = c(\"Pure error\", \"Lack of fit\", \"Outlier\", \"Influence\")\n)\n\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 4) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"red\", linewidth = 1.5) +\n  scale_x_continuous(breaks = seq(0,20,2)) +\n  scale_y_continuous(breaks = seq(0,12,2)) +\n  stat_ellipse(level = 0.5, color=col, type=\"norm\") +\n  geom_label(data=desc, aes(label = label), x=6, y=12) +\n  facet_wrap(~dataset, labeller = label_both) \n\nThe subplots are labeled with the statistical idea they reflect:\n\ndataset 1: Pure error. This is the typical case with well-behaved data. Variation of the points around the line reflect only measurement error or unreliability in the response, \\(y\\). \ndataset 2: Lack of fit. The data is clearly curvilinear, and would be very well described by a quadratic, y ~ poly(x, 2). This violates the assumption of linear regression that the fitted model has the correct form. \ndataset 3: Outlier. One point, second from the right, has a very large residual. Because this point is near the extreme of \\(x\\), it pulls the regression line towards it, as you can see by imagining a line through the remaining points. \ndataset 4: Influence. All but one of the points have the same \\(x\\) value. The one unusual point has sufficient influence to force the regression line to fit it exactly. \n\nOne moral from this example:\n\nLinear regression only “sees” a line. It does its’ best when the data are really linear. Because the line is fit by least squares, it pulls the line toward discrepant points to minimize the sum of squared residuals.\n\n\n\n\n\n\n\nDatasaurus Dozen\n\n\n\nThe method Anscombe used to compose his quartet is unknown, but it turns out that that there is a method to construct a wider collection of datasets with identical statistical properties. After all, in a bivariate dataset with \\(n\\) observations, the correlation has \\((n-2)\\) degrees of freedom, so it is possible to choose \\(n-2\\) of the \\((x, y)\\) pairs to yield any given value. As it happens, it is also possible to create any number of datasets with the same means, standard deviations and correlations with nearly any shape you like — even a dinosaur! \nThe Datasaurus Dozen was first publicized by Alberto Cairo in a blog post and are available in the datasauRus package (Gillespie et al., 2025). As shown in Figure 2.2 the sets include a star, cross, circle, bullseye, horizontal and vertical lines, and, of course the “dino”. The method (Matejka & Fitzmaurice, 2017) uses simulated annealing, an iterative process that perturbs the points in a scatterplot, moving them towards a given shape while keeping the statistical summaries close to the fixed target value.\nThe datasauRus package just contains the datasets, but a general method, called statistical metamers, for producing such datasets has been described by Elio Campitelli and implemented in the metamer package.\n\n\n\n\n\n\n\n\n\nFigure 2.2: Animation of the Dinosaur Dozen datasets. Source: https://youtu.be/It4UA75z_KQ\n\n\n\n\n\n\n\n\n\n\nQuartets\n\n\n\nThe essential idea of a statistical “quartet” is to illustrate four quite different datasets or circumstances that seem superficially the same, but yet are paradoxically very different when you look behind the scenes.\nFor example, in the context of causal analysis Gelman et al. (2023), illustrated sets of four graphs, within each of which all four represent the same average (latent) causal effect but with much different patterns of individual effects. McGowan et al. (2023) provide another illustration with four seemingly identical data sets each generated by a different causal mechanism. \nAs an example of machine learning models, Biecek et al. (2023), introduced the “Rashamon Quartet”, a synthetic dataset for which four models from different classes (linear model, regression tree, random forest, neural network) have practically identical predictive performance. In all cases, the paradox is solved when their visualization reveals the distinct ways of understanding structure in the data.  The quartets package (D’Agostino McGowan, 2023) contains these and other variations on this theme. \n\n\n\n\n2.1.2 One lousy point can ruin your day\nIn the mid 1980s, a consulting client had a strange problem.2 She was conducting a study of the relation between body image and weight preoccupation in exercising and non-exercising people (Davis, 1990). As part of the design, the researcher wanted to know if self-reported weight could be taken as a reliable indicator of true weight measured on a scale. It was expected that the correlations between reported and measured weight should be close to 1.0, and the slope of the regression lines for men and women should also be close to 1.0. The dataset is carData::Davis.\nShe was therefore very surprised to see the following numerical results: For men, the correlation was nearly perfect, but not so for women. \n\ndata(Davis, package=\"carData\")\nDavis &lt;- Davis |&gt;\n  drop_na()          # drop missing cases\nDavis |&gt;\n  group_by(sex) |&gt;\n  select(sex, weight, repwt) |&gt;\n  summarise(r = cor(weight, repwt))\n# # A tibble: 2 × 2\n#   sex       r\n#   &lt;fct&gt; &lt;dbl&gt;\n# 1 F     0.501\n# 2 M     0.979\n\nSimilarly, the regression lines showed the expected slope for men, but that for women was only 0.26.\n\nDavis |&gt;\n  nest(data = -sex) |&gt;\n  mutate(model = map(data, ~ lm(repwt ~ weight, data = .)),\n         tidied = map(model, tidy)) |&gt;\n  unnest(tidied) |&gt;\n  filter(term == \"weight\") |&gt;\n  select(sex, term, estimate, std.error)\n# # A tibble: 2 × 4\n#   sex   term   estimate std.error\n#   &lt;fct&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n# 1 M     weight    0.990    0.0229\n# 2 F     weight    0.262    0.0459\n\n“What could be wrong here?”, the client asked. The consultant replied with the obvious question:\n\nDid you plot your data?\n\nThe answer turned out to be one discrepant point, a female (case 12), whose measured weight was 166 kg (366 lbs!). This single point exerted so much influence that it pulled the fitted regression line down to a slope of only 0.26. \n\n# shorthand to position legend inside the figure\nlegend_inside &lt;- function(position) {          # simplify legend placement\n  theme(legend.position = \"inside\",\n        legend.position.inside = position)\n}\n\nDavis |&gt;\n  ggplot(aes(x = weight, y = repwt, \n             color = sex, shape = sex, linetype = sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  labs(x = \"Measured weight (kg)\", \n       y = \"Reported weight (kg)\") +\n  scale_linetype_manual(values = c(F = \"longdash\", \n                                   M = \"solid\")) +\n  legend_inside(c(.8, .8))\n\n\n\n\n\n\nFigure 2.3: Regression for Davis’ data on reported weight and measures weight for men and women. Separate regression lines, predicting reported weight from measured weight are shown for males and females. One highly unusual point is highlighted.\n\n\n\n\nIn this example, it was arguable that \\(x\\) and \\(y\\) axes should be reversed, to determine how well measured weight can be predicted from reported weight. In ggplot this can easily be done by reversing the x and y aesthetics.\n\nDavis |&gt;\n  ggplot(aes(y = weight, x = repwt, color = sex, shape=sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  labs(y = \"Measured weight (kg)\", \n       x = \"Reported weight (kg)\") +\n    geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  legend_inside(c(.8, .8))\n\n\n\n\n\n\nFigure 2.4: Regression for Davis’ data on reported weight and measures weight for men and women. Separate regression lines, predicting measured weight from reported weight are shown for males and females. The highly unusual point no longer has an effect on the fitted lines.\n\n\n\n\nIn Figure 2.4, this discrepant observation again stands out like a sore thumb, but it makes very little difference in the fitted line for females. The reason is that this point is well within the range of the \\(x\\) variable (repwt). To impact the slope of the regression line, an observation must be unusual in both \\(x\\) and \\(y\\). We take up the topic of how to detect influential observations and what to do about them in Chapter 6.\nThe value of such plots is not only that they can reveal possible problems with an analysis, but also help identify their reasons and suggest corrective action. What went wrong here? Examination of the original data showed that this woman (case 12) mistakenly switched the values, recording her reported weight in the box for measured weight and vice versa.\n\n\n2.1.3 Shaken, not stirred: The 1970 Draft Lottery\n\nAlthough we often hear that data speak for themselves, their voices can be soft and sly.—Frederick Mosteller (1983), Beginning Statistics with Data Analysis, p. 234.\n\nThe power of graphics is particularly evident when data contains a weak signal embedded in a field of noise. To the casual glance, there may seem to be nothing going on, but the signal can be made apparent in an incisive graph.\nA dramatic example of this occurred in 1969 when the U.S. military conducted a lottery, the first since World War II, to determine which young men would be called up to serve in the Vietnam War for 1970. The U.S. Selective Service had devised a system to rank eligible men according to a random drawing of their birthdays. There were 366 blue plastic capsules containing birth dates placed in a transparent glass container and drawn by hand to assign ranked order-of-call numbers to all men within the 18-26 age range.\n\n\n\n\n\n\n\nFigure 2.5: Congressman Alexander Pirnie (R-NY) drawing the first capsule for the Selective Service draft, Dec 1, 1969. Source: Wikipedia https://bit.ly/45c23sB\n\n\n\n\n\nIn an attempt to make the selection process also transparent, the proceeding was covered on radio, TV and film and the dates posted in order on a large display board. The first capsule—drawn by Congressman Alexander Pirnie (R-NY) of the House Armed Services Committee—contained the date September 14, so all men born on September 14 in any year between 1944 and 1950 were assigned lottery number 1, and would be drafted first. April 24 was drawn next, then December 30, February 14, and so on until June 8, selected last. At the time of the drawing, US officials stated that those with birthdays drawn in the first third would almost certainly be drafted, while those in the last third would probably avoid the draft (Fienberg, 1971).\nI watched this unfold with considerable interest because I was eligible for the Draft that year. I was dismayed when my birthday, May 7, came up ranked 35. Ugh! Could some data analysis and graphics get me out of my funk?\nThe data, from the official Selective Service listing are contained in the dataset vcdExtra::Draft1970, ordered by Month and birthdate (Day), with Rank as the order in which the birthdates were drawn.\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\ndata(Draft1970, package = \"vcdExtra\")\ndplyr::glimpse(Draft1970)\n# Rows: 366\n# Columns: 3\n# $ Day   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n# $ Rank  &lt;int&gt; 305, 159, 251, 215, 101, 224, 306, 199, 194, 325, 32…\n# $ Month &lt;ord&gt; Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Ja…\n\nA basic scatterplot, slightly prettified, is shown in Figure 2.6. The points are colored by month, and month labels are shown at the bottom.\n\nShow the code# make markers for months at their mid points\nmonths &lt;- data.frame(\n  month =unique(Draft1970$Month),\n  mid = seq(15, 365-15, by = 30))\n\nggplot2:: theme_set(theme_bw(base_size = 16))\ngg &lt;- ggplot(Draft1970, aes(x = Day, y = Rank)) +\n  geom_point(size = 2.5, shape = 21, \n             alpha = 0.3, \n             color = \"black\", \n             aes(fill=Month)\n  ) +\n  scale_fill_manual(values = rainbow(12)) +\n  geom_text(data=months, aes(x=mid, y=0, label=month), nudge_x = 5) +\n  geom_smooth(method = \"lm\", formula = y ~ 1,\n              col = \"black\", fill=\"grey\", linetype = \"dashed\", alpha=0.6) +\n  labs(x = \"Day of the year\",\n       y = \"Lottery rank\") +\n  theme(legend.position = \"none\") \ngg\n\n\n\n\n\n\nFigure 2.6: Basic scatterplot of 1970 Draft Lottery data plotting rank order of selection against birthdates in the year. Points are colored by month. The horizontal line is at the average rank.\n\n\n\n\nThe ranks do seem to be essentially random. Is there any reason to suspect a flaw in the selection process, as I firmly hoped at the time?\nIf you stare at the graph in Figure 2.6 long enough, you just can make out a sparsity of points in the upper right corner and also in the lower left corner compared to the opposite corners. But probably not until I told you where to look.\nVisual smoothers\nFitting a linear regression line or a smoothed (loess) curve can bring out the signal lurking in the background of a field of nearly random points. Figure 2.7 shows a definite trend to lower ranks for birthdays toward the end of the year. Those born earlier in the year were more likely to be given lower ranks, calling them up sooner for the draft. \n\nShow the codeggplot(Draft1970, aes(x = Day, y = Rank)) +\n  geom_point(size = 2.5, shape = 21, \n             alpha = 0.3, \n             color = \"black\", \n             aes(fill=Month)) +\n  scale_fill_manual(values = rainbow(12)) +\n  geom_smooth(method = \"lm\", formula = y~1,\n              se = FALSE,\n              col = \"black\", fill=\"grey\", linetype = \"dashed\", alpha=0.6) +\n  geom_smooth(method = \"loess\", formula = y~x,\n              color = \"blue\", se = FALSE,\n              alpha=0.25) +\n  geom_smooth(method = \"lm\", formula = y~x,\n              color = \"darkgreen\",\n              fill = \"darkgreen\", \n              alpha=0.25) +\n  geom_text(data=months, aes(x=mid, y=0, label=month), nudge_x = 5) +\n  labs(x = \"Day of the year\",\n       y = \"Lottery rank\") +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\nFigure 2.7: Enhanced scatterplot of 1970 Draft Lottery data adding a linear regression line and loess smooth.\n\n\n\n\nIs this a real effect? Even though the points seem to be random over the year, linear regression of Rank on Day shows a highly significant negative effect even though the correlation3 is small (\\(r = -0.226\\)). The slope, -0.226, means that for each additional day in the year the lottery rank decreases about 1/4 toward the front of the draft line; that’s nearly 7 ranks per month.\n\ndraft.mod &lt;- lm(Rank ~ Day, data=Draft1970)\nwith(Draft1970, cor(Day, Rank))\n# [1] -0.226\ncoef(draft.mod)\n# (Intercept)         Day \n#     224.913      -0.226\n\nSo, smoothing the data, using either the linear regression line or a nonparametric smoother is one important technique for seeing a weak signal in a noisy background.\nVisual summaries\nAnother way to enhance the signal-to-noise ratio of a graph is to plot summaries of the messy data points. For example, you might make boxplots of the ranks by month, or calculate and plot the mean or median rank by month and plot those together with some indication of variability within month.\nFigure 2.8 plots the average Rank for each month with error bars showing the mean \\(\\pm 1\\) standard errors against the average Day. The message of rank decreasing nearly linearly with month is now more dramatic, partly because I decreased the range of the y-axis.4 The correlation between the means is \\(r = -0.867\\); the slope is -0.231, similar to what we found for the raw data.\n\nCodemeans &lt;- Draft1970 |&gt;\n  group_by(Month) |&gt;\n  summarize(Day = mean(Day),\n            se = sd(Rank/ sqrt(n())),\n            Rank = mean(Rank)) \n\nggplot(aes(x = Day, y = Rank), data=means) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", formula = y~x,\n              color = \"blue\", fill = \"blue\", alpha = 0.1) +\n  geom_errorbar(aes(ymin = Rank-se, ymax = Rank+se), \n                width = 8, linewidth = 1.3) +\n  geom_text(data=months, aes(x=mid, y=100, label=month), nudge_x = 5) +\n  ylim(100, 250) +\n  labs(x = \"Average day of the year\",\n       y = \"Average lottery rank\")\n\n\n\n\n\n\nFigure 2.8: Plot of the average rank per month with \\(\\pm 1\\) standard error bars. The line shows the least squares regression line, treating months as equally spaced. The vertical axis has been truncated to highlight the decrease in lottery rank over the year.\n\n\n\n\nThe visual impression of a linearly decreasing trend in lottery rank is much stronger in Figure 2.8 than in Figure 2.7 for two reasons:\n\nReplacing the data points with their means strengthens the signal in relation to noise, which is essentially eliminated by plotting means and error bars rather than the raw data. This is an example of visual thinning (Section 3.2.7), reducing visual complexity to highlight an overall pattern.\nThe narrower vertical range (100–250) in the plot of means makes the slope of the line appear steeper. (However, the slope of the means, \\(b = -0.231\\) is nearly the same as that for the data points.) The narrower range also makes deviations from the regression line more noticeable.\nWhat happened here?\nPrevious lotteries carried out by drawing capsules from a container had occasionally suffered the embarrassment that an empty capsule was selected because of vigorous mixing (Fienberg, 1971). So for the 1970 lottery, the birthdate capsules were put in cardboard boxes, one for each month and these were carefully emptied into the glass container in order of month: Jan., Feb., through Dec., and gently shaken in atop the pile already there.\nAll might have been well had the persons drawing the capsules put their hand in truly randomly, but generally they picked from toward the top of the container. Consequently, those born later in the year had a greater chance of being picked earlier.\nThere was considerable criticism of this procedure once the flaw had been revealed by analyses such as described here. In the following year, the Selective Service called upon the National Bureau of Standards to devise a better procedure. In 1971 they used two drums, one with the dates of the year and another with the rank numbers 1-366. As a date capsule was drawn randomly from the first drum, another from the numbers drum was picked simultaneously, giving a doubly-randomized sequence.\nOf course, if they had R, the entire process could have been done using sample():\n\nset.seed(42)\ndate = seq(as.Date(\"1971-01-01\"), \n           as.Date(\"1971-12-31\"), by=\"+1 day\")\nrank = sample(seq_along(date))\ndraft1971 &lt;- data.frame(date, rank)\n\nhead(draft1971, 3)\n#         date rank\n# 1 1971-01-01   49\n# 2 1971-01-02  321\n# 3 1971-01-03  153\ntail(draft1971, 3)\n#           date rank\n# 363 1971-12-29    8\n# 364 1971-12-30  333\n# 365 1971-12-31  132\n\nAnd, what would have happened to me and all others born on a May 7th, if they did it this way? My lottery rank would have 274!5\n\nme &lt;- as.Date(\"1971-05-07\")\ndraft1971[draft1971$date == me,]\n#           date rank\n# 127 1971-05-07  274",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "03-getting_started.html#sec-plots-data-analysis",
    "href": "03-getting_started.html#sec-plots-data-analysis",
    "title": "2  Getting Started",
    "section": "\n2.2 Plots for data analysis",
    "text": "2.2 Plots for data analysis\nVisualization methods take an enormous variety of forms, so it is useful to distinguish several broad categories according to their use in data analysis:\n\ndata plots: primarily plot the raw data, often with annotations to aid interpretation. 1D plots include boxplots, violin plots and dot plots, sometimes in combination; univariate distributions can also be portrayed in histograms and density estimates. 2D plots are most often scatterplots, favorably enhanced using regression lines and smooths, data ellipses, rug plots and marginal distributions. A survey of these methods is presented in Section 3.1. \nreconnaissance plots: with more than a few variables, reconnaissance plots provide a high-level, bird’s-eye overview of the data, allowing you to see patterns that might not be visible in a set of separate plots. Some examples are scatterplot matrices (Section 3.7) showing all bivariate plots of variables in a dataset; correlation diagrams (Section 3.8), using visual glyphs to represent the correlations between all pairs of variables and “trellis” or faceted plots that show how a focal relation of one or more variables differs across values of other variables. \nmodel plots: plot the results of a fitted model, such as a regression line or curve to show uncertainty, or a regression surface in 3D, or a plot of coefficients in model together with confidence intervals. Figure 2.8 is a simple example. \n\nOther model plots try to take into account that a fitted model may involve more variables than can be shown in a static 2D plot. Some examples of these are added variable plots (Section 6.3), and marginal effect plots (Section 6.4), both of which attempt to show the net relation of two focal variables, controlling or adjusting for other variables in a model. \n\ndiagnostic plots: indicating potential problems with the fitted model. For linear models, these include residual plots, influence plots, plots for testing homogeneity of variance and so forth, illustrated in Section 6.1.2. Plots for diagnosing problems with multivariate models are discussed in Section 10.7. \ndimension reduction plots : plot representations of the data in a space of fewer dimensions than the number of variables in the analysis. Simple examples include principal components analysis (PCA) and the related biplots, and multidimensional scaling (MDS) methods. This is the topic of Chapter 4, but this powerful idea runs through the rest of the book. I refer to such plots as multivariate juicers, because they can squeeze the essence of your data into low-D package, enhancing the flavor.\n\n\n2.2.1 Diagnostic plots\nHaving fit a model, your next step should usually be to try to criticize it by checking whether the assumptions of the model are met in your data.\nFor example, the plot of the Davis data in Figure 2.3 effectively fits a separate regression line for males and females, which is expressed by the model formula repwt ~ weight * sex. Having fit the model using lm(), the plot method for a \"lm\" object produces a set of four diagnostic plots (called the “regression quartet” Section 6.1) designed to highlight problems with the fitted model.\n\ndavis.mod &lt;- lm(repwt ~ weight * sex, data=Davis)  \nplot(davis.mod, \n     cex.lab = 1.2, cex = 1.1, \n     id.n = 2, cex.id = 1.2, lwd = 2)\n\n\n\n\n\n\nFigure 2.9: Regression quartet: Four diagnostic plots for the linear model fit to the Davis data. The details of these plots are discussed in Section 6.1.\n\n\n\n\nThe details of such plots are discussed and illustrated in Section 6.1, so I will just point out one useful feature here: that of point identification: the id.n argument controls the number of most-extreme points to be labeled in each panel. The point for the erroneous case 12 female who mis-recorded her height as her weight sticks out a mile in all four panels.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "03-getting_started.html#principles-of-graphical-display",
    "href": "03-getting_started.html#principles-of-graphical-display",
    "title": "2  Getting Started",
    "section": "\n2.3 Principles of graphical display",
    "text": "2.3 Principles of graphical display\nTODO: This could be a separate chapter, supplementary materials or excluded here\n\nCriteria for assessing graphs: communication goals\nEffective data display:\n\nMake the data stand out\nMake graphical comparison easy\nEffect ordering: For variables and unordered factors, arrange them according to the effects to be seen\n\n\nVisual thinning: As the data becomes more complex, focus more on impactful summaries",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "03-getting_started.html#what-have-we-learned",
    "href": "03-getting_started.html#what-have-we-learned",
    "title": "2  Getting Started",
    "section": "\n2.4 What have we learned?",
    "text": "2.4 What have we learned?\nThis chapter demonstrates why visualization isn’t just a “nice-to-have” feature in data analysis–—it’s absolutely essential. Through compelling historical examples and modern techniques, we’ve discovered some fundamental idea that every data analyst should embrace:\n\nSummary statistics can conceal the truth: Anscombe’s Quartet reveals that datasets can have identical means, correlations, and regression coefficients yet tell completely different stories. The quartet’s four plots–—pure error, lack of fit, outliers, and influence—–show that numerical summaries without visualization can lead us wildly astray. Modern extensions like the Datasaurus Dozen prove this isn’t just a quirky historical example–—you can literally hide a dinosaur in your data while maintaining identical statistical properties! Talk about the dinosaur in the room!\nOne rogue data point can hijack your entire analysis: Plotting the raw data facilitates critical engagement with our statistical models. The Davis weight study demonstrates how a single influential observation (one participant who accidentally switched their reported and measured weights) can completely distort relationships between variables. What appeared to be a puzzling gender difference in the reliability of self-reported weight vanished once the data was plotted and the outlier revealed itself as a simple recording error.\nMeaning becomes more apparent through thoughtful visualizations of well-considered models: Statistical modeling helps guide your attention in what might otherwise be a chaotic plot of raw data. The 1970 Draft Lottery story shows how graphics can reveal systematic bias hiding in apparently random data. While individual lottery numbers seemed random, smoothing techniques and summary plots exposed a clear pattern–—later birthdays were systematically favored due to poor mixing of the lottery capsules. Sometimes the most important patterns are the ones that whisper rather than shout.\nDifferent plot types serve different purposes: The chapter introduces a taxonomy of visualization goals that helps us choose the right tool for each analytical task. Data plots show raw observations, reconnaissance plots provide bird’s-eye overviews of complex datasets, model plots reveal fitted relationships, diagnostic plots expose model problems, and dimension reduction plots tame high-dimensional complexity. Each serves a distinct role in the analytical process.\nVisual enhancement amplifies signal over noise: Whether through smoothing lines, statistical summaries, or careful use of color and annotation, the chapter shows how thoughtful visual design can make weak patterns stand out dramatically. The Draft Lottery analysis becomes far more convincing when we plot monthly averages rather than individual data points, transforming a subtle correlation into an unmistakable trend.\n\nThe overarching message is clear: in an era where we can compute any statistic imaginable, the not-so-humble graph remains our most powerful tool for understanding what our data are really trying to tell us. As the Farquhar brothers noted over a century ago, getting information from tables alone is like extracting sunlight from a cucumber—possible in theory, but why make it so hard on yourself?\n\n\n\n\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. The American Statistician, 27, 17–21.\n\n\nBiecek, P., Baniecki, H., Krzyzinski, M., & Cook, D. (2023). Performance is not enough: A story of the rashomon’s quartet. https://arxiv.org/abs/2302.13356\n\n\nD’Agostino McGowan, L. (2023). Quartets: Datasets to help teach statistics. https://r-causal.github.io/quartets/\n\n\nDavis, C. (1990). Body image and weight preoccupation: A comparison between exercising and non-exercising women. Appetite, 16(1), 84. https://doi.org/10.1016/0195-6663(91)90115-9\n\n\nFarquhar, A. B., & Farquhar, H. (1891). Economic and industrial delusions: A discourse of the case for protection. Putnam.\n\n\nFienberg, S. E. (1971). Randomization and social affairs: The 1970 draft lottery. Science, 171, 255–261.\n\n\nFriendly, M. (2008). The Golden Age of statistical graphics. Statistical Science, 23(4), 502–535. https://doi.org/10.1214/08-STS268\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034\n\n\nFunkhouser, H. G. (1937). Historical development of the graphical representation of statistical data. Osiris, 3(1), 269–405. http://tinyurl.com/32ema9\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGelman, A., Hullman, J., & Kennedy, L. (2023). Causal quartets: Different ways to attain the same average treatment effect. http://www.stat.columbia.edu/~gelman/research/unpublished/causal_quartets.pdf\n\n\nGillespie, C., Locke, S., Davies, R., & D’Agostino McGowan, L. (2025). datasauRus: Datasets from the datasaurus dozen. https://doi.org/10.32614/CRAN.package.datasauRus\n\n\nGuerry, A.-M. (1833). Essai sur la statistique morale de la France. Crochard.\n\n\nHerschel, J. F. W. (1833). On the investigation of the orbits of revolving double stars: Being a supplement to a paper entitled \"micrometrical measures of 364 double stars\". Memoirs of the Royal Astronomical Society, 5, 171–222.\n\n\nMatejka, J., & Fitzmaurice, G. (2017, May). Same stats, different graphs. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3025453.3025912\n\n\nMcGowan, L. D., Gerke, T., & Barrett, M. (2023). Causal inference is not just a statistics problem. Journal of Statistics and Data Science Education, 1–9. https://doi.org/10.1080/26939169.2023.2276446\n\n\nPearson, K. (1896). Contributions to the mathematical theory of evolution—III, regression, heredity and panmixia. Philosophical Transactions of the Royal Society of London, 187, 253–318.\n\n\nPlayfair, W. (1786). Commercial and political atlas: Representing, by copper-plate charts, the progress of the commerce, revenues, expenditure, and debts of england, during the whole of the eighteenth century. Debrett; Robinson;; Sewell. http://ucpj.uchicago.edu/Isis/journal/demo/v000n000/000000/000000.fg4.html\n\n\nPlayfair, W. (1801). Statistical breviary; shewing, on a principle entirely new, the resources of every state and kingdom in Europe. Wallis.\n\n\nTufte, E. R. (1983). The visual display of quantitative information. Graphics Press.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "03-getting_started.html#footnotes",
    "href": "03-getting_started.html#footnotes",
    "title": "2  Getting Started",
    "section": "",
    "text": "For example, a cell in a table can be used to show a “sparklines” (Tufte (1983)), tiny versions of a line graph or bar chart. As well, table rows and/or columns can be sorted to show trends or background colors can be used to show unusual values.↩︎\nThis story is told apocryphally. The consulting client actually did plot the data, but needed help in understanding what went wrong in her analyses and in making better graphs.↩︎\nBecause both days of the year and rank in the lottery are the integers, 1 to 366, the Pearson correlation and Spearman rank order correlation are identical.↩︎\nRestricting the y-axis range in plots can sometimes be a graphical sin. It can distort the visual representation of the data, making differences appear larger than they actually are, and potentially misleading the viewer. But this is not a sin, when it serves a communication goal, which in Figure 2.8 is to focus attention on the relative changes in lottery rank over the year. Using a visual cue like a broken axis in the axis is one way to avoid misinterpretation. For ggplot graphs, the ggbreak package is useful for this purpose.↩︎\nA personal note: I escaped being drafted, but I moved to Canada in 1971. Looking back today, it’s one of the best decisions I ever made.↩︎",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html",
    "href": "04-multivariate_plots.html",
    "title": "3  Plots of Multivariate Data",
    "section": "",
    "text": "3.1 Bivariate summaries\nThese quotes from John Tukey remind us that data analysis should nearly always start with graphs to help us understand the main features of our data. It is important to understand the general patterns and trends: Are relationships increasing or decreasing? Are they approximately linear or non-linear? But it is also important to spot anomalies: “unusual” observations, groups of points that seem to differ from the rest, and so forth. As we saw with Anscombe’s quartet (Section 2.1.1) and the Davis weight data (Section 2.1.2) numerical summaries hide features that are immediately apparent in a plot.\nThis chapter introduces a toolbox of basic graphical methods for visualizing multivariate datasets. It starts with some simple techniques to enhance the basic scatterplot with graphical annotations such as fitted lines, curves (Section 3.1) and data ellipses (Section 3.2) which serve to add visual summaries of the relation between two variables.\nTo visualize more than two variables, we can view all pairs of variables in a scatterplot matrix (Section 3.7) or shift gears entirely to show multiple variables along a set of parallel axes (Section 3.10).\nAs the number of variables increases, we may need to suppress details with stronger summaries (Section 3.2.7) for a high-level reconnaissance of our data terrain, as we do by zooming out on a map. For example, we can simply remove the data points or make them nearly transparent to focus on the visual summaries provided by fitted lines or other graphical summaries.\nAnother approach to visualizing high-D data uses animated tours (Section 3.11) to show the data in a sequence of low-D projections, typically 2D, designed to reveal interesting features that might not otherwise be visible. At a higher level of abstraction, network diagrams (Section 3.12) representing the structure of correlations among a possibly large set of variables offer another useful technique.\nPackages\nIn this chapter I use the following packages. Load them now:\nThe basic scatterplot is the workhorse of multivariate data visualization, showing how one variable, \\(y\\), often an outcome to be explained by or varies with another, \\(x\\). It is a building block for many useful techniques, so it is helpful to understand how it can be used as a tool for thinking in a wider, multivariate context.\nThe essential idea is that we can start with a simple version of the scatterplot and add annotations to show interesting features more clearly. I consider the following here:",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-bivariate_summaries",
    "href": "04-multivariate_plots.html#sec-bivariate_summaries",
    "title": "3  Plots of Multivariate Data",
    "section": "",
    "text": "Smoothers: Showing overall trends, perhaps in several forms, as visual summaries such as fitted regression lines or curves and nonparametric smoothers.\n\nStratifiers: Using color, shape or other features to identify subgroups; more generally, conditioning on other variables in multi-panel displays;\n\nData ellipses: A compact 2D visual summary of bivariate linear relations and uncertainty assuming normality; more generally, contour plots of bivariate density. \n\n\nExample 3.1 Academic salaries\nLet’s start with data on the academic salaries of faculty members collected at a U.S. college for the purpose of assessing salary differences between male and female faculty members, and perhaps address anomalies in compensation. The dataset carData::Salaries gives data on nine-month salaries and other variables for 397 faculty members in the 2008-2009 academic year.\n\ndata(Salaries, package = \"carData\")\nstr(Salaries)\n# 'data.frame': 397 obs. of  6 variables:\n#  $ rank         : Factor w/ 3 levels \"AsstProf\",\"AssocProf\",..: 3 3 1 3 3 2 3 3 3 3 ...\n#  $ discipline   : Factor w/ 2 levels \"A\",\"B\": 2 2 2 2 2 2 2 2 2 2 ...\n#  $ yrs.since.phd: int  19 20 4 45 40 6 30 45 21 18 ...\n#  $ yrs.service  : int  18 16 3 39 41 6 23 45 20 18 ...\n#  $ sex          : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 1 ...\n#  $ salary       : int  139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ...\n\nThe most obvious, but perhaps naive, predictor of salary is years.since.phd. For simplicity, I’ll refer to this as years of “experience.” Before looking at differences between males and females, we would want consider faculty rank (related also to yrs.service) and discipline, recorded here as \"A\" (“theoretical” departments) or \"B\" (“applied” departments). But, for a basic plot, we will ignore these for now to focus on what can be learned from plot annotations.\n\n\ngg1 &lt;- ggplot(Salaries, \n       aes(x = yrs.since.phd, y = salary)) +\n  geom_jitter(size = 2) +\n  scale_y_continuous(labels = scales::dollar_format(\n    prefix=\"$\", scale = 0.001, suffix = \"K\")) +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") \n\ngg1 + geom_rug(position = \"jitter\", alpha = 1/4)\n\n\n\n\n\n\nFigure 3.1: Naive scatterplot of Salary vs. years since PhD, ignoring other variables, and without graphical annotations.\n\n\n\n\nThere is quite a lot we can see “just by looking” at this simple plot, but the main things are:\n\nSalary increases generally from 0 - 40 years since the PhD, but then maybe begins to drop off (partial retirement?);\nVariability in salary increases among those with the same experience, a “fan-shaped” pattern that signals a violation of homogeneity of variance in simple regression;\nData beyond 50 years is thin, but there are some quite low salaries there. Adding rug plots to the X and Y axes is a simple but effective way to show the marginal distributions of the observations. Jitter and transparency helps to avoid overplotting due to discrete values.\n\n\n\n3.1.1 Smoothers\nSmoothers are among the most useful graphical annotations you can add to such plots, giving a visual summary of how \\(y\\) changes with \\(x\\). The most common smoother is a line showing the linear regression for \\(y\\) given \\(x\\), expressed in math notation as \\(\\mathbb{E} (y | x) = b_0 + b_1 x\\). If there is doubt that a linear relation is an adequate summary, you can try a quadratic or other polynomial smoothers.\nIn ggplot2, these are easily added to a plot using geom_smooth() with method = \"lm\", and a model formula, which (by default) is y ~ x for a linear relation or y ~ poly(x, k) for a polynomial of degree \\(k\\).\n\nShow the codegg1 + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\", \n              color = \"red\", fill= \"pink\",\n              linewidth = 2) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x,2)\", \n              color = \"darkgreen\", fill = \"lightgreen\",\n              linewidth = 2) \n\n\n\n\n\n\nFigure 3.2: Scatterplot of Salary vs. years since PhD, showing linear and quadratic smooths with 95% confidence bands.\n\n\n\n\n\nThis serves to highlight some of our impressions from the basic scatterplot shown in Figure 3.1, making them more apparent. And that’s precisely the point: the regression smoother draws attention to a possible pattern that we can consider as a visual summary of the data. You can think of this as showing what a linear (or quadratic) regression “sees” in the data. Statistical tests  can help you decide if there is more evidence for a quadratic fit compared to the simpler linear relation. \nIt is useful to also show some indication of uncertainty (or inversely, precision) associated with the predicted values. Both the linear and quadratic trends are shown in Figure 3.2 with 95% pointwise confidence bands.1 These are necessarily narrower in the center of the range of \\(x\\) where there is typically more data; they get wider toward the highest values of experience where the data are thinner. \nNon-parametric smoothers\nThe most generally useful idea is a smoother that tracks an average value, \\(\\mathbb{E} (y | x)\\), of \\(y\\) as \\(x\\) varies across its’ range without assuming any particular functional form, and so avoiding the necessity to choose among y ~ poly(x, 1), or y ~ poly(x, 2), or y ~ poly(x, 3), etc.\nNon-parametric smoothers attempt to estimate \\(\\mathbb{E} (y | x) = f(x)\\) where \\(f(x)\\) is some smooth function. These typically use a collection of weighted local regressions for each \\(x_i\\) within a window centered at that value. In the method called lowess or loess (Cleveland, 1979; Cleveland & Devlin, 1988), a weight function is applied, giving greatest weight to \\(x_i\\) and a weight of 0 outside a window containing a certain fraction, \\(s\\), called span, of the nearest neighbors of \\(x_i\\). The fraction, \\(s\\), is usually within the range \\(1/3 \\le s \\le 2/3\\), and it determines the smoothness of the resulting curve; smaller values produce a wigglier curve and larger values giving a smoother fit (an optimal span can be determined by \\(k\\)-fold cross-validation to minimize a measure of overall error of approximation).\nNon-parametric regression is a broad topic; see Fox (2016), Ch. 18 for a more general treatment including smoothing splines, and Wood (2006) for generalized additive models, fit using method = \"gam\" in ggplot2, which is the default when the largest group has more than 1,000 observations.\nFigure 3.3 shows the addition of a loess smooth to the plot in Figure 3.2, suppressing the confidence band for the linear regression. The loess fit is nearly coincident with the quadratic fit, but has a slightly wider confidence band.\n\nCodegg1 + \n  geom_smooth(method = \"loess\", formula = \"y ~ x\", \n              color = \"blue\", fill = scales::muted(\"blue\"),\n              linewidth = 2) +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE,\n              color = \"red\",\n              linewidth = 2) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x,2)\", \n              color = \"darkgreen\", fill = \"lightgreen\",\n              linewidth = 2) \n\n\n\n\n\n\nFigure 3.3: Scatterplot of Salary vs. years since PhD, adding the loess smooth. The loess smooth curve and confidence band in green is nearly indistinguishable from a quadratic fit in blue.\n\n\n\n\nBut now comes an important question: is it reasonable that academic salary should increase up to about 40 years since the PhD degree and then decline? The predicted salary for someone still working 50 years after earning their degree is about the same as a person at 15 years. What else is going on here?\n\n3.1.2 Stratifiers\nVery often, we have a main relationship of interest, but various groups in the data are identified by discrete factors (like faculty rank and sex, their type of discipline here), or there are quantitative predictors for which the main relation might vary. In the language of statistical models such effects are interaction terms, as in y ~ group + x + group:x, where the term group:x fits a different slope for each group and the grouping variable is often called a moderator variable. Common moderator variables are ethnicity, health status, social class and level of education. Moderators can also be continuous variables as in y ~ x1 + x2 + x1:x2.\nI call these stratifiers, recognizing that we should consider breaking down the overall relation to see whether and how it changes over such “other” variables. Such variables are most often factors, but we can cut a continuous variable into ranges (shingles) and do the same graphically. There are two general graphical techniques for stratifying:\n\nGrouping: Identify subgroups in the data by assigning different visual attributes, such as color, shape, line style, etc. within a single plot, as shown in Figure 3.4 below. This is quite natural for factors; quantitative predictors can be accommodated by cutting their range into ordered intervals. Grouping has the advantage that the levels of a grouping variable can be shown within the same plot, facilitating direct comparison.\nConditioning: Showing subgroups in different plot panels, as in Figure 3.6. This has the advantages that relations for the individual groups more easily discerned and one can easily stratify by two (or more) other variables jointly. But visual comparison is more difficult because the eye must scan from one panel to another.\n\n\n\n\n\n\n\nHistory Corner: Coplots and faceting\n\n\n\nThe syntax you use in specifying a plot plays an important role in visual thinking. Ideally, you want the shortest path between having a graphic idea in your head and specifying that in code to see the result rendered on your screen. \nRecognition of the roles of visual grouping by factors within a panel and conditioning in multi-panel displays was an important advance in the development of modern statistical graphics software. It began at A.T.& T. Bell Labs in Murray Hill, NJ in conjunction with the S language, the mother of R.\nConditioning displays (originally called coplots (Chambers & Hastie, 1991)) are simply a collection of 1D, 2D or 3D plots separate panels for subsets of the data broken down by one or more factors, or, for quantitative variables, subdivided into a factor with several overlapping intervals (shingles). The first implementation was in Trellis plots (Becker et al., 1996; Cleveland, 1985). \nTrellis displays were extended in the lattice package (Sarkar, 2008), which offered:\n\nA graphing syntax similar to that used in statistical model formulas: y ~ x | g conditions the data by the levels of g, with | read as “given”; two or more conditioning are specified as y ~ x | g1 + g2 + ..., with + read as “and”.\n\nPanel functions define what is plotted in a given panel. panel.xyplot() is the default for scatterplots, plotting points, but you can add panel.lmline() for regression lines, latticeExtra::panel.smoother() for loess smooths and a wide variety of others.\n\nThe car package (Fox & Weisberg, 2019) supports this graphing syntax in many of its functions. ggplot2 does not; it uses aesthetics (aes()), which map variables in the data to visual characteristics in displays.\n\n\nUsing grouping For the Salaries data, the most obvious variable that affects academic salary is rank, because faculty typically get an increase in salary with a promotion that carries through in their future salary. What can we see if we group by rank and fit a separate smoothed curve for each?\nIn ggplot2 thinking, grouping is accomplished simply by adding an aesthetic, such as color = rank. What happens then is that points, lines, smooths and other geom_*() inherit the feature that they are differentiated by color. In the case of geom_smooth(), we get a separate fit for each subset of the data, according to rank.\n\nCode# make some re-useable pieces to avoid repetitions\nscale_salary &lt;-   scale_y_continuous(\n  labels = scales::dollar_format(prefix=\"$\", \n                                 scale = 0.001, \n                                 suffix = \"K\")) \n# position the legend inside the plot\nlegend_pos &lt;- theme(legend.position = \"inside\",\n                    legend.position.inside = c(.1, 0.95), \n                    legend.justification = c(0, 1))\n\nggplot(Salaries, \n       aes(x = yrs.since.phd, y = salary, \n           color = rank, shape = rank)) +\n  geom_point() +\n  scale_salary +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  geom_smooth(aes(fill = rank),\n                  method = \"loess\", formula = \"y ~ x\", \n                  linewidth = 2)  +\n  legend_pos\n\n\n\n\n\n\nFigure 3.4: Scatterplot of Salary vs. years since PhD, grouped by rank.\n\n\n\n\nWell, there is a different story here. Salaries generally occupy separate vertical levels, increasing with academic rank. The horizontal extents of the smoothed curves show their ranges. Within each rank there is some initial increase after promotion, and then some tendency to decline with increasing years. But, by and large, years since the PhD doesn’t make as much difference once we’ve taken academic rank into account.\nWhat about the discipline which is classified, perhaps peculiarly, as “theoretical” vs. “applied”? The values are just \"A\" and \"B\", so I map these to more meaningful labels before making the plot.\n\nCodeSalaries &lt;- Salaries |&gt;\n  mutate(discipline = \n           factor(discipline, \n                  labels = c(\"A: Theoretical\", \"B: Applied\")))\n\nSalaries |&gt;\n  ggplot(aes(x = yrs.since.phd, y = salary, color = discipline)) +\n    geom_point() +\n  scale_salary +\n  geom_smooth(aes(fill = discipline ),\n                method = \"loess\", formula = \"y ~ x\", \n                linewidth = 2) + \n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  legend_pos \n\n\n\n\n\n\nFigure 3.5: Scatterplot of Salary vs. years since PhD, grouped by discipline.\n\n\n\n\nThe story in Figure 3.5 is again different. Faculty in applied disciplines on average earn about 10,000$ more per year on average than their theoretical colleagues.\n\nSalaries |&gt;\n  group_by(discipline) |&gt;\n  summarize(mean = mean(salary)) \n# # A tibble: 2 × 2\n#   discipline        mean\n#   &lt;fct&gt;            &lt;dbl&gt;\n# 1 A: Theoretical 108548.\n# 2 B: Applied     118029.\n\nFor both groups, there is an approximately linear relation up to about 30–40 years, but the smoothed curves then diverge into the region where the data is thinner.\nThis result is more surprising than differences among faculty ranks. The effect of annotation with smoothed curves as visual summaries is apparent, and provides a stimulus to think about why these differences (if they are real) exist between theoretical and applied professors, and maybe should theoreticians be paid more!\n\n3.1.3 Conditioning\nThe previous plots use grouping by color to plot the data for different subsets inside the same plot window, making comparison among groups easier, because they can be directly compared along a common vertical scale 2. This gets messy, however, when there are more than just a few levels, or worse—when there are two (or more) variables for which we want to show separate effects. In such cases, we can plot separate panels using the ggplot2 concept of faceting. There are two options: facet_wrap() takes one or more conditioning variables and produces a ribbon of plots for each combination of levels; facet_grid(row ~ col) takes two or more conditioning variables and arranges the plots in a 2D array identified by the row and col variables.\nLet’s look at salary broken down by the combinations of discipline and rank. Here, I chose to stratify using color by rank within each of panels faceting by discipline. Because there is more going on in this plot, a linear smooth is used to represent the trend.\n\nCodeSalaries |&gt;\n  ggplot(aes(x = yrs.since.phd, y = salary, \n             color = rank, shape = rank)) +\n  geom_point() +\n  scale_salary +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  geom_smooth(aes(fill = rank),\n              method = \"lm\", formula = \"y ~ x\", \n              linewidth = 2, alpha = 1/4) +\n  facet_wrap(~ discipline) +\n  legend_pos\n\n\n\n\n\n\nFigure 3.6: Scatterplot of Salary vs. years since PhD, grouped by rank, with separate panels for discipline.\n\n\n\n\nOnce both of these factors are taken into account, there does not seem to be much impact of years of service. Salaries in theoretical disciplines are noticeably greater than those in applied disciplines at all ranks, and there are even greater differences among ranks.\nFinally, to shed light on the question that motivated this example— are there anomalous differences in salary for men and women— we can look at differences in salary according to sex, when discipline and rank are taken into account. To do this graphically, condition by both variables, but use facet_grid(discipline ~ rank) to arrange their combinations in a grid whose rows are the levels of discipline and columns are those of rank. I want to make the comparison of males and females most direct, so I use color = sex to stratify the panels. The smoothed regression lines and error bands are calculated separately for each combination of discipline, rank and sex.\n\nCodeSalaries |&gt;\n  ggplot(aes(x = yrs.since.phd, y = salary, color = sex)) +\n  geom_point() +\n  scale_salary +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  geom_smooth(aes(fill = sex),\n              method = \"lm\", formula = \"y ~ x\",\n              linewidth = 2, alpha = 1/4) +\n  facet_grid(discipline ~ rank) +\n  legend_pos\n\n\n\n\n\n\nFigure 3.7: Scatterplot of Salary vs. years since PhD, grouped by sex, faceted by discipline and rank.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-data-ellipse",
    "href": "04-multivariate_plots.html#sec-data-ellipse",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.2 Data Ellipses",
    "text": "3.2 Data Ellipses\nThe data ellipse (Monette, 1990), or concentration ellipse (Dempster, 1969) is a remarkably simple and effective display for viewing and understanding bivariate relationships in multivariate data. The data ellipse is typically used to add a visual summary to a scatterplot, that shows all together the means, standard deviations, correlation, and slope of the regression line for two variables, perhaps stratified by another variable.\nUnder the classical assumption that the data are bivariate normally distributed, the data ellipse is also a sufficient visual summary, in the sense that it captures all relevant features of the data. See Friendly et al. (2013) for a complete discussion of the role of ellipsoids in statistical data visualization.\nThe data ellipse is based on the idea that in a bivariate normal distribution, the contours of equal probability form a series of concentric ellipses. If the variables were uncorrelated and had the same variances, these would be circles, and Euclidean distance would measure the distance of each observation from the mean. When the variables are correlated, a different measure, Mahalanobis distance is the proper measure of how far a point is from the mean, taking the correlation into account. \n\n\n\n\n\n\n\nFigure 3.8: 2D data with curves of constant distance from the centroid. The blue solid ellipse shows a contour of constant Mahalanobis distance, taking the correlation into account; the dashed red circle is a contour of equal Euclidean distance. Given the data points, which of the points A and B is further from the mean (X)? Source: Re-drawn from Ou Zhang\n\n\n\n\n\nTo illustrate, Figure 3.8 shows a scatterplot with labels for two points, “A” and “B”. Which is further from the mean, “X”? A contour of constant Euclidean distance, shown by the red dashed circle, ignores the apparent negative correlation, so point “A” is further. The blue ellipse for Mahalanobis distance takes the correlation into account, so point “B” has a greater distance from the mean.\nMathematically, Euclidean (squared) distance (\\(D_E^2(y)\\)) for \\(p\\) variables, \\(j = 1, 2, \\dots , p\\), is just a generalization of the square of a univariate standardized (\\(z\\)) score, \\(z^2 = [(y - \\bar{y}) / s]^2\\),\n\\[\nD_E^2 (\\mathbf{y}) = \\sum_j^p z_j^2 = \\mathbf{z}^\\textsf{T}  \\mathbf{z} = (\\mathbf{y} - \\bar{\\mathbf{y}})^\\textsf{T} \\operatorname{diag}(\\mathbf{S})^{-1} (\\mathbf{y} - \\bar{\\mathbf{y}}) \\; ,\n\\] where \\(\\mathbf{S}\\) is the sample variance-covariance matrix, \\(\\mathbf{S} = ({n-1})^{-1} \\sum_{i=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})^\\textsf{T} (\\mathbf{y}_i - \\bar{\\mathbf{y}})\\).\nMahalanobis distance takes the correlations into account simply by using the covariances as well as the variances,\n\\[\nD_M^2 (\\mathbf{y}) = (\\mathbf{y} - \\bar{\\mathbf{y}})^\\mathsf{T} S^{-1} (\\mathbf{y} - \\bar{\\mathbf{y}}) \\; .\n\\tag{3.1}\\]\nIn Equation 3.1, the inverse \\(S^{-1}\\) serves to “divide” the matrix \\((\\mathbf{y} - \\bar{\\mathbf{y}})^\\mathsf{T} (\\mathbf{y} - \\bar{\\mathbf{y}})\\) of squared distances by the variances (and covariances) of the variables, as in the univariate case.\nFor \\(p\\) variables, the data ellipsoid \\(\\mathcal{E}_c\\) of size \\(c\\) is a \\(p\\)-dimensional ellipse, defined as the set of points \\(\\mathbf{y} = (y_1, y_2, \\dots y_p)\\) whose squared Mahalanobis distance, \\(D_M^2 ( \\mathbf{y} )\\) is less than or equal to \\(c^2\\),\n\\[\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{S}) := \\{ D_M^2 (\\mathbf{y}) \\le c^2 \\} \\; .\n\\]\n\n3.2.1 Drawing data ellipses\nA computational definition of the data ellipsoid recognizes that the boundary of an ellipsoid can be found by (a) starting with a unit a unit sphere \\(\\mathcal{P}\\) centered at the origin, (b) transforming that by a “square root” of the covariance matrix, denoted \\(\\mathbf{S}^{1/2}\\), and then (c) shifting that to centroid of the data.\nThe unit sphere is defined as the contour of Euclidean distance 1 from the origin, \\(\\mathcal{P} := \\{ \\mathbf{x}^\\textsf{T} \\mathbf{x}= 1\\}\\). Using the notation \\(\\oplus\\) to represent translation to a new centroid at the variable means, \\(\\bar{\\mathbf{y}}\\), the data ellipsoid becomes,\n\\[\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{S}) = \\bar{\\mathbf{y}} \\; \\oplus \\; \\mathbf{S}^{1/2} \\, \\mathcal{P} \\:\\: .\n\\] The matrix \\(\\mathbf{S}^{1/2}\\) represents a rotation and scaling of the sphere and is commonly computed as the Cholesky factor of \\(\\mathbf{S}\\) given in R by chol(S). You can imagine this in 2D by thinking of how the dashed red circle in Figure 3.8 could be transformed into the blue ellipse.\nSlightly abusing notation and taking the unit sphere \\(\\mathcal{P}\\) like an identity matrix \\(\\mathbf{I}\\) that vanishes in multiplication, we can write the data ellipsoid as simply:\n\\[\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{S}) = \\bar{\\mathbf{y}} \\; \\oplus \\; c\\, \\sqrt{\\mathbf{S}} \\:\\: .\n\\tag{3.2}\\]\nContour levels (\\(c\\))\nWhen \\(\\mathbf{y}\\) is (at least approximately) bivariate normal, \\(D_M^2(\\mathbf{y})\\) has a large-sample \\(\\chi^2_2\\) distribution (\\(\\chi^2\\) with 2 df), so ellipses of various conventional sizes can be calculated using qchisq():\n\n\n\\(c^2 = \\chi^2_2 (0.5) = 1.39\\) gives a data ellipse covering 50% of the data points, a bivariate analog of the central box of a boxplot.\n\n\\(c^2 = \\chi^2_2 (0.68) = 2.28\\) gives a “1 standard deviation bivariate ellipse,” an analog of the standard interval \\(\\bar{y} \\pm 1 s\\),\n\n\\(c^2 = \\chi^2_2 (0.95) = 5.99 \\approx 6\\) gives a data ellipse of 95% coverage.\n\nIn not-large samples, the radius \\(c\\) of the ellipsoid is better approximated by a multiple of a \\(F_{p, n-p}\\) distribution, with \\(p\\) variables and \\(n-p\\) degrees of freedom for \\(\\mathbf{S}\\). This gives a radius \\(c =\\sqrt{ 2 F_{2, n-2}^{1-\\alpha} }\\) in the bivariate case (\\(p=2\\)) for coverage \\(1-\\alpha\\).\nThese three conventional cases are illustrated in Figure 3.9 with ellipses of 50%, 68% and 95% coverage for a the matrix \\(\\mathbf{S}\\) defined below and \\(\\bar{\\mathbf{y}} = \\mathbf{0}\\). Here, the variance of \\(\\mathbf{y}_1\\) is twice that of \\(\\mathbf{y}_2\\) and the correlation works out to \\(r(y_1, y_2) = 0.35\\).\n\nybar &lt;- c(0, 0)\nS &lt;- matrix(c(1, .5, .5, 2), 2, 2)\nrownames(S) &lt;- colnames(S) &lt;- c(\"y1\", \"y2\")\nS\n#     y1  y2\n# y1 1.0 0.5\n# y2 0.5 2.0\n\n\nStatistical ellipses are conveniently drawn using car::ellipse(). heplots::ellipse.label() provides flexible ways to add labels to ellipses at various locations around the ellipse shown in Figure 3.9. These are called repeatedly to overlay the three ellipses.\n\nShow the codelevels &lt;- c(0.50, 0.68, 0.95)\nc &lt;- qchisq(levels, df = 2) |&gt; round(2) |&gt; print()\n# [1] 1.39 2.28 5.99\n\n# labels for ellipses, using plotmath\nlab1 &lt;- bquote(paste(\"c =\", chi[2]^2, \"(\", .(levels[1]), \") =\", .(c[1])))\nlab2 &lt;- bquote(paste(\"c =\", chi[2]^2, \"(\", .(levels[2]), \") =\", .(c[2])))\nlab3 &lt;- bquote(paste(\"c =\", chi[2]^2, \"(\", .(levels[3]), \") =\", .(c[3])))\n\ne1 &lt;- ellipse(ybar, S, radius=qchisq(levels[1], 2), \n        col = \"blue\", fill=TRUE, fill.alpha = 0.5,\n        add=FALSE, \n        xlim=c(-8, 8), ylim=c(-9.5, 9.5), \n        asp=1, grid = FALSE,\n        xlab = expression(y[1]), \n        ylab = expression(y[2]),\n        cex.lab = 1.5)\nlabel.ellipse(e1, label = lab1, label.pos = \"S\",\n              cex = 1.2)\n\ne2 &lt;- ellipse(ybar, S, radius=qchisq(levels[2], 2), \n        col=\"blue\", fill=TRUE, fill.alpha=0.3)\nlabel.ellipse(e2, label = lab2, label.pos = \"N\",\n              cex = 1.2)\n\ne3 &lt;- ellipse(ybar, S, radius=qchisq(levels[3], 2), \n        col=\"blue\", fill=TRUE, fill.alpha=0.1)\nlabel.ellipse(e3, label = lab3, label.pos = \"N\",\n              cex = 1.2)\n\n\n\n\n\n\nFigure 3.9: Data ellipses of 50%, 68% and 95% coverage when the means are \\(\\bar{\\mathbf{y}} = \\mathbf{0}\\) and the variance-covariance matrix is \\(\\mathbf{S}\\).\n\n\n\n\nAs always, graphic details matter. Figure 3.9 uses asp = 1 so that units in the plot are the same for \\(\\mathbf{y}_1\\) and \\(\\mathbf{y}_2\\), and we can see the greater variance for \\(\\mathbf{y}_2\\) as well as the correlation. Facilities of grDevices::plotmath() are used to provide mathematical annotations in the plot.\n\n3.2.2 Ellipse properties\nThe essential ideas of correlation and regression and their relation to ellipses go back to Galton (1886). Galton’s goal was to predict (or explain) how a heritable trait, \\(Y\\), (e.g., height) of children was related to that of their parents, \\(X\\). He made a semi-graphic table of the frequencies of 928 observations of the average height of father and mother versus the height of their child, shown in Figure 3.10. (Today, we would put child height on the \\(y\\) axis, but Galton was working from the table, so he organized it with parent height as the rows.) He then drew smoothed contour lines of equal frequencies and had the wonderful visual insight that these formed concentric shapes that were tolerably close to ellipses.\nHe then calculated summaries, \\(\\text{Ave}(Y | X)\\), and, for symmetry, \\(\\text{Ave}(X | Y)\\), and plotted these as lines of means on his diagram. Lo and behold, he had a second visual insight: the lines of means of (\\(Y | X\\)) and (\\(X | Y\\)) corresponded approximately to the loci of horizontal and vertical tangents to the concentric ellipses. To complete the picture, he added lines showing the geometric major and minor axes of the family of ellipses (which turned out to be the principal components) with the result shown in Figure 3.10. \n\n\n\n\n\n\n\nFigure 3.10: Galton’s 1886 diagram, showing the relationship of height of children to the average of their parents’ height. The diagram is essentially an overlay of a geometrical interpretation on a bivariate grouped frequency distribution, shown as numbers.\n\n\n\n\nFor two variables, \\(x\\) and \\(y\\), the remarkable properties of the data ellipse are illustrated in Figure 3.11, a modern reconstruction of Galton’s diagram.\n\nPartial code for this figuredata(Galton, package = \"HistData\")\nsunflowerplot(parent ~ child, data=Galton, \n      xlim=c(61,75), \n      ylim=c(61,75), \n      seg.col=\"black\", \n      xlab=\"Child height\", \n      ylab=\"Mid Parent height\")\n\ny.x &lt;- lm(parent ~ child, data=Galton)     # regression of y on x\nabline(y.x, lwd=2)\nx.y &lt;- lm(child ~ parent, data=Galton)     # regression of x on y\ncc &lt;- coef(x.y)\nabline(-cc[1]/cc[2], 1/cc[2], lwd=2, col=\"gray\")\n\nwith(Galton, \n     car::dataEllipse(child, parent, \n         plot.points=FALSE, \n         levels=c(0.40, 0.68, 0.95), \n         lty=1:3)\n    )\n\n\n\n\n\n\n\n\n\nFigure 3.11: Sunflower plot of Galton’s data on heights of parents and their children (in.), with 40%, 68% and 95% data ellipses and the regression lines of \\(y\\) on \\(x\\) (black) and \\(x\\) on \\(y\\) (grey).\n\n\n\n\n\nThe ellipses have the mean vector \\((\\bar{x}, \\bar{y})\\) as their center.\nThe lengths of arms of the blue dashed central cross show the standard deviations of the variables, which correspond to the shadows of the ellipse covering 40% of the data. These are the bivariate analogs of the standard intervals \\(\\bar{x} \\pm 1 s_x\\) and \\(\\bar{y} \\pm 1 s_y\\).\n\nMore generally, shadows (projections) on the coordinate axes, or any linear combination of them, give any standard interval, \\(\\bar{x} \\pm k s_x\\) and \\(\\bar{y} \\pm k s_y\\). Those with \\(k=1, 1.5, 2.45\\), have bivariate coverage 40%, 68% and 95% respectively, corresponding to these quantiles of the \\(\\chi^2\\) distribution with 2 degrees of freedom, i.e., \\(\\chi^2_2 (.40) \\approx 1^2\\), \\(\\chi^2_2 (.68) \\approx 1.5^2\\), and \\(\\chi^2_2 (.95) \\approx 2.45\\). The shadows of the 68% ellipse are the bivariate analog of a univariate \\(\\bar{x} \\pm 1 s_x\\) interval.\n\n\nThe regression line predicting \\(y\\) from \\(x\\) goes through the points where the ellipses have vertical tangents. The other regression line, predicting \\(x\\) from \\(y\\) goes through the points of horizontal tangency.\nThe correlation \\(r(x, y)\\) is the ratio of the vertical segment from the mean of \\(y\\) to the regression line to the vertical segment going to the top of the ellipse as shown at the right of the figure. It is \\(r = 0.46\\) in this example.\nThe residual standard deviation, \\(s_e = \\sqrt{MSE} = \\sqrt{\\Sigma (y - \\bar{y})^2 / n-2}\\), is the half-length of the ellipse at the mean \\(\\bar{x}\\).\n\nBecause Galton’s values of parent and child height were recorded in class intervals of 1 in., they are shown as sunflower symbols in Figure 3.11, with multiple ‘petals’ reflecting the number of observations at each location. This plot (except for annotations) is constructed using sunflowerplot() and car::dataEllipse() for the ellipses.\n\nsunflowerplot(parent ~ child, data=Galton, \n      xlim=c(61,75), \n      ylim=c(61,75), \n      seg.col=\"black\", \n      xlab=\"Child height\", \n      ylab=\"Mid Parent height\")\n\ny.x &lt;- lm(parent ~ child, data=Galton)    # regression of y on x\nabline(y.x, lwd=2)\nx.y &lt;- lm(child ~ parent, data=Galton)    # regression of x on y\ncc &lt;- coef(x.y)\nabline(-cc[1]/cc[2], 1/cc[2], lwd=2, col=\"gray\")\n\nwith(Galton, \n     car::dataEllipse(child, parent, \n         plot.points=FALSE, \n         levels=c(0.40, 0.68, 0.95), \n         lty=1:3)\n    )\n\nFinally, as Galton noted in his diagram, the principal major and minor axes of the ellipse have important statistical properties. Pearson (1901) would later show that their directions are determined by the eigenvectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\dots\\) of the covariance matrix \\(\\mathbf{S}\\) and their radii by the square roots, \\(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}, \\dots\\) of the corresponding eigenvalues.\n\n3.2.3 R functions for data ellipses\nA number of packages provide functions for drawing data ellipses in a scatterplot, with various features.\n\n\ncar::scatterplot(): uses base R graphics to draw 2D scatterplots, with a wide variety of plot enhancements including linear and non-parametric smoothers (loess, gam), a formula method, e.g., y ~ x | group, and marking points and lines using symbol shape, color, etc. Importantly, the car package generally allows automatic identification of “noteworthy” points by their labels in the plot using a variety of methods. For example, method = \"mahal\" labels cases with the most extreme Mahalanobis distances; method = \"r\" selects points according to their value of abs(y), which is appropriate in residual plots.\n\ncar::dataEllipse(): plots classical or robust data ellipses for one or more groups, with the same facilities for point identification. The robust version (robust=TRUE) uses the multivariate \\(t\\) distribution (using MASS::cov/trob()) rather than the Gaussian,\n\nheplots::covEllipses(): draws classical or robust data ellipses for one or more groups in a one-way design and optionally for the pooled total sample, where the focus is on homogeneity of within-group covariance matrices.\n\nggplot2::stat_ellipse(): uses the calculation methods of car::dataEllipse() to add unfilled (geom = \"path\") or filled (geom = polygon\") data ellipses in a ggplot scatterplot, using inherited aesthetics.\n\n\nExample 3.2 Canadian occupational prestige\nThese examples use the data on the prestige of 102 occupational categories and other measures from the 1971 Canadian Census, recorded in Prestige.3 Our interest is in understanding how prestige (the Pineo & Porter (2008) prestige score for an occupational category, derived from a social survey) is related to census measures of the average education, income, percent women of incumbents in those occupations. Occupation type is a factor with levels \"bc\" (blue collar), \"wc\" (white collar) and \"prof\" (professional).\n\n\ndata(Prestige, package=\"carData\")\n# `type` is really an ordered factor. Make it so.\nPrestige$type &lt;- ordered(Prestige$type,\n                         levels=c(\"bc\", \"wc\", \"prof\"))\nstr(Prestige)\n# 'data.frame': 102 obs. of  6 variables:\n#  $ education: num  13.1 12.3 12.8 11.4 14.6 ...\n#  $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...\n#  $ women    : num  11.16 4.02 15.7 9.11 11.68 ...\n#  $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...\n#  $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...\n#  $ type     : Ord.factor w/ 3 levels \"bc\"&lt;\"wc\"&lt;\"prof\": 3 3 3 3 3 3 3 3 3 3 ...\n\n\nI first illustrate the relation between income and prestige in Figure 3.12 using car::scatterplot() with many of its bells and whistles, including marginal boxplots for the variables, the linear regression line, loess smooth and the 68% data ellipse.\n\n\nscatterplot(prestige ~ income, data=Prestige,\n  pch = 16, cex.lab = 1.25,\n  regLine = list(col = \"red\", lwd=3),\n  smooth = list(smoother=loessLine, \n                lty.smooth = 1, lwd.smooth=3,\n                col.smooth = \"darkgreen\", \n                col.var = \"darkgreen\"),\n  ellipse = list(levels = 0.68),\n  id = list(n=4, method = \"mahal\", col=\"black\", cex=1.2))\n# general.managers          lawyers        ministers       physicians \n#                2               17               20               24\n\n\n\n\n\n\nFigure 3.12: Scatterplot of prestige vs. income, showing the linear regression line (red), the loess smooth with a confidence envelope (darkgreen) and a 68% data ellipse. Points with the 4 largest Mahalanobis \\(D^2\\) values are labeled.\n\n\n\n\nThere is a lot that can be seen here:\n\n\nincome is positively skewed, as is often the case.\nThe loess smooth, on the scale of income, shows prestige increasing up to $15,000 (these are 1971 incomes), and then leveling off.\nThe bivariate 1 standard deviation data ellipse, centered at the means encloses approximately 68% of the data points. It adds visual information about the correlation and precision of the linear regression; but here, the non-linear trend for higher incomes strongly suggests a different approach.\nThe four points identified by their labels are those with the largest Mahalanobis distances. scatterplot() prints their labels to the console.\n\nFigure 3.13 shows a similar plot for education, which from the boxplot appears to be reasonably symmetric. The smoothed curve is quite close to the linear regression, according to which prestige increases on average coef(lm(prestige ~ education, data=Prestige))[\"education\"] = 5.361 with each year of education.\n\nscatterplot(prestige ~ education, data=Prestige,\n  pch = 16, cex.lab = 1.3,\n  regLine = list(col = \"red\", lwd=3),\n  smooth = list(smoother=loessLine, \n                lty.smooth = 1, lwd.smooth=3,\n                col.smooth = \"darkgreen\", \n                col.var = \"darkgreen\"),\n  ellipse = list(levels = 0.68),\n  id = list(n=4, method = \"mahal\", col=\"black\", cex=1.2))\n#  physicians file.clerks    newsboys     farmers \n#          24          41          53          67\n\n\n\n\n\n\nFigure 3.13: Scatterplot of prestige vs. education, showing the linear regression line (red), the loess smooth with a confidence envelope (darkgreen) and a 68% data ellipse.\n\n\n\n\nIn this plot, farmers, newsboys, file.clerks and physicians are identified as noteworthy, for being furthest from the mean by Mahalanobis distance. In relation to their typical level of education, these are mostly understandable, but it is nice that farmers are rated of higher prestige than their level of education would predict.\nNote that the method argument for point identification can take a vector of case numbers indicating the points to be labeled. So, to label the observations with large absolute standardized residuals in the linear model m, you can use method = which(abs(rstandard(m)) &gt; 2).\n\nm &lt;- lm(prestige ~ education, data=Prestige)\nscatterplot(prestige ~ education, data=Prestige,\n    pch = 16, cex.lab = 1.3,\n    boxplots = FALSE,\n    regLine = list(col = \"red\", lwd=3),\n    smooth = list(smoother=loessLine,\n                  lty.smooth = 1, lwd.smooth=3,\n                  col.smooth = \"black\", \n                  col.var = \"darkgreen\"),\n    ellipse = list(levels = 0.68),\n    id = list(n=4, method = which(abs(rstandard(m))&gt;2), \n              col=\"black\", cex=1.2)) |&gt; \n  invisible()\n\n\n\n\n\n\nFigure 3.14: Scatterplot of prestige vs. education, labeling points whose absolute standardized residual is &gt; 2.\n\n\n\n\n\n3.2.4 Handling nonlinearity: Plotting on a log scale\nA typical remedy for the non-linear relationship of income to prestige is to plot income on a log scale. This usually makes sense, and expresses a belief that a multiple of or percentage increase in income has a constant impact on prestige, as opposed to the additive interpretation for income itself.\nFor example, the slope of the linear regression line in Figure 3.12 is given by coef(lm(prestige ~ income, data=Prestige))[\"income\"] = 0.003. Multiplying this by 1000 says that a $1000 increase in income is associated with with an average increase of prestige of 2.9.\nIn the plot below, scatterplot(..., log = \"x\") re-scales the x-axis to the \\(\\log_e()\\) scale. The slope, coef(lm(prestige ~ log(income), data=Prestige))[\"log(income)\"] = 21.556 says that a 1% increase in salary is associated with an average change of 21.55 / 100 in prestige.\n\nscatterplot(prestige ~ income, data=Prestige,\n  log = \"x\",\n  pch = 16, cex.lab = 1.3,\n  regLine = list(col = \"red\", lwd=3),\n  smooth = list(smoother=loessLine,\n                lty.smooth = 1, lwd.smooth=3,\n                col.smooth = \"darkgreen\", col.var = \"darkgreen\"),\n  ellipse = list(levels = 0.68),\n  id = list(n=4, method = \"mahal\", col=\"black\", cex=1.2))\n# general.managers        ministers         newsboys      babysitters \n#                2               20               53               63\n\n\n\n\n\n\nFigure 3.15: Scatterplot of prestige vs. log(income).\n\n\n\n\nThe smoothed curve in Figure 3.15 exhibits a slight tendency to bend upwards, but a linear relation is a reasonable approximation.\n\n3.2.5 Stratifying\nBefore going further, it is instructive to ask what we could see in the relationship between income and prestige if we stratified by type of occupation, fitting separate regressions and smooths for blue collar, white collar and professional incumbents in these occupations.\nThe formula prestige ~ income | type (read: income given type) is a natural way to specify grouping by type; separate linear regressions and smooths are calculated for each group, applying the color and point shapes specified by the col and pch arguments.\n\nscatterplot(prestige ~ income | type, data=Prestige,\n  col = c(\"blue\", \"red\", \"darkgreen\"),\n  pch = 15:17, cex.lab = 1.3,\n  grid = FALSE,\n  legend = list(coords=\"bottomright\"),\n  regLine = list(lwd=3, lty = \"longdash\"),\n  smooth=list(smoother=loessLine, \n              var=FALSE, lwd.smooth=2, lty.smooth=1))\n\n\n\n\n\n\nFigure 3.16: Scatterplot of prestige vs. income, stratified by occupational type. This implies a different interpretation, where occupation type is a moderator variable, so different regressions apply to each type. The linear regression line is dashed, while the loess smooth is solid.\n\n\n\n\nThis visual analysis offers a different interpretation of the dependence of prestige on income, which appeared to be non-linear when occupation type was ignored. Instead, Figure 3.16 suggests an interaction of income by type. In a model formula this would be expressed as one of:\nlm(prestige ~ income + type + income:type, data = Prestige)\nlm(prestige ~ income * type, data = Prestige)\nThese models signify that there are different slopes (and intercepts) for the three occupational types. In this interpretation, type is a moderator variable, with a different story. The slopes of the fitted lines suggest that among blue collar workers, prestige increases sharply with their income. For white collar and professional workers, there is still an increasing relation of prestige with income, but the effect of income (slope) diminishes with higher occupational category. A different fitted relationship entails a different story.\n\n3.2.6 Meet the Penguins\nThe penguins dataset from the palmerpenguins package (Horst et al., 2020) provides further instructive examples of plots and analyses of multivariate data. The data consists of measurements of body size (flipper length, body mass, bill length and depth) of 344 penguins collected at the Palmer Research Station in Antarctica.\n\n\n\n\n\n\n\nFigure 3.17: Penguin species observed in the Palmer Archipelago. This is a cartoon, but it illustrates some features of penguin body size measurements, and the colors typically used for species. Image credit: Allison Horst\n\n\n\n\nThere were three different species of penguins (Adélie, Chinstrap & Gentoo) collected from 3 islands in the Palmer Archipelago between 2007–2009 (Gorman et al., 2014). The purpose was to examine differences in size or appearance of these species, particularly those between the sexes (sexual dimorphism) in relation to foraging and habitat.\nHere, I use a slightly altered version of the dataset, heplots::peng, constructed by renaming variables to remove the units, making factors of character variables and deleting a few cases with missing data.4\n\ndata(penguins, package = \"palmerpenguins\")\npeng &lt;- penguins |&gt;\n  rename(\n    bill_length = bill_length_mm, \n    bill_depth = bill_depth_mm, \n    flipper_length = flipper_length_mm, \n    body_mass = body_mass_g\n  ) |&gt;\n  mutate(species = as.factor(species),\n         island = as.factor(island),\n         sex = as.factor(substr(sex,1,1))) |&gt;\n  tidyr::drop_na()\n\nglimpse(peng)\n# Rows: 333\n# Columns: 8\n# $ species        &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Ade…\n# $ island         &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen,…\n# $ bill_length    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 4…\n# $ bill_depth     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 1…\n# $ flipper_length &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191…\n# $ body_mass      &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3…\n# $ sex            &lt;fct&gt; m, f, f, f, m, f, m, f, m, m, f, f, m, f, m…\n# $ year           &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2…\n\nThere are quite a few variables to choose for illustrating data ellipses in scatterplots. Here I focus on the measures of their bills, bill_length and bill_depth (indicating curvature) and show how to use ggplot2 for these plots.\nI’ll be using the penguins data quite a lot, so it is useful to set up custom colors like those used in Figure 3.17. My versions are shown in Figure 3.18 with their color codes. These are shades of:\n\n\nAdélie: orange,\n\nChinstrap: purple, and\n\nGentoo: green.\n\n\n\n\n\n\n\n\nFigure 3.18: Color palettes used for penguin species. Each species has a primary hue, which can be used in light, medium or dark versions.\n\n\n\n\nTo use these in ggplot2 I define a function peng.colors() that allows shades of light, medium and dark and then functions scale_*_penguins() for color and fill.\n\nCodepeng.colors &lt;- function(shade=c(\"medium\", \"light\", \"dark\")) {\n  shade = match.arg(shade)\n  #             light      medium     dark\n  oranges &lt;- c(\"#FDBF6F\", \"#F89D38\", \"#F37A00\")  # Adelie\n  purples &lt;- c(\"#CAB2D6\", \"#9A78B8\", \"#6A3D9A\")  # Chinstrap\n  greens &lt;-  c(\"#B2DF8A\", \"#73C05B\", \"#33a02c\")  # Gentoo\n  \n  cols.vec &lt;- c(oranges, purples, greens)\n  cols.mat &lt;- \n    matrix(cols.vec, 3, 3, \n           byrow = TRUE,\n           dimnames = list(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n                           shade = c(\"light\", \"medium\", \"dark\")))\n  # get shaded colors\n  cols.mat[, shade ]\n}\n\n# define color and fill scales\nscale_fill_penguins &lt;- function(shade=c(\"medium\", \"light\", \"dark\"), ...){\n  shade = match.arg(shade)\n  ggplot2::discrete_scale(\n    \"fill\",\"penguins\",\n     scales:::manual_pal(values = peng.colors(shade)), ...)\n}\n\nscale_colour_penguins &lt;- function(shade=c(\"medium\", \"light\", \"dark\"), ...){\n  shade = match.arg(shade)\n  ggplot2::discrete_scale(\n    \"colour\",\"penguins\",\n    scales:::manual_pal(values = peng.colors(shade)), ...)\n}\nscale_color_penguins &lt;- scale_colour_penguins\n\n\nThis is used to define a theme_penguins() function that I use to simply change the color and fill scales for plots below. I also define a convenience function legend_inside() to make it less verbose to position a legend inside a plot window, because outside legends reduce resolution within the plot.\n\ntheme_penguins &lt;- function(shade=c(\"medium\", \"light\", \"dark\"), \n                           ...) {\n  shade = match.arg(shade)\n  list(scale_color_penguins(shade=shade),\n       scale_fill_penguins(shade=shade)\n      )\n}\n\nlegend_inside &lt;- function(position) {          # simplify legend placement\n  theme(legend.position = \"inside\",\n        legend.position.inside = position)\n}\n\nAn initial plot using ggplot2 shown in Figure 3.19 uses color and point shape to distinguish the three penguin species. I annotate the plot of points using the linear regression lines, loess smooths to check for non-linearity and 95% data ellipses to show precision of the linear relation.\n\nCodeggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, shape = species, fill=species)) +\n  geom_point(size=2) +\n  geom_smooth(method = \"lm\", formula = y ~ x,\n              se=FALSE, linewidth=2) +\n  geom_smooth(method = \"loess\",  formula = y ~ x,\n              linewidth = 1.5, se = FALSE, alpha=0.1) +\n  stat_ellipse(geom = \"polygon\", level = 0.95, alpha = 0.2) +\n  theme_penguins(\"dark\") +\n  legend_inside(c(0.85, 0.15))\n\n\n\n\n\n\nFigure 3.19: Penguin bill length and bill depth according to species.\n\n\n\n\n\n3.2.7 Visual thinning\nOverall, the three species occupy different regions of this 2D space and for each species the relation between bill length and depth appears reasonably linear. Given this, we can suppress plotting the data points to get a visual summary of the data using the fitted regression lines and data ellipses, as shown in Figure 3.20.\nThis idea, of visual thinning a graph to focus on what should be seen, becomes increasingly useful as the data becomes more complex. The ggplot2 framework encourages this, because we can think of various components as layers, to be included or not. In Figure 3.20 I chose to include only the regression line and add data ellipses of 40%, 68% and 95% coverage to highlight the increasing bivariate density around the group means.\n\nCodeggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, shape = species, fill=species)) +\n  geom_smooth(method = \"lm\",  se=FALSE, linewidth=2) +\n  stat_ellipse(geom = \"polygon\", level = 0.95, alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", level = 0.68, alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", level = 0.40, alpha = 0.2) +\n  theme_penguins(\"dark\") +\n  legend_inside(c(0.85, 0.15))\n\n\n\n\n\n\nFigure 3.20: Visual thinning: Suppressing the data points gives a visual summary of the relation between bill length and bill depth using the regression line and data ellipses.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-bagplots",
    "href": "04-multivariate_plots.html#sec-bagplots",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.3 Bagplots",
    "text": "3.3 Bagplots\nIf you are concerned about the assumption of bivariate normality entailed by the data ellipse, a very nice non-parametric and robust alternative is a 2D generalization of a boxplot called a bagplot, introduced by Rousseeuw et al. (1999). The idea is very simple. The bagplot consists of three nested polygons, called the “bag”, the “fence”, and the “loop”:\n\nbag: The central 50% box of the boxplot is replaced by a polygon called the “bag”, constructed on the basis of depth of points from the bivariate depth median point. Depth generalizes the univariate concept of rank, but counted from the medians point outward in any direction. The bag contains at most 50% of the most central data points. \nfence: The univariate fences are replaced by a “fence” polygon, by expanding the bag outward by a factor (coef), usually 3, from the depth median.\nloop: Points beyond the fence (the “loop”) are potential outliers, sometimes plotted as a convex hull surrounding all of the observations, but better rendered simply as a scatterplot of just those points.\n\nIn this way, the bagplot visualizes the bivariate location (median), spread, correlation, skewness, and tails of the data. Compared with a standard data ellipse, it serves as a visual test of normality and provides a simple way to identify outliers.\nBagplots are implemented in gggda (Brunson & Gracey, 2025) in the ggplot2 framework as geom_bagplot() with computations done via stat_bagplot(). For the Penguin data, the bagplot version of Figure 3.20 is shown in Figure 3.21.\n\nCodeggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, shape = species, fill=species)) +\n  geom_smooth(method = \"lm\", formula = y ~ x,\n              se=FALSE, linewidth=2) +\n  geom_bagplot(bag.alpha = 0.5,\n               outlier.size = 5,\n               fraction = 0.5,    # bag fraction\n               coef = 2.5,        # fence factor\n               show.legend = FALSE) +\n  theme_penguins(\"dark\") +\n  legend_inside(c(0.87, 0.15)) \n\n\n\n\n\n\nFigure 3.21: Bagplot: For each Penguin species the darker inner (bag) polygon reflects the innermost 50% of the data points. The outer (fence) polygon corresponds to points enclosed by a multiple of the bag. Points outside the fence for each species are ploted individually.\n\n\n\n\nCompared with the data ellipses shown in Figure 3.20, the bag polygons have shapes very close to ellipses, giving credence to the assumption of (approximate) normality. Using the factor coef = 2.5 causes five points to be flagged as potential outliers. Bivariate skewness is indicated when the bag and fence is not symmetric around the central median in some direction, as is slightly true for all three species.\nI discuss other visual tests of multivariate normality in Section 3.6 and consider outlier identification for the Penguin data in Section 3.6.2 below.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-bivar-density",
    "href": "04-multivariate_plots.html#sec-bivar-density",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.4 Non-parametric bivariate density plots",
    "text": "3.4 Non-parametric bivariate density plots\nWhile I emphasize data ellipses (because I like their beautiful geometry), other visual summaries of the bivariate density are possible and often useful. To see more detail about the “shape” of bivariate data in a non-parametric way you can use one of the methods described below to see a model-free representation of your data.\nFor a single variable, stats::density() and ggplot2::geom_density() calculate a smoothed estimate of the density using nonparametric kernel methods (Silverman, 1986) whose smoothness is controlled by a bandwidth parameter, analogous to the span in a loess smoother. This idea extends to two (and more) variables (Scott, 1992). For bivariate data, MASS::kde2d() estimates the density on a square \\(n \\times n\\) grid over the ranges of the variables.\nggplot2 provides geom_density_2d() which uses MASS::kde2d() and displays these as contours— horizontal slices of the 3D surface at equally-spaced heights and projects these onto the 2D plane. The ggdensity package (Otto & Kahle, 2023) extends this with geom_hdr(), computing the high density regions that bound given levels of probability and maps these to the alpha transparency aesthetic. A method argument allows you to specify various nonparametric (method =\"kde\" is the default) and parametric (method =\"mvnorm\" gives normal data ellipses) ways to estimate the underlying bivariate distribution. \nFigure 3.22 shows these side-by-side for comparison. With geom_density_2d() you can specify either the number of contour bins or the width of these bins (binwidth). For geom_hdr(), the probs argument gives a result that is easier to understand.\n\nCodelibrary(ggdensity)\nlibrary(patchwork)\np1 &lt;- ggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species)) +\n  geom_smooth(method = \"lm\",  se=FALSE, linewidth=2) +\n  geom_density_2d(linewidth = 1.1, bins = 8) +\n  ggtitle(\"geom_density_2d\") +\n  theme_penguins() +\n  legend_inside(c(0.85, 0.15))\n\np2 &lt;- ggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, fill = species)) +\n  geom_smooth(method = \"lm\",  se=FALSE, linewidth=2) +\n  geom_hdr(probs = c(0.95, 0.68, 0.4), show.legend = FALSE) +\n  ggtitle(\"ggdensity::geom_hdr\") +\n  theme_penguins() +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\nFigure 3.22: Bivariate densities show the contours of the 3D surface representing the frequency in the joint distribution of bill length and bill depth.\n\n\n\n\nCompared with the data ellipse, which is highly smoothed by the Gaussian assumption and the bagplot, which is smoothed by the concept of contours of data depth, the plots in Figure 3.22 show considerably more detail, and an intriguing suggestion of two peaks for our Chinstrap penguins.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-simpsons",
    "href": "04-multivariate_plots.html#sec-simpsons",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.5 Simpson’s paradox: marginal and conditional relationships",
    "text": "3.5 Simpson’s paradox: marginal and conditional relationships\nBecause it provides a visual representation of means, variances, and correlations, the data ellipse is ideally suited as a tool for illustrating and explicating various phenomena that occur in the analysis of linear models. One class of simple, but important, examples concerns the difference between the marginal relationship between variables, ignoring some important factor or covariate, and the conditional relationship, adjusting (controlling) for that variable.\nAn important example is Simpson’s paradox (Simpson, 1951) which occurs when the marginal and conditional relationships differ in direction. That is, the overall correlation in a model y ~ x might be negative, while the within-group correlations in separate models for each group y[g] ~ x[g] might be positive, or vice versa. For Flatlanders in 2D space, this is a puzzlement.\nWe can see this paradox in the plots of bill length against bill depth for the penguin data shown in Figure 3.23. Ignoring penguin species, the marginal, total-sample correlation is slightly negative as seen in panel (a). The individual-sample ellipses in panel (b) show that the conditional, within-species correlations are all positive, with approximately equal regression slopes. However the group means have a negative relationship, accounting for the negative marginal correlation when species is ignored.\n\n\n\n\n\n\n\nFigure 3.23: Marginal (a), conditional (b), and pooled within-sample (c) relationships of bill length and depth in the Penguins data. Each plot shows the 68% data ellipse and regression line(s) with 95% confidence bands.\n\n\n\n\nThe regression line in panel (a) is that for the linear model lm(bill_depth ~ bill_length), while the separate lines in panel (b) are those for the model lm(bill_depth ~ bill_length * species) which allows a different slope and intercept for each species.\nA correct analysis of the (conditional) relationship between these variables, controlling or adjusting for mean differences among species, is based on the pooled within-sample covariance matrix, a weighted average of the individual within-group \\(\\mathbf{S}_i\\),\n\\[\n\\mathbf{S}_{\\textrm{within}}  =\n\\sum_{i=1}^g\n(n_i - 1) \\mathbf{S}_i \\, / \\, (N - g)\n\\:\\: ,\n\\]\nwhere \\(N = \\sum n_i\\). The result is shown in panel (c) of Figure 3.23.\nIn this graph, the data for each species were first transformed to deviations from the species means on both variables and then translated back to the grand means. You can also see here that the shapes and sizes of the individual data ellipses are roughly comparable, but perhaps not identical. This visual idea of centering groups to a common mean will become important in Chapter 12 when we want to test the assumption of equality of error covariances in multivariate models.\nThe ggplot2 code for the panels in this figure are shown below. Note that for components that will be the same across panels, you can define elements (e.g., labels, theme_penguins()) once, and then re-use these across several graphs.\nTODO This panel tabset looks fine in HTML but is awkward in PDF\n\n\n(a) Ignoring species\n(b) By species\n(c) Within species\n\n\n\n\nlabels &lt;- labs(\n  x = \"Bill length (mm)\",\n  y = \"Bill depth (mm)\",\n  color = \"Species\",\n  shape = \"Species\",\n  fill = \"Species\") \n\nplt1 &lt;- ggplot(data = peng,\n               aes(x = bill_length,\n                   y = bill_depth)) +\n  geom_point(size = 1.5) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              se = TRUE, color = \"gray50\") +\n  stat_ellipse(level = 0.68, linewidth = 1.1) +\n  ggtitle(\"Ignoring species\") +\n  labels\n\nplt1\n\n\n\n\nplt2 &lt;- ggplot(data = peng,\n               aes(x = bill_length,\n                   y = bill_depth,\n                   color = species,\n                   shape = species,\n                   fill = species)) +\n  geom_point(size = 1.5,\n             alpha = 0.8) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              se = TRUE, alpha = 0.3) +\n  stat_ellipse(level = 0.68, linewidth = 1.1) +\n  ggtitle(\"By species\") +\n  labels +\n  theme_penguins(\"dark\") +\n  legend_inside(c(0.83, 0.16)) \n\nplt2\n\n\n\n\n# center within groups, translate to grand means\nmeans &lt;- colMeans(peng[, 3:4])\npeng.centered &lt;- peng |&gt;\n  group_by(species) |&gt;\n  mutate(bill_length = means[1] + scale(bill_length, scale = FALSE),\n         bill_depth  = means[2] + scale(bill_depth, scale = FALSE))\n\nplt3 &lt;- ggplot(data = peng.centered,\n               aes(x = bill_length,\n                   y = bill_depth,\n                   color = species,\n                   shape = species,\n                   fill = species)) +\n  geom_point(size = 1.5,\n             alpha = 0.8) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              se = TRUE, alpha = 0.3) +\n  stat_ellipse(level = 0.68, linewidth = 1.1) +\n  labels +\n  ggtitle(\"Within species\") +\n  theme_penguins(\"dark\") +\n  legend_inside(c(0.83, 0.16)) \n\nplt3\n\n\n\n\nTODO: Add stuff on 3D scatterplots, using the R/penguin/peng-3D-rgl.R example. A start on this is in child/03-3D-scat.qmd",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-multivar-normality",
    "href": "04-multivariate_plots.html#sec-multivar-normality",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.6 Multivariate normality and outliers",
    "text": "3.6 Multivariate normality and outliers\nThe relation of the data ellipsoid for \\(p\\) variables to the \\(\\chi^2_p\\) distribution with \\(p\\) degrees of freedom described in Section 3.2 is based on the assumption that the data in \\(\\mathbf{y}\\) is a sample from a multivariate normal distribution (MVN), with a mean vector \\(\\boldsymbol{\\mu}\\) and variance-covariance matrix \\(\\boldsymbol{\\Sigma}\\), so each one implies the other:\n\\[\n\\mathbf{y}_{p \\times 1} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\Longleftrightarrow D^2_M (\\mathbf{y}) \\sim \\chi^2_p\\; .\n\\]\nThis fact can be used to assess whether sample data in \\(\\mathbf{y}\\) does indeed follow a MVN distribution by plotting the quantiles5 of the sorted sample Mahalanobis \\(D^2\\) values (denoted \\(D^2_{(i)}\\)) calculated from Equation 3.1 against the corresponding \\(\\chi^2_p\\) quantiles found from qchisq(df = p). This is called a \\(\\chi^2\\) QQ plot.\nThe essential idea is that the plotted points should then approximately fall along a 45\\(^o\\) line of slope = 1 when the data are MVN (and axes are scaled equally). This also provides a simple method to identify potential outliers as those points which are furthest from the centroid; that is those for which \\(D^2_{(i)}\\) is greater than the \\(1 - \\alpha\\) quantile of \\(\\chi^2_p\\).\nThe topics of assessing multivariate normality and detecting multivariate outliers are larger than I consider here. These topics are also intertwined, because outliers inflate the variance (\\(\\mathbf{S}\\)) of the data, making extreme observations appear less extreme. A variety of robust methods described later (Section 13.5) work by down-weighting outliers, which is particularly important in assessing whether the residuals from a multivariate linear model are multivariate normal.\nIn this section, I focus on the \\(\\chi^2\\) QQ plot and graphical methods to relate the the points identified as potential outliers to plots in data space.\n\n3.6.1 Galton data\nMahalanobis \\(D^2\\) values are calculated by heplots::Mahalanobis(). I illustrate finding the largest \\(D^2_{(i)}\\) for the Galton data as shown below. I’ve used \\(\\alpha = 0.01\\), giving \\(\\chi^2_p (0.99) = 9.21\\) as an outlier cutoff, just for the sake of this example. With 928 cases, we would expect about 1% or 9 to have larger squared distances than this cutoff. (Normally, allowing for the fact that we are looking at the largest values in a sample of size \\(n\\), we would use a much smaller individual significance level, say \\(\\alpha = 0.001\\) or smaller still.) Three cases are identified here.\n\ndata(Galton, package = \"HistData\")\nDSQ &lt;- Mahalanobis(Galton)\nalpha &lt;- 0.01\ncutoff &lt;- (qchisq(p = 1 - alpha, df = ncol(Galton))) |&gt; \n  print()\n# [1] 9.21\noutliers &lt;- which(DSQ &gt; cutoff) |&gt;\n  print()\n# [1]   1  13 897\nGaltonD &lt;- cbind(Galton, DSQ = DSQ)\nGaltonD[outliers,]\n#     parent child   DSQ\n# 1     70.5  61.7 13.67\n# 13    70.5  63.2  9.45\n# 897   65.5  72.2  9.49\n\n\\(\\chi^2\\) QQ plots are constructed by heplots::cqplot(). For the Galton data, the result is shown in Figure 3.24, where I’ve asked for the id.n = 3 with the greatest \\(D^2_{(i)}\\) values to be identified with their row numbers in the plot. The function returns (invisibly) the \\(D^2_{(i)}\\) and corresponding \\(\\chi^2_2\\) quantile along with the upper-tail \\(p\\)-value.\n\nout &lt;- cqplot(Galton, id.n = 3)\nout\n#       DSQ quantile        p\n# 1   13.67     15.1 0.000539\n# 897  9.49     12.9 0.001616\n# 13   9.45     11.8 0.002694\n\n\n\n\n\n\nFigure 3.24: Chi-square QQ plot of Galton’s data on heights of parents and their offspring, with a 95% pointwise confidence envelope\n\n\n\n\nIn the typical use of QQ plots, it essential to have something in the nature of a confidence band around the points to be able to appreciate whether, and to what degree the observed data points differ from the reference distribution. For cqplot(), this helps to assess whether the data are reasonably distributed as multivariate normal and also to flag potential outliers.\nThe pointwise 95% confidence envelope is calculated as \\(D^2_{(i)} \\pm 1.96 \\times \\text{se} ( D^2_{(i)} )\\) where the standard error is calculated (Chambers et al., 1983, sec. 8.6) as\n\\[\n\\text{se} ( D^2_{(i)} ) = \\frac{\\hat{b}}{d ( q_i)} \\times \\sqrt{  p_i (1-p_i) / n }  \\:\\: .\n\\] Here, \\(\\hat{b}\\) is an estimate of the actual slope of the reference line obtained from from the ratio of the interquartile range of the \\(D^2\\) values to that of the corresponding \\(\\chi^2_p\\) distribution. \\(d(q_i)\\) is the density of the chi square distribution at the quantile \\(q_i\\) and \\(p_i\\) is the corresponding percentile.\nSo, in Figure 3.24, you can see the same three points, with largest \\(D^2\\) identified earlier. The confidence band shows that uncertainty increases a points get further from the mean, but by and large, nearly all the points lie within it. The fact that larger values fall beneath the reference line indicates that the sample \\(D^2\\) are somewhat less concentrated (has a shorter upper tail) than values in the \\(\\chi^2_2\\) distribution. \nTo help understand what we are seeing in this example, it is helpful to view the data in a scatterplot equivalent of Figure 3.11, with the same three observations labeled and made distinctive in the plot. For this plot, because the heights are recorded in whole inches, I draw the data ellipses first without plotting the points and then draw the points using jitter() on top of the data ellipses.\n\nset.seed(47)\ndataEllipse(parent ~ child, data = GaltonD,\n            levels = c(0.68, 0.95),\n            add = FALSE, plot.points = FALSE,\n            center.pch = \"+\", center.cex = 3,\n            cex.lab = 1.5)\nwith(GaltonD,{\n  points(jitter(child), jitter(parent),\n         col = ifelse(DSQ &gt; cutoff, \"red\", \"black\"),\n         pch = ifelse(DSQ &gt; cutoff, 16, 1),\n         cex = ifelse(DSQ &gt; cutoff, 2, 0.8))\n  text(child[outliers], parent[outliers], labels = outliers, pos = 3)\n  }\n)\n\n\n\n\n\n\nFigure 3.25: Scatterplot of Galton’s data showing 68% and 95% data ellipses for the observations on parent and child height. The discrete points have been jittered to avoid overplotting. The three observations identified as having large \\(D^2\\) values are labeled with their observation number and given distinctive size, color and shape.\n\n\n\n\nIt is clear from this plot that case 897 is for an exceptionally tall child of quite short parents, while case 1 and 13 are from very short children of tall parents; you could call them poster children for regression toward the mean.\n\n3.6.2 Penguin data\nLet’s do a similar analysis to assess multivariate normality and identify possible outliers for the Penguin data. In the cqplot() shown in Figure 3.26, I use the same point symbols and colors as in Figure 3.19. For illustration, I again label the three most extreme points.\n\nclr &lt;- peng.colors(\"dark\")\npch &lt;- c(19, 17, 15)   # ggplot symbol defaults for a factor\n\nout &lt;- cqplot(peng[, 3:6], \n   id.n = 3,\n   col = clr[peng$species],\n   pch = pch[peng$species],\n   ref.col = \"grey\",\n   what = \"Penguin numeric variables\",\n   cex.lab = 1.25)\nout\n#      DSQ quantile       p\n# 283 27.8     17.6 0.00150\n# 10  13.3     15.1 0.00450\n# 35  12.4     13.9 0.00751\n\n\n\n\n\n\nFigure 3.26: Chi-square QQ plot of the Pengiun data. The three cases with the largest \\(D^2\\) values are identified with their case numbers.\n\n\n\n\nOne point (case 283) stands out as an extreme multivariate outlier. The other two (10, 35) are well within the confidence envelope.\nTo relate this to the data, we can plot the data, as was done in Figure 3.19, and label the points identified as noteworthy. It is a bit tricky to label points selectively in ggplot2 when the criterion for which points to label is complex, or involves variables outside the data frame. Here, I create a logical variable note, which is TRUE for the noteworthy ones. This is then used to subset the data for geom_text() that writes the case id numbers. Figure 3.27 shows the resulting plot.\n\nDSQ &lt;- Mahalanobis(peng[, 3:6])\nnoteworthy &lt;- order(DSQ, decreasing = TRUE)[1:3] |&gt; print()\n# [1] 283  10  35\n\npeng_plot &lt;- peng |&gt;\n  tibble::rownames_to_column(var = \"id\") |&gt; \n  mutate(note = id %in% noteworthy)\n\nggplot(peng_plot, \n       aes(x = bill_length, y = bill_depth,\n           color = species, shape = species, fill=species)) +\n  geom_point(aes(size=note), show.legend = FALSE) +\n  scale_size_manual(values = c(1.5, 4)) +\n  geom_text(data = subset(peng_plot, note==TRUE),\n            aes(label = id),\n            nudge_y = .4, color = \"black\", size = 5) +\n  geom_smooth(method = \"lm\", formula = y ~ x,\n              se=FALSE, linewidth=2) +\n  stat_ellipse(geom = \"polygon\", level = 0.95, alpha = 0.1) +\n  theme_penguins() +\n#  theme_bw(base_size = 14) +\n  legend_inside(c(0.85, 0.15)) \n\n\n\n\n\n\nFigure 3.27: Plot of bill length and bill depth, with the noteworthy points labeled. Only one (case 283) is a true multivariate outlier.\n\n\n\n\nTwo cases (10, 283) appear to be very unusual in this plot, in relation to other members of their species. But only case 283 is a true multivariate outlier, as shown in the cqplot Figure 3.26. We could call this long-billed penguin “Cyrano”.6 I’ll call case 10, with a very short and curved bill, “Hook Nose”.\nOf course, we are only looking at the data in the 2D space of the bill variables, but possible outliers exist in four dimensional Penguin space. Case 35 is well inside the Adélie ellipse, so perhaps it is unusual on one of the other variables. It turns out that multivariate outliers can most often be easily seen as unusual observations in a projection of the data into the space of the smallest principal components. I return to this example in Section 4.7.\nIt bears noting that for linear models, multivariate normality is not required for the response variables or predictors, but rather is an assumption for the residuals from a model. As well, multivariate outliers in the responses may not turn out to be unusual when the predictor variables are taken into account. In this example, a multivariate model would include the effect of species,\npeng.mod &lt;- lm(cbind(bill_length, bill_depth, flipper_length, body_mass) ~ species, \n               data = peng)\nand there would be cause for concern if the residuals from this model, residuals(peng.mod) were highly non-normal or showed outliers.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-scatmat",
    "href": "04-multivariate_plots.html#sec-scatmat",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.7 Scatterplot matrices",
    "text": "3.7 Scatterplot matrices\nGoing beyond bivariate scatterplots, a pairs plot (or scatterplot matrix) displays all possible \\(p \\times p\\) pairs of \\(p\\) variables in a matrix-like display where variables \\((x_i, x_j)\\) are shown in a plot for row \\(i\\), column \\(j\\). This idea, due to Hartigan (1975b), uses small multiple plots, so that the eye can easily scan across a row or down a column to see how a given variable is related to all the others.\nThe most basic version is provided by pairs() in base R. When one variable is considered as an outcome or response, it is usually helpful to put this in the first row and column. For the Prestige data, in addition to income and education, we also have a measure of % women in each occupational category.\nPlotting these together gives Figure 3.28. In such plots, the diagonal cells give labels for the variables, but they are also a guide to interpreting what is shown. In each row, say row 2 for income, income is the vertical \\(y\\) variable in plots against other variables. In each column, say column 3 for education, education is the horizontal \\(x\\) variable.\n\npairs(~ prestige + income + education + women,\n      data=Prestige)\n\n\n\n\n\n\nFigure 3.28: Scatterplot matrix of the variables in the Prestige dataset produced by pairs()\n\n\n\n\nThe plots in the first row show what we have seen before for the relations between prestige and income and education, adding to those the plot of prestige vs. % women. Plots in the first column show the same data, but with \\(x\\) and \\(y\\) interchanged.\nBut this basic pairs() plot is very limited. A more feature-rich version is provided by car::scatterplotMatrix() which can add the regression lines, loess smooths and data ellipses for each pair, as shown in Figure 3.29.\nThe diagonal panels show density curves for the distribution of each variable; for example, education appears to be multi-modal and that of women shows that most of the occupations have a low percentage of women.\nThe combination of the regression line with the loess smoothed curve, but without their confidence envelopes, provides about the right amount of detail to take in at a glance where the relations are non-linear. We’ve already seen (Figure 3.12) the non-linear relation between prestige and income (row 1, column 2) when occupational type is ignored. But all relations with income in column 2 are non-linear, reinforcing our idea (Section 3.2.4) that effects of income should be assessed on a log scale.\n\nscatterplotMatrix(~ prestige + income + education + women,\n  data=Prestige,\n  regLine = list(method=lm, lty=1, lwd=2, col=\"black\"),\n  smooth=list(smoother=loessLine, spread=FALSE,\n              lty.smooth=1, lwd.smooth=3, col.smooth=\"red\"),\n  ellipse=list(levels=0.68, fill.alpha=0.1))\n\n\n\n\n\n\nFigure 3.29: Scatterplot matrix of the variables in the Prestige dataset from car::scatterplotMatrix().\n\n\n\n\nscatterplotMatrix() can also label points using the id = argument (though this can get messy) and can stratify the observations by a grouping variable with different symbols and colors. For example, Figure 3.30 uses the syntax ~ prestige + education + income + women | type to provide separate regression lines, smoothed curves and data ellipses for the three types of occupations. (The default colors are somewhat garish, so I use scales::hue_pal() to mimic the discrete color scale used in ggplot2).\n\nscatterplotMatrix(~ prestige + income + education + women | type,\n  data = Prestige,\n  col = scales::hue_pal()(3),\n  pch = 15:17,\n  smooth=list(smoother=loessLine, spread=FALSE,\n              lty.smooth=1, lwd.smooth=3, col.smooth=\"black\"),\n  ellipse=list(levels=0.68, fill.alpha=0.1))\n\n\n\n\n\n\nFigure 3.30: Scatterplot matrix of the variables in the Prestige dataset from car::scatterplotMatrix(), stratified by type of occupation.\n\n\n\n\nIt is now easy to see why education is multi-modal: blue collar, white collar and professional occupations have largely non-overlapping years of education. As well, the distribution of % women is much higher in the white collar category.\nFor the penguins data, given what we’ve seen before in Figure 3.19 and Figure 3.20, we may wish to suppress details of the points (plot.points = FALSE) and loess smooths (smooth = FALSE) to focus attention on the similarity of regression lines and data ellipses for the three penguin species. In Figure 3.31, I’ve chosen to show boxplots rather than density curves in the diagonal panels in order to highlight differences in the means and interquartile ranges of the species, and to show 68% and 95% data ellipses in the off-diagonal panels.\n\nscatterplotMatrix(~ bill_length + bill_depth + flipper_length + body_mass | species,\n  data = peng, \n  col = peng.colors(\"medium\"), \n  legend=FALSE,\n  ellipse = list(levels = c(0.68, 0.95), \n                 fill.alpha = 0.1),\n  regLine = list(lwd=3),\n  diagonal = list(method = \"boxplot\"),\n  smooth = FALSE,\n  plot.points = FALSE,\n  cex.labels=1) \n\n\n\n\n\n\nFigure 3.31: Scatterplot matrix of the quantitative variables in the penguins dataset, stratified by species.\n\n\n\n\n\nIt can be seen that the species are widely separated in most of the bivariate plots. As well, the regression lines for species have similar slopes and the data ellipses have similar size and shape in most of the plots. From the boxplots, we can also see that Adelie penguins have shorter bill lengths than the others, while Gentoo penguins have smaller bill depth, but longer flippers and are heavier than Chinstrap and Adelie penguins.\n\n\n\n\n\n\nLooking ahead\n\n\n\nFigure 3.31 provides a reasonably complete visual summary of the data in relation to multivariate models that ask “do the species differ in their means on these body size measures?” This corresponds to the MANOVA model,\n\npeng.mod &lt;- lm(cbind(bill_length, bill_depth, flipper_length, body_mass) ~ species, \n               data=peng)\n\nHypothesis-error (HE) plots, described in Chapter 11 provide a better summary of the evidence for the MANOVA test of differences among means on all variables together. These give an \\(\\mathbf{H}\\) ellipse reflecting the differences among means, to be compared with an \\(\\mathbf{E}\\) ellipse reflecting within-group variation and a visual test of significance.\nA related question is “how well are the penguin species distinguished by these body size measures?” Here, the relevant model is linear discriminant analysis (LDA), where species plays the role of the response in the model,\n\npeng.lda &lt;- MASS:lda( species ~ cbind(bill_length, bill_depth, flipper_length, body_mass), \n               data=peng)\n\nBoth MANOVA and LDA depend on the assumption that the variances and correlations between the variables are the same for all groups. This assumption can be tested and visualized using the methods in Chapter 12.\n\n\n\n3.7.1 Visual thinning\nWhat can you do if there are even more variables than in these examples? If what you want is a high-level, zoomed-out display summarizing the pairwise relations more strongly, you can apply the idea of visual thinning to show only the most important features.\nThis example uses data on the rate of various crimes in the 50 U.S. states from the United States Statistical Abstracts, 1970, used by Hartigan (1975a) and Friendly (1991). These are ordered in the dataset roughly by seriousness of crime or from crimes of violence to property crimes.\n\ndata(crime, package = \"ggbiplot\")\nstr(crime)\n# 'data.frame': 50 obs. of  10 variables:\n#  $ state   : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n#  $ murder  : num  14.2 10.8 9.5 8.8 11.5 6.3 4.2 6 10.2 11.7 ...\n#  $ rape    : num  25.2 51.6 34.2 27.6 49.4 42 16.8 24.9 39.6 31.1 ...\n#  $ robbery : num  96.8 96.8 138.2 83.2 287 ...\n#  $ assault : num  278 284 312 203 358 ...\n#  $ burglary: num  1136 1332 2346 973 2139 ...\n#  $ larceny : num  1882 3370 4467 1862 3500 ...\n#  $ auto    : num  281 753 440 183 664 ...\n#  $ st      : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n#  $ region  : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n\nFigure 3.32 displays the scatterplot matrix for these seven variables, using only the regression line and data ellipse to show the linear relation and the loess smooth to show potential non-linearity.  To make this even more schematic, the axis tick marks and labels are also removed using the par() settings xaxt = \"n\", yaxt = \"n\".\n\ncrime |&gt;\n  select(where(is.numeric)) |&gt;\n  scatterplotMatrix(\n    plot.points = FALSE,\n    ellipse = list(levels = 0.68, fill=FALSE),\n    smooth = list(spread = FALSE, \n                  lwd.smooth=2, lty.smooth = 1, \n                  col.smooth = \"red\"),\n    cex.labels = 2,\n    xaxt = \"n\", yaxt = \"n\")\n\n\n\n\n\n\nFigure 3.32: Visual thinning: Scatterplot matrix of the crime data, showing only high-level summaries of the linear and nonlinear relations betgween each pair of variables.\n\n\n\n\nWe can see that all pairwise correlations are positive, pairs closer to the main diagonal tend to be more highly correlated and in most cases the nonparametric smooth doesn’t differ much from the linear regression line. Exceptions to this appear mainly in the columns for robbery and auto (auto theft).",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-corrgram",
    "href": "04-multivariate_plots.html#sec-corrgram",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.8 Corrgrams",
    "text": "3.8 Corrgrams\nTODO: Perhaps split the chapter here\nWhat if you want to summarize the data even further simple visual thinning. For example with many variables you might want to show only the value of the correlation for each pair of variables, but do so in a way to help see patterns in the correlations that would be invisible in just a table.\nA corrgram (Friendly, 2002) is a visual display of a correlation matrix, where the correlation can be rendered in a variety of ways to show the direction and magnitude: circular “pac-man” (or pie) symbols, ellipses, colored vars or shaded rectangles, as shown in Figure 3.33.\nAnother aspect is that of effect ordering (Friendly & Kwan, 2003), ordering the levels of factors and variables in graphic displays to make important features most apparent. For variables, this means that we can arrange the variables in a matrix-like display in such a way as to make the pattern of relationships easiest to see. Methods to achieve this include using principal components and cluster analysis to put the most related variables together as described in Chapter 4.\n\n\n\n\n\n\n\nFigure 3.33: Corrgrams: Some renderings for the value of a correlation in a corrgram display, conveying sign and magnitude in different ways.\n\n\n\n\nIn R, these diagrams can be created using the corrgram (Wright, 2021) and corrplot (Wei & Simko, 2024) packages, with different features. corrgram::corrgram() is closest to Friendly (2002), in that it allows different rendering functions for the lower, upper and diagonal panels as illustrated in Figure 3.33. For example, a corrgram similar to Figure 3.32 can be produced as follows (not shown here):\n\ncrime |&gt;\n  select(where(is.numeric)) |&gt;\n  corrgram(lower.panel = panel.ellipse,\n           upper.panel = panel.ellipse,\n           diag.panel = panel.density)\n\nWith the corrplot package, corrplot() provides the rendering methods c(\"circle\", \"square\", \"ellipse\", \"number\", \"shade\", \"color\", \"pie\"), but only one can be used at a time. The function corrplot.mixed() allows different options to be selected for the lower and upper triangles. The iconic rendering shape is colored with a gradient in relation to the correlation value. For comparison, Figure 3.34 uses ellipses below the diagonal and filled pie charts below the diagonal using a gradient of the fill color in both cases.\n\ncrime.cor &lt;- crime |&gt;\n  dplyr::select(where(is.numeric)) |&gt; \n  cor()\n\ncorrplot.mixed(crime.cor,\n   lower = \"ellipse\",\n   upper = \"pie\",\n   tl.col = \"black\",\n   tl.srt = 0,\n   tl.cex = 1.25,\n   addCoef.col = \"black\",\n   addCoefasPercent = TRUE)\n\n\n\n\n\n\nFigure 3.34: Mixed corrplot of the crime data, showing the correlation between each pair of variables with an ellipse (lower) and a pie chart symbol (upper), all shaded in proportion to the correlation value, also shown numerically.\n\n\n\n\nThe combination of renderings shown in Figure 3.34 is instructive. Small differences among correlation values are easier to see with the pie symbols than with the ellipses; for example, compare the values for murder with larceny and auto theft in row 1, columns 6-7 with those in column 1, rows 6-7, where the former are easier to distinguish. The shading color adds another visual cue.\nThe variables in Figure 3.32 and Figure 3.34 are arranged by their order in the dataset, which is not often the most useful. A better idea is to arrange the variables so that the most highly correlated variables are adjacent.\nA general method described in Section 4.5 orders the variables according to the angles of the first two eigenvectors from a principal components analysis (PCA) around a unit circle. The function corrMatOrder() provides several methods (order = c(\"AOE\", \"FPC\", \"hclust\", \"alphabet\")) for doing this, and PCA ordering is order = \"AOE\". Murder and auto theft are still first and last, but some of the intermediate crimes have been rearranged.\n\nord &lt;- corrMatOrder(crime.cor, order = \"AOE\")\nrownames(crime.cor)[ord]\n# [1] \"murder\"   \"assault\"  \"rape\"     \"robbery\"  \"burglary\"\n# [6] \"larceny\"  \"auto\"\n\n\n\nUsing this ordering in corrplot() produces Figure 3.35.\n\ncorrplot.mixed(crime.cor,\n  order = \"AOE\", \n  lower = \"ellipse\",\n  upper = \"ellipse\",\n  tl.col = \"black\",\n  tl.srt = 0,\n  tl.cex = 1.25,\n  addCoef.col = \"black\",\n  addCoefasPercent = TRUE)\n\n\n\n\n\n\nFigure 3.35: Corrplot of the crime data with the variables reordered according to the angles of variable eigenvectors. Correlations are rendered with ellipses shaded in proportion to their magnitude.\n\n\n\n\nIn this case, where the correlations among the crime variables are all positive, the effect of variable re-ordering is subtle, but note that there is now a slightly pronounced pattern of highest correlations near the diagonal, and decreasing away from the diagonal. Figure 4.27 and Figure 4.29 in Section 4.5 provide a more dramatic example of variable ordering using this method.\nVariations of corrgrams are worthy replacements for a numeric table of correlations, which are often presented in publications only for archival value. Including the numeric value (rounded here, for presentation purposes), makes this an attractive alternative to boring tables of correlations.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-ggpairs",
    "href": "04-multivariate_plots.html#sec-ggpairs",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.9 Generalized pairs plots",
    "text": "3.9 Generalized pairs plots\nWhen a dataset contains one or more discrete variables, the traditional pairs plot cannot cope, because the discrete categories would plot as many overlaid points. This cannot be represented using only color and/or point symbols in a meaningful scatterplot.\nBut the associations between categorical variables in a frequency table can be shown in mosaic displays (Friendly, 1994), using an array of tiles whose areas are depict the cell frequencies. For an \\(n\\)-way frequency, an analog of the scatterplot matrix uses mosaic plots for each pair of variables. The vcd package (Meyer et al., 2024) implements very general pairs() methods for \"table\" objects and vcdExtra (Friendly, 2025) extends this to wide classes of loglinear models (Friendly, 1999) See Friendly (1999) and my book Discrete Data Analysis with R (Friendly & Meyer, 2016) for mosaic plots and matrices.\nFor example, we can tabulate the distributions of penguin species by sex and the island where they were observed using xtabs(). ftable() prints this three-way table more compactly. (In this example, and what follows in the chapter, I’ve changed the labels for sex from (“f”, “m”) to (“Female”, “Male”)).\n\n# use better labels for sex\npeng &lt;- peng |&gt;\n  mutate(sex = factor(sex, labels = c(\"Female\", \"Male\")))\npeng.table &lt;- xtabs(~ species + sex + island, data = peng)\n\nftable(peng.table)\n#                  island Biscoe Dream Torgersen\n# species   sex                                 \n# Adelie    Female            22    27        24\n#           Male              22    28        23\n# Chinstrap Female             0    34         0\n#           Male               0    34         0\n# Gentoo    Female            58     0         0\n#           Male              61     0         0\n\nWe can see immediately that the penguin species differ by island: only Adélie were observed on all three islands; Biscoe Island had no Chinstraps and Dream Island had no Gentoos.\nvcd::pairs() produces all pairwise mosaic plots, as shown in Figure 3.36. The diagonal panels show the one-way frequencies by width of the divided bars. Each off-diagonal panel shows the bivariate counts, breaking down each column variable by splitting the bars in proportion to a second variable. Consequently, the frequency of each cell is represented by its’ area. The purpose is to show the pattern of association between each pair, and so, the tiles in the mosaic are shaded according to the signed standardized residual, \\(d_{ij} = (n_{ij} - \\hat{n}_{ij}) / \\sqrt{\\hat{n}_{ij}}\\) in a simple \\(\\chi^2 = \\Sigma_{ij} \\; d_{ij}^2\\) test for association— blue where the observed frequency \\(n_{ij}\\) is significantly greater than expected \\(\\hat{n}_{ij}\\) under independence, and red where it is less than expected. The tiles are unshaded when \\(| d_{ij} | &lt; 2\\).\n\nlibrary(vcd)\npairs(peng.table, shade = TRUE,\n      lower_panel_args = list(labeling = labeling_values()),\n      upper_panel_args = list(labeling = labeling_values()))\n\n\n\n\n\n\nFigure 3.36: Mosaic pairs plot for the combinations of species, sex and island. Diagnonal plots show the marginal frequency of each variable by the width of each rectangle. Off-diagonal mosaic plots subdivide by the conditional frequency of the second variable, shown numerically in the tiles. The tiles are shaded according to the departures from independence.\n\n\n\n\nThe shading patterns in cells (1,3) and (3,1) of Figure 3.36 show what we’ve seen before in the table of frequencies: The distribution of species varies across island because on each island one or more species did not occur. Row 2 and column 2 show that sex is nearly exactly proportional among species and islands, indicating independence, \\(\\text{sex} \\perp \\{\\text{species}, \\text{island}\\}\\). More importantly, mosaic pairs plots can show, at a glance, all (bivariate) associations among multivariate categorical variables.\nThe next step, by John Emerson and others (Emerson et al., 2013) was to recognize that combinations of continuous and discrete, categorical variables could be plotted in different ways.\n\nTwo continuous variables can be shown as a standard scatterplot of points and/or bivariate density contours, or simply by numeric summaries such as a correlation value;\nA pair of one continuous and one categorical variable can be shown as side-by-side boxplots or violin plots, histograms or density plots;\nTwo categorical variables could be shown in a mosaic plot or by grouped bar plots.\n\nIn the ggplot2 framework, these displays are implemented using the ggpairs() function from the GGally package (Schloerke et al., 2025). This allows different plot types to be shown in the lower and upper triangles and in the diagonal cells of the plot matrix. As well, aesthetics such as color and shape can be used within the plots to distinguish groups directly. As illustrated below, you can define custom functions to control exactly what is plotted in any panel.\nThe basic, default plot shows scatterplots for pairs of continuous variables in the lower triangle and the values of correlations in the upper triangle. A combination of a discrete and continuous variables is plotted as histograms in the lower triangle and boxplots in the upper triangle. Figure 3.37 includes sex to illustrate the combinations.\n\n\nCodeggpairs(peng, columns=c(3:6, 7),\n        aes(color=species, alpha=0.5),\n        progress = FALSE) +\n  theme_penguins() +\n  theme(axis.text.x = element_text(angle = -45))\n\n\n\n\n\n\n\n\n\n\nFigure 3.37: Basic ggpairs() plot of penguin size variables and sex, stratified by species.\n\n\n\n\nTo my eye, printing the values of correlations in the upper triangle is often a waste of graphic space. But in this example the correlations show something peculiar and interesting if you look closely: In all pairs among the penguin size measurements, there are positive correlations within each species, as we can see in Figure 3.31. Yet, in three of these panels, the overall correlation ignoring species is negative. For example, the overall correlation between bill depth and flipper length is \\(r = -0.579\\) in row 2, column 3; the scatterplot in the diagonally opposite cell, row 3, column 2 shows the data. These cases, of differing signs for an overall correlation, ignoring a group variable and the within group correlations are examples of Simpson’s Paradox, explored later in Section 6.3.2.\nThe last row and column, for sex in Figure 3.37, provides an initial glance at the issue of sex differences among penguin species that motivated the collection of these data. We can go further by also examining differences among species and island, but first we need to understand how to display exactly what we want for each pairwise plot.\nggpairs() is extremely general in that for each of the lower, upper and diag sections you can assign any of a large number of built-in functions (of the form ggally_NAME), or your own custom function for what is plotted, depending on the types of variables in each plot.\n\n\ncontinuous: both X and Y are continuous variables, supply this as the NAME part of a ggally_NAME() function or the name of a custom function;\n\ncombo: one X of and Y variable is discrete while the other is continuous, using the same convention;\n\ndiscrete: both X and Y are discrete variables.\n\nThe defaults, which were used in Figure 3.37, are:\n\nupper = list(continuous = \"cor\",          # correlation values\n             combo = \"box_no_facet\",      # boxplots \n             discrete = \"count\")          # rectangles ~ count\nlower = list(continuous = \"points\",       # just data points\n             combo = \"facethist\",         # faceted histograms\n             discrete = \"facetbar\")       # faceted bar plots\ndiag  = list(continuous = \"densityDiag\",  # density plots\n             discrete = \"barDiag\")        # bar plots\n\nThus, ggpairs() uses ggally_cor() to print the correlation values for pairs of continuous variables in the upper triangle, and uses ggally_points() to plot scatterplots of points in the lower portion. The diagonal panels as shown as density plots (ggally_densityDiag()) for continuous variables but as bar plots (ggally_barDiag()) for discrete factors.\nSee the vignette, ggally_plots for an illustrated list of available high-level plots. For our purpose here, which is to illustrate enhanced displays, note that for scatterplots of continuous variables, there are two functions which plot the points and also add a smoother, _lm or _loess.\n\nls(getNamespace(\"GGally\")) |&gt; \n  stringr::str_subset(\"^ggally_smooth_\")\n# [1] \"ggally_smooth_lm\"    \"ggally_smooth_loess\"\n\nA customized display for scatterplots of continuous variables can be any function that takes data and mapping arguments and returns a \"ggplot\" object. The mapping argument supplies the aesthetics, e.g., aes(color=species, alpha=0.5), but only if you wish to override what is supplied in the ggpairs() call.\nHere is a function, my_panel() that plots the data points, regression line and loess smooth:\n\nmy_panel &lt;- function(data, mapping, ...){\n  p &lt;- ggplot(data = data, mapping = mapping) + \n    geom_point() + \n    geom_smooth(method=lm, formula = y ~ x, se = FALSE, ...) +\n    geom_smooth(method=loess, formula = y ~ x, se = FALSE, ...)\n  p\n}\n\nFor this example, I want only simple summaries of for the scatterplots, so I don’t want to plot the data points, but do want to add the regression line and a data ellipse.\n\nmy_panel1 &lt;- function(data, mapping, ...){\n  p &lt;- ggplot(data = data, mapping = mapping) + \n     geom_smooth(method=lm, formula = y ~ x, se = FALSE, ...) +\n     stat_ellipse(geom = \"polygon\", level = 0.68, ...)\n  p\n}\n\nThen, to show what can be done, Figure 3.38 uses my_panel1() for the scatterplots in the 4 x 4 block of plots in the upper left. The combination of the continuous body size measures and the discrete factors species, island and sex are shown in upper triangle by boxplots but by faceted histograms in the lower portion. The factors are shown as rectangles with area proportional to count (poor-man’s mosaic plots) above the diagonal and as faceted bar plots below.\n\n\nCodeggpairs(peng, columns=c(3:6, 1, 2, 7),\n        mapping = aes(color=species, fill = species, alpha=0.2),\n        lower = list(continuous = my_panel1),\n        upper = list(continuous = my_panel1),\n        progress = FALSE) +\n  theme_penguins() +\n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank()) + \n  theme(axis.text.x = element_text(angle = -45))\n\n\n\n\n\n\n\n\n\n\nFigure 3.38: Customized ggpairs() plot of penguin size variables, together with species, island and sex.\n\n\n\n\nThere is certainly a lot going on in Figure 3.38, but it does show a high-level overview of all the variables (except year) in the penguins dataset. It is probably easiest to “read” this figure by focusing on the four blocks for the combinations of 4 continuous and 3 categorical measures. In the upper left block, visual thinning of the scatterplots, showing only the data ellipses and regression lines gives a simple view as it did in Figure 3.31.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-parcoord",
    "href": "04-multivariate_plots.html#sec-parcoord",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.10 Parallel coordinate plots",
    "text": "3.10 Parallel coordinate plots\nAs we have seen above, scatterplot matrices and generalized pairs plots extend data visualization to multivariate data, but these variables share one 2D space, so resolution decreases as the number of variable increase. You need a very large screen or sheet of paper to see more than, say 5-6 variables with any clarity.\nParallel coordinate plots are an attractive alternative, with which we can visualize an arbitrary number of variables to get a visual summary of a potentially high-dimensional dataset, and perhaps recognize outliers and clusters in the data in a different way. In these plots, each variable is shown on a separate, parallel axis. A multivariate observation is then plotted by connecting their respective values on each axis with lines across all the axes.\nThe geometry of parallel coordinates is interesting, because what is a point in \\(n\\)-dimensional (Euclidean) data space becomes a line in the projective parallel coordinate space with \\(n\\) axes, and vice-versa: lines in parallel coordinate space correspond to points in data space. Thus, a collection of points in data space map to lines that intersect in a point in projective space. What this does is to map \\(n\\)-dimensional relations into 2D patterns we can see in a parallel coordinates plot.\n\n\n\n\n\n\nHistory Corner: Who invented parallel coordinates?\n\n\n\n\nThose who don’t know history are doomed to plagiarize it —The author\n\nThe theory of projective geometry originated with the French mathematician Maurice d’Ocagne (1885) who sought a way to provide graphic calculation of mathematical functions with alignment diagrams or nomograms using parallel axes with different scales. A three-variable equation, for example, could be solved using three parallel axes, where known values could be marked on their scales, a line drawn between them, and an unknown read on its scale at the point where the line intersects that scale.\nHenry Gannet (1880), in work preceding the Statistical Atlas of the United States for the 1890 Census (Gannett, 1898), is widely credited with being the first to use parallel coordinates plots to show data, in his case, to show the rank ordering of US states by 10 measures including population, occupations, wealth, manufacturing, agriculture and so on.\nHowever, both d’Ocagne and Gannet were far preceded in this by Andre-Michel Guerry (1833) who used this method to show how the rank order of various crimes changed with age of the accused. See Friendly (2022), Figure 7 for his version and for an appreciation of the remarkable contributions of this amateur statistician to the history of data visualization.\n\nThe use of parallel coordinates for display of multidimensional data was rediscovered by Alfred Inselberg (1985) and extended by Edward Wegman (1990), neither of whom recognized the earlier history. Somewhat earlier, David Andrews (1972) proposed mapping multivariate observations to smooth Fourrier functions composed of alternating \\(\\sin()\\) and \\(\\cos()\\) terms. And in my book, SAS System for Statistical Graphics (Friendly, 1991), I implemented what I called profile plots without knowing their earlier history as parallel coordinate plots.\n\n\nParallel coordinate plots present a challenge for graphic developers, in that they require a different way to think about plot construction for multiple variables, which can be quantitative, as in the original idea, or categorical factors, all to be shown along parallel axes.\nHere, I use the ggpcp package (Hofmann et al., 2022), best described in VanderPlas et al. (2023), who also review the modern history.7 This takes some getting used to, because they develop pcp_*() extensions of the ggplot2 grammar of graphics framework to allow:\n\n\npcp_select(): selections of the variables to be plotted and their horizontal order on parallel axes,\n\npcp_scale(): methods for scaling of the variables to each axis,\n\npcp_arrange(): methods for breaking ties in factor variables to space them out.\n\nThen, it provides geom_pcp_*() functions to control the display of axes with appropriate aesthetics, labels for categorical factors and so forth. Figure 3.39 illustrates this type of display, using sex and species in addition to the quantitative variables for the penguin data.\n\n\nCodepeng |&gt;\n  pcp_select(bill_length:body_mass, sex, species) |&gt;\n  pcp_scale(method = \"uniminmax\") |&gt;\n  pcp_arrange() |&gt;\n  ggplot(aes_pcp()) +\n  geom_pcp_axes() +\n  geom_pcp(aes(colour = species), alpha = 0.8, overplot = \"none\") +\n  geom_pcp_labels() +\n  scale_colour_manual(values = peng.colors()) +\n  labs(x = \"\", y = \"\") +\n  theme(axis.title.y = element_blank(), axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(), legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nFigure 3.39: Parallel coordinates plot of penguin size variables, together with sex and species.\n\n\n\n\nRearranging the order of variables and the ordering of factor levels can make a difference in what we can see in such plots. For a simple example (following VanderPlas et al. (2023)), we reorder the levels of species and islands to make it clearer which species occur on each island.\n\nCodepeng1 &lt;- peng |&gt;\n  mutate(species = factor(species, levels = c(\"Chinstrap\", \"Adelie\", \"Gentoo\"))) |&gt;\n  mutate(island = factor(island, levels = c(\"Dream\", \"Torgersen\", \"Biscoe\")))\n\npeng1 |&gt;\n  pcp_select(species, island, bill_length:body_mass) |&gt;\n  pcp_scale() |&gt;\n  pcp_arrange(method = \"from-left\") |&gt;\n  ggplot(aes_pcp()) +\n  geom_pcp_axes() +\n  geom_pcp(aes(colour = species), alpha = 0.6, overplot = \"none\") +\n  geom_pcp_boxes(fill = \"white\", alpha = 0.5) +\n  geom_pcp_labels() +\n  scale_colour_manual(values = peng.colors()[c(2,1,3)]) +\n  theme_bw() +\n  labs(x = \"\", y = \"\") +\n  theme(axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(),\n        legend.position = \"none\") \n\n\n\n\n\n\n\n\n\n\nFigure 3.40: Parallel coordinates plot of penguin size variables, with the levels of species and island reordered.\n\n\n\n\nThe order of variables in this plot emphasizes the relation between penguin species and the island where they were observed and then shows the values of the quantitative body size measurements.\nMore generally, quantitative variables can, and probably should, be ordered to place the most highly correlated variables adjacently to minimize the degree of crossing lines from one variable to the next (Martí & Laguna, 2003). When variables are highly negatively correlated (such as bill_depth and flipper_length here), crossings can be reduced simply by reversing the scale of one of the variables, e.g., by plotting -bill_depth.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-animated-tours",
    "href": "04-multivariate_plots.html#sec-animated-tours",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.11 Animated tours",
    "text": "3.11 Animated tours\nIn the mid 17\\(^{th}\\) to early 19\\(^{th}\\)-century the Grand Tour became a coming-of-age custom for young Europeans (mainly British nobility and landed gentry) of sufficient rank and means to undertake a journey to the principal sites of Europe (Paris, Geneva, Rome, Athens, …) to complete their education by learning something of the cultural legacies in history, art, and music from antiquity to the Renaissance. Thereby, they could gain a wider appreciation of history and be prepared to play a role in polite society or in their chosen endeavors.\nTravels in high-dimensional data space might be less thrilling than a journey from London through Paris and Millan to Rome. Yet, in both cases it is useful to think of the path taken, and what might be seen along the way. But there are different kinds of tours. We might simply take a meandering tour, exploring all the way, or want to plan a tour to see the most interesting sites in travel or have a tour guided by an expert. Similarly in data space, we might travel randomly to see what we can find or be guided to find interesting features such as clusters, outliers or non-linear relations in data. \nFollowing the demonstration in PRIM-9 (Section 5) of exploring multidimensional data space by rotation Asimov (1985) developed the idea of the grand tour, a computer method for viewing multivariate statistical data via orthogonal projections onto an animated sequence of low-dimensional subspaces, like a movie. In contrast to a scatterplot matrix which shows a static view of a data cloud projected onto all pairwise variable axes, a statistical tour is like the view of an eye moving smoothly in high-dimensional space, capturing what it sees from a given location onto the 2-d plane of the computer screen.\nMore generally, statistical tours are a type of dynamic projections onto orthogonal axes (called a basis) that embed data in a smaller \\(p\\)-dimensional space into a \\(d\\)-dimensional viewing subspace with \\(d &lt; p\\). Typically, \\(d=2\\), where the result can displayed as scatterplots, together with vectors representing the projections of the data variables in this space. But the projected data can be rendered in 1D as densities or histograms, or in more dimensions (\\(d &gt; 2\\)) as glyphs, or even as parallel coordinate plots. The essential idea is that we can define, and animate, a tour path as a smooth sequence of such projections over small changes to the projection basis, which gives the orientation of the data in the viewing space.\n\n3.11.1 Projections\nThe idea of a projection is fundamental to touring methods and other visualizations of high-D data, so it is useful to understand what a projection is. Quite simply, you can think of a projection as the shadow of an object or cloud of points.\nThis is nicely illustrated by the cover image (Figure 3.41) used for Douglas Hofstadter’s (1979) Gödel, Bach and Escher which shows two 3D solid shapes illuminated by light sources so their shadows form the letters G, B and E projected onto the planes formed by pairs of the three coordinate axes. The set of three 2D views is essentially the same that we see in a scatterplot matrix, where a 3D dataset is portrayed by the set of shadows of the points on planes formed by pairs of coordinate axes.\n\n\n\n\n\n\n\nFigure 3.41: The cover image from Hofstadter (1979) illustrates how projections are shadows of an object cast by a light from a given direction.\n\n\n\n\nIn the simplest case, a data point \\(\\mathbf{x} = (x_1, x_2)\\) in two dimensions can be represented geometrically as a vector from the origin as shown in Figure 3.42. This point can be projected on any one-dimensional axis \\(\\mathbf{p}\\) by dropping a line perpendicular to \\(\\mathbf{p}\\), which is the idea of a shadow. Mathematically, this is calculated as the product \\(\\mathbf{x}^\\mathsf{T} \\mathbf{p} = x_1 p_1 + x_2 p_2\\) and suitably normalized to give the correct length. …\n\n\n\n\n\n\n\nFigure 3.42: Projection of a point x onto a direction or axis p.\n\n\n\n\nMore generally, a projection of an \\((n \\times p)\\) data matrix \\(\\mathbf{X}\\) representing \\(n\\) observations in \\(p\\) dimensions onto a \\(d\\)-dimensional viewing space \\(\\mathbf{Y}_{n \\times d}\\) is represented by a \\(p \\times d\\) projection matrix \\(\\mathbf{P}\\) as \\(\\mathbf{Y} = \\mathbf{X} \\mathbf{P}\\), where the columns of \\(\\mathbf{P}\\) are orthogonal and of unit length,i.e., \\(\\mathbf{P}^\\mathsf{T} \\mathbf{P} = \\mathbf{I}_{(d \\times d)}\\).\nFor example, to project a data matrix \\(\\mathbf{X}\\) in three dimensions onto a 2D plane, we would multiply it by a \\((3 \\times 2)\\) orthonormal matrix \\(\\mathbf{P}\\). The matrix \\(\\mathbf{P}_1\\) below simply selects the first two columns of \\(\\mathbf{X}\\).8\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n    0 & 0 & 0 \\\\\n    0 & 0 & 10 \\\\\n    0 & 10 & 0 \\\\\n    0 & 10 & 10 \\\\\n    10 & 0 & 0 \\\\\n    10 & 0 & 10 \\\\\n    10 & 10 & 0 \\\\\n    10 & 10 & 10 \\\\\n\\end{bmatrix}_{8 \\times 3}\n;\\;\n\\mathbf{P_1} =\n\\begin{bmatrix}\n    1 & 0 \\\\\n    0 & 1 \\\\\n    0 & 0 \\\\\n\\end{bmatrix}_{3 \\times 2}\n\\;\\Rightarrow\\quad\n\\mathbf{Y} = \\mathbf{X} \\; \\mathbf{P_1} =\n\\begin{bmatrix}\n    0 & 0 \\\\\n    0 & 0 \\\\\n    0 & 10 \\\\\n    0 & 10 \\\\\n    10 & 0 \\\\\n    10 & 0 \\\\\n    10 & 10 \\\\\n    10 & 10 \\\\\n\\end{bmatrix}_{8 \\times 2}\n\\] An oblique projection using all three dimensions is given by \\(\\mathbf{P_2}\\) below. This produces a new 2D view in \\(\\mathbf{Y}\\):\n\\[\n\\mathbf{P_2} =\n\\begin{bmatrix}\n    0.71 & -0.42 \\\\\n    0.71 & 0.42 \\\\\n    0 & 0.84 \\\\\n\\end{bmatrix}_{3 \\times 2}\n\\quad\\Rightarrow\\quad\n\\mathbf{Y} = \\mathbf{X} \\; \\mathbf{P_2} =\n\\begin{bmatrix}\n    0 & 0 \\\\\n    0 & 8.4 \\\\\n    7.1 & 4.2 \\\\\n    7.1 & 12.6 \\\\\n    7.1 & -4.2 \\\\\n    7.1 & 4.2 \\\\\n    14.2 & 0 \\\\\n    14.2 & 8.4 \\\\\n\\end{bmatrix}\n\\]\nThe columns in \\(\\mathbf{Y}\\) are simply the linear combinations of those of \\(\\mathbf{X}\\) using the weights in each column of \\(\\mathbf{P_2}\\)\n\\[\\begin{aligned}\n\\mathbf{y}_1 & = & 0.71 \\mathbf{x}_1 + 0.71 \\mathbf{x}_2 + 0 \\mathbf{x}_3\\\\\n\\mathbf{y}_2 & = & -0.42 \\mathbf{x}_1 + 0.42 \\mathbf{x}_2 + 0.84 \\mathbf{x}_3 \\\\\n\\end{aligned}\\]\n\nCodevals &lt;- c(0, 10)\nX &lt;- expand.grid(x1 = vals, x2=vals, x3=vals) |&gt; as.matrix()\n\n# project on just x1, x2 plane\nP1 &lt;- rbind(diag(2), c(0,0))\nY1 &lt;- X %*% P1\n\n# oblique projection\nP2 &lt;- matrix(c(0.71, 0.71, 0, -0.42, .42, 0.84), ncol=2)\nY2 &lt;- X %*% P2\n\n\n\nIn this example, the matrix \\(\\mathbf{X}\\) consists of 8 points at the vertices of a cube of size 10, as shown in Figure 3.43 (a). The projections \\(\\mathbf{Y}_1 = \\mathbf{P}_1 \\mathbf{X}\\) and \\(\\mathbf{Y}_2 = \\mathbf{P}_2 \\mathbf{X}\\) are shown in panels (b) and (c). To make it easier to relate the points in different views, shapes and colors are assigned so that each point has a unique combination of these attributes.\n\n\npch &lt;- rep(15:18, times = 2)\ncolors &lt;- c(\"red\", \"blue\", \"darkgreen\", \"brown\")\ncol &lt;- rep(colors, each = 2)\ndata.frame(X, pch, col)\n#   x1 x2 x3 pch       col\n# 1  0  0  0  15       red\n# 2 10  0  0  16       red\n# 3  0 10  0  17      blue\n# 4 10 10  0  18      blue\n# 5  0  0 10  15 darkgreen\n# 6 10  0 10  16 darkgreen\n# 7  0 10 10  17     brown\n# 8 10 10 10  18     brown\n\n\n\n\n\n\n\n\nFigure 3.43: Projection example: (a) The 8 points in X form a cube of size 10; (b) the projection by P1 is the view ignoring x3 (two points coincide at each vertex); (c) the projection by P2 is an oblique view.\n\n\n\n\nBut, if we are traveling in the projection space of \\(\\mathbf{Y}\\), we need some signposts to tell us how the new dimensions relate to those of \\(\\mathbf{X}\\). The answer is provided simply by plotting the rows of \\(\\mathbf{P}\\) as vectors, as shown in Figure 3.44. In these plots, each row of \\(\\mathbf{P}_1\\) and \\(\\mathbf{P}_2\\) appears as a vector from the origin. It’s direction shows the contribution each of \\(\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\) make to the new coordinates \\(\\mathbf{y}_1\\) and \\(\\mathbf{y}_2\\).\nShow the codelibrary(matlib)     # circle(), vectors()\nxlim &lt;- ylim &lt;- c(-1.1, 1.1)\naxes.x &lt;- c(-1, 1, NA, 0, 0)\naxes.y &lt;- c(0, 0, NA, -1, 1)\nlabs &lt;- c(expression(x[1]), expression(x[2]), expression(x[3]))\n\nplot(xlim, ylim, type = \"n\", asp=1,\n     xlab = expression(y[1]), ylab = expression(y[2]),\n     cex.lab = 1.8)\ncircle(0, 0, 1, col = adjustcolor(\"skyblue\", alpha = 0.2))\nlines(axes.x, axes.y, col = \"grey\")\nvectors(P1, labels = labs, cex.lab = 1.8, lwd = 3, pos.lab = c(4, 2, 1))\n\nplot(xlim, ylim, type = \"n\", asp=1,\n     xlab = expression(y[1]), ylab = expression(y[2]),\n     cex.lab = 1.8)\ncircle(0, 0, 1, col = adjustcolor(\"skyblue\", alpha = 0.2))\nlines(axes.x, axes.y, col = \"grey\")\nvectors(P2, labels = labs, cex.lab = 1.8, lwd = 3)\n\n\n\n\n\n\n\n\n\n\nFigure 3.44: Variable vectors: Data variables viewed as vectors in the space of their projections. The angles of the x vectors with respect to the y coordinate axes show their relative contributions to each. The lengths of the x vectors show the relative degree to which they are represented in the space of ys. Left: the P1 projection; right: the P2 projection.\n\n\n\n\nIn \\(\\mathbf{P}_1\\), the projected variable \\(\\mathbf{y}_1\\) is related only to \\(\\mathbf{x}_1\\), while \\(\\mathbf{y}_2\\) is related only to \\(\\mathbf{x}_2\\). \\(\\mathbf{x}_3\\) makes no contribution, and appears at the origin. However in the projection given by \\(\\mathbf{P}_2\\), \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) make the same contribution to \\(\\mathbf{y}_1\\), while \\(\\mathbf{x}_3\\) has no contribution to that horizontal axis. The vertical axis, \\(\\mathbf{y}_2\\) here is completely aligned with \\(\\mathbf{x}_3\\); \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) have vertical components that are half of that for \\(\\mathbf{x}_3\\) in absolute value.\n` #### Vector lengths In Figure 3.44, the lengths of the \\(\\mathbf{x}\\) vectors reflect the relative degree to which each variable is represented in the space of the projection, and this is important for interpretation. For the \\(\\mathbf{P}_1\\) projection, \\(\\mathbf{x}_3\\) is of length 0, while \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) fill the unit circle. In the projection given by \\(\\mathbf{P}_2\\), all three \\(\\mathbf{x}\\) are approximately the same length.\nIn algebra, the length of a vector \\(\\mathbf{x}\\) is \\(||\\mathbf{x}|| = (\\mathbf{x}^\\mathsf{T} \\mathbf{x})^{1/2} = \\sqrt{\\Sigma x_i^2}\\), the Euclidean distance of the tip of the vector from the origin. In R, we calculate the lengths of row vectors in a projection matrix by transposing and using matlib::len().\n\nP1 |&gt; t() |&gt; matlib::len()\n# [1] 1 1 0\nP2 |&gt; t() |&gt; matlib::len()\n# [1] 0.825 0.825 0.840\n\n\n3.11.1.1 Joint-views\nTo interpret such projections, we want to see both the projected data and the signposts that tell us where we are in relation to the original variables. To do this, we can overlay the variable vectors represented by the rows of the projection matrix \\(\\mathbf{P}\\) onto plots like Figure 3.43 (b) and Figure 3.43 (c) to see how the axes in a projection relate to those in the data. To place these together on the same plot, we can either center the columns of \\(\\mathbf{Y}\\) at their means or shift the the columns of \\(\\mathbf{P}\\) to colMeans(Y). It is only the directions of the vectors that matters, so we are free to scale their lengths by any convenient factor.\n\nCodeY2s &lt;- scale(Y2, scale=FALSE)       # center Y2\nplot(Y2s, cex = 3, \n     asp = 1,\n     pch = pch, col = col,\n     xlab = expression(y[1]), ylab = expression(y[2]),\n     xlim = c(-10, 10), ylim = c(-10, 10), cex.lab = 1.8)\nr &lt;- 7\nvecs &lt;- (r*diag(3) %*% P2)\nvectors(vecs, labels = labs, cex.lab = 1.8, lwd = 2)\nvectors(-vecs, labels = NULL, lty = 1, angle = 1, col = \"gray\")\n\n\nThe plot in Figure 3.45 illustrates this, centering \\(\\mathbf{Y}\\), and multiplying the vectors in \\(\\mathbf{P}\\) by 7. To check your understanding, try to see if you can relate what is shown in this plot to the 3D plot in Figure 3.43 (a).\n\n\n\n\n\n\n\nFigure 3.45: The P2 projection of the data showing vectors for the original variables in the space of Y.\n\n\n\n\nThe idea of viewing low-dimensional projections of data together with vectors representing the contributions of the original variables to the dimensions shown in a display is also the basis of biplot techniques (Section 4.3) we will use in relation to principal components analysis.\n\n3.11.2 Touring methods\nThe trick of statistical touring methods is to generate a smooth sequence of interpolated projections \\(\\mathbf{P}_{(t)}\\) indexed by time \\(t\\), \\(\\mathbf{P}_{(1)}, \\mathbf{P}_{(2)}, \\mathbf{P}_{(3)}, \\dots, \\mathbf{P}_{(T)}\\). This gives a path of views \\(\\mathbf{Y}_{(t)} = \\mathbf{X} \\mathbf{P}_{(t)}\\), that can be animated in successive frames, as shown schematically in Figure 3.46.\n\n\n\n\n\n\n\nFigure 3.46: Interpolations: Illustration of a grand tour of interpolations of projection planes showing 2D scatterplots of the Penguin dataset. The seqeunce of views moves smoothly from an initial frame P(1) to a final frame P(T) where the penguin species are widely separated.\n\n\n\n\nAsimov’s (1985) original idea of the grand tour was that of a random path, picking orthogonal projections \\(\\mathbf{P}_{(i)}\\) at random. Given enough time, the grand tour gives a space-filling path and would eventually show every possible projection of the data. But it does so smoothly, by interpolating from one projection to the next. In the travel analogy, the path by road from London to Paris might go smoothly through Kent to Dover, thence via Amiens and Beauvais before reaching Paris. By air, the tour would follow a smoother geodesic path, and this is what the grand tour does. The sense in watching an animation of a statistical grand tour is that of continuous motion. The grand tour algorithm is described in detail by Buja et al. (2005) and Cook et al. (2008). \n\n\n3.11.2.1 Guided tours\nThe next big idea was that rather than traveling randomly in projection space one could take a guided tour, following a path that leads to “interesting projections”, such as those that reveal clusters, gaps in data space or outliers. This idea, called projection pursuit (Cook et al., 1995), works by defining a measure of interestingness of a data projection. In a guided tour, the next projection is chosen to increase that index, so over time the projection moves toward one that is maximizes that index.\nIn the time since Asimov (1985), there have been many implementations of touring visualization methods. XGobi (Swayne et al., 1998) for X-Windows displays on Linux systems provided a test-bed for dynamic, interactive graphic methods; it’s successor, GGobi (Cook & Swayne, 2007; Swayne et al., 2003) extended the range of touring methods to include a wider variety of projection pursuit indices.\n\n3.11.2.2 tourr package\nThe current state of art is best captured in the tourr package for R (Wickham et al., 2011; Wickham & Cook, 2025). It defines a tour to consist of three components:\n\n\ndata: An \\((n \\times p)\\) numerical data matrix to be viewed.\n\npath: A tour path function that produces a smoothed sequence of projection matrices \\(\\mathbf{P}_{(p \\times d)}\\) in \\(d\\). dimensions, for example grand_tour(d = 2) or guided_tour(index = holes).\n\ndisplay: A function that renders the projected data, for example display_xy() for a scatterplot, display_depth() for a 3D plot with simulated depth, or display_pcp() for a parallel coordinates plots\n\nThis very nicely separates the aspects of a tour, and allows one to think of and define new tour path methods and display methods. The package defines two general tour functions: animate() produces a real-time animation on a display device and render() saves image frames to disk, such as a .gif file.\n\nanimate(data, tour_path, display_method)\nrender(data, tour_path, display_method)\n\nThe tourr package provides a wide range of tour path methods and display methods:\n\n# tour path methods\ngrep(\"_tour$\", lsf.str(\"package:tourr\"), value = TRUE)\n#  [1] \"dependence_tour\"     \"frozen_guided_tour\" \n#  [3] \"frozen_tour\"         \"grand_tour\"         \n#  [5] \"guided_anomaly_tour\" \"guided_section_tour\"\n#  [7] \"guided_tour\"         \"little_tour\"        \n#  [9] \"local_tour\"          \"new_tour\"           \n# [11] \"planned_tour\"        \"planned2_tour\"      \n# [13] \"radial_tour\"\n\n# display methods\ngrep(\"display_\", lsf.str(\"package:tourr\"), value = TRUE)\n#  [1] \"display_andrews\"   \"display_density2d\" \"display_depth\"    \n#  [4] \"display_dist\"      \"display_faces\"     \"display_groupxy\"  \n#  [7] \"display_idx\"       \"display_image\"     \"display_pca\"      \n# [10] \"display_pcp\"       \"display_sage\"      \"display_scatmat\"  \n# [13] \"display_slice\"     \"display_stars\"     \"display_stereo\"   \n# [16] \"display_trails\"    \"display_xy\"\n\nTour path methods take a variety of optional arguments to specify the detailed behavior of the method. For example, most allow you to specify the number of dimension (d =) of the projections. The guided_tour() is of particular interest here.\n\nargs(guided_tour)\n# function (index_f, d = 2, cooling = 0.99, max.tries = 25, max.i = Inf, \n#     search_f = search_geodesic, n_jellies = 30, n_sample = 100, \n#     alpha = 0.5, ...) \n# NULL\n\nIn this, index_f specifies a function that the method tries to optimize on its path and package defines four indices:\n\nHoles (holes()): This is sensitive to projections with separated clusters of points, with few points near the origin\nCentral mass (cmass()): Sensitive to projections with lots of points in the center, but perhaps with some outliers\nLinear discriminant analysis (lda_pp()): For data with a grouping factor, optimizes a measure of separation of the group means as in MANOVA or linear discriminant analysis.\nPDA analysis (pda_pp()): A penalized version of lda_pp() for cases of large \\(p\\) relative to sample size \\(n\\) (E.-K. Lee & Cook, 2009).\n\nIn addition, there is now a guided_anomaly_tour() that looks for the best projection of observations that are outside the data ellipsoid, finding a view showing observations with large Mahalanobis distances from the centroid.\nPenguin tours\nPenguins are a traveling species. They make yearly travels inland to breeding sites in early spring, repeating the patterns of their ancestors. Near the beginning of summer, adult penguins and their chicks return to the sea and spend the rest of the summer feeding there, but they have different strategies to survive the polar winter (Black et al., 2018). The data scientists among them might wonder about the relations among among their cousins of different species on various islands and take a data-centric tour of their 4D measurements of body size with trusty R tools.\n\nFor example, using the Penguins dataset, the following calls produce grand tours in 2, 3, and 4 dimensions. The 2D tour is displayed as a scatterplot, the 3D tour using simulated depth as shown by variation in point size and transparency, and the 4D tour is shown using a parallel coordinate plot.\n\n\ndata(peng, package = \"heplots\")\npeng_scaled &lt;- scale(peng[,3:6])\ncolnames(peng_scaled) &lt;- c(\"BL\", \"BD\", \"FL\", \"BM\")\n\nanimate(peng_scaled, grand_tour(d = 2), display_xy())\nanimate(peng_scaled, grand_tour(d = 3), display_depth())\nanimate(peng_scaled, grand_tour(d = 4), display_pcp())\n\n\n\n\n\n\n\n\nFigure 3.47: Snapshops of grand tours of the penguin dataset in 2, 3, and 4 dimensions using different display_*() methods.\n\n\n\n\n\nTo illustrate, I’ll start with a grand tour designed to explore this 4D space of penguins. I’ll abbreviate the variables to two characters, “BL” = bill_length, “BD” = bill_depth, “FL” = flipper_length, and “BM” = body_mass and identify the penguin species using point shape (pch) and color (col).\nAs you watch this pay attention to the separation of the species and any other interesting features. What do you see?\n\ndata(peng, package = \"heplots\")\npeng_scaled &lt;- scale(peng[,3:6])\ncolnames(peng_scaled) &lt;- c(\"BL\", \"BD\", \"FL\", \"BM\")\n\npch &lt;- c(15, 16, 17)[peng$species] \ncex = 1.2\n\nset.seed(1234)\nanimate(peng_scaled,\n        tour_path = grand_tour(d=2),\n        display_xy(col = peng$species,\n                   palette = peng.colors(\"dark\"),\n                   pch = pch, cex = cex,\n                   axis.col = \"black\", \n                   axis.text.col = \"black\", \n                   axis.lwd = 1.5))\n\n\n\n\n\n\n\n\nFigure 3.48: Animation of a grand tour of the Penguin data.\n\n\n\n\nFigure 3.49 shows three frames from this movie. The first (a) is the initial frame that shows the projection in the plane of bill depth and bill length. The variable vectors indicate that bill length differentiates Adélie penguins from the others. In frame (b), the three species are widely separated, with bill depth distinguishing Gentoo from the others. In frame (c) the three species are largely mixed, but two points stand out as outliers, with exceptionally long bills compared to the rest.\n\n\n\n\n\n\n\n\n\n(a) Initial frame\n\n\n\n\n\n\n\n\n\n(b) Clusters\n\n\n\n\n\n\n\n\n\n(c) Outliers\n\n\n\n\n\n\nFigure 3.49: Three frames from the grand tour of the Penguin data. (a) The initial frame is the projection showing only BD and BL, where bill length conveniently separates Adélie from the other two species. (b) A frame that shows the three species more widely separated. (c) A frame that shows two outliers with very large bills.\n\n\n\n\n\n\n\n\n\n\nLet’s take the penguins on a guided tour, trying to find views that show the greatest separations among their species; that is, a guided tour, optimizing the lda_pp() index of group separation.\n\nset.seed(1234)\nanimate(peng_scaled, \n        guided_tour(lda_pp(peng$species)),\n        display_xy(col = peng$species,\n                   palette = peng.colors(\"dark\"),\n                   pch = pch,\n                   cex = cex)\n)\n\n\n\n\n\n\n\n\nFigure 3.50: Animation of a guided tour of the Penguin data, using a tour criterion designed to find an optimal separation among the penguin species. The animation shows three loops of the sequence of projections and stops when the LDA criterion cannot be improved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Optimizing lda_pp()\n\n\n\n\n\n\n\n\n\n(b) Optimizing anomaly_index()\n\n\n\n\n\n\nFigure 3.51: Guided tours: These figures show the final frame in the animations of guided tours designed to find the projection that optimize an index. (a) The lda_pp() criterion optimizes the separation of the means for species relative to within-group variation. (b) The anomalies_index() optimizes the average Mahalanobis distance of points from the centroid\n\n\nThese examples are intended to highlight what is possible with dynamic graphics for exploring high-dimensional data visually. Cook & Laa (2024) extend the discussion of these methods from Cook & Swayne (2007) (which used Ggobi) to the tourr package. They illustrate dimension reduction, various cluster analysis methods, trees and random forests and some machine-learning techniques.\n\nIdeally, we should be able interact with a tour,\n\npausing when we see something interesting and saving the view for later analysis;\nselecting or highlighting unusual points,\nchanging tour methods or variables displayed on the fly, and so forth.\n\nSome packages that provide these capabilities are: detourr(Hart & Wang, 2022) langevitour(Harrison, 2023a, 2023b) and liminal (S. Lee, 2025)  The loon package (Waddell & Oldford, 2025) is a general toolkit that enables highly interactive data visualization. It provides a loon.tour package (Xu & Oldford, 2021) for using touring methods within the loon environment.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#sec-network",
    "href": "04-multivariate_plots.html#sec-network",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.12 Network diagrams",
    "text": "3.12 Network diagrams\nA major theme throughout this chapter has been to understand how to extend data visualization from simple bivariate scatterplots to increasingly more complex situations with larger datasets. With a moderate number of variables, techniques such as smoothing, summarizing with data ellipses and fitted curves, and visual thinning can be used to tame “big \\(N\\)” datasets with thousands of cases.\nHowever “big \\(p\\)” datasets, with more than a moderate number (\\(p\\)) of variables still remain a challenge. It is hard to see how the more advanced methods (corrgrams, parallel coordinate) described earlier could cope with \\(p = 20, 50, 100, 500, \\dots\\) variables. At some point, each of these begins to break down for the purpose of visualizing associations among many variables. We are forced to thin the information presented in graphs more and more as the number of variables increases.\nIt turns out that there is a way to increase the number of variables displayed dramatically, if we are mainly interested in the pairwise correlations for reasonably normally distributed data. A graphical network diagram portrays variables by nodes (vertices), connected by (weighted) edges whose properties reflect the strength of connections between pairs, such as a correlation. Such diagrams can reveal properties not readily seen by other means.\nAs an example consider Figure 3.52, which portrays the correlations among 25 self-report items reflecting 5 factors (the “Big Five”) considered in personality psychology to represent the dominant aspects of all of personality. These factors are easily remembered by the acronum OCEAN: Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism. The dataset, bfi, contains data from an online sample of \\(n=2800\\) with 5 items for each scale.\nIn this figure (taken from Rodrigues (2021)), the item nodes are labeled according to the OCEAN factor they are assumed to measure. For 25 items, there are \\(25 \\times 24 / 2 = 300\\) correlations, way too much to see. A clearer picture arises when we reduce the number of edges shown according to some criterion. Here, edges are drawn only between nodes where the correlation is considered important by a method (“glasso” = graphical LASSO) designed to make the graph optimally sparse.\n\n\n\n\n\n\n\n\nFigure 3.52: Network diagram of the correlations among 25 items from a Big-Five personality scale, 5 items for each scale. The magnitude of a correlation is shown by the thickness and transparency of the edge between two item nodes. The sign of a correlation is shown by edge color and style: solid blue for positive and dashed red for negative. Source: Rodrigues (2021)\n\n\n\n\nThe edges shown in Figure 3.52 reflect the Pearson correlation between a given pair of items by the visual attributes of color and line style: magnitude is shown by both the thickness and transparency of the edge; the sign of the correlation is shown by color and line type: solid blue for positive correlations and dashed red for negative ones.\nAccording to some theories, the five personality factors should be largely non-overlapping, so there should not be many edges connecting items of one factor with those of another. Yet, there are quite a few cross-factor connections in Figure 3.52, so perhaps the theory is wrong, or, more likely, the 25 items are not good representatives of these underlying dimensions. The network diagram shown here is a visual tool for thought and refinement. See Costantini et al. (2015) for a tutorial on network analysis of personality data in R.\nNetwork diagrams stem from mathematical graph theory (Bondy & Murty, 2008; West, 2001) of the abstract properties of nodes and edges used to represent pairwise relationships. These can be used to model many types of relations and processes in physical, biological, social and other sciences, where such properties as connectedness, centrality, cliques of connected nodes and so forth provide a vocabulary used to understand and explain complex systems.\nFor one example, Grandjean (2016) used network analysis to study the connections among 2500 Twitter users (nodes) who identified as belonging to a “digital humanities” community from the relations (edges) of who follows whom. Grandjean also used these methods to study the relationships among characters in Shakespeare’s tragedies in terms of the characters (nodes) and edges representing how often they appeared in the same scene.\nThe wide applicability of these ideas has led to what is now called network science (Barab’asi, 2016) encompassing computer networks, biological networks, cognitive and semantic networks, and social networks. Recent developments in psychology led to a framework of network psychometrics (Isvoranu et al., 2022), where, for example, symptoms of psychopathology (phobias, anxiety, substance abuse) can be conceptualized as an interconnected network of clusters and studied for possible causal relations (Robinaugh et al., 2019).\nBecause a network diagram can potentially reflect hundreds of variables, various graph layout algorithms have been developed to automatically position the nodes so as to generate aesthetically pleasing network visualizations that emphasize important structural properties, like clusters and central nodes, while minimizing visual clutter (many crossing lines) to promote understandability and usability.\nThere are quite a few R packages for constructing network diagrams, both static and dynamic / interactive, and these differ considerably in how the information required for a graph is structured as R objects, and the flexibility to produce attractive graphs. Among these, igraph (Csárdi et al., 2024) structures the data as a dataset of vertices and edges with properties\n-&gt; packages: qgraph, …\n\n3.12.1 Crime data\nFor the present purposes, let’s see what network diagrams can tell us about the crime data analyzed earlier. Here, I first reorder the variables as in Figure 3.35. In the call to qgraph(), the argument minimum = \"sig\" says to show only the edges for significant correlations (at \\(\\alpha = 0.01\\) here). In Figure 3.53, the variable nodes are positioned around a circle (layout = \"circle\"), which is the default.\n\n\nlibrary(qgraph)\nord &lt;- corrMatOrder(crime.cor, order = \"AOE\")\nrownames(crime.cor)[ord]\n# [1] \"murder\"   \"assault\"  \"rape\"     \"robbery\"  \"burglary\"\n# [6] \"larceny\"  \"auto\"\ncrime.cor &lt;- crime.cor[ord, ord]\n\n# \"association graph\": network of correlations\nqgraph(crime.cor, \n  title = \"Crime data:\\ncorrelations\", title.cex = 1.5,\n  graph = \"cor\",\n  layout = \"circle\",\n  minimum = \"sig\", sampleSize = nrow(crime), alpha = 0.01,\n  color = grey(.9), vsize = 12,\n  labels = rownames(crime.cor),\n  posCol = \"blue\")\n\n\n\n\n\n\nFigure 3.53: Network diagram depicting the correlations among the crime variables. Only edges for correlations that are significant at the \\(\\alpha = 0.01\\) level are displayed.\n\n\n\n\n\nIn this figure, you can see the group of property crimes (auto theft, larceny, burglary) at the left separated from the violent crimes against persons at the right.\n\n3.12.2 Partial correlations\nAmong the more important statistical applications of network graph theory is the idea that you can also use them to study the the partial (conditional) associations among variables with the contributions of all other variables removed in what are called Graphical Gaussian Models (GGMs) (Højsgaard et al., 2012; Lauritzen, 1996). In a network diagram of these partial associations, \n\nThe edges between nodes represent the partial correlations between those variables.\nThe absence of an edge between two nodes indicates their variables are conditionally independent, given the other variables.\n\nSo, whereas a network diagram of correlations shows marginal associations ignoring other variables, one of partial correlations allows you to visualize the direct relationship between each pair of variables, removing the indirect effects that might be mediated through all other variables.\nFor a set of variables \\(X = \\{x_1, x_2, \\dots, x_p \\}\\), the partial correlation between \\(x_i\\) and \\(x_i\\), controlling for all other variables \\(Z = X \\setminus \\{x_i, x_j\\} = x_\\text{others}\\) is equivalent to the correlation between the residuals of the linear regressions of \\(x_i\\) on all other \\(\\mathbf{Z}\\) and \\(x_j\\) on \\(\\mathbf{Z}\\). (The notation \\(X \\setminus \\{x_i, x_j\\}\\) is read as “\\(X\\) without the set \\(\\{x_i, x_j\\}\\)”).\nMathematically, let \\(\\hat{x}_i\\) and \\(\\hat{x}_j\\) be the predicted values from the linear regressions of \\(x_i\\) on \\(\\mathbf{Z}\\) and of \\(x_j\\) on \\(\\mathbf{Z}\\), respectively. The partial correlation \\(p_{ij}\\) between \\(x_i\\) and \\(x_j\\) controlling for \\(\\mathbf{Z}\\) is given by:\n\\[\np_{x_i,x_j|\\mathbf{Z}} = r( x_i, x_j \\mid \\text{others}) = \\text{cor}[ (x_i - \\hat{x}_i),\\; (x_j - \\hat{x}_j)]\n\\tag{3.3}\\]\nBut, rather than running all these linear regressions, they can all be computed from the inverse of the correlation matrix (Whittaker, 1990, Ch. 5), a relation first noted by Dempster (1972). Let \\(\\mathbf{R}\\) be the correlation matrix of the variables. Then, the matrix \\(\\mathbf{P}\\) of partial correlations can be obtained from the negative inverse, \\(-\\mathbf{R}^{-1}\\), standardized to a correlation matrix by dividing by the square root of product of its diagonal elements,\n\\[\nP_{ij} = - \\frac{R^{-1}_{ij}}{\\sqrt{(R^{-1}_{ii} \\cdot R^{-1}_{jj})}} \\:\\: .\n\\]\n\nThe practical implications of this are:\n\nIf a partial correlation is close to zero, it suggests the relationship between two variables is primarily mediated through other variables.\nNon-zero partial correlations indicate a direct relationship that persists after controlling for other variables.\n\nFigure 3.54 shows the partial correlation network for the crime data, using the qgraph() argument graph = \"pcor\" To provide a more interpretable result, the argument layout = \"spring\" positions the nodes using a force-embedded algorithm where edges act like springs, pulling connected nodes together and unconnected nodes repel each other, pushing them apart.\n\nqgraph(crime.cor, \n       title = \"Crime data:\\npartial correlations\", title.cex = 1.5,\n       graph = \"pcor\",\n       layout = \"spring\", repulsion = 1.2,\n       minimum = \"sig\", sampleSize = nrow(crime), alpha = 0.05,\n       color = grey(.9), vsize = 14,\n       labels = rownames(crime.cor),\n       edge.labels = TRUE, edge.label.cex = 1.7,\n       posCol = \"blue\")\n\n\n\n\n\n\nFigure 3.54: Network diagram of partial correlations among the crime variables, controlling for all others. Variable nodes have been positioned by a “spring” layout method …\n\n\n\n\nFigure 3.54 shows that, once all other crime variables are controlled for each pair, there remain only a few partial correlations at the \\(\\alpha = 0.05\\) level. Of these, only the largest three in absolute value are significant at \\(\\alpha = 0.01\\).\nThus, once all other variables are taken into account, what remains is mainly a strong positive association between burglary and larceny and a moderate one between auto theft and robbery. There also remains a moderate negative correlation between murder and larceny. The spring layout makes it clear that, with suppression of weak edges, auto theft and robbery form a cluster separated from the other variables.\n\n3.12.3 Visualizing partial correlations\nJust as you can visualize marginal association between variables in a scatterplot, you can also visualize conditional association. A partial variables plot is simply a scatterplot of the partial residuals \\(e_i = (x_i - \\hat{x}_i)\\) from a regression of \\(x_i\\) on the other variables \\(Z\\) against those \\(e_j = (x_j - \\hat{x}_j)\\) for another variable \\(x_j\\).\nIn this, you can use all the bells and whistles of standard scatterplots (regression lines, smooths, data ellipses, …) to listen more attentively to the story partial association has to tell. The function pvPlot() calculates the partial residuals and then calls car::dataEllipse() for display. The five most “unusual” observations by Mahalanobis \\(D^2\\) are identified with their abbreviated state labels. Figure 3.55 shows these plots for the variable pairs with the two largest partial correlations.\n\nTODO: Put pvPlot into heplots package or car\n\nsource(\"R/pvPlot.R\")\n# select numeric, make `st` into rownames\ncrime.num &lt;- crime |&gt;\n  tibble::column_to_rownames(\"st\") |&gt;\n  dplyr::select(where(is.numeric))\n\npvPlot(crime.num, vars = c(\"burglary\", \"larceny\"), \n       id = list(n=5),\n       cex.lab = 1.5)\npvPlot(crime.num, vars = c(\"robbery\", \"auto\"),\n       id = list(n=5),\n       cex.lab = 1.5)\n\n\n\n\n\n\n\n\nFigure 3.55: Partial variables plots for burglary and larceny (left) and for robbery and auto theft (right) in the network diagram for partial correlations of the crime variables.\n\n\n\n\nIn the pvPlot for burglary and larceny, you can see that the high partial correlation is largely driven by the extreme points at the left and and right sides. Once all other variables are taken into account, Arizona (AZ) and Hawaii (HI) have larger incidence of both crimes, while Arkansas (AK) are smaller on both.\nIn the pvPlot for robbery and auto theft, New York stands out as an influential, high-leverage point (see Section 6.5); Massachusetts (MA) is noteworthy because auto theft in that state is considerably higher than what would be predicted from all other variables.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#what-have-we-learned",
    "href": "04-multivariate_plots.html#what-have-we-learned",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.13 What have we learned?",
    "text": "3.13 What have we learned?\nThis chapter provides a comprehensive toolkit for visualizing multivariate relationships, transforming the humble scatterplot into a powerful engine for data exploration and discovery.\n\nEnhance your plots: The basic scatterplot becomes far more informative when we add smoothers (regression lines, loess curves), stratifiers (grouping by color, shape, or panels), and data ellipses that capture correlation, variance, and regression relationships in a single elegant geometric form. These annotations help turn static dots into informative stories about relationships in your data.\nData ellipses are a visualization Multi-Tool: These simple geometric summaries encode a remarkable amount of information—means, standard deviations, correlation, regression slopes, and confidence regions—all in one visual package. When your data are roughly bivariate normal, the ellipse becomes a sufficient summary, telling you everything you need to know about the relationship between two variables. Relaxing the normality assumption, robust methods like the bagplot can serve nearly as well.\nSimpson’s Paradox lurks everywhere: The grouping variables you ignore can completely reverse the relationships you see, once you properly account for the grouping variables. Marginal correlations can be negative while all within-group correlations—conditional on the grouping variable–are positive (or vice versa). This fundamental lesson reminds us that context matters—always consider what variables you might be overlooking.\nScatterplot matrices scale gracefully: When you have multiple variables, pairs plots let you see all pairwise relationships simultaneously. Add visual thinning (removing points, keeping only smoothers and ellipses) and effect ordering (arranging variables by similarity) to handle even larger numbers of variables while maintaining interpretability.\nGeneralized pairs plots bridge data types: Modern extensions handle mixtures of continuous and categorical variables elegantly, using appropriate plot types for each combination—scatterplots for continuous pairs, box plots for continuous-categorical combinations, and mosaic plots for categorical pairs. This unified framework means no variable gets left behind.\nParallel coordinates reveal high-dimensional patterns: When scatterplot matrices reach their limits, parallel coordinate plots let you visualize dozens of variables simultaneously. Each observation becomes a connected line across parallel axes, revealing clusters, outliers, and multivariate patterns that would be invisible in traditional 2D projections.\nTours provide dynamic exploration: Animated statistical tours take you on guided journeys through high-dimensional data space, smoothly rotating through different 2D projections. Whether taking a random grand tour or a guided tour optimized for specific features (clusters, outliers, group separation), these methods reveal structures hidden in static displays.\nNetwork diagrams conquer “big p” data: When correlations become too numerous to display traditionally, network graphs show variables as nodes connected by edges representing associations. Partial correlation networks reveal direct relationships between variables with all other influences removed—the difference between marginal and conditional independence becomes visually apparent.\nVisual thinning is a superpower: As datasets grow in size and complexity, strategic removal of visual elements (points, axes, labels) while retaining essential summaries (trends, ellipses, connections) lets you maintain clarity and insight. Less can indeed be more when every remaining element carries a greater share of graphic information.\n\nHaving understood these lessons of multivariate thinking and visualization in data space, you are now prepared to take the next step in Chapter 4 to consider how to enhance your understanding of multivariate data using multivariate juicers which project high-dimensional data into a lower-dimensional space in which important effects can be more easily seen.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#exercises",
    "href": "04-multivariate_plots.html#exercises",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.14 Exercises",
    "text": "3.14 Exercises\n\nExercise 3.1 Using the Salaries dataset, create one or more plots to compare different smoothing methods for the relationship between yrs.since.phd and salary shown in Figure 3.5. Include linear regression, quadratic polynomial, and loess smoothers,\nlibrary(ggplot2)\ndata(Salaries, package = \"carData\")\n# Your code here\n\n\nExercise 3.2 One alternative to a loess smooth, which allows a span argument to control the degree of smoothing is a natural spline, that can be used in geom_smooth() using the argument formula = y ~ splines::ns(x, df=), where df is the equivalent number of degrees of freedom for the spline smoother. Re-do Exercise 3.1, but trying out this smoothing method for several values of df.\n\n\n\n\n\n\n\nAndrews, D. F. (1972). Plots of high dimensional data. Biometrics, 28, 123–136.\n\n\nAsimov, D. (1985). Grand tour. SIAM Journal of Scientific and Statistical Computing, 6(1), 128–143.\n\n\nBarab’asi, A.-L. (2016). Network science. Cambridge University Press.\n\n\nBecker, R. A., Cleveland, W. S., & Shyu, M.-J. (1996). The visual design and control of trellis display. Journal of Computational and Graphical Statistics, 5(2), 123–155.\n\n\nBlack, C., Southwell, C., Emmerson, L., Lunn, D., & Hart, T. (2018). Time-lapse imagery of Adélie penguins reveals differential winter strategies and breeding site occupation. PLOS ONE, 13(3), e0193532. https://doi.org/10.1371/journal.pone.0193532\n\n\nBlishen, B., Carroll, W., & Moore, C. (1987). The 1981 socioeconomic index for occupations in canada. Canadian Review of Sociology/Revue Canadienne de Sociologie, 24(4), 465–488. https://doi.org/10.1111/j.1755-618x.1987.tb00639.x\n\n\nBondy, J. A., & Murty, U. S. R. (2008). Graph theory. Springer.\n\n\nBrunson, J. C., & Gracey, J. (2025). Gggda: A ’ggplot2’ extension for geometric data analysis. https://github.com/corybrunson/gggda\n\n\nBuja, A., Cook, D., Asimov, D., & Hurley, C. (2005). Computational methods for high-dimensional rotations in data visualization. In J. S. CR Rao EJ Wegman (Ed.), Handbook of statistics (pp. 391–413). Elsevier. https://doi.org/10.1016/s0169-7161(04)24014-7\n\n\ncagne, M. (1885). Coordonnées parallèles et axiales: Méthode de transformation géométrique et procédé nouveau de calcul graphique déduits de la considération des coordonnées parallèlles. Gauthier-Villars. http://historical.library.cornell.edu/cgi-bin/cul.math/docviewer?did=00620001&seq=3\n\n\nChambers, J. M., Cleveland, W. S., Kleiner, B., & Tukey, P. A. (1983). Graphical methods for data analysis. Wadsworth.\n\n\nChambers, J. M., & Hastie, T. J. (1991). Statistical models in s (p. 624). Chapman & Hall/CRC.\n\n\nCleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association, 74, 829–836.\n\n\nCleveland, W. S. (1985). The elements of graphing data. Wadsworth Advanced Books.\n\n\nCleveland, W. S., & Devlin, S. J. (1988). Locally weighted regression: An approach to regression analysis by local fitting. Journal of the American Statistical Association, 83, 596–610.\n\n\nCleveland, W. S., & McGill, R. (1984). Graphical perception: Theory, experimentation and application to the development of graphical methods. Journal of the American Statistical Association, 79, 531–554.\n\n\nCleveland, W. S., & McGill, R. (1985). Graphical perception and graphical methods for analyzing scientific data. Science, 229, 828–833.\n\n\nCook, D., Buja, A., Cabrera, J., & Hurley, C. (1995). Grand tour and projection pursuit. Journal of Computational and Graphical Statistics, 4(3), 155. https://doi.org/10.2307/1390844\n\n\nCook, D., Buja, A., Lee, E.-K., & Wickham, H. (2008). Grand tours, projection pursuit guided tours, and manual controls. In Handbook of data visualization (pp. 295–314). Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-33037-0_13\n\n\nCook, D., & Laa, U. (2024). Interactively exploring high-dimensional data and models in R. Online. https://dicook.github.io/mulgar_book/\n\n\nCook, D., & Swayne, D. F. (2007). Interactive and dynamic graphics for data analysis : With R and GGobi. Springer. http://www.ggobi.org/book/\n\n\nCostantini, G., Epskamp, S., Borsboom, D., Perugini, M., Mõttus, R., Waldorp, L. J., & Cramer, A. O. J. (2015). State of the aRt personality research: A tutorial on network analysis of personality data in R. Journal of Research in Personality, 54, 13–29. https://doi.org/10.1016/j.jrp.2014.07.003\n\n\nCsárdi, G., Nepusz, T., Traag, V., Horvát, S., Zanini, F., Noom, D., & Müller, K. (2024). igraph: Network analysis and visualization in r. https://doi.org/10.5281/zenodo.7682609\n\n\nDempster, A. P. (1969). Elements of continuous multivariate analysis. Addison-Wesley.\n\n\nDempster, A. P. (1972). Covariance selection. Biometrics, 28(1), 157–175.\n\n\nEmerson, J. W., Green, W. A., Schloerke, B., Crowley, J., Cook, D., Hofmann, H., & Wickham, H. (2013). The generalized pairs plot. Journal of Computational and Graphical Statistics, 22(1), 79–91. https://doi.org/10.1080/10618600.2012.694762\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (Third edition.). SAGE.\n\n\nFox, J., & Weisberg, S. (2019). An R companion to applied regression (Third). Sage. https://www.john-fox.ca/Companion/\n\n\nFriendly, M. (1991). SAS System for statistical graphics (1st ed.). SAS Institute. http://www.sas. com/service/doc/pubcat/uspubcat/ind_files/56143.html\n\n\nFriendly, M. (1994). Mosaic displays for multi-way contingency tables. Journal of the American Statistical Association, 89, 190–200. http://www.jstor.org/stable/2291215\n\n\nFriendly, M. (1999). Extending mosaic displays: Marginal, conditional, and partial views of categorical data. Journal of Computational and Graphical Statistics, 8(3), 373–395. http://datavis.ca/papers/drew/drew.pdf\n\n\nFriendly, M. (2002). Corrgrams: Exploratory displays for correlation matrices. The American Statistician, 56(4), 316–324. https://doi.org/10.1198/000313002533\n\n\nFriendly, M. (2022). The life and works of andré-michel guerry, revisited. Sociological Spectrum, 42(4-6), 233–259. https://doi.org/10.1080/02732173.2022.2078450\n\n\nFriendly, M. (2025). vcdExtra: ’Vcd’ extensions and additions. https://doi.org/10.32614/CRAN.package.vcdExtra\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays. Computational Statistics and Data Analysis, 43(4), 509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGannett, H. (1898). Statistical atlas of the united states, eleventh (1890) census. U.S. Government Printing Office.\n\n\nGorman, K. B., Williams, T. D., & Fraser, W. R. (2014). Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis). PLoS ONE, 9(3), e90081. https://doi.org/10.1371/journal.pone.0090081\n\n\nGrandjean, M. (2016). A social network analysis of Twitter: Mapping the digital humanities community. Cogent Arts &Amp; Humanities, 3(1), 1171458. https://doi.org/10.1080/23311983.2016.1171458\n\n\nGuerry, A.-M. (1833). Essai sur la statistique morale de la France. Crochard.\n\n\nHarrison, P. (2023b). Langevitour: Smooth interactive touring of high dimensions, demonstrated with scRNA-seq data. The R Journal, 15(2), 206–219. https://doi.org/10.32614/RJ-2023-046\n\n\nHarrison, P. (2023a). Langevitour: Smooth interactive touring of high dimensions, demonstrated with scRNA-seq data. The R Journal, 15(2), 206–219. https://doi.org/10.32614/RJ-2023-046\n\n\nHart, C., & Wang, E. (2022). Detourr: Portable and performant tour animations. https://CRAN.R-project.org/package=detourr\n\n\nHartigan, J. A. (1975a). Clustering algorithms. John Wiley; Sons.\n\n\nHartigan, J. A. (1975b). Printer graphics for clustering. Journal of Statistical Computing and Simulation, 4, 187–213.\n\n\nHofmann, H., VanderPlas, S., & Ge, Y. (2022). Ggpcp: Parallel coordinate plots in the ’ggplot2’ framework. https://doi.org/10.32614/CRAN.package.ggpcp\n\n\nHofstadter, D. R. (1979). Gödel, escher, bach: An eternal golden braid. Basic Books.\n\n\nHøjsgaard, S., Edwards, D., & Lauritzen, S. (2012). Graphical models with R. Springer Science & Business Media.\n\n\nHorst, A. M., Hill, A. P., & Gorman, K. B. (2020). Palmerpenguins: Palmer archipelago (antarctica) penguin data. https://doi.org/10.5281/zenodo.3960218\n\n\nInselberg, A. (1985). The plane with parallel coordinates. The Visual Computer, 1, 69–91.\n\n\nIsvoranu, A.-M., Epskamp, S., Waldorp, L. J., & Borsboom, D. (2022). Network psychometrics with r: A guide for behavioral and social scientists. Routledge. https://doi.org/10.4324/9781003111238\n\n\nLauritzen, S. L. (1996). Graphical models. Oxford University Press.\n\n\nLee, E.-K., & Cook, D. (2009). A projection pursuit index for large p small n data. Statistics and Computing, 20(3), 381–392. https://doi.org/10.1007/s11222-009-9131-1\n\n\nLee, S. (2025). Liminal: Multivariate data visualization with tours and embeddings. https://github.com/sa-lee/liminal\n\n\nMartí, R., & Laguna, M. (2003). Heuristics and meta-heuristics for 2-layer straight line crossing minimization. Discrete Applied Mathematics, 127(3), 665–678.\n\n\nMeyer, D., Zeileis, A., Hornik, K., & Friendly, M. (2024). Vcd: Visualizing categorical data. https://doi.org/10.32614/CRAN.package.vcd\n\n\nMonette, G. (1990). Geometry of multiple regression and interactive 3-D graphics. In J. Fox & S. Long (Eds.), Modern methods of data analysis (pp. 209–256). SAGE Publications.\n\n\nOtto, J., & Kahle, D. (2023). Ggdensity: Interpretable bivariate density visualization with ’ggplot2’. https://doi.org/10.32614/CRAN.package.ggdensity\n\n\nPearson, K. (1901). On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 6(2), 559–572.\n\n\nPineo, P. O., & Porter, J. (2008). Occupational prestige in canada. Canadian Review of Sociology, 4(1), 24–40. https://doi.org/10.1111/j.1755-618x.1967.tb00472.x\n\n\nRobinaugh, D. J., Hoekstra, R. H. A., Toner, E. R., & Borsboom, D. (2019). The network approach to psychopathology: A review of the literature 2008–2018 and an agenda for future research. Psychological Medicine, 50(3), 353–366. https://doi.org/10.1017/s0033291719003404\n\n\nRousseeuw, P. J., Ruits, I., & Tukey, J. W. (1999). The bagplot: A bivariate boxplot. The American Statistician, 53(4), 382–387.\n\n\nSarkar, D. (2008). Lattice: Multivariate data visualization with r. Springer. http://lmdvr.r-forge.r-project.org\n\n\nSchloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M., Thoen, E., Elberg, A., & Crowley, J. (2025). GGally: Extension to ’ggplot2’. https://doi.org/10.32614/CRAN.package.GGally\n\n\nScott, D. W. (1992). Multivariate density estimation: Theory, practice, and visualization. Wiley.\n\n\nSilverman, B. W. (1986). Density estimation for statistics and data analysis. Chapman & Hall.\n\n\nSimpson, E. H. (1951). The interpretation of interaction in contingency tables. Journal of the Royal Statistical Society, Series B, 30, 238–241.\n\n\nSwayne, D. F., Cook, D., & Buja, A. (1998). XGobi: Interactive dynamic data visualization in the x window system. Journal of Computational and Graphical Statistics, 7(1), 113–130. https://doi.org/10.1080/10618600.1998.10474764\n\n\nSwayne, D. F., Lang, D. T., Buja, A., & Cook, D. (2003). GGobi: Evolving from XGobi into an extensible framework for interactive data visualization. Computational Statistics &Amp; Data Analysis, 43(4), 423–444. https://doi.org/10.1016/s0167-9473(02)00286-4\n\n\nTukey, J. W. (1977). Exploratory data analysis. Addison Wesley.\n\n\nVanderPlas, S., Ge, Y., Unwin, A., & Hofmann, H. (2023). Penguins go parallel: A grammar of graphics framework for generalized parallel coordinate plots. Journal of Computational and Graphical Statistics, 1–16. https://doi.org/10.1080/10618600.2023.2195462\n\n\nWaddell, A., & Oldford, R. W. (2025). Loon: Interactive statistical data visualization. https://doi.org/10.32614/CRAN.package.loon\n\n\nWegman, E. J. (1990). Hyperdimensional data analysis using parallel coordinates. Journal of the American Statistical Association, 85(411), 664–675.\n\n\nWei, T., & Simko, V. (2024). R package ’corrplot’: Visualization of a correlation matrix. https://github.com/taiyun/corrplot\n\n\nWest, D. B. (2001). Introduction to graph theory. Prentice hall.\n\n\nWhittaker, J. (1990). Graphical models in applied multivariate statistics. John Wiley; Sons.\n\n\nWickham, H., & Cook, D. (2025). Tourr: Tour methods for multivariate data visualisation. https://github.com/ggobi/tourr\n\n\nWickham, H., Cook, D., Hofmann, H., & Buja, A. (2011). Tourr: An R package for exploring multivariate data with projections. Journal of Statistical Software, 40(2). https://doi.org/10.18637/jss.v040.i02\n\n\nWood, S. N. (2006). Generalized additive models: An introduction with r. Chapman; Hall/CRC Press.\n\n\nWright, K. (2021). Corrgram: Plot a correlogram. https://doi.org/10.32614/CRAN.package.corrgram\n\n\nXu, Z., & Oldford, R. W. (2021). Loon.tour: Tour in ’loon’. https://cran.r-project.org/package=loon.tourr",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-multivariate_plots.html#footnotes",
    "href": "04-multivariate_plots.html#footnotes",
    "title": "3  Plots of Multivariate Data",
    "section": "",
    "text": "Confidence bands allow us to visualize the uncertainty around a fitted regression curve. These can be of two types: pointwise intervals or simultaneous intervals. The default setting in `ggplot2::geom_smooth() calculates pointwise intervals (using stats::predict.lm(..., interval=\"confidence\") at a confidence level \\(1-\\alpha\\) for the predicted response at each value \\(x_i\\) of a predictor, and have the frequentist interpretation that over repeated sampling only \\(100\\;\\alpha\\) of the predictions at \\(x_i\\) will be outside that interval. In contrast, simultaneous intervals are calculated so that \\(1 - \\alpha\\) is the probability that all of them cover their corresponding true values simultaneously. These are necessarily wider than pointwise intervals. Commonly used methods for constructing simultaneous confidence bands in regression are the Bonferroni and Scheffé methods, which control the family-wise error rate over all values of \\(x_i\\). See  for precise definitions of these terms. These are different from a prediction band, which is used to represent the uncertainty about the value of a new data-point on the curve, but subject to the additional variance reflected in one observation.↩︎\nThe classic study by Cleveland & McGill (1984);Cleveland & McGill (1985) shows that judgements of magnitude along a common scale are more accurate than those along separate, aligned scales.↩︎\nThe dataset was collected by Bernard Blishen, William Carroll and Catherine Moore, but apparently unpublished. A version updated to the 1981 census is described in Blishen et al. (1987).↩︎\nIn R 4.5.0, a revised version of the palmerpenguins dataset, named penguins was added to the base R datasets package. This corrected a few of the problems that led me to create heplots::peng, but still retains the cases with missing data.↩︎\nThe \\(p^{th}\\)-quantile of a random variable \\(Y\\) is a value, denoted by \\(Q_Y(p)\\), such that \\(X \\leq Q_Y(p)\\) with probability \\(p\\), i.e., \\(\\Pr (Y \\leq Q_Y(p)) = p\\). For continuous variables, the quantile is the inverse of the cumulative distribution function.↩︎\nAfter the long-nosed Cyrano de Bergerac from Edmund Rostand’s play. Actually, bird 283 is a female. But, it would be a mistake to call her “Rozanne” instead.↩︎\nOther implementations of parallel coordinate plots in R include: MASS::parcoord(), GGally::ggparcoord() and PairViz::pcp(). The ggpcp version used here is the most general.↩︎\nThis example was modified from one used by Cook et al. (2008).↩︎",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html",
    "href": "05-pca-biplot.html",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "4.1 Flatland and Spaceland\nThere was a cloud in the sky above Flatland one day. But it was a huge, multidimensional cloud of sparkly points that might contain some important message, perhaps like the hidden EUREKA (Figure 5), or perhaps forecasting the upcoming harvest, if only Flatlanders could appreciate it.\nA leading citizen, A SQUARE, who had traveled once to Spaceland and therefore had an inkling of its majesty beyond the simple world of his life in the plane looked at that cloud and had a brilliant thought, an OMG moment:\nAs it happened, our Square friend, although he could never really see in three dimensions, he could now at least think of a world described by height as well as breadth and width, and think of the shadow cast by a cloud as something mutable, changing size and shape depending on its’ orientation over Flatland.\nAnd what a world it was, inhabited by Pyramids, Cubes and wondrous creatures called Polyhedrons with many \\(C\\)orners, \\(F\\)aces and \\(E\\)dges. Not only that, but all those Polyhedra were forced in Spaceland to obey a magic formula: \\(C + F - E = 2\\).1 How cool was that!\nIndeed, there were even exalted Spheres, having so many faces that its surface became as smooth as a baby’s bottom with no need for pointed corners or edges, just as Circles were the smoothest occupants of his world with far too many sides to count. It was his dream of a Sphere passing through Flatland (Figure 1) that first awakened him to a third dimension.\nHe also marveled at Ellipsoids, as smooth as Spheres, but in Spaceland having three natural axes of different extent and capable of being appearing fatter or slimmer when rotated from different views. An Ellipsoid had magical properties: it could appear as so thin in one or more dimensions that it became a simple 2D ellipse, or a 1D line, or even a 0D point (Friendly et al., 2013).\nAll of these now arose in Square’s richer 3D imagination. And, all of this came from just one more dimension than his life in Flatland.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html#sec-spaceland",
    "href": "05-pca-biplot.html#sec-spaceland",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "It is high time that I should pass from these brief and discursive notes about Flatland to the central event of this book, my initiation into the mysteries of Space. THAT is my subject; all that has gone before is merely preface — Edwin Abbott, Flatland, p. 57.\n\n\n\n\nOh, can I, in my imagination, rotate that cloud and squeeze its juice so that it rains down on Flatland with greatest joy?”\n\n\n\n\n\n\n\n4.1.1 Multivariate juicers\nUp to now, we have also been living in Flatland. We have been trying to understand data in data space of possibly many dimensions, but confined to the 2D plane of a graph window. Scatterplot matrices and parallel coordinate plots provided some relief. The former did so by projecting the data into sets of 2D views in the coordinates of data space; the latter did so by providing multiple axes in a 2D space along which we could trace the paths of individual observations.\nThis chapter is about seeing data in a different projection—a low-dimensional (usually 2D) space—that squeezes out the most juice from multidimensional data for a particular purpose (Figure 4.1), where what we want to understand can be more easily seen.\n\n\n\n\n\n\n\nFigure 4.1: A multivariate juicer takes data from possibly high-dimensional data space and transforms it to a lower-dimenional space in which important effects can be more easily seen.\n\n\n\n\nHere, I concentrate on principal components analysis (PCA), whose goal reflects A Square’s desire to see that sparkly cloud of points in \\(nD\\) space in the plane showing the greatest variation (squeezing the most juice) among all other possible views. This appealed to his sense of geometry, but left him wondering how the variables in that high-D cloud were related to the dimensions he could see in a best-fitting plane. \nThe idea of a biplot, showing the data points in the plane, together with thick pointed arrows—variable vectors— in one view is the other topic explained in this chapter (Section 4.3). The biplot is the simplest example of a multivariate juicer. The essential idea is to project the cloud of data points in \\(n\\) dimensions into the 2D space of principal components and simultaneously show how the original variables relate to this space. For exploratory analysis to get an initial, incisive view of a multivariate dataset, a biplot is often my first choice. \n\n\n\n\n\n\nLooking ahead\n\n\n\nI’m using the term multivariate juicer here to refer the wider class of dimension reduction techniques, used for various purposes in data analysis and visualization. PCA is the simplest example and illustrates the general ideas.\nThe key point is that these methods are designed to transform the data into a low-dimensional space for a particular goal or purpose. In PCA, the goal is to extract the greatest amount of total variability in the data. In the context of univariate multiple regression, the goal is often to reduce the number of predictors necessary to account for an outcome variable, called feature extraction in the machine learning literature.\nWhen the goal is to best distinguish among groups discriminant analysis finds uncorrelated weighted sums of predictors on which the means of groups are most widely separated in a reduced space of hopefully fewer dimensions.\nThe methods I cover in this book are all linear methods, but there is also a wide variety of non-linear dimension reduction techniques.\n\n\nPackages\nIn this chapter I use the following packages. Load them now:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(ggbiplot)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(car)\nlibrary(ggpubr)\nlibrary(matlib)",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html#sec-pca",
    "href": "05-pca-biplot.html#sec-pca",
    "title": "4  Dimension Reduction",
    "section": "\n4.2 Principal components analysis (PCA)",
    "text": "4.2 Principal components analysis (PCA)\nWhen Francis Galton (1886) first discovered the idea of regression toward the mean and presented his famous diagram (Figure 3.10), he had little thought that he had provided a window to a higher-dimensional world, beyond what even A Square could imagine. His friend, Karl Pearson (1896) took that idea and developed it into a theory of regression and a measure of correlation that would bear his name, Pearson’s \\(r\\).\nBut then Pearson (1901) had a further inspiration, akin to that of A Square. If he also had a cloud of sparkly points in \\(2, 3, 4, ..., p\\) dimensions, could he find a point (\\(0D\\)), or line (\\(1D\\)), or plane (\\(2D\\)), or even a hyperplane (\\(nD\\)) that best summarized — squeezed out the most juice—from multivariate data? This was the first truly multivariate problem in the history of statistics (Friendly & Wainer, 2021, p. 186).\nThe best \\(0D\\) point was easy— it was simply the centroid, the means of each of the variables in the data, \\((\\bar{x}_1, \\bar{x}_2, ..., \\bar{x}_p)\\), because that was “closest” to the data in the sense of minimizing the sum of squared differences, \\(\\Sigma_i\\Sigma_j (x_{ij} - \\bar{x}_j)^2\\). In higher dimensions, his solution was also an application of the method of least squares, but he argued it geometrically and visually as shown in Figure 4.2.\n\n\n\n\n\n\n\nFigure 4.2: Karl Pearson’s (1901) geometric, visual argument for finding the line or plane of closest fit to a collection of points, P1, P2, P3, …\n\n\n\n\nFor a \\(1D\\) summary, the line of best fit to the points \\(P_1, P_2, \\dots P_n\\) is the line that goes through the centroid and made the average squared length of the perpendicular segments from those points to a line as small as possible. This was different from the case in linear regression, for fitting \\(y\\) from \\(x\\), where the average squared length of the vertical segments, \\(\\Sigma_i (y_i - \\hat{y}_i)^2\\) was minimized by least squares.\nHe went on to prove the visual insights from simple smoothing of Galton (1886) (shown in Figure 3.10) regarding the regression lines of y ~ x and x ~ y. More importantly, he proved that the cloud of points is captured, for the purpose of finding a best line, plane or hyperplane, by the ellipsoid that encloses it, as seen in his diagram, Figure 4.3. The major axis of the 2D ellipse is the line of best fit, along which the data points have the smallest average squared distance from the line. The axis at right angles to that—the minor axis— is labeled “line of worst fit” with the largest average squared distance.\n\n\n\n\n\n\n\nFigure 4.3: Karl Pearson’s diagram showing the elliptical geometry of regression and principal components analysis … Source: Pearson (1901), p. 566.\n\n\n\n\nEven more importantly— and this is the basis for PCA — he recognized that the two orthogonal axes of the ellipse gave new coordinates for the data which were uncorrelated, whatever the correlation of \\(x\\) and \\(y\\).\n\nPhysically, the axes of the correlation type-ellipse are the directions of independent and uncorrelated variation. — Pearson (1901), p. 566.\n\nIt was but a small step to recognize that for two variables, \\(x\\) and \\(y\\):\n\nThe line of best fit, the major axis (PC1) had the greatest variance of points projected onto it.\nThe line of worst fit, the minor axis (PC2), had the least variance.\nThese could be seen as a rotation of the data space of \\((x, y)\\) to a new space (PC1, PC2) with uncorrelated variables.\nThe total variation of the points in data space, \\(\\text{Var}(x) + \\text{Var}(y)\\), being unchanged by rotation, was equally well expressed as the total variation \\(\\text{Var}(PC1) + \\text{Var}(PC2)\\) of the scores on what are now called the principal component axes.\n\nIt would have appealed to Pearson (and also to A Square) to see these observations demonstrated in a 3D video.  Figure 4.4 shows a 3D plot of the variables Sepal.Length, Sepal.Width and Petal.Length in Edgar Anderson’s iris data, with points colored by species and the 95% data ellipsoid. This is rotated smoothly by interpolation until the first two principal axes, PC1 and PC2 are aligned with the horizontal and vertical dimensions. Because this is a rigid rotation of the cloud of points, the total variability is obviously unchanged.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Animation of PCA as a rotation in 3D space. The plot shows three variables for the iris data, initially in data space and its’ data ellipsoid, with points colored according to species of the iris flowers. This is rotated smoothly until the first two principal axes are aligned with the horizontal and vertical directions in the final frame.\n\n\n\n\n\n4.2.1 PCA by springs\nBefore delving into the mathematics of PCA, it is useful to see how Pearson’s problem, and fitting by least squares generally, could be solved in a physical realization.\nFrom elementary statistics, you may be familiar with a physical demonstration that the mean, \\(\\bar{x}\\), of a sample is the value for which the sum of deviations, \\(\\Sigma_i (x_i - \\bar{x})\\) is zero, so the mean can be visualized as the point of balance on a line where those differences \\((x_i - \\bar{x})\\) are placed. Equally well, there is a physical realization of the mean as the point along an axis where weights connected by springs will minimize the sum of squared differences, because springs with a constant stiffness, \\(k\\), exert forces proportional to \\(k (x_i - \\bar{x}) ^2\\). That’s the reason it is useful as a measure of central tendency: it minimizes the average squared error.\nIn two dimensions, imagine that we have points, \\((x_i, y_i)\\) and these are attached by springs of equal stiffness \\(k\\), to a line anchored at the centroid, \\((\\bar{x}, \\bar{y})\\) as shown in Figure 4.5. If we rotate the line to some initial position and release it, the springs will pull the line clockwise or counterclockwise and the line will bounce around until the forces, proportional to the squares of the lengths of the springs, will eventually balance out at the position (shown by the red fixed line segments at the ends). This is the position that minimizes the the sum of squared lengths of the connecting springs, and also minimizes the kinetic energy in the system.\nIf you look closely at Figure 4.5 you will see something else: When the line is at its final position of minimum squared length and energy, the positions of the red points on this line are spread out furthest, i.e., have maximum variance. Conversely, when the line is at right angles to its final position (shown by the black line at 90\\(^o\\)) the projected points have the smallest possible variance.\n\n\n\n\n\n\n\n\nFigure 4.5: Animation of PCA fitted by springs. The blue data points are connected to their projections on the red line by springs perpendicular to that line. From an initial position, the springs pull that line in proportion to their squared distances, until the line finally settles down to the position where the forces are balanced and the minimum is achieved. Source: Amoeba, https://bit.ly/46tAicu.\n\n\n\n\n4.2.2 Mathematics and geometry of PCA\nAs the ideas of principal components developed, there was a happy marriage of Galton’s geometrical intuition and Pearson’s mathematical analysis. The best men at the wedding were ellipses and higher-dimensional ellipsoids. The bridesmaids were eigenvectors, pointing in as many different directions as space would allow, each sized according to their associated eigenvalues. Attending the wedding were the ghosts of uncles, Leonhard Euler, Jean-Louis Lagrange, Augustin-Louis Cauchy and others who had earlier discovered the mathematical properties of ellipses and quadratic forms in relation to problems in physics.\nThe key idea in the statistical application was that, for a set of variables \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_p\\), the \\(p \\times p\\) covariance matrix \\(\\mathbf{S}\\) could be expressed exactly as a matrix product involving a matrix \\(\\mathbf{V}\\), whose columns are eigenvectors (\\(\\mathbf{v}_i\\)) and a diagonal matrix \\(\\boldsymbol{\\Lambda}\\), whose diagonal elements (\\(\\lambda_i\\)) are the corresponding eigenvalues.\nTo explain this, it is helpful to use a bit of matrix math:\n\\[\n\\begin{aligned}\n\\mathbf{S}_{p \\times p} & = \\mathbf{V}_{p \\times p} \\phantom{0000000000}\n                            \\boldsymbol{\\Lambda}_{p \\times p} \\phantom{00000000000000}\n                            \\mathbf{V}_{p \\times p}^\\mathsf{T} \\\\\n           & = \\left( \\mathbf{v}_1, \\, \\mathbf{v}_2, \\,\\dots, \\, \\mathbf{v}_p \\right)\n           \\begin{pmatrix}\n             \\lambda_1 &  &  & \\\\\n             & \\lambda_2  &   & \\\\\n             &  & \\ddots & \\\\\n             &  &  & \\lambda_p\n            \\end{pmatrix}\n            \\;\n            \\begin{pmatrix}\n            \\mathbf{v}_1^\\mathsf{T}\\\\\n            \\mathbf{v}_2^\\mathsf{T}\\\\\n            \\vdots\\\\\n            \\mathbf{v}_p^\\mathsf{T}\\\\\n            \\end{pmatrix}\n           \\\\\n           & = \\lambda_1 \\mathbf{v}_1 \\mathbf{v}_1^\\mathsf{T} + \\lambda_2 \\mathbf{v}_2 \\mathbf{v}_2^\\mathsf{T} + \\cdots + \\lambda_p \\mathbf{v}_p \\mathbf{v}_p^\\mathsf{T}\n\\end{aligned}\n\\tag{4.1}\\]\nIn this equation,\n\nThe last line follows because \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix, so \\(\\mathbf{S}\\) is expressed as a sum of outer products of each \\(\\mathbf{v}_i\\) with itself, times the eigenvalue \\(\\lambda_i\\).\nThe columns of \\(\\mathbf{V}\\) are the eigenvectors of \\(\\mathbf{S}\\). They are orthogonal and of unit length, so \\(\\mathbf{V}^\\mathsf{T} \\mathbf{V} = \\mathbf{I}\\). Thus they represent orthogonal (uncorrelated) directions in data space.\nThe column \\(\\mathbf{v}_i\\) gives the weights applied to the variables to produce the scores on the \\(i\\)-th principal component _i$. For example, the first principal component is the weighted sum:\n\n\\[\n\\text{PC}_1 = v_{11} \\mathbf{x}_1 + v_{12} \\mathbf{x}_2 + \\cdots + v_{1p} \\mathbf{x}_p \\:\\: .\n\\]\n\nThe matrix of all scores on the principal components can be calculated by multiplying the data matrix \\(\\mathbf{X}\\) by the eigenvectors, \\(\\mathbf{PC} = \\mathbf{X} \\mathbf{V}\\).\nThe eigenvalues, \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_p\\) are the variances of the the components, because \\(\\mathbf{v}_i^\\mathsf{T} \\;\\mathbf{S} \\; \\mathbf{v}_i = \\lambda_i\\).\nIt is usually the case that the variables \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_p\\) are linearly independent, which means that none of these is an exact linear combination of the others. In this case, all eigenvalues \\(\\lambda_i\\) are positive and the covariance matrix \\(\\mathbf{S}\\) is said to have rank \\(p\\). (Rank is the number of non-zero eigenvalues.)\nHere is a key fact: If, as usual, the eigenvalues are arranged in order, so that \\(\\lambda_1 &gt; \\lambda_2 &gt; \\dots &gt; \\lambda_p\\), then the first \\(d\\) components give a \\(d\\)-dimensional approximation to \\(\\mathbf{S}\\). This approximation accounts for \\(\\Sigma_i^d \\lambda_i\\) of the \\(\\Sigma_i^p \\lambda_i\\) total variance, and is usually interpreted as the proportion, \\((\\Sigma_i^d \\lambda_i) / (\\Sigma_i^p \\lambda_i)\\).\n\nFor the case of two variables, \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) Figure 4.6 shows the transformation from data space to component space. The eigenvectors, \\(\\mathbf{v}_1, \\mathbf{v}_2\\) are the major and minor axes of the data ellipse, whose lengths are the square roots \\(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}\\) of the eigenvalues.\n\n\n\n\n\n\n\nFigure 4.6: Geometry of PCA as a rotation from data space to principal component space, defined by the eigenvectors v1 and v2 of a covariance matrix\n\n\n\n\n\n\nExample 4.1 Workers’ experience and income\nFor a small example, consider the relation between years of experience and income in a small (contrived) sample (\\(n = 10\\)) of workers in a factory. The dataset workers contains these and other variables. In a wider context, we might want to fit a regression model to predict Income, but here we focus on a PCA of just these two variables.\n\ndata(workers, package = \"matlib\") \nhead(workers)\n#         Income Experience Skill Gender\n# Abby        20          0     2 Female\n# Betty       35          5     5 Female\n# Charles     40          5     8   Male\n# Doreen      30         10     6 Female\n# Ethan       50         10    10   Male\n# Francie     50         15     7 Female\n\nLet’s start with a simple scatterplot of Income vs. Experience, with points labeled by Name (and colored by Gender). There’s a fairly strong correlation (\\(r\\) = 0.853). How does a PCA capture this?\n\n\nvars &lt;- c(\"Experience\", \"Income\")\nplot(workers[, vars],\n     pch = 16, cex = 1.5,\n     cex.lab = 1.5)\ntext(workers[, vars], \n     labels = rownames(workers),\n     col = ifelse(workers$Gender == \"Female\", \"red\", \"blue\"),\n     pos = 3, xpd = TRUE)\n\n\n\n\n\n\nFigure 4.7: Scatterplot of Income vs. Experience for the workers data.\n\n\n\n\nTo carry out a PCA of these variables, first calculate the vector of means (\\(\\bar{\\mathbf{x}}\\)) and covariance matrix \\(\\mathbf{S}\\).\n\nmu &lt;- colMeans(workers[, vars]) |&gt; print()\n# Experience     Income \n#       15.5       46.5\nS &lt;- cov(workers[, vars]) |&gt; print()\n#            Experience Income\n# Experience        136    152\n# Income            152    234\n\nThe eigenvalues and eigenvectors of S are calculated by eigen(). This returns a list with components values for the \\(\\lambda_i\\) and vectors for \\(\\mathbf{V}\\).\n\nS.eig &lt;- eigen(S)\nLambda &lt;- S.eig$values |&gt; print()\n# [1] 344.3  25.1\nV &lt;- S.eig$vectors |&gt; print()\n#       [,1]   [,2]\n# [1,] 0.589 -0.808\n# [2,] 0.808  0.589\n\nFrom this, you can verify the points above regarding the relations between variances of the variables and the eigenvalues:\n\n#total variances of the variables = sum of eigenvalues\nsum(diag(S))\n# [1] 369\nsum(Lambda)\n# [1] 369\n\n# percent of variance of each PC\n100 * Lambda / sum(Lambda)\n# [1] 93.2  6.8\n\nUsing these, you can express the eigenvalue decomposition of \\(\\mathbf{S}\\) in Equation 4.1 with latexMatrix() and Eqn() from the matlib package (Friendly et al., 2024) as:\n\n\noptions(digits = 3)\nrownames(S) &lt;- colnames(S) &lt;- c(\"\\\\small \\\\text{Exp}\", \n                                \"\\\\small \\\\text{Inc}\")\nspacer &lt;- \"\\\\phantom{00000000000000}\"\nEqn(\"\\\\mathbf{S} & = \\\\mathbf{V}\", spacer,\n    \"\\\\mathbf{\\\\Lambda}\", spacer,  \n    \"\\\\mathbf{V}^\\\\top\", Eqn_newline(),\n    latexMatrix(S), \"& =\", \n    latexMatrix(V), \"  \", diag(Lambda), \"  \", latexMatrix(V, transpose=TRUE),\n    align = TRUE)\n\n\n\\[\\begin{aligned}\n\\mathbf{S} & = \\mathbf{V} \\phantom{00000000000000}\n     \\boldsymbol{\\Lambda} \\phantom{00000000000000}  \n     \\mathbf{V}^\\top \\\\\n\\begin{matrix}\n  &  \\begin{matrix} \\phantom{i} Exp & Inc\n  \\end{matrix} \\\\\n\\begin{matrix}  \n   Exp\\\\\n   Inc\\\\\n\\end{matrix}  &\n\\begin{pmatrix}  \n136 & 152 \\\\\n152 & 234 \\\\\n\\end{pmatrix}\n\\\\\n\\end{matrix}\n& =\\begin{pmatrix}\n0.589 & -0.808 \\\\\n0.808 &  0.589 \\\\\n\\end{pmatrix}\n  \\begin{pmatrix}\n344.3 &   0.0 \\\\\n  0.0 &  25.1 \\\\\n\\end{pmatrix}\n  \\begin{pmatrix}\n0.589 & -0.808 \\\\\n0.808 &  0.589 \\\\\n\\end{pmatrix}^\\top\n\\end{aligned}\\]\nThe “scores” on the principal components can be calculated (point (5) above) as \\(\\mathbf{PC} = \\mathbf{X} \\mathbf{V}\\):\n\nPC &lt;- as.matrix(workers[, vars]) %*% V\ncolnames(PC) &lt;- paste0(\"PC\", 1:2)\nhead(PC)\n#          PC1   PC2\n# Abby    16.2 11.78\n# Betty   31.2 16.57\n# Charles 35.3 19.52\n# Doreen  30.1  9.59\n# Ethan   46.3 21.37\n# Francie 49.2 17.32\n\nThen, you can visualize the geometry of PCA as in Figure 4.6 (left) by plotting the data ellipse for the points, along with the PCA axes (heplots::ellipse.axes()). Figure 4.8 also shows the bounding box of the data ellipse, which are parallel to the PC axes and scaled to have the same “radius” as the data ellipse.\n\n\nCode# calculate conjugate axes for PCA factorization\npca.fac &lt;- function(x) {\n  xx &lt;- svd(x)\n  ret &lt;- t(xx$v) * sqrt(pmax( xx$d,0))\n  ret\n}\n\ndataEllipse(Income ~ Experience, data=workers,\n    pch = 16, cex = 1.5, \n    center.pch = \"+\", center.cex = 2,\n    cex.lab = 1.5,\n    levels = 0.68,\n    grid = FALSE,\n    xlim = c(-10, 40),\n    ylim = c(10, 80),\n    asp = 1)\nabline(h = mu[2], v = mu[1], \n       lty = 2, col = \"grey\")\n\n# axes of the ellipse = PC1, PC2\nradius &lt;- sqrt(2 * qf(0.68, 2, nrow(workers)-1 ))\nheplots::ellipse.axes(S, mu, \n     radius = radius,\n     labels = TRUE,\n     col = \"red\", lwd = 2,\n     cex = 1.8)\n\n# bounding box of the ellipse\nlines(spida2::ellplus(mu, S, radius = radius,\n              box = TRUE, fac = pca.fac),\n      col = \"darkgreen\",\n      lwd = 2, lty=\"longdash\")\n\n\n\n\n\n\nFigure 4.8: Geometry of the PCA for the workers data, showing the data ellipse, the eigenvectors of \\(\\mathbf{S}\\), whose half-lengths are the square roots \\(\\sqrt{\\lambda_i}\\) of the eigenvalues, and the bounding box of the ellipse.\n\n\n\n\nFinally, to preview the methods of the next section, the results calculated “by hand” above can be obtained using prcomp(). The values labeled \"Standard deviations\" are the square roots \\(\\sqrt{\\lambda}_i\\) of the two eigenvalues. The eigenvectors are labeled \"Rotation\" because \\(\\mathbf{V}\\) is the matrix that rotates the data matrix to produce the component scores.\n\nworkers.pca &lt;- prcomp(workers[, vars]) |&gt; \n  print()\n# Standard deviations (1, .., p=2):\n# [1] 18.56  5.01\n# \n# Rotation (n x k) = (2 x 2):\n#              PC1    PC2\n# Experience 0.589  0.808\n# Income     0.808 -0.589\n\n\n\n4.2.3 Finding principal components\nIn R, PCA is most easily carried out using stats::prcomp() or stats::princomp() or similar functions in other packages such as FactomineR::PCA(). The FactoMineR package (Husson et al., 2017; Lê et al., 2008) has extensive capabilities for exploratory analysis of multivariate data (PCA, correspondence analysis, cluster analysis).\nA particular strength of FactoMineR for PCA is that it allows the inclusion of supplementary variables (which can be categorical or quantitative) and supplementary points for individuals. These are not used in the analysis, but are projected into the plots to facilitate interpretation. For example, in the analysis of the crime data described below, it would be useful to have measures of other characteristics of the U.S. states, such as poverty and average level of education (Section 4.3.5).\nUnfortunately, although all of these functions perform similar calculations, the options for analysis and the details of the result they return differ.\nThe important options for analysis include:\n\nwhether or not the data variables are centered, to a mean of \\(\\bar{x}_j =0\\)\n\nwhether or not the data variables are scaled, to a variance of \\(\\text{Var}(x_j) =1\\).\n\nIt nearly always makes sense to center and scale the variables. This choice of scaling determines whether the correlation matrix is analyzed, so that each variable contributes equally to the total variance that is to be accounted for. The unscaled choice analyzes the covariance matrix, where each variable contributes its own variance to the total. Analysis of the covariance matrix makes little sense when the variables are measured in different units2, unless you want to interpret total variance on the scales of the different variables.\nYou don’t need to scale your data in advance, but be aware of the options: prcomp() has default options center = TRUE, scale. = FALSE3 so in most cases you should specify scale. = TRUE. I mostly use this. The older princomp() has only the option cor = FALSE which centers the data and uses the covariance matrix, so in most cases you should set cor = TRUE.\nTo illustrate, the analysis of the workers data presented above used scale. = FALSE by default, so the eigenvalues reflected the variances of Experience and Income. The analogous result, using standardized variables (\\(z\\)-scores) can be computed in any of the forms shown below, using either scale. = FALSE or standardizing first using scale():\n\nprcomp(workers[, vars], scale. = TRUE)\n# Standard deviations (1, .., p=2):\n# [1] 1.361 0.383\n# \n# Rotation (n x k) = (2 x 2):\n#              PC1    PC2\n# Experience 0.707  0.707\n# Income     0.707 -0.707\n\n# same as (output suppressed):\nworkers[, vars] |&gt; prcomp(scale. = TRUE) |&gt; invisible()\nworkers[, vars] |&gt; scale() |&gt; prcomp() |&gt; invisible()\n\nIn this form, each of Experience and Income have variance = 1, and the \"Standard deviations\" reported are the square roots (\\(\\sqrt{\\lambda}_i\\)) of the eigenvalues \\(\\lambda_i\\) of the correlation matrix \\(\\mathbf{R}\\). The eigenvalues of a correlation matrix always sum to \\(p\\), the number of variables. This fact prompted the rough rule of thumb to extract principal components whose eigenvalues exceed 1.0, which is their average value, \\(\\bar{\\lambda} = (\\Sigma^p \\lambda_i) / p = p / p\\).\n\nprcomp(workers[, vars], scale. = TRUE)$sdev\n# [1] 1.361 0.383\n\n# eiven values of correlation matrix\nR &lt;- cor(workers[, vars])\nR.eig &lt;- eigen(R)\nLambda &lt;- R.eig$values |&gt; print()\n# [1] 1.853 0.147\nsum(Lambda)\n# [1] 2\n\nExample: Crime data\nThe dataset crime, analysed in Section 3.8, showed all positive correlations among the rates of various crimes in the corrgram, Figure 3.34. What can we see from a PCA? Is it possible that a few dimensions can account for most of the juice in this data?\nIn this example, you can easily find the PCA solution using prcomp() in a single line in base-R. You need to specify the numeric variables to analyze by their columns in the data frame. The most important option here is scale. = TRUE.\n\ndata(crime, package = \"ggbiplot\")\ncrime.pca &lt;- prcomp(crime[, 2:8], scale. = TRUE)\n\nThe tidy equivalent is more verbose, but also more expressive about what is being done. It selects the variables to analyze by a function, is.numeric() applied to each of the columns and feeds the result to prcomp().\n\ncrime.pca &lt;- \n  crime |&gt; \n  dplyr::select(where(is.numeric)) |&gt;\n  prcomp(scale. = TRUE)\n\nAs is typical with models in R, the result, crime.pca of prcomp() is an object of class \"prcomp\", a list of components, and there are a variety of methods for \"prcomp\" objects. Among the simplest is summary(), which gives the contributions of each component to the total variance in the dataset.\n\nsummary(crime.pca) |&gt; print(digits=2)\n# Importance of components:\n#                         PC1  PC2  PC3   PC4   PC5   PC6   PC7\n# Standard deviation     2.03 1.11 0.85 0.563 0.508 0.471 0.352\n# Proportion of Variance 0.59 0.18 0.10 0.045 0.037 0.032 0.018\n# Cumulative Proportion  0.59 0.76 0.87 0.914 0.951 0.982 1.000\n\nThe object, crime.pca returned by prcomp() is a list of the following the following elements:\n\nnames(crime.pca)\n# [1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"\n\nOf these, for \\(n\\) observations and \\(p\\) variables,\n\n\nsdev is the length \\(p\\) vector of the standard deviations of the principal components (i.e., the square roots \\(\\sqrt{\\lambda_i}\\) of the eigenvalues of the covariance/correlation matrix). When the variables are standardized, the sum of squares of the eigenvalues is equal to \\(p\\).\n\nrotation is the \\(p \\times p\\) matrix of weights or loadings of the variables on the components; the columns are the eigenvectors of the covariance or correlation matrix of the data;\n\nx is the \\(n \\times p\\) matrix of scores for the observations on the components, the result of multiplying (rotating) the data matrix by the loadings. These are uncorrelated, so cov(x) is a \\(p \\times p\\) diagonal matrix whose diagonal elements are the eigenvalues \\(\\lambda_i\\) = sdev^2.\n\ncenter gives the means of the variables when the option center. = TRUE (the default)\n\n4.2.4 Visualizing variance proportions: screeplots\nFor a high-D dataset, such as the crime data in seven dimensions, a natural question is how much of the variation in the data can be captured in 1D, 2D, 3D, … summaries and views. This is answered by considering the proportions of variance accounted by each of the dimensions, or their cumulative values. The components returned by various PCA methods have (confusingly) different names, so broom::tidy() provides methods to unify extraction of these values.\n\n(crime.eig &lt;- crime.pca |&gt; \n  broom::tidy(matrix = \"eigenvalues\"))\n# # A tibble: 7 × 4\n#      PC std.dev percent cumulative\n#   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n# 1     1   2.03   0.588       0.588\n# 2     2   1.11   0.177       0.765\n# 3     3   0.852  0.104       0.868\n# 4     4   0.563  0.0452      0.914\n# 5     5   0.508  0.0368      0.951\n# 6     6   0.471  0.0317      0.982\n# 7     7   0.352  0.0177      1\n\nThen, a simple visualization is a plot of the proportion of variance for each component (or cumulative proportion) against the component number, usually called a screeplot. The idea, introduced by Cattell (1966), is that after the largest, dominant components, the remainder should resemble the rubble, or scree formed by rocks falling from a cliff.\nFrom this plot, imagine drawing a straight line through the plotted eigenvalues, starting with the largest one. The typical rough guidance is that the last point to fall on this line represents the last component to extract, the idea being that beyond this, the amount of additional variance explained is non-meaningful. Another rule of thumb is to choose the number of components to extract a desired proportion of total variance, usually in the range of 80 - 90%.\nstats::plot(crime.pca) would give a bar plot of the variances of the components, however ggbiplot::ggscreeplot() gives nicer and more flexible displays as shown in Figure 4.9.\n\np1 &lt;- ggscreeplot(crime.pca) +\n  stat_smooth(data = crime.eig |&gt; filter(PC&gt;=4), \n              aes(x=PC, y=percent), method = \"lm\", \n              se = FALSE,\n              fullrange = TRUE) +\n  theme_bw(base_size = 14)\n\np2 &lt;- ggscreeplot(crime.pca, type = \"cev\") +\n  geom_hline(yintercept = c(0.8, 0.9), color = \"blue\") +\n  theme_bw(base_size = 14)\n\np1 + p2\n\n\n\n\n\n\nFigure 4.9: Screeplots for the PCA of the crime data. The left panel shows the traditional version, plotting variance proportions against component number, with linear guideline for the scree rule of thumb. The right panel plots cumulative proportions, showing cutoffs of 80%, 90%.\n\n\n\n\nFrom this we might conclude that four components are necessary to satisfy the scree criterion or to account for 90% of the total variation in these crime statistics. However two components, giving 76.5%, might be enough juice to tell a reasonable story.\n\n4.2.5 Visualizing PCA scores and variable vectors\nTo see and attempt to understand PCA results, it is useful to plot both the scores for the observations on a few of the largest components and also the loadings or variable vectors that give the weights for the variables in determining the principal components.\nIn Section 4.3 I discuss the biplot technique that plots both in a single display. However, I do this directly here, using tidy processing to explain what is going on in PCA and in these graphical displays.\nScores\nThe (uncorrelated) principal component scores can be extracted as crime.pca$x or using purrr::pluck(\"x\"). As noted above, these are uncorrelated and have variances equal to the eigenvalues of the correlation matrix.\n\nscores &lt;- crime.pca |&gt; purrr::pluck(\"x\") \ncov(scores) |&gt; zapsmall()\n#      PC1  PC2  PC3  PC4  PC5  PC6  PC7\n# PC1 4.11 0.00 0.00 0.00 0.00 0.00 0.00\n# PC2 0.00 1.24 0.00 0.00 0.00 0.00 0.00\n# PC3 0.00 0.00 0.73 0.00 0.00 0.00 0.00\n# PC4 0.00 0.00 0.00 0.32 0.00 0.00 0.00\n# PC5 0.00 0.00 0.00 0.00 0.26 0.00 0.00\n# PC6 0.00 0.00 0.00 0.00 0.00 0.22 0.00\n# PC7 0.00 0.00 0.00 0.00 0.00 0.00 0.12\n\nFor plotting, it is more convenient to use broom::augment() which extracts the scores (named .fittedPC*) and appends these to the variables in the dataset.\n\ncrime.pca |&gt;\n  broom::augment(crime) |&gt; head()\n# # A tibble: 6 × 18\n#   .rownames state      murder  rape robbery assault burglary larceny\n#   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n# 1 1         Alabama      14.2  25.2    96.8    278.    1136.   1882.\n# 2 2         Alaska       10.8  51.6    96.8    284     1332.   3370.\n# 3 3         Arizona       9.5  34.2   138.     312.    2346.   4467.\n# 4 4         Arkansas      8.8  27.6    83.2    203.     973.   1862.\n# 5 5         California   11.5  49.4   287      358     2139.   3500.\n# 6 6         Colorado      6.3  42     171.     293.    1935.   3903.\n# # ℹ 10 more variables: auto &lt;dbl&gt;, st &lt;chr&gt;, region &lt;fct&gt;,\n# #   .fittedPC1 &lt;dbl&gt;, .fittedPC2 &lt;dbl&gt;, .fittedPC3 &lt;dbl&gt;,\n# #   .fittedPC4 &lt;dbl&gt;, .fittedPC5 &lt;dbl&gt;, .fittedPC6 &lt;dbl&gt;,\n# #   .fittedPC7 &lt;dbl&gt;\n\nThen, we can use ggplot() to plot any pair of components. To aid interpretation, I label the points by their state abbreviation and color them by region of the U.S.. A geometric interpretation of the plot requires an aspect ratio of 1.0 (via coord_fixed()) so that a unit distance on the horizontal axis is the same length as a unit distance on the vertical. To demonstrate that the components are uncorrelated, I also added their data ellipse.\n\ncrime.pca |&gt;\n  broom::augment(crime) |&gt; # add original dataset back in\n  ggplot(aes(.fittedPC1, .fittedPC2, color = region)) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_point(size = 1.5) +\n  geom_text(aes(label = st), nudge_x = 0.2) +\n  stat_ellipse(color = \"grey\") +\n  coord_fixed(ylim = c(-3,3), ratio = 1) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"PC Dimension 1\", y = \"PC Dimension 2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"top\") \n\n\n\n\n\n\nFigure 4.10: Plot of component scores on the first two principal components for the crime data. States are colored by region.\n\n\n\n\nTo interpret such plots, it is useful consider the observations that are a high and low on each of the axes as well as other information, such as region here, and ask how these differ on the crime statistics. The first component, PC1, contrasts Nevada and California with North Dakota, South Dakota and West Virginia. The second component has most of the southern states on the low end and Massachusetts, Rhode Island and Hawaii on the high end. However, interpretation is easier when we also consider how the various crimes contribute to these dimensions.\nWhen, as here, there are more than two components that seem important in the scree plot, we could obviously go further and plot other pairs.\nVariable vectors\nYou can extract the variable loadings using either crime.pca$rotation or purrr::pluck(\"rotation\"), similar to what I did with the scores.\n\ncrime.pca |&gt; purrr::pluck(\"rotation\")\n#             PC1     PC2     PC3     PC4     PC5     PC6     PC7\n# murder   -0.300 -0.6292 -0.1782  0.2321  0.5381  0.2591  0.2676\n# rape     -0.432 -0.1694  0.2442 -0.0622  0.1885 -0.7733 -0.2965\n# robbery  -0.397  0.0422 -0.4959  0.5580 -0.5200 -0.1144 -0.0039\n# assault  -0.397 -0.3435  0.0695 -0.6298 -0.5067  0.1724  0.1917\n# burglary -0.440  0.2033  0.2099  0.0576  0.1010  0.5360 -0.6481\n# larceny  -0.357  0.4023  0.5392  0.2349  0.0301  0.0394  0.6017\n# auto     -0.295  0.5024 -0.5684 -0.4192  0.3698 -0.0573  0.1470\n\nBut note something important in this output: All of the weights for the first component are negative. In PCA, the directions of the eigenvectors are completely arbitrary, in the sense that the vector \\(-\\mathbf{v}_i\\) gives the same linear combination as \\(\\mathbf{v}_i\\), but with its’ sign reversed. For interpretation, it is useful (and usually recommended) to reflect the loadings to a positive orientation by multiplying them by -1. In general, you are free to reflect any of the components for ease of interpretation, and not necessarily if all the signs are negative.\nTo reflect the PCA loadings (multiplying PC1 and PC2 by -1) and get them into a convenient format for plotting with ggplot(), it is necessary to do a bit of processing, including making the row.names() into an explicit variable for the purpose of labeling.\n\n\n\n\n\n\n\nrownames in R\n\n\n\nR software evolved over many years, particularly in conventions for labeling cases in printed output and graphics. In base-R, the convention was that the row.names() of a matrix or data.frame served as observation labels in all printed output and plots, with a default to use numbers 1:n if there were no rownames.\nIn ggplot2 and the tidyverse framework, the decision was made that observation labels had to be an explicit variable in a “tidy” dataset, so it could be used as a variable in constructs like geom_text(aes(label = label)) as in this example. This change often requires extra steps in pre-tidy software that uses the rownames convention.\n\n\n\nvectors &lt;- crime.pca |&gt; \n  purrr::pluck(\"rotation\") |&gt;\n  as.data.frame() |&gt;\n  mutate(PC1 = -1 * PC1, PC2 = -1 * PC2) |&gt;      # reflect axes\n  tibble::rownames_to_column(var = \"label\") \n\nvectors[, 1:3]\n#      label   PC1     PC2\n# 1   murder 0.300  0.6292\n# 2     rape 0.432  0.1694\n# 3  robbery 0.397 -0.0422\n# 4  assault 0.397  0.3435\n# 5 burglary 0.440 -0.2033\n# 6  larceny 0.357 -0.4023\n# 7     auto 0.295 -0.5024\n\nThen, I plot these using geom_segment(), taking some care to use arrows from the origin with a nice shape and add geom_text() labels for the variables positioned slightly to the right. Again, coord_fixed() ensures equal scales for the axes, which is important because we want to interpret the angles between the variable vectors and the PCA coordinate axes.\n\narrow_style &lt;- arrow(\n  angle = 20, ends = \"first\", type = \"closed\", \n  length = grid::unit(8, \"pt\")\n)\n\nvectors |&gt;\n  ggplot(aes(PC1, PC2)) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(xend = 0, yend = 0, \n               linewidth = 1, \n               arrow = arrow_style,\n               color = \"brown\") +\n  geom_text(aes(label = label), \n            size = 5,\n            hjust = \"outward\",\n            nudge_x = 0.05, \n            color = \"brown\") +\n  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 0.5),  color = gray(.50)) +\n  xlim(-0.5, 0.9) + \n  ylim(-0.8, 0.8) +\n  coord_fixed() +         # fix aspect ratio to 1:1\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\nFigure 4.11: Plot of component loadings the first two principal components for the crime data. These are interpreted as the contributions of the variables to the components.\n\n\n\n\nThe variable vectors (arrows) shown in Figure 4.11 have the following interpretations:\n\nThe lengths of the variable vectors, \\(\\lVert\\mathbf{v}_i\\rVert = \\sqrt{\\Sigma_{j} \\; v_{ij}^2}\\) give the relative proportion of variance of each variable accounted for in a two-dimensional display.\nEach vector points in the direction in component space with which that variable is most highly correlated: the value, \\(v_{ij}\\), of the vector for variable \\(\\mathbf{x}_i\\) on component \\(j\\) reflects the correlation of that variable with the \\(j\\)th principal component. Thus,\n\n* A variable that is perfectly correlated with a component is parallel to it.\n* A variable uncorrelated with an component is perpendicular to it.\n\nThe angle between vectors shows the strength and direction of the correlation between those variables: the cosine of the angle \\(\\theta\\) between two variable vectors, \\(\\mathbf{v}_i\\) and \\(\\mathbf{v}_j\\), which is \\(\\cos(\\theta) = \\mathbf{v}_i^\\prime \\; \\mathbf{v}_j \\;/ \\; \\| \\mathbf{v}_i \\| \\cdot \\| \\mathbf{v}_j \\|\\) gives the approximation of the correlation \\(r_{ij}\\) between \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) that is shown in this space. This means that:\n\n* two variable vectors that point in the same direction are highly correlated; $r = 1$ if they are completely aligned.\n* Variable vectors at right angles are approximately uncorrelated, while those\npointing in opposite directions are negatively correlated; \\(r = -1\\) if they are at 180\\(^o\\).\nTo illustrate point (1), the following indicates that almost 70% of the variance of murder is represented in the the 2D plot shown in Figure 4.10, but only 40% of the variance of robbery is captured. For point (2), the correlation of murder with the dimensions is 0.3 for PC1 and 0.63 for PC2. For point (3), the angle between murder and burglary looks to be about 90\\(^o\\), but the actual correlation is 0.39.\n\n\n\n\n\n\n\n\n\nvectors |&gt; select(label, PC1, PC2) |&gt; \n  mutate(length = sqrt(PC1^2 + PC2^2))\n#      label   PC1     PC2 length\n# 1   murder 0.300  0.6292  0.697\n# 2     rape 0.432  0.1694  0.464\n# 3  robbery 0.397 -0.0422  0.399\n# 4  assault 0.397  0.3435  0.525\n# 5 burglary 0.440 -0.2033  0.485\n# 6  larceny 0.357 -0.4023  0.538\n# 7     auto 0.295 -0.5024  0.583",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html#sec-biplot",
    "href": "05-pca-biplot.html#sec-biplot",
    "title": "4  Dimension Reduction",
    "section": "\n4.3 Biplots",
    "text": "4.3 Biplots\nThe biplot is a visual multivariate juicer. It is the simple and powerful idea that came from the recognition that you can overlay a plot of observation scores in a principal components analysis with the information of the variable loadings (weights) to give a simultaneous display that is easy to interpret. In this sense, a biplot is generalization of a scatterplot, projecting from data space to PCA space, where the observations are shown by points, as in the plots of component scores in Figure 4.10, but with the variables also shown by vectors (or scaled linear axes aligned with those vectors).\nThe idea of the biplot was introduced by Ruben Gabriel (1971, 1981) and later expanded in scope by Gower & Hand (1996). The book by Greenacre (2010) gives a practical overview of the many variety of biplots. Gower et al. (2011) Understanding biplots provides a full treatment of many topics, including how to calibrate biplot axes, 3D plots, and so forth.\nBiplot methodolgy is far more general than I cover here. Categorical variables can be incorporated in PCA using points that represent the levels of discrete categories. Two-way frequency tables of categorical variables can be analysed using correspondence analysis, which is similar to PCA, but designed to account for the maximum amount of the \\(\\chi^2\\) statistic for association; multiple correspondence analysis extends this to method to multi-way tables (Friendly & Meyer, 2016; Greenacre, 1984).\n\n4.3.1 Constructing a biplot\nThe biplot is constructed by using the singular value decomposition (SVD) to obtain a low-rank approximation to the data matrix \\(\\mathbf{X}_{n \\times p}\\) (centered, and optionally scaled to unit variances) whose \\(n\\) rows are the observations and whose \\(p\\) columns are the variables.\n\n\n\n\n\n\n\nFigure 4.12: The singular value decomposition expresses a data matrix X as the product of a matrix U of observation scores, a diagonal matrix \\(\\Lambda\\) of singular values and a matrix V of variable weights.\n\n\n\n\nUsing the SVD, the matrix \\(\\mathbf{X}\\), of rank \\(r \\le p\\) can be expressed exactly as:\n\\[\n\\mathbf{X} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{V}'\n                 = \\sum_i^r \\lambda_i \\mathbf{u}_i \\mathbf{v}_i' \\; ,\n\\tag{4.2}\\]\nwhere\n\n\n\\(\\mathbf{U}\\) is an \\(n \\times r\\) orthonormal matrix of uncorrelated observation scores; these are also the eigenvectors of \\(\\mathbf{X} \\mathbf{X}'\\),\n\n\\(\\boldsymbol{\\Lambda}\\) is an \\(r \\times r\\) diagonal matrix of singular values, \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\lambda_r\\), which are also the square roots of the eigenvalues of \\(\\mathbf{X} \\mathbf{X}'\\).\n\n\\(\\mathbf{V}\\) is an \\(r \\times p\\) orthonormal matrix of variable weights and also the eigenvectors of \\(\\mathbf{X}' \\mathbf{X}\\).\n\nThen, a rank 2 (or 3) PCA approximation \\(\\widehat{\\mathbf{X}}\\) to the data matrix used in the biplot can be obtained from the first 2 (or 3) singular values \\(\\lambda_i\\) and the corresponding \\(\\mathbf{u}_i, \\mathbf{v}_i\\) as:\n\\[\n\\mathbf{X} \\approx \\widehat{\\mathbf{X}} = \\lambda_1 \\mathbf{u}_1 \\mathbf{v}_1' + \\lambda_2 \\mathbf{u}_2 \\mathbf{v}_2' \\; .\n\\]\nThe variance of \\(\\mathbf{X}\\) accounted for by each term is \\(\\lambda_i^2\\).\nA biplot is then obtained by overlaying two scatterplots that share a common set of axes and have a between-set scalar product interpretation. Typically, the observations (rows of \\(\\mathbf{X}\\)) are represented as points and the variables (columns of \\(\\mathbf{X}\\)) are represented as vectors from the origin.\nThe scale factor, \\(\\alpha\\) allows the variances of the components to be apportioned between the row points and column vectors, with different interpretations, by representing the approximation \\(\\widehat{\\mathbf{X}}\\) as the product of two matrices,\n\\[\n\\widehat{\\mathbf{X}} = (\\mathbf{U} \\boldsymbol{\\Lambda}^\\alpha) (\\boldsymbol{\\Lambda}^{1-\\alpha} \\mathbf{V}') = \\mathbf{A} \\mathbf{B}'\n\\]\nThis notation uses a little math trick involving a power, \\(0 \\le \\alpha \\le 1\\): When \\(\\alpha = 1\\), \\(\\boldsymbol{\\Lambda}^\\alpha = \\boldsymbol{\\Lambda}^1  =\\boldsymbol{\\Lambda}\\), and \\(\\boldsymbol{\\Lambda}^{1-\\alpha} = \\boldsymbol{\\Lambda}^0  =\\mathbf{I}\\). \\(\\alpha = 1/2\\) gives the diagonal matrix \\(\\boldsymbol{\\Lambda}^{1/2}\\) whose elements are the square roots of the singular values.\nThe choice \\(\\alpha = 1\\) assigns the singular values totally to the left factor; then, the angle between two variable vectors, reflecting the inner product \\(\\mathbf{x}_j^\\mathsf{T}, \\mathbf{x}_{j'}\\) approximates their correlation or covariance, and the distance between the points approximates their Mahalanobis distances. \\(\\alpha = 0\\) gives a distance interpretation to the column display. \\(\\alpha = 1/2\\) gives a symmetrically scaled biplot. TODO: Explain this better.\nWhen the singular values are assigned totally to the left or to the right factor, the resultant coordinates are called principal coordinates and the sum of squared coordinates on each dimension equal the corresponding singular value. The other matrix, to which no part of the singular values is assigned, contains the so-called standard coordinates and have sum of squared values equal to 1.0.\n\n4.3.2 Biplots in R\nThere are a large number of R packages providing biplots. The most basic, stats::biplot(), provides methods for \"prcomp\" and \"princomp\" objects. Among other packages, factoextra (Kassambara & Mundt, 2020), an extension of FactoMineR (Lê et al., 2008), is perhaps the most comprehensive and provides ggplot2 graphics. In addition to biplot methods for quantitative data using PCA (fviz_pca()), it offers biplots for categorical data using correspondence analysis (fviz_ca()) and multiple correspondence analysis (fviz_mca()); factor analysis with mixed quantitative and categorical variables (fviz_famd()) and cluster analysis (fviz_cluster()). The adegraphics package (Dray & Siberchicot, 2025) produces lovely biplots using lattice graphics, but with its own analytic framework.\nHere, I use the ggbiplot package (Vu & Friendly, 2024), which aims to provide a simple interface to biplots within the ggplot2 framework. I also use some convenient utility functions from factoextra.\n\n4.3.3 Example: Crime data\nA basic biplot of the crime data, using standardized principal components and labeling the observation by their state abbreviation is shown in Figure 4.13. The correlation circle reflects the data ellipse of the standardized components. This reminds us that these components are uncorrelated and have equal variance in the display.\n\n\ncrime.pca &lt;- reflect(crime.pca) # reflect the axes\n\nggbiplot(crime.pca,\n   obs.scale = 1, var.scale = 1,\n   labels = crime$st ,\n   circle = TRUE,\n   varname.size = 4,\n   varname.color = \"brown\") +\n  theme_minimal(base_size = 14) \n\n\n\n\n\n\nFigure 4.13: Basic biplot of the crime data. State abbreviations are shown at their standardized scores on the first two dimensions. The variable vectors reflect the correlations of the variables with the biplot dimensions.\n\n\n\n\nIn this dataset the states are grouped by region and we saw some differences among regions in the plot (Figure 4.10) of component scores. ggbiplot() provides options to include a groups = variable, used to color the observation points and also to draw their data ellipses, facilitating interpretation.\n\nggbiplot(crime.pca,\n   obs.scale = 1, var.scale = 1,\n   groups = crime$region,\n   labels = crime$st,\n   labels.size = 4,\n   var.factor = 1.4,\n   ellipse = TRUE, \n   ellipse.prob = 0.5, ellipse.alpha = 0.1,\n   circle = TRUE,\n   varname.size = 4,\n   varname.color = \"black\",\n   clip = \"off\") +\n  labs(fill = \"Region\", color = \"Region\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.direction = 'horizontal', legend.position = 'top')\n\n\n\n\n\n\nFigure 4.14: Enhanced biplot of the crime data, grouping the states by region and adding data ellipses.\n\n\n\n\nThis plot provides what is necessary to interpret the nature of the components and also the variation of the states in relation to these. In this, the data ellipses for the regions provide a visual summary that aids interpretation.\n\nFrom the variable vectors, it seems that PC1, having all positive and nearly equal loadings, reflects a total or overall index of crimes. Nevada, California, New York and Florida are highest on this, while North Dakota, South Dakota and West Virginia are lowest.\nThe second component, PC2, shows a contrast between crimes against persons (murder, assault, rape) at the top and property crimes (auto theft, larceny) at the bottom. Nearly all the Southern states are high on personal crimes; states in the North East are generally higher on property crimes.\nWestern states tend to be somewhat higher on overall crime rate, while North Central are lower on average. In these states there is not much variation in the relative proportions of personal vs. property crimes.\n\nMoreover, in this biplot you can interpret the the value for a particular state on a given crime by considering its projection on the variable vector, where the origin corresponds to the mean, positions along the vector have greater than average values on that crime, and the opposite direction have lower than average values. For example, Massachusetts has the highest value on auto theft, but a value less than the mean. Louisiana and South Carolina on the other hand are highest in the rate of murder and slightly less than average on auto theft.\nThese 2D plots account for only 76.5% of the total variance of crimes, so it is useful to also examine the third principal component, which accounts for an additional 10.4%. The choices = option controls which dimensions are plotted.\n\nggbiplot(crime.pca,\n         choices = c(1,3),\n         obs.scale = 1, var.scale = 1,\n         groups = crime$region,\n         labels = crime$st,\n         labels.size = 4,\n         var.factor = 2,\n         ellipse = TRUE, \n         ellipse.prob = 0.5, ellipse.alpha = 0.1,\n         circle = TRUE,\n         varname.size = 4,\n         varname.color = \"black\",\n         clip = \"off\") +\n  labs(fill = \"Region\", color = \"Region\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.direction = 'horizontal', legend.position = 'top')\n\n\n\n\n\n\nFigure 4.15: Biplot of dimensions 1 & 3 of the crime data, with data ellipses for the regions.\n\n\n\n\nDimension 3 in Figure 4.15 is more subtle. One interpretation is a contrast between larceny, which is a larceny (simple theft) and robbery, which involves stealing something from a person and is considered a more serious crime with an element of possible violence. In this plot, murder has a relatively short variable vector, so does not contribute very much to differences among the states.\n\n4.3.4 Biplot contributions and quality\nTo better understand how much each variable contributes to the biplot dimensions, it is helpful to see information about the variance of variables along each dimension. Graphically, this is nothing more than a measure of the lengths of projections of the variables on each of the dimensions. factoextra::get_pca_var() calculates a number of tables from a \"prcomp\" or similar object.\n\nvar_info &lt;- factoextra::get_pca_var(crime.pca)\nnames(var_info)\n# [1] \"coord\"   \"cor\"     \"cos2\"    \"contrib\"\n\nThe component cor gives correlations of the variables with the dimensions and contrib gives their variance contributions as percents, where each row and column sums to 100.\n\ncontrib &lt;- var_info$contrib\ncbind(contrib, Total = rowSums(contrib)) |&gt;\n  rbind(Total = c(colSums(contrib), NA)) |&gt; \n  round(digits=2)\n#           Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7 Total\n# murder     9.02  39.59   3.18   5.39  28.96   6.71   7.16   100\n# rape      18.64   2.87   5.96   0.39   3.55  59.79   8.79   100\n# robbery   15.75   0.18  24.59  31.14  27.04   1.31   0.00   100\n# assault   15.73  11.80   0.48  39.67  25.67   2.97   3.68   100\n# burglary  19.37   4.13   4.41   0.33   1.02  28.73  42.01   100\n# larceny   12.77  16.19  29.08   5.52   0.09   0.16  36.20   100\n# auto       8.71  25.24  32.31  17.58  13.67   0.33   2.16   100\n# Total    100.00 100.00 100.00 100.00 100.00 100.00 100.00    NA\n\nThese contributions can be visualized as sorted barcharts for a given axis using factoextra::fviz_contrib(). The dashed horizontal lines are at the average value for each dimension.\n\np1 &lt;- fviz_contrib(crime.pca, choice = \"var\", axes = 1,\n                   fill = \"lightgreen\", color = \"black\")\np2 &lt;- fviz_contrib(crime.pca, choice = \"var\", axes = 2,\n                   fill = \"lightgreen\", color = \"black\")\np1 + p2\n\n\n\n\n\n\nFigure 4.16: Contributions of the crime variables to dimensions 1 (left) & 2 (right) of the PCA solution. The dashed horizontal lines show the average contribution for each dimension.\n\n\n\n\nA simple rubric for interpreting the dimensions in terms of the variable contributions is to mention those that are largest or above average on each dimension. So, burglary and rape contribute most to the first dimension, while murder and auto theft contribute most to the second.\nAnother useful measure is called cos2, the quality of representation, meaning how much of a variable is represented in a given component. The columns sum to the eigenvalue for each dimension. The rows each sum to 1.0, meaning each variable is completely represented on all components, but we can find the quality of a \\(k\\)-D solution by summing the values in the first \\(k\\) columns. These can be plotted in a style similar to Figure 4.16 using factoextra::fviz_cos2().\n\nquality &lt;- var_info$cos2\nrowSums(quality)\n#   murder     rape  robbery  assault burglary  larceny     auto \n#        1        1        1        1        1        1        1\n\ncolSums(quality)\n# Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 \n# 4.115 1.239 0.726 0.316 0.258 0.222 0.124\n\ncbind(quality[, 1:2], \n      Total = rowSums(quality[, 1:2])) |&gt;\n  round(digits = 2)\n#          Dim.1 Dim.2 Total\n# murder    0.37  0.49  0.86\n# rape      0.77  0.04  0.80\n# robbery   0.65  0.00  0.65\n# assault   0.65  0.15  0.79\n# burglary  0.80  0.05  0.85\n# larceny   0.53  0.20  0.73\n# auto      0.36  0.31  0.67\n\nIn two dimensions, murder and burglary are best represented; robbery and larceny are the worst, but as we saw above (Figure 4.15), these crimes are implicated in the third dimension.\n\n4.3.5 Supplementary variables\nAn important feature of biplot methodology is that once you have a reduced-rank display of the relations among a set of variables, you can use other available data to help interpret what is shown in the biplot. In a sense, this is what I did above in Figure 4.14 and Figure 4.15 using region as a grouping variable and summarizing the variability in the scores for states with their data ellipses by region.\nWhen we have other quantitative variables on the same observations, these can be represented as supplementary variables in the same space. Geometrically, this amounts to projecting the new variables on the space of the principal components. It is carried out by regressions of these supplementary variables on the scores for the principal component dimensions.\nFor example, the left panel of Figure 4.17 depicts the vector geometry of a regression of a variable \\(\\mathbf{y}\\) on two predictors, \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). The fitted vector, \\(\\widehat{\\mathbf{y}}\\), is the perpendicular projection of \\(\\mathbf{y}\\) onto the plane of \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). In the same way, in the right panel, a supplementary variable is projected into the plane of two principal component axes shown as an ellipse. The black fitted vector shows how that additional variable relates to the biplot dimensions.\n\n\n\n\n\n\n\nFigure 4.17: Fitting supplementary variables in a biplot is analogous (right) to regression on the principal component dimensions (left). Source: Aluja et al. (2018), Figure 2.11\n\n\n\n\nFor this example, it happens that some suitable supplementary variables to aid interpretation of crime rates are available in the dataset datsets::state.x77, which was obtained from the U.S. Bureau of the Census Statistical Abstract of the United States for 1977. I select a few of these below and make the state name a column variable so it can be merged with the crime data.\n\nsupp_data &lt;- state.x77 |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(var = \"state\") |&gt;\n  rename(Life_Exp = `Life Exp`,\n         HS_Grad = `HS Grad`) |&gt;\n  select(state, Income:Life_Exp, HS_Grad) \n\nhead(supp_data)\n#        state Income Illiteracy Life_Exp HS_Grad\n# 1    Alabama   3624        2.1     69.0    41.3\n# 2     Alaska   6315        1.5     69.3    66.7\n# 3    Arizona   4530        1.8     70.5    58.1\n# 4   Arkansas   3378        1.9     70.7    39.9\n# 5 California   5114        1.1     71.7    62.6\n# 6   Colorado   4884        0.7     72.1    63.9\n\nThen, we can merge the crime data with the supp_data dataset to produce something suitable for analysis using factoMineR::PCA().\n\ncrime_joined &lt;-\n  dplyr::left_join(crime[, 1:8], supp_data, by = \"state\")\nnames(crime_joined)\n#  [1] \"state\"      \"murder\"     \"rape\"       \"robbery\"   \n#  [5] \"assault\"    \"burglary\"   \"larceny\"    \"auto\"      \n#  [9] \"Income\"     \"Illiteracy\" \"Life_Exp\"   \"HS_Grad\"\n\nPCA() can only get the labels for the observations from the row.names() of the dataset, so I assign them explicitly. The supplementary variables are specified by the argument quanti.sup as the indices of the columns in what is passed as the data argument.\n\nrow.names(crime_joined) &lt;- crime$st\ncrime.PCA_sup &lt;- PCA(crime_joined[,c(2:8, 9:12)], \n                     quanti.sup = 8:11,\n                     scale.unit=TRUE, \n                     ncp=3, \n                     graph = FALSE)\n\nThe essential difference between the result of prcomp() used earlier to get the crime.pca object and the result of PCA() with supplementary variables is that the crime.PCA_sup object now contains a quanti.sup component containing the coordinates for the supplementary variables in PCA space.\nThese can be calculated directly as a the coefficients of a multivariate regression of the standardized supplementary variables on the PCA scores for the dimensions, with no intercept—which forces the fitted vectors to go through the origin. For example, in the plot below (Figure 4.18), the vector for Income has coordinates (0.192, -0.530) on the first two PCA dimensions.\n\nreg.data &lt;- cbind(scale(supp_data[, -1]), \n                  crime.PCA_sup$ind$coord) |&gt;\n  as.data.frame()\n\nsup.mod &lt;- lm(cbind(Income, Illiteracy, Life_Exp, HS_Grad) ~ \n                    0 + Dim.1 + Dim.2 + Dim.3, \n              data = reg.data )\n\n(coefs &lt;- t(coef(sup.mod)))\n#             Dim.1  Dim.2   Dim.3\n# Income      0.192  0.530  0.0482\n# Illiteracy  0.112 -0.536  0.1689\n# Life_Exp   -0.131  0.649 -0.2158\n# HS_Grad     0.103  0.610 -0.4095\n\nNote that, because the supplementary variables are standardized, these coefficients are the same as the correlations between the supplementary variables and the scores on the principal components, up to a scaling factor for each dimension. This provides a general way to relate dimensions found in other methods to the original data variables using vectors as in biplot techniques.\n\n(R &lt;- cor(reg.data[, 1:4], reg.data[, 5:7]))\n#             Dim.1  Dim.2   Dim.3\n# Income      0.393  0.596  0.0415\n# Illiteracy  0.230 -0.602  0.1453\n# Life_Exp   -0.268  0.730 -0.1857\n# HS_Grad     0.211  0.686 -0.3524\n\nR / coefs\n#            Dim.1 Dim.2 Dim.3\n# Income      2.05  1.12 0.861\n# Illiteracy  2.05  1.12 0.861\n# Life_Exp    2.05  1.12 0.861\n# HS_Grad     2.05  1.12 0.861\n\nThe PCA() result can then be plotted using FactoMiner::plot() or various factoextra functions like fviz_pca_var() for a plot of the variable vectors or fviz_pca_biplot() for a biplot. When a quanti.sup component is present, supplementary variables are also shown in the displays.\nFor simplicity I use FactoMiner::plot() here and only show the variable vectors. For consistency with earlier plots, I first reflect the orientation of the 2nd PCA dimension so that crimes of personal violence are at the top, as in Figure 4.11.\n\n# reverse coordinates of Dim 2\ncrime.PCA_sup &lt;- ggbiplot::reflect(crime.PCA_sup, columns = 2)\nplot(crime.PCA_sup, choix = \"var\")\n\n\n\n\n\n\nFigure 4.18: PCA plot of variables for the crime data, with vectors for the supplementary variables showing their association with the principal component dimensions.\n\n\n\n\nRecall that from earlier analyses, I interpreted the the dominant PC1 dimension as reflecting overall rate of crime. The contributions to this dimension, which are the projections of the variable vectors on the horizontal axis in Figure 4.11 and Figure 4.14 were shown graphically by barcharts in the left panel of Figure 4.16.\nBut now in Figure 4.18, with the addition of variable vectors for the supplementary variables, you can see how income, rate of illiteracy, life expectancy and proportion of high school graduates are related to the variation in rates of crimes for the U.S. states.\nOn dimension 1, what stands out is that life expectancy is associated with lower overall crime, while other supplementary variable have positive associations. On dimension 2, crimes against persons (murder, assault, rape) are associated with greater rates of illiteracy among the states, which as we earlier saw (Figure 4.14) were more often Southern states. Crimes against property (auto theft, larceny) at the bottom of this dimension are associated with higher levels of income and high school graduates.\n\n4.3.6 Example: Diabetes data\nAs another example, consider the data from Reaven & Miller (1979) on measures of insulin and glucose shown in Figure 6 and that led to the discovery of two distinct types of development of Type 2 diabetes (Section 5). This dataset is available as Diabetes. The three groups are Normal, Chemical_Diabetic and Overt_Diabetic, and the (numerical) diagnostic variables are:\n\n\nrelwt: relative weight, the ratio of actual to expected weight, given the person’s height,\n\nglufast: fasting blood plasma glucose level\n\nglutest: test blood plasma glucose level, a measure of glucose intolerance\n\ninstest: plasma insulin during test, a measure of insulin response to oral glucose\n\nsspg: steady state plasma glucose, a measure of insulin resistance\n\nTODO: Should introduce 3D plots earlier, in Ch3 before Section 3.7.\nFirst, let’s try to create a 3D plot, analogous to the artist’s drawing from PRIM-9 shown in Figure 7. For this, I use car::scatter3d() which can show data ellipsoids summarizing each group. The formula notation, z ~ x + y assigns the z variable to the vertical direction in the plot, and the x and y variable form a base plane. \n\ncols &lt;- c(\"darkgreen\", \"blue\", \"red\")\nscatter3d(sspg ~ instest + glutest, data=Diabetes,\n          groups = Diabetes$group,\n          ellipsoid = TRUE,\n          surface = FALSE,\n          col = cols,\n          surface.col = cols)\n\ncar::scatter3d() uses the rgl package (Murdoch & Adler, 2025) to render 3D graphics on a display device, which means that it has facilities for perspective, lighting and other visual properties. You can interactively zoom in or out or rotate the display in any of the three dimensions and use rgl::spin3d() to animate rotations around any axes and record this a a movie3d(). Figure 4.19 shows two views of this plot, one from the front and one from the back. The data ellipsoids are not as evocative as the artist’s rendering, but they give a sense of the relative sizes and shapes of the clouds of points for the three diagnostic groups.\n\n\n\n\n\n\n\nFigure 4.19: Two views of a 3D scatterplot of three main diagnostic variables in the Diabetes dataset. The left panel shows an orientation similar to that of Figure 7; the right panel shows a view from the back.\n\n\n\n\nThe normal group is concentrated near the origin, with relatively low values on all three diagnostic measures. The chemical diabetic group forms a wing with higher values on insulin response to oral glucose (instest), while the overt diabetics form the other wing, with higher values on glucose intolerance (glutest). The relative sizes and orientations of the data ellipsoids are also informative.\nGiven this, what can we see in a biplot view based on PCA? The PCA of these data shows that 83% of the variance is captured in two dimensions and 96% in three. The result for 3D is interesting, in that the view from PRIM-9 shown in Figure 7 and Figure 4.19 nearly captured all available information. \n\ndata(Diabetes, package=\"heplots\")\n\ndiab.pca &lt;- \n  Diabetes |&gt; \n  dplyr::select(where(is.numeric)) |&gt;\n  prcomp(scale. = TRUE)\nsummary(diab.pca)\n# Importance of components:\n#                          PC1   PC2   PC3    PC4     PC5\n# Standard deviation     1.662 1.177 0.818 0.3934 0.17589\n# Proportion of Variance 0.552 0.277 0.134 0.0309 0.00619\n# Cumulative Proportion  0.552 0.829 0.963 0.9938 1.00000\n\nA 2D biplot, with data ellipses for the groups, can be produced as before, but I also want to illustrate labeling the groups directly, rather than in a legend.\n\nplt &lt;- ggbiplot(diab.pca,\n     obs.scale = 1, var.scale = 1,\n     groups = Diabetes$group,\n     var.factor = 1.4,\n     ellipse = TRUE, \n     ellipse.prob = 0.5, ellipse.alpha = 0.1,\n     circle = TRUE,\n     point.size = 2,\n     varname.size = 4) +\n  labs(fill = \"Group\", color = \"Group\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\nThen, find the centroids of the component scores and use geom_label() to plot the group labels.\n\nscores &lt;- data.frame(diab.pca$x[, 1:2], group = Diabetes$group)\ncentroids &lt;- scores |&gt;\n  group_by(group) |&gt;\n  summarize(PC1 = mean(PC1),\n            PC2 = mean(PC2))\n\nplt + geom_label(data = centroids, \n                 aes(x = PC1, y = PC2, \n                     label=group, color = group),\n                 nudge_y = 0.2)\n\n\n\n\n\n\nFigure 4.20: 2D biplot of the Diabetes data\n\n\n\n\nWhat can we see here, and how does it relate to the artist’s depiction shown earlier in Figure 7? The variables instest, sspg and glutest correspond approximately to the coordinate axes in the artist’s drawing. glutest and glufast primarily separate the overt diabetics from the others. The chemical diabetics are distinguished by having larger values of insulin response (instest) and are also higher in relative weight (relwt).",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html#sec-nonlinear",
    "href": "05-pca-biplot.html#sec-nonlinear",
    "title": "4  Dimension Reduction",
    "section": "\n4.4 Nonlinear dimension reduction",
    "text": "4.4 Nonlinear dimension reduction\nThe world of dimension reduction methods reflected by PCA is a simple and attractive one in which relationships among variable are at least approximately linear, and can be made visible in a lower-dimensional view by linear transformations and projections. PCA does an optimal job of capturing global linear relationships in the data. But many phenomena defy linear description or involve local nonlinear relationships and clusters within the data. Our understanding of high-D data can sometimes be improved by nonlinear dimension reduction techniques.\nTo see why, consider the data shown in the left panel of Figure 4.21 and suppose we want to be able to separate the two classes by a line. The groups are readily seen in this simple 2D example, but there is no linear combination or projection that shows them as distinct categories. The right panel shows the same data after a nonlinear transformation to polar coordinates, where the two groups are readily distinguished by radius. Such problems arise in higher dimensions where direct visualization is far more difficult and nonlinear methods become attractive.\n\n\n\n\n\n\n\nFigure 4.21: Nonlinear patterns: Two representations of the same data are shown. In the plot at the left, the clusters are clear to the eye, but there is no linear relation that separates them. Transforming the data nonlinearly, to polar coordinates in the plot at the right, makes the two groups distinct.\n\n\n\n\n\n4.4.1 Multidimensional scaling\nOne way to break out of the “linear-combination, maximize-variance PCA” mold is to consider a more intrinsic property of points in Spaceland: similarity or distance. The earliest expression of this idea was in multidimensional scaling (MDS) by Torgerson (1952), which involved trying to determine a metric low-D representation of objects from their interpoint distances via an application of the SVD.\nThe break-through for nonlinear methods came from Roger Shepard and William Kruskal (Kruskal, 1964; Shepard, 1962a, 1962b) who recognized that a more general, nonmetric version (nMDS) could be achieved using only the rank order of input distances \\(d_{ij}\\) among objects. nMDS maps these into a low-D spatial representation of points, \\(\\mathbf{x}_i, \\mathbf{x}_j\\) whose fitted distances, \\(\\hat{d}_{ij} = \\lVert\\mathbf{x}_i - \\mathbf{x}_j\\rVert\\) matches the order of the \\(d_{ij}\\) as closely as possible. That is, rather than assume that the observed distances are linearly related to the fitted \\(\\hat{d}_{ij}\\), nMDS assumes only that their order is the same. Borg & Groenen (2005) and Borg et al. (2018) give a comprehensive overview of modern developments in MDS.\nThe impetus for MDS stemmed largely from psychology and the behavioral sciences, where simple experimental measures of similarity or dissimilarity of psychological objects (color names, facial expressions, words, Morse code symbols) could be obtained by direct ratings, confusions, or other tasks (Shepard et al., 1972b, 1972a). MDS was revolutionary in that it provided a coherent method to study the dimensions of perceptual and cognitive space in applications where the explanation of a cognitive process was derived directly from an MDS solution (Shoben, 1983).\nTo perform nMDS, you need to calculate the matrix of distances between all pairs of observations (dist()). The basic function is MASS::isoMDS().4 In the call, you can specify the number of dimensions (k) desired, with k=2 as default. It returns the coordinates in a dataset called points.\n\ndiab.dist &lt;- dist(Diabetes[, 1:5])\nmds &lt;- diab.dist |&gt;\n  MASS::isoMDS(k = 2, trace = FALSE) |&gt;\n  purrr::pluck(\"points\") \n\ncolnames(mds) &lt;- c(\"Dim1\", \"Dim2\")\nmds &lt;- bind_cols(mds, group = Diabetes$group)\nmds |&gt; sample_n(6)\n# # A tibble: 6 × 3\n#     Dim1   Dim2 group            \n#    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;            \n# 1 -213.  -42.1  Normal           \n# 2  191.   47.3  Overt_Diabetic   \n# 3   12.0 -63.2  Overt_Diabetic   \n# 4   25.0 -38.1  Chemical_Diabetic\n# 5  774.    9.44 Overt_Diabetic   \n# 6   79.0 136.   Overt_Diabetic\n\nThe method works by trying to minimize a measure, “Stress”, of the average difference between the fitted distances \\(\\hat{d}_{ij}\\) and an optimal monotonic (order-preserving) transformation, \\(f_{\\text{mon}}(d_{ij})\\), of the distances in the data. Values of Stress around 5-8% and smaller are generally considered adequate.\nUnlike PCA, where you can fit all possible dimensions once and choose the number of components to retain by examining the eigenvalues or variance proportions, in MDS it is necessary to fit the data for several values of k and consider the trade-off between goodness of fit and complexity.\n\nstress &lt;- vector(length = 5)\nfor(k in 1:5){\n  res &lt;- MASS::isoMDS(diab.dist, k=k, trace = FALSE)\n  stress[k] &lt;- res$stress\n}\nround(stress, 3)\n# [1] 17.755  3.525  0.256  0.000  0.000\n\nPlotting these shows that a 3D solution is nearly perfect, while a 2D solution is certainly adequate. This plot is the MDS analog of a screeplot for PCA.\n\nplot(stress, type = \"b\", pch = 16, cex = 2,\n     xlab = \"Number of dimensions\",\n     ylab = \"Stress (%)\")\n\n\n\n\n\n\nFigure 4.22: Badness of fit (Stress) of the MDS solution in relation to number of dimensions.\n\n\n\n\nTo plot the 2D solution, I’ll use ggpubr::ggscatter() here because it handles grouping, provides concentration ellipses and other graphical features.\n\nlibrary(ggpubr)\ncols &lt;- scales::hue_pal()(3) |&gt; rev()\nmplot &lt;-\nggscatter(mds, x = \"Dim1\", y = \"Dim2\", \n          color = \"group\",\n          shape = \"group\",\n          palette = cols,\n          size = 2,\n          ellipse = TRUE,\n          ellipse.level = 0.5,\n          ellipse.type = \"t\") +\n  geom_hline(yintercept = 0, color = \"gray\") +\n  geom_vline(xintercept = 0, color = \"gray\") \n\nFor this and other examples using MDS, it would be nice to also show how the dimensions of this space relate to the original variables, as in a biplot. Using the idea of correlations between variables and dimensions from Section 4.3.5, I do this as shown below. Only the relative directions and lengths of the variable vectors matter, so you can choose any convenient scale factor to make the vectors fill the plot region.\n\nvectors &lt;- cor(Diabetes[, 1:5], mds[, 1:2])\nscale_fac &lt;- 500\nmplot + \n  coord_fixed() +\n  geom_segment(data=vectors,\n               aes(x=0, xend=scale_fac*Dim1, y=0, yend=scale_fac*Dim2),\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"),\n               linewidth = 1.1) +\n  geom_text(data = vectors,\n            aes(x = 1.15*scale_fac*Dim1, y = 1.07*scale_fac*Dim2, \n                label=row.names(vectors)),\n            nudge_x = 4,\n            size = 4) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(.8, .8))\n\n\n\n\n\n\nFigure 4.23: Nonmetric MDS representation of the Diabetes data. The vectors reflect the correlations of the variables with the MDS dimensions.\n\n\n\n\nThe configuration of the groups in Figure 4.23 is similar to that of the biplot in Figure 4.20, but the groups are more widely separated along the first MDS dimension. The variable vectors are also similar, except that relwt is not well-represented in the MDS solution.\n\n4.4.2 t-SNE\nWith the rise of “machine learning” methods for “feature extraction” in “supervised” vs. “unsupervised” settings, a variety of new algorithms have been proposed for the task of finding low-D representations of high-D data. Among these, t-distributed Stochastic Neighbor Embedding (t-SNE) developed by Maaten & Hinton (2008) is touted as method for revealing local structure and clustering better in possibly complex high-D data and at different scales.\nt-SNE differs from MDS in what it tries to preserve in the mapping to low-D space: Multidimensional scaling aims to preserve the distances between pairs of data points, focusing on pairs of distant points in the original space. t-SNE, on the other hand focuses on preserving neighboring data points. Data points that are close in the original data space will be tight in the t-SNE embeddings.\n\n“The t-SNE algorithm models the probability distribution of neighbors around each point. Here, the term neighbors refers to the set of points which are closest to each point. In the original, high-dimensional space, this is modeled as a Gaussian distribution. In the 2-dimensional output space this is modeled as a \\(t\\)-distribution. The goal of the procedure is to find a mapping onto the 2-dimensional space that minimizes the differences between these two distributions over all points. The fatter tails of a \\(t\\)-distribution compared to a Gaussian help to spread the points more evenly in the 2-dimensional space.” (Jake Hoare, How t-SNE works and Dimensionality Reduction).\nt-SNE also uses the idea of mapping distance measures into a low-D space, but converts Euclidean distances into conditional probabilities. Stochastic neighbor embedding means that t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability.\nAs van der Maaten and Hinton explained: “The similarity of datapoint \\(\\mathbf{x}_{j}\\) to datapoint \\(\\mathbf{x}_{i}\\) is the conditional probability, \\(p_{j|i}\\), that \\(\\mathbf{x}_{i}\\) would pick \\(\\mathbf{x}_{j}\\) as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian distribution centered at \\(\\mathbf{x}_{i}\\).” For \\(i \\ne j\\), they define:\n\\[\np_{j\\mid i} = \\frac{\\exp(-\\lVert\\mathbf{x}_i - \\mathbf{x}_j\\rVert^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\lVert\\mathbf{x}_i - \\mathbf{x}_k\\rVert^2 / 2\\sigma_i^2)} \\;.\n\\] and set \\(p_{i\\mid i} = 0\\). \\(\\sigma^2_i\\) is the variance of the normal distribution that centered on datapoint \\(\\mathbf{x}_{i}\\) and serves as a tuning bandwidth so smaller values of \\(\\sigma _{i}\\) are used in denser parts of the data space. These conditional probabilities are made symmetric via averaging, giving \\(p_{ij} = \\frac{p_{j\\mid i} + p_{i\\mid j}}{2n}\\).\nt-SNE defines a similar probability distribution \\(q_{ij}\\) over the points \\(\\mathbf{y}_i\\) in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map,\n\\[\nD_\\mathrm{KL}\\left(P \\parallel Q\\right) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} \\; ,\n\\] a measure of how different the distribution of \\(P\\) in the data is from that of \\(Q\\) in the low-D representation. The t in t-SNE comes from the fact that the probability distribution of the points \\(\\mathbf{y}_i\\) in the embedding space is taken to be a heavy-tailed \\(t_{(1)}\\) distribution with one degree of freedom to spread the points more evenly in the 2-dimensional space, rather than the Gaussian distribution for the points in the high-D data space.\nt-SNE is implemented in the Rtsne package (Krijthe, 2023) which is capable of handling thousands of points in very high dimensions. It uses a tuning parameter, “perplexity” to choose the bandwidth \\(\\sigma^2_i\\) for each point. This value effectively controls how many nearest neighbors are taken into account when constructing the embedding in the low-dimensional space. It can be thought of as the means to balance between preserving the global and the local structure of the data.5\nRtsne::Rtsne() finds the locations of the points in the low-D space, of dimension k=2 by default. It returns the coordinates in a component named Y. The package has no print(), summary() or plot methods, so you’re on your own.\n\nlibrary(Rtsne)\nset.seed(123) \ndiab.tsne &lt;- Rtsne(Diabetes[, 1:5], scale = TRUE)\ndf2 &lt;- data.frame(diab.tsne$Y, group = Diabetes$group) \ncolnames(df2) &lt;- c(\"Dim1\", \"Dim2\", \"group\")\n\nYou can plot this as shown below:\n\n\np2 &lt;- ggplot(df2, aes(x=Dim1, y=Dim2, color = group, shape=group)) + \n  geom_point(size = 3) + \n  stat_ellipse(level = 0.68, linewidth=1.1) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  scale_color_manual(values = cols) +\n  labs(x = \"Dimension 1\",\n       y = \"Dimension 2\") + \n  ggtitle(\"tSNE\") +\n  theme_bw(base_size = 16) +\n  theme(legend.position = \"bottom\") \np2\n\n\n\n\n\n\nFigure 4.24: t-SNE representation of the Diabetes data with data ellipses for the diagnostic groups.\n\n\n\n\n\n4.4.2.1 Comparing solutions\nFor the Diabetes data, I’ve shown the results of three different dimension reduction techniques, PCA (Figure 4.20), MDS (Figure 4.23), and t-SNE (Figure 4.24). How are these similar, and how do they differ?\nOne way is to view them side by side as shown in Figure 4.25. To an initial glance, the t-SNE solution looks like a rotated version of the PCA solution, but there are differences in the shapes of the clusters as well.\n\n\n\n\n\n\n\n\nFigure 4.25: Comparison of the PCA and t-SNE 2D representations of the Diabetes data.\n\n\n\n\nAnother way to compare these two views is to animate the transition from the PCA to the t-SNE representation by a series of smooth interpolated views. This is a more generally useful visualization technique, so it is useful to spell out the details.\nThe essential idea is calculate interpolated views as a weighted average of the two endpoints using a weight \\(\\gamma\\) that is varied from 0 to 1.\n\\[\n\\mathbf{X}_{\\text{View}} = \\gamma \\;\\mathbf{X}_{\\text{PCA}} + (1-\\gamma) \\;\\mathbf{X}_{\\text{t-SNE}}\n\\] The same idea can be applied to other graphical features: lines, paths (ellipses), and so forth. These methods are implemented in the gganimate package (Pedersen & Robinson, 2025).\nIn this case, to create an animation you can extract the coordinates for the PCA, \\(\\mathbf{X}_{\\text{PCA}}\\), as a data.frame df1, and those for the t-SNE, \\(\\mathbf{X}_{\\text{t-SNE}}\\) as df2, each with a constant method variable. These two are then stacked (using rbind()) to give a combined df3. The animation can then interpolate over method going from pure PCA to pure t-SNE.\n\ndiab.pca &lt;- prcomp(Diabetes[, 1:5], scale = TRUE, rank.=2) \ndf1 &lt;- data.frame(diab.pca$x, group = Diabetes$group) \ncolnames(df1) &lt;- c(\"Dim1\", \"Dim2\", \"group\")\ndf1 &lt;- cbind(df1, method=\"PCA\")\n\nset.seed(123) \ndiab.tsne &lt;- Rtsne(Diabetes[, 1:5], scale = TRUE)\ndf2 &lt;- data.frame(diab.tsne$Y, group = Diabetes$group) \ncolnames(df2) &lt;- c(\"Dim1\", \"Dim2\", \"group\")\ndf2 &lt;- cbind(df2, method=\"tSNE\")\n\n# stack the PCA and t-SNE solutions\ndf3 &lt;- rbind(df1, df2) \n\nThen, plot the configuration of the points and add data ellipses as before. The key thing for animating the difference between the solutions is to add transition_states(method, ...), tweening from PCA to t-SNE. The state_length argument transition_states() controls the relative length of the pause between states.\nThis animated graphic is shown only in the online version of the book.\n\nlibrary(gganimate)\nanimated_plot &lt;- \n  ggplot(df3, aes(x=Dim1, y=Dim2, color=group, shape=group)) + \n  geom_point(size = 3) + \n  stat_ellipse(level = 0.68, linewidth=1.1) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  scale_color_manual(values = cols) +\n  labs(title = \"PCA vs. tSNE Dimension Reduction: {closest_state}\",\n       subtitle = \"Frame {frame} of {nframes}\",\n       x = \"Dimension 1\",\n       y = \"Dimension 2\") + \n  transition_states( method, transition_length = 3, state_length = 2 ) + \n  view_follow() + \n  theme_bw(base_size = 16) +\n  theme(legend.position = \"bottom\") \n\nanimated_plot\n\n\n\n\n\n\n\n\nFigure 4.26: Animation of the relationship of PCA to the t-SNE embedding for the Diabetes data. The method name in the title reflects the closest state\n\n\n\n\nYou can see that the PCA configuration is morphed into the that for t-SNE largely by rotation 90\\(^o\\) clockwise, so that dimension 1 in PCA becomes dimension 2 in t-SNE. This is not unexpected, because PCA finds the dimensions in to order of maximum variance, whereas t-SNE is only trying to match the distances in the data to those in the solution. To interpret the result from t-SNE you are free to interchange the axes, or indeed to rotate the solution arbitrarily.\nIt is more interesting that the sizes and shapes of the group clusters change from one solution to the other. The normal group is most compact in the PCA solution, but becomes the least compact in t-SNE.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html#sec-var-order",
    "href": "05-pca-biplot.html#sec-var-order",
    "title": "4  Dimension Reduction",
    "section": "\n4.5 Application: Variable ordering for data displays",
    "text": "4.5 Application: Variable ordering for data displays\nIn many multivariate data displays, such as scatterplot matrices, parallel coordinate plots and others reviewed in Chapter 3, the order of different variables might seem arbitrary. They might appear in alphabetic order, or more often in the order they appear in your dataset, for example when you use pairs(mydata). Yet, the principle of effect ordering (Friendly & Kwan (2003)) for variables says you should try to arrange the variables so that adjacent ones are as similar as possible.6\nFor example, the mtcars dataset contains data on 32 automobiles from the 1974 U.S. magazine Motor Trend and consists of fuel comsumption (mpg) and 10 aspects of automobile design (cyl: number of cyliners; hp: horsepower, wt: weight) and performance (qsec: time to drive a quarter-mile). What can we see from a simple corrplot() of their correlations? No coherent pattern stands out in Figure 4.27.\n\ndata(mtcars)\nlibrary(corrplot)\nR &lt;- cor(mtcars)\ncorrplot(R, \n         method = 'ellipse',\n         title = \"Dataset variable order\",\n         tl.srt = 0, tl.col = \"black\", tl.pos = 'd',\n         mar = c(0,0,1,0))\n\n\n\n\n\n\nFigure 4.27: Corrplot of mtcars data, with the variables arranged in the order they appear in the dataset.\n\n\n\n\nIn this display you can scan the rows and columns to “look up” the sign and approximate magnitude of a given correlation; for example, the correlation between mpg and cyl appears to be about -0.9, while that between mpg and gear is about 0.5. Of course, you could print the correlation matrix to find the actual values (-0.86 and 0.48 respectively):\n\nprint(floor(100*R))\n#      mpg cyl disp  hp drat  wt qsec  vs  am gear carb\n# mpg  100 -86  -85 -78   68 -87   41  66  59   48  -56\n# cyl  -86 100   90  83  -70  78  -60 -82 -53  -50   52\n# disp -85  90  100  79  -72  88  -44 -72 -60  -56   39\n# hp   -78  83   79 100  -45  65  -71 -73 -25  -13   74\n# drat  68 -70  -72 -45  100 -72    9  44  71   69  -10\n# wt   -87  78   88  65  -72 100  -18 -56 -70  -59   42\n# qsec  41 -60  -44 -71    9 -18  100  74 -23  -22  -66\n# vs    66 -82  -72 -73   44 -56   74 100  16   20  -57\n# am    59 -53  -60 -25   71 -70  -23  16 100   79    5\n# gear  48 -50  -56 -13   69 -59  -22  20  79  100   27\n# carb -56  52   39  74  -10  42  -66 -57   5   27  100\n\nBecause the angles between variable vectors in the biplot reflect their correlations, Friendly & Kwan (2003) defined principal component variable ordering as the order of angles, \\(a_i\\) of the first two eigenvectors, \\(\\mathbf{v}_1, \\mathbf{v}_2\\) around the unit circle. These values are calculated going counter-clockwise from the 12:00 position as:\n\\[\na_i =\n  \\begin{cases}\n    \\tan^{-1} (v_{i2}/v_{i1}), & \\text{if $v_{i1}&gt;0$;}\n     \\\\\n    \\tan^{-1} (v_{i2}/v_{i1}) + \\pi, & \\text{otherwise.}\n  \\end{cases}     \n\\tag{4.3}\\]\nIn Equation 4.3 \\(\\tan^{-1}(x)\\) is read as “the angle whose tangent is \\(x\\)”, and so the angles are determined by the tangent ratios “opposite” / “adjacent” = \\(v_{i2} / v_{i1}\\) in the right triangle defined by the vector and the horizontal axis.\n\nFor the mtcars data the biplot in Figure 4.28 accounts for 84% of the total variance so a 2D representation is fairly good. The plot shows the variables as widely dispersed. There is a collection at the left of positively correlated variables and another positively correlated set at the right.\n\nmtcars.pca &lt;- prcomp(mtcars, scale. = TRUE)\nggbiplot(mtcars.pca,\n         circle = TRUE,\n         point.size = 2.5,\n         varname.size = 6,\n         varname.color = \"brown\") +\n  theme_minimal(base_size = 14) \n\n\n\n\n\n\nFigure 4.28: Biplot of the mtcars data. The order of the variables around the circle, starting from “gear” (say) arranges them so that the most similar variables are adjacent in graphical displays.\n\n\n\n\nIn corrplot() principal component variable ordering is implemented using the order = \"AOE\" option. There are a variety of other methods based on hierarchical cluster analysis described in the package vignette.\nFigure 4.29 shows the result of ordering the variables by this method. A nice feature of corrplot() is the ability to manually highlight blocks of variables that have a similar pattern of signs by outlining them with rectangles. From the biplot, the two main clusters of positively correlated variables seemed clear, and are outlined in the plot using corrplot::corrRect(). What became clear in the corrplot is that qsec, the time to drive a quarter-mile from a dead start didn’t quite fit this pattern, so I highlighted it separately.\n\ncorrplot(R, \n         method = 'ellipse', \n         order = \"AOE\",\n         title = \"PCA variable order\",\n         tl.srt = 0, tl.col = \"black\", tl.pos = 'd',\n         mar = c(0,0,1,0)) |&gt;\n  corrRect(c(1, 6, 7, 11))\n\n\n\n\n\n\nFigure 4.29: Corrplot of mtcars data, with the variables ordered according to the variable vectors in the biplot.\n\n\n\n\nBut wait, there is something else to be seen in Figure 4.29. Can you see one cell that doesn’t fit with the rest?\nYes, the correlation of number of forward gears (gear) and number of carburators (carb) in the upper left and lower right corners stands out as moderately positive (0.27) while all the others in their off-diagonal blocks are negative. This is another benefit of effect ordering: when you arrange the variables so that the most highly related variable are together, features that deviate from dominant pattern become visible.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html#sec-eigenfaces",
    "href": "05-pca-biplot.html#sec-eigenfaces",
    "title": "4  Dimension Reduction",
    "section": "\n4.6 Application: Eigenfaces",
    "text": "4.6 Application: Eigenfaces\nThere are many applications of principal components analysis beyond the use for visualization for multivariate data covered here, that rely on its’ ability as a dimension reduction technique, that is, to find a low-dimensional approximation to a high-dimensional dataset.\n\n\n\n\n\n\nMachine learning uses\n\n\n\nIn machine learning, for example, PCA is a method used to reduce model complexity and avoid overfitting by feature extraction, which amounts to fitting a response variable in a low-D space of the predictors. This is just another name for principal components regression, where, instead of regressing the dependent variable on all the explanatory variables directly, a smaller number principal components of the explanatory variables is used as predictors. This has the added benefit that it avoids problems of collinearity (section-ref) due to high correlations of the predictors, because the principal component scores are necessarily uncorrelated. When the goal is model explanation rather than pure prediction, it has the disadvantage that the components may be hard to interpret.\n\n\nAn interesting class of problems have to do with image processing, where an image of size width \\(\\times\\) height in pixels can be represented by a \\(w \\times h\\) array of greyscale values \\(x_{ij}\\) in the range of [0, 1] or \\(h \\times w \\times 3\\) array \\(x_{ijk}\\) of (red, green, blue) color values. For example a single \\(640 \\times 640\\) photo is comprised of about 400K pixels in B/W and 1200K pixels in color.\nThe uses here include\n\n\nImage compression: a process applied to a graphics file to minimize its size in bytes for storage or transmission, without degrading image quality below an acceptable threshold\n\nimage enhancement: improving the quality of an image, with applications in Computer Vision tasks, remote sensing, and satellite imagery.\n\nfacial recognition: classifying or matching a facial image against a large corpus of stored images.\n\nWhen PCA is used on facial images, you can think of the process as generating eigenfaces (Turk & Pentland (1991)) a representation of the pixels in the image in terms of an eigenvalue decomposition. Dimension reduction means that a facial image can be considerably compressed by removing the components associated with small dimensions.\nAs an example, consider the black and white version of the Mona Lisa shown in Figure 4.30. The idea and code for this example is adapted from this blog post by Kieran Healy.7\n\n\n\n\n\n\n\n\nFigure 4.30: 640 x 954 black and white image of the Mona Lisa. Source: Wikipedia\n\n\n\n\nIt would take too long to explain the entire method, so I’ll just sketch the essential parts here. The complete script for this example is contained in PCA-MonaLisa.R. …\n\nAn image can be imported using imager::load.image() which creates a \"cimg\" object, a 4-dimensional array with dimensions named x,y,z,c. x and y are the usual spatial dimensions, z is a depth dimension (which would correspond to time in a movie), and c is a color dimension containing R, G, B values.\n\nlibrary(imager)\nimg &lt;- imager::load.image(here::here(\"images\", \"MonaLisa-BW.jpg\"))\ndim(img)\n# [1] 640 954   1   1\n\n\nAn as.data.frame() method converts this to a data frame with x and y coordinates. Each x-y pair is a location in the 640 by 954 pixel grid, and the value is a grayscale value ranging from zero to one.\n\nimg_df_long &lt;- as.data.frame(img)\nhead(img_df_long)\n#   x y value\n# 1 1 1 0.431\n# 2 2 1 0.337\n# 3 3 1 0.467\n# 4 4 1 0.337\n# 5 5 1 0.376\n# 6 6 1 0.361\n\nHowever, to do a PCA we will need a matrix of data in wide format containing the grayscale pixel values. We can do this using tidyr::pivot_wider(), giving a result with 640 rows and 954 columns.\n\nimg_df &lt;- pivot_wider(img_df_long, \n                     names_from = y, \n                     values_from = value) |&gt;\n  select(-x)\ndim(img_df)\n# [1] 640 954\n\nMona’s PCA is produced from this img_df with prcomp():\n\nimg_pca &lt;- img_df |&gt;\n  prcomp(scale = TRUE, center = TRUE)\n\nWith 955 columns, the PCA comprises 955 eigenvalue/eigenvector pairs. However, the rank of a matrix is the smaller of the number of rows and columns, so only 640 eigenvalues can be non-zero. Printing the first 10 shows that the first three dimensions account for 46% of the variance and we only get to 63% with 10 components.\n\nimg_pca |&gt;\n  broom::tidy(matrix = \"eigenvalues\") |&gt; head(10)\n# # A tibble: 10 × 4\n#       PC std.dev percent cumulative\n#    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#  1     1   14.1  0.209        0.209\n#  2     2   11.6  0.141        0.350\n#  3     3   10.1  0.107        0.457\n#  4     4    7.83 0.0643       0.522\n#  5     5    6.11 0.0392       0.561\n#  6     6    4.75 0.0237       0.585\n#  7     7    3.70 0.0143       0.599\n#  8     8    3.52 0.0130       0.612\n#  9     9    3.12 0.0102       0.622\n# 10    10    2.86 0.00855      0.631\n\nFigure 4.31 shows a screeplot of proportions of variance. Because there are so many components and most of the information is concentrated in the largest dimensions, I’ve used a \\(\\log_{10}()\\) scale on the horizontal axis. Beyond 10 or so dimensions, the variance of additional components looks quite tiny.\n\nggscreeplot(img_pca) +\n  scale_x_log10()\n\n\n\n\n\n\nFigure 4.31: Screeplot of the variance proportions in the Mona Lisa PCA.\n\n\n\n\nThen, if \\(\\mathbf{M}\\) is the \\(640 \\times 955\\) matrix of pixel values, a best approximation \\(\\widehat{\\mathbf{M}}_k\\) using \\(k\\) dimensions can be obtained as \\(\\widehat{\\mathbf{M}}_k = \\mathbf{X}_k\\;\\mathbf{V}_k^\\mathsf{T}\\) where \\(\\mathbf{X}_k\\) are the principal component scores and \\(\\mathbf{V}_k\\) are the eigenvectors corresponding to the \\(k\\) largest eigenvalues. The function approx_pca() does this, and also undoes the scaling and centering carried out in PCA.\n\n\nCodeapprox_pca &lt;- function(n_comp = 20, pca_object = img_pca){\n  ## Multiply the matrix of rotated data (component scores) by the transpose of \n  ## the matrix of eigenvectors (i.e. the component loadings) to get back to a \n  ## matrix of original data values\n\n  recon &lt;- pca_object$x[, 1:n_comp] %*% t(pca_object$rotation[, 1:n_comp])\n  \n  ## Reverse any scaling and centering that was done by prcomp()\n  if(all(pca_object$scale != FALSE)){\n    ## Rescale by the reciprocal of the scaling factor, i.e. back to\n    ## original range.\n    recon &lt;- scale(recon, center = FALSE, scale = 1/pca_object$scale)\n  }\n  if(all(pca_object$center != FALSE)){\n    ## Remove any mean centering by adding the subtracted mean back in\n    recon &lt;- scale(recon, scale = FALSE, center = -1 * pca_object$center)\n  }\n  \n  ## Make it a data frame that we can easily pivot to long format\n  ## for drawing with ggplot\n  recon_df &lt;- data.frame(cbind(1:nrow(recon), recon))\n  colnames(recon_df) &lt;- c(\"x\", 1:(ncol(recon_df)-1))\n\n  ## Return the data to long form \n  recon_df_long &lt;- recon_df |&gt;\n    tidyr::pivot_longer(cols = -x, \n                        names_to = \"y\", \n                        values_to = \"value\") |&gt;\n    mutate(y = as.numeric(y)) |&gt;\n    arrange(y) |&gt;\n    as.data.frame()\n  \n  recon_df_long\n}\n\n\nFinally, the recovered images, using 2, 3 , 4, 5, 10, 15, 20, 50, and 100 principal components can be plotted using ggplot. In the code below, the approx_pca() function is run for each of the 9 values specified by n_pcs giving a data frame recovered_imgs containing all reconstructed images, with variables x, y and value (the greyscale pixel value).\n\nn_pcs &lt;- c(2:5, 10, 15, 20, 50, 100)\nnames(n_pcs) &lt;- paste(\"First\", n_pcs, \"Components\", sep = \"_\")\n\nrecovered_imgs &lt;- map_dfr(n_pcs, \n                          approx_pca, \n                          .id = \"pcs\") |&gt;\n  mutate(pcs = stringr::str_replace_all(pcs, \"_\", \" \"), \n         pcs = factor(pcs, levels = unique(pcs), ordered = TRUE))\n\nIn ggplot(), each is plotted using geom_raster(), using value to as the fill color. A quirk of images imported to R is that origin is taken as the upper left corner, so the Y axis scale needs to be reversed. The 9 images are then plotted together using facet_wrap().\n\np &lt;- ggplot(data = recovered_imgs, \n            mapping = aes(x = x, y = y, fill = value))\np_out &lt;- p + geom_raster() + \n  scale_y_reverse() + \n  scale_fill_gradient(low = \"black\", high = \"white\") +\n  facet_wrap(~ pcs, ncol = 3) + \n  guides(fill = \"none\") + \n  labs(title = \"Recovering Mona Lisa from PCA of her pixels\") + \n  theme(strip.text = element_text(face = \"bold\", size = rel(1.2)),\n        plot.title = element_text(size = rel(1.5)))\n\np_out\n\nThe result, in Figure 4.32 is instructive about how much visual information is contained in lower-dimensional reconstructions, or conversely, how much the image can be compressed by omitting the many small dimensions.\n\n\n\n\n\n\n\nFigure 4.32: Re-construction of the Mona Lisa using 2, 3 , 4, 5, 10, 15, 20, 50, and 100 principal components.\n\n\n\n\nIn this figure, with 4-5 components most people would recognize this as a blury image of the world’s most famous portrait. It is certainly clear that this is the Mona Lisa with 10–15 components. Details of the portrait and backgound features become recognizable with 20–50 components, and with 100 components it compares favorably with the original in Figure 4.30. In numbers, the original \\(640 \\times 955\\)) image is of size 600 Kb. The 100 component version is only 93 Kb, 15.6% of this.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html#sec-outlier-detection",
    "href": "05-pca-biplot.html#sec-outlier-detection",
    "title": "4  Dimension Reduction",
    "section": "\n4.7 Elliptical insights: Outlier detection",
    "text": "4.7 Elliptical insights: Outlier detection\nThe data ellipse (Section 3.2), or ellipsoid in more than 2D is fundamental in regression. But, as Pearson showed, it is also key to understanding principal components analysis, where the principal component directions are simply the axes of the ellipsoid of the data. As such, observations that are unusual in data space may not stand out in univariate views of the variables, but will stand out in principal component space, usually on the smallest dimension.\nAs an illustration, I created a dataset of \\(n = 100\\) observations with a linear relation, \\(y = x + \\mathcal{N}(0, 1)\\) and then added two discrepant points at (1.5, -1.5), (-1.5, 1.5).\n\nset.seed(123345)\nx &lt;- c(rnorm(100),             1.5, -1.5)\ny &lt;- c(x[1:100] + rnorm(100), -1.5, 1.5)\n\nWhen these are plotted with a data ellipse in Figure 4.33 (left), you can see the discrepant points labeled 101 and 102, but they do not stand out as unusual on either \\(x\\) or \\(y\\). The transformation to from data space to principal components space, shown in Figure 4.33 (right), is simply a rotation of \\((x, y)\\) to a space whose coordinate axes are the major and minor axes of the data ellipse, \\((PC_1, PC_2)\\). In this view, the additional points appear a univariate outliers on the smallest dimension, \\(PC_2\\).\n\n\n\n\n\n\n\nFigure 4.33: Outlier demonstration: The left panel shows the original data and highlights the two discrepant points, which do not appear to be unusual on either x or y. The right panel shows the data rotated to principal components, where the labeled points stand out on the smallest PCA dimension.\n\n\n\n\nTo see this more clearly, Figure 4.34 shows an animation of the rotation from data space to PCA space. This uses heplots::interpPlot() to interpolate linearly from the positions of the points in data space to their locations in PCA space.\n\n\n\n\n\n\n\n\nFigure 4.34: Animation of rotation from data space to PCA space.\n\n\n\n\n\n4.7.1 Example: Penguin data\nIn Section 3.6.2 we examined the questions of multivariate normality and outliers for the penguin data. From a \\(\\chi^2\\) QQ plot (Figure 3.26) of the Mahalanobis \\(D^2\\) values, three Penguins (283, 10, 35) were identified as noteworthy, deserving a closer look to see why they are unusual. It was pointed out (Figure 3.27) that 2D plots of the data variables were only partially revealing. Let’s see where they appear in biplots.\nFirst, I find the noteworthy points with the three the largest \\(D^2\\) values as before:\n\ndata(peng, package=\"heplots\")\n\n# find potential multivariate outliers\nDSQ &lt;- heplots::Mahalanobis(peng[, 3:6])\nnoteworthy &lt;- order(DSQ, decreasing = TRUE)[1:3] |&gt; print()\n# [1] 283  10  35\n\nThe PCA shows that the first two components account for 88% of variance, so this is probably an adequate representation of the overall structure of our penguins:\n\npeng.pca &lt;- prcomp(\n  ~ bill_length + bill_depth + flipper_length + body_mass,\n                   data=peng, scale. = TRUE\n  )\nsummary(peng.pca)\n# Importance of components:\n#                          PC1   PC2    PC3   PC4\n# Standard deviation     1.657 0.882 0.6072 0.328\n# Proportion of Variance 0.686 0.195 0.0922 0.027\n# Cumulative Proportion  0.686 0.881 0.9730 1.000\n\n\n\n\n\n\n\n\nFigure 4.35: Biplot of the first two dimensions of the Penguin data. The points for the three noteworthy cases are labeled, but none of these appear to be unusual in this view.\n\n\n\n\nFigure 4.35 gives the biplot for the first two dimensions. It can be seen that:\n\nPC1 is largely determined by flipper length and body mass. We can interpret this as an overall measure of penguin size. On this dimension, Gentoos are the largest, by quite a lot, compared with Adélie and Chinstrap.\nPC2 is mainly determined by variation in the two beak variables: bill length and depth. Chinstrap are lower than the other two species on bill length and depth, but bill length further distinguishes the Gentoos from the Adélies. A penguin biologist could almost certainly provide an explanation, but I’ll call this beak shape.\nBut, our three suspected outliers are well-within the bulk of their species.\n\nThat’s the point of this exercise. The projection of the data into the space that accounts for the greatest total variance usually does not reveal a few unusual points.\n\nShow the codesource(\"R/penguin/penguin-colors.R\")\n# create vector of labels, blank except for the noteworthy\nlab &lt;- 1:nrow(peng)\nlab &lt;- ifelse(lab %in% noteworthy, lab, \"\")\n\nggbiplot(peng.pca,\n         choices = 1:2,\n         groups = peng$species, \n         ellipse = TRUE, ellipse.alpha = 0.1,\n         circle = TRUE,\n         var.factor = 1,\n         geom.ind = c(\"point\", \"text\"),\n         point.size = 1,\n         labels = lab, labels.size = 6,\n         varname.size = 5,\n         clip = \"off\") +\n  theme_minimal(base_size = 14) +\n  theme_penguins(\"dark\") +\n  scale_shape_discrete() +\n  theme(legend.direction = 'horizontal', legend.position = 'top') \n\n\nNow, plotting dimensions 3 and 4 gives Figure 4.36. Dimension 3, accounting for 9%, is largely determined by a contrast of bill length with bill depth and body mass, while dimension 4 involves a contrast between body mass and flipper length. The Chinstraps here have the longest, straightest beaks.\n\n\n\n\n\n\n\nFigure 4.36: Biplot of dimensions 3-4 of the Penguin data. The three noteworthy birds stand out in this view.\n\n\n\n\nRecall that the perpendicular projection of observation \\(i\\) on the vector for variable \\(j\\) gives an approximation of \\(\\hat{x}_{ij}\\) shown in that space. Our friend Cyrano (case 283), the only true multivariate outlier, lies at the extreme ends of both dimensions, with his exceptionally long, straight bill. Case 10 (Hook Nose) stands out at the high end of dimension 3 with a highly curved beak. case 35 is at the high end of dimension 4, so probably is much heavier than most and has short flippers.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html#what-have-we-learned",
    "href": "05-pca-biplot.html#what-have-we-learned",
    "title": "4  Dimension Reduction",
    "section": "\n4.8 What have we learned?",
    "text": "4.8 What have we learned?\nWelcome to the world of multivariate juicers–—those magical tools that squeeze the most meaningful information from high-dimensional data clouds! This chapter has taken us on a journey from Flatland to Spaceland, revealing how dimension reduction methods can transform overwhelming complexity into interpretable insights.\n• PCA is your geometric friend, helping you compress N-dimensional data: Principal Components Analysis finds the directions of maximum variance in your data, creating uncorrelated orthogonal dimensions that capture the most “juice” from your multivariate cloud. Think of it as finding the best viewpoint to see a 3D sculpture when you can only look at a 2D photograph—PCA rotates and compresses your data to show you the best 2D viewing angle.\n• Biplots are visualization gold, helping you view compressed N-dimensional data: These elegant displays build off of PCA by simultaneously showing both your observations (as points) and your variables (as vectors) in the same reduced space. The magic lies in the interpretation: variable vectors pointing in similar directions are correlated, and you can read approximate values by projecting points onto variable vectors. It’s like having X-ray vision for multivariate relationships!\n• Eigenvalues tell the variance story: The screeplot becomes your guide for deciding how many dimensions to keep. Look for the “elbow” where the eigenvalues start to resemble scree (rubble) rather than meaningful signal. Generally, 80-90% cumulative variance gives you a solid foundation for interpretation.\n• Supplementary variables enhance interpretation: Once you’ve found your reduced-dimension view, you can project additional variables into the space to aid interpretation—like adding helpful annotations to a map. This technique bridges the gap between statistical discovery and domain knowledge.\n• Nonlinear methods reveal hidden structures: When relationships aren’t linear, techniques like multidimensional scaling (MDS) and t-SNE can uncover patterns that PCA might miss. These methods focus on preserving local neighborhoods and distances rather than global variance, often revealing clusters and nonlinear manifolds lurking in your data.\n• Variable ordering creates visual clarity: When similar variables are placed adjacent to each other, patterns emerge and anomalies become visible—it’s like organizing a messy bookshelf by subject. In biplots, the angles of variable vectors provide a natural ordering that are used to make correlation matrices and other displays much more interpretable.\n• Outlier detection gets multidimensional power: Points that seem normal in individual variables can reveal themselves as true multivariate outliers when viewed in principal component space, especially along the smallest dimensions. The data ellipse becomes your guide to understanding what’s typical versus what deserves a closer look.\n• Real applications abound: From compressing the Mona Lisa using eigenfaces to understanding crime patterns across U.S. states, dimension reduction methods bridge the gap between statistical technique and practical insight. These aren’t just mathematical curiosities—they’re essential tools for making sense of our increasingly high-dimensional world.\n\n\n\n\n\nAluja, T., Morineau, A., & Sanchez, G. (2018). Principal component analysis for data science. https://pca4ds.github.io/\n\n\nBorg, I., & Groenen, P. J. F. (2005). Modern Multidimensional Scaling: Theory and Applications. Springer.\n\n\nBorg, I., Groenen, P. J. F., & Mair, P. (2018). Applied multidimensional scaling and unfolding. In SpringerBriefs in Statistics. Springer International Publishing. https://doi.org/10.1007/978-3-319-73471-2\n\n\nCattell, R. B. (1966). The scree test for the number of factors. Multivariate Behavioral Research, 1(2), 245–276. https://doi.org/10.1207/s15327906mbr0102_10\n\n\nDray, S., & Siberchicot, A. (2025). Adegraphics: An S4 lattice-based package for the representation of multivariate data. http://pbil.univ-lyon1.fr/ADE-4/\n\n\nEuler, L. (1758). Elementa doctrinae solidorum. Novi Commentarii Academiae Scientiarum Petropolitanae, 4, 109–140. https://scholarlycommons.pacific.edu/euler-works/230/\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix functions for teaching and learning linear algebra and multivariate statistics. https://github.com/friendly/matlib\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays. Computational Statistics and Data Analysis, 43(4), 509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with application to principal components analysis. Biometrics, 58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGabriel, K. R. (1981). Biplot display of multivariate matrices for inspection of data and diagnosis. In V. Barnett (Ed.), Interpreting multivariate data (pp. 147–173). John Wiley; Sons.\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman & Hall.\n\n\nGower, J. C., Lubbe, S. G., & Roux, N. J. L. (2011). Understanding biplots. Wiley. http://books.google.ca/books?id=66gQCi5JOKYC\n\n\nGreenacre, M. (1984). Theory and applications of correspondence analysis. Academic Press.\n\n\nGreenacre, M. (2010). Biplots in practice. Fundación BBVA. https://books.google.ca/books?id=dv4LrFP7U\\_EC\n\n\nHahsler, M., Buchta, C., & Hornik, K. (2024). Seriation: Infrastructure for ordering objects using seriation. https://github.com/mhahsler/seriation\n\n\nHusson, F., Le, S., & Pagès, J. (2017). Exploratory multivariate analysis by example using r. Chapman & Hall. https://doi.org/10.1201/b21874\n\n\nKassambara, A., & Mundt, F. (2020). Factoextra: Extract and visualize the results of multivariate data analyses. https://doi.org/10.32614/CRAN.package.factoextra\n\n\nKrijthe, J. (2023). Rtsne: T-distributed stochastic neighbor embedding using a barnes-hut implementation. https://github.com/jkrijthe/Rtsne\n\n\nKruskal, J. B. (1964). Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29(1), 1–27. https://doi.org/10.1007/bf02289565\n\n\nLê, S., Josse, J., & Husson, F. (2008). FactoMineR: A package for multivariate analysis. Journal of Statistical Software, 25(1), 1–18. https://doi.org/10.18637/jss.v025.i01\n\n\nMaaten, L. van der, & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9, 2579–2605. http://www.jmlr.org/papers/v9/vandermaaten08a.html\n\n\nMurdoch, D., & Adler, D. (2025). Rgl: 3D visualization using OpenGL. https://doi.org/10.32614/CRAN.package.rgl\n\n\nOksanen, J., Simpson, G. L., Blanchet, F. G., Kindt, R., Legendre, P., Minchin, P. R., O’Hara, R. B., Solymos, P., Stevens, M. H. H., Szoecs, E., Wagner, H., Barbour, M., Bedward, M., Bolker, B., Borcard, D., Borman, T., Carvalho, G., Chirico, M., De Caceres, M., … Weedon, J. (2025). Vegan: Community ecology package. https://doi.org/10.32614/CRAN.package.vegan\n\n\nPearson, K. (1896). Contributions to the mathematical theory of evolution—III, regression, heredity and panmixia. Philosophical Transactions of the Royal Society of London, 187, 253–318.\n\n\nPearson, K. (1901). On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 6(2), 559–572.\n\n\nPedersen, T. L., & Robinson, D. (2025). Gganimate: A grammar of animated graphics. https://doi.org/10.32614/CRAN.package.gganimate\n\n\nReaven, G. M., & Miller, R. G. (1979). An attempt to define the nature of chemical diabetes using a multidimensional analysis. Diabetologia, 16, 17–24.\n\n\nShepard, R. N. (1962a). The analysis of proximities: Multidimensional scaling with an unknown distance function. i. Psychometrika, 27(2), 125–140. https://doi.org/10.1007/bf02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities: Multidimensional scaling with an unknown distance function. II. Psychometrika, 27(3), 219–246. https://doi.org/10.1007/bf02289621\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S. (1972a). Multidimensional scaling; theory and applications in the behavioral sciences: Vols. II. Applications. Seminar Press. https://books.google.ca/books?id=PpFAAQAAIAAJ\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S. (1972b). Multidimensional scaling: Theory and applications in the behavioral sciences: Vols. I. Theory. Seminar Press. https://books.google.ca/books?id=pJRAAQAAIAAJ\n\n\nShoben, E. J. (1983). Applications of multidimensional scaling in cognitive psychology. Applied Psychological Measurement, 7(4), 473–490. https://doi.org/10.1177/014662168300700406\n\n\nTorgerson, W. S. (1952). Multidimensional scaling: I. Theory and method. Psychometrika, 17(4), 401–419. https://doi.org/10.1007/bf02288916\n\n\nTurk, M., & Pentland, A. (1991). Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1), 71–86. https://doi.org/10.1162/jocn.1991.3.1.71\n\n\nVu, V. Q., & Friendly, M. (2024). Ggbiplot: A grammar of graphics implementation of biplots. https://doi.org/10.32614/CRAN.package.ggbiplot",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-pca-biplot.html#footnotes",
    "href": "05-pca-biplot.html#footnotes",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "This is Euler’s (1758) formula, which states that any convex polyhedron must obey the formula \\(V + F - E = 2\\) where \\(V\\) is the number of vertexes (corners), \\(F\\) is the number of faces and \\(E\\) is the number of edges. For example, a tetrahedron or pyramid has \\((V, F, E) = (4, 4, 6)\\) and a cube has \\((V, F, E) = (8, 6, 12)\\). Stated in words, for all solid bodies confined by planes, the sum of the number of vertexes and the number of faces is two less than the number of edges.↩︎\nFor example, if two variables in the analysis are height and weight, changing the unit of height from inches to centimeters would multiply its variance by \\(2.54^2\\); changing weight from pounds to kilograms would divide its variance by \\(2.2^2\\).↩︎\nThe unfortunate default scale. = FALSE was for consistency with S, the precursor to R but in general scaling is usually advisable.↩︎\nThe vegan package (Oksanen et al., 2025) provides vegan::metaMDS() which allows a wide range of distance measures …↩︎\nThe usual default, perplexity = 30 focuses on preserving the distances to the 30 nearest neighbors and puts virtually no weight on preserving distances to the remaining points. For data sets with a small number of points e.g. \\(n=100\\), this will uncover the global structure quite well since each point will preserve distances to a third of the data set. For larger problems, e.g., \\(n = 10,000\\) points, using a higher perplexity value e.g. 500, will do a much better job for of uncovering the global structure. (This description comes from https://opentsne.readthedocs.io/en/latest/parameters.html)↩︎\nThe general topic of arranging items (variables, factor values) in an orderly sequence is called seriation, and stems from methods of dating in archaeology, used to arrange stone tools, pottery fragments, and other artifacts in time order. In R, the seriation package (Hahsler et al., 2024) provides a wide range of techniques. …↩︎\nhttps://kieranhealy.org/blog/archives/2019/10/27/reconstructing-images-using-pca/↩︎",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "06-linear_models.html",
    "href": "06-linear_models.html",
    "title": "5  Overview of Linear models",
    "section": "",
    "text": "5.1 The General Linear Model\nAlthough this book is primarily about multivariate models, it is useful to have an overview of the range of available techniques for univariate response models to see their uses and to appreciate how easily univariate models generalize to multivariate ones. Hence, this chapter reviews the characteristics of the standard univariate methods for explaining or predicting a single outcome variable from a set of predictors.\nThe key ideas are:\nFigure 5.1 summarizes a variety of methods for linear models, classified by number of predictors and number of response variables, and whether these are quantitative vs. discrete. For the present purposes, the key columns are the first two, for the case of one or more quantitative outcome variables.\nWhen the predictors are also quantitative, simple regression (\\(p=1\\)) generalizes to multivariate regression with two or more outcomes (\\(q &gt; 1\\)). For example we might want to predict weight and body mass index jointly from a person’s height.\nThe situation is more interesting when there are \\(p&gt;1\\) predictors. The most common multivariate generalization is multivariate multiple regression (MMRA), where each outcome is regressed on the predictors, as if done separately for each outcome, but using multivariate tests that take correlations among the response variables into account. Other methods for this case include canonical correlation analysis, which tries to explain all relations between \\(\\mathbf{Y}\\) and a set of \\(\\mathbf{x}\\)s through maximally correlated linear combinations of each.\nWhen the predictor variables are all discrete or categorical, such as gender or level of education, methods like the simple \\(t\\)-test, one-way ANOVA and factorial ANOVA with \\(q=1\\) outcome measures all have simple extensions to the case of \\(q&gt;1\\) outcomes.\nPackages\nIn this chapter I use the following packages. Load them now:\nTo establish notation and terminology, it is worthwhile to state the general linear model formally. For convenience, I use vector and matrix notation. This expresses a response variable, \\(\\mathbf{y} = (y_1, y_2, \\dots , y_n)^\\mathsf{T}\\) for \\(n\\) observations, as a sum of terms involving \\(p\\) regressors, \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots , \\mathbf{x}_p\\), each of length \\(n\\).\n\\[\n\\begin{aligned}\n\\mathbf{y} & = \\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\cdots + \\beta_p \\mathbf{x}_p + \\boldsymbol{\\epsilon} \\\\\n           & = \\left[ \\mathbf{1},\\; \\mathbf{x}_1,\\; \\mathbf{x}_2,\\; \\dots ,\\; \\mathbf{x}_p \\right] \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\\\\n\\end{aligned}\n\\tag{5.1}\\]\nor, expressed in matrices,\n\\[\n\\large{\\mathord{\\mathop{\\mathbf{y}}\\limits_{n \\times 1}} = \\mathord{\\mathop{\\mathbf{X}}\\limits_{n \\times (p+1)}}\\; \\mathord{\\mathop{\\mathbf{\\boldsymbol{\\beta}}}\\limits_{(p+1) \\times 1}} + \\boldsymbol{\\epsilon}}\n\\]\nThe matrix \\(\\mathbf{X}\\) is called the model matrix and contains the numerical representations of the predictor variables called regressors. The essential thing about a linear model is that it is linear in the parameters \\(\\beta_i\\). That is, the predicted value of \\(\\mathbf{y}\\) is a linear combination of some \\(\\mathbf{x}_i\\) with weights \\(\\beta_i\\). An example of a nonlinear model is the exponential growth model, \\(y = \\beta_0 + e^{\\beta_1 x}\\), where the parameter \\(\\beta_1\\) appears as an exponent.1",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "06-linear_models.html#sec-GLM",
    "href": "06-linear_models.html#sec-GLM",
    "title": "5  Overview of Linear models",
    "section": "",
    "text": "These can be quantitative variables like age, salary or years of education. But they can also be transformed versions, like sqrt(age) or log(salary).\nA quantitative variable can be represented by more than one model regressor, for example if it is expressed as a polynomial like poly(age, degree=2) or a natural spline like ns(salary, df=5). The model matrix portion for such terms contains one column for each degree of freedom (df) and there are df coefficients in the corresponding portion of \\(\\boldsymbol{\\beta}\\).\nA categorical or discrete predictor– a factor variable in R– with \\(d\\) levels is expressed as \\(d - 1\\) columns in \\(\\mathbf{X}\\). Typically these are contrasts or comparisons between a baseline or reference level and each of the remaining ones, but any set of \\(d - 1\\) linearly independent contrasts can be used by assigning to contrasts(factor). For example, contrasts(factor) &lt;- contr.treatment(4) for a 4-level factor assigns 3 contrasts representing comparisons with a baseline level, typically the first (in alphabetic order). For an ordered factor, such as one for political knowledge with levels “low”, “medium”, “high”, contrasts.poly() returns the coefficients of orthogonal polynomial contrasts representing linear and quadratic trends.\nInteractions between predictors are represented as the direct products of the corresponding columns of \\(\\mathbf{X}\\). This allows the effect of one predictor on the response to depend on values of other predictors. For example, the interaction of two quantitative variables, \\(\\mathbf{x}_1, \\mathbf{x}_2\\) is represented by the product \\(\\mathbf{x}_1 \\times \\mathbf{x}_2\\). More generally, for variables or factors \\(A\\) and \\(B\\) with degrees of freedom \\(\\text{df}_A\\) and \\(\\text{df}_B\\) the regressors in \\(\\mathbf{X}\\) are the \\(\\text{df}_A \\times \\text{df}_B\\) products of each column for \\(A\\) with each column for \\(B\\).\n\n\n5.1.1 Model formulas\nStatistical models in R, such as those fit by lm(), glm() and many other modelling function in R are expressed in a simple notation that was developed by Wilkinson & Rogers (1973) for the GENSTAT software system at the Rothamsted Research Station. It solves the problem of having a compact way to specify any model consisting of any combinations of quantitative and discrete factor variables, interactions of these and arbitrary transformations of these.\nIn this, a model formula take the forms\n\nresponse ~ terms\nresponse ~ term1 + term2 + ...\n\nwhere the left-hand side, response specifies the response variable in the model and the right-hand side specifies the terms in the model specifying the columns in the \\(\\mathbf{X}\\) matrix of Equation 5.1; the coefficients \\(\\beta\\) are implied and not represented explicitly in the formula.\nThe notation y ~ x is read as “y is modeled by x”. The left-hand side is usually a variable name (such as height), but it could be an expression that evaluates to the the response, such as log(salary) or weight/height^2 which represents the body mass index.\nOn the right-hand side (RHS), the usual arithmetic operator, +, -, *, /, ^ have special meanings as described below. The most fundamental is that y ~ a + b is interpreted as “y is modeled by a and b”; that is, the sum of linear terms for a and b.\nSome examples for regression-like models using only quantitative variables, x, x1, x2, x3, ... are shown below:\n\ny ~ x                      # simple linear regression\ny ~ x - 1                  # no intercept: regression through the origin \ny ~ x + I(x^2)             # quadratic model\ny ~ poly(x, 3)             # cubic model\ny ~ x1 * x2                # crossing: x1 + x2  +  x1 : x2\ny ~ x1 + x2 + x3           # multiple regression\ny ~ (x1 + x2 + x3)^2       # response surface: all quadratics & two-way interactions\nlog(y) ~ x1 + poly(x, 2)   # arbitrary transformation of response\ny1 + y2 ~ x1 + x2 + x3     # response is sum of y1 and y2\n\nThe intercept \\(\\beta_0\\) is automatically included in the model without need to specify it explicitly. The minus sign, - on the right-hand side removes terms from the model, so a model with no intercept \\(\\beta_0 = 0\\) can be specifies as y ~ X -1 (or perhaps more naturally, y ~ 0 + X).\nFunction calls on the RHS, such as poly(x, 3) are evaluated directly, but to use a special model operator, like ^ must be “protected” by wrapping the term in I(), meaning “identity” or “inhibit”. Thus, the model y ~ x + I(x^2) means the quadratic model \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\). This differs from the model y ~ poly(x, 2) in that the former uses the raw x, x^2 values (which are necessarily positively correlated) while poly() converts these to orthogonal polynomial scores, which are uncorrelated (and therefore free from problems of collinearity).\n\nExample 5.1 Workers data: Regression models\nFor the workers data (Example 4.1) you can fit simple regression models predicting income from years of experience using a linear and quadratic model as follows.\n\ndata(workers, package = \"matlib\")\nworkers.mod1 &lt;- lm(Income ~ Experience, data=workers)\ncoef(workers.mod1) |&gt; t() |&gt; t()\n#              [,1]\n# (Intercept) 29.16\n# Experience   1.12\n\nworkers.mod2 &lt;- lm(Income ~ poly(Experience, 2), data=workers)\ncoef(workers.mod2) |&gt; t() |&gt; t()\n#                       [,1]\n# (Intercept)           46.5\n# poly(Experience, 2)1  39.1\n# poly(Experience, 2)2 -11.2\n\n(In this code, coef(workers.mod2) |&gt; t() |&gt; t() is used to print the coefficients as a one-column matrix.)\nIt is simplest to understand these models by plotting the data overlaid with the fitted regressions. This uses geom_smooth()  and specifies the smoothing model as method = \"lm\" with a formula, which is y ~ x for the linear model and y ~ poly(x, 2) for the quadratic.\n\nggplot(data = workers,\n       aes(x = Experience, y = Income)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y ~ x,\n              se = FALSE, linewidth = 2,\n              color = \"blue\") +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2),\n              se = FALSE, linewidth = 2,\n              color = \"red\") \n\n\n\n\n\n\nFigure 5.2: Workers data with fitted linear and quadratic models for years of experience.\n\n\n\n\nThe coefficients of the linear model are also easy to interpret:\n\n\\[\n\\operatorname{\\widehat{Income}} = 29.162 + 1.119(\\operatorname{Experience})\n\\]\n\nSo a worker with zero years of experience can expect an income of $29162 and this should increase by $1119 for each additional year. However, it is not so simple to interpret the coefficients when a poly() term is used. Naively plugging in the coefficients for workers.mod2 gives\n\n\\[\n\\operatorname{\\widehat{Income}} = 46.5 + 39.111(\\operatorname{Experience}) - 11.16(\\operatorname{Experience^2})\n\\]\n\nThe problem is that \\(x\\) = Experience in model workers.mod is represented not by the raw values, but rather by values of \\(x\\) and \\(x^2\\) that have been made to be uncorrelated. If you really want to interpret the coefficient values in terms of years of experience, use the option raw = TRUE in poly():\n\nworkers.mod2b &lt;- lm(Income ~ poly(Experience, 2, raw = TRUE), \n                    data=workers) |&gt;\n  print()\n# \n# Call:\n# lm(formula = Income ~ poly(Experience, 2, raw = TRUE), data = workers)\n# \n# Coefficients:\n#                      (Intercept)  poly(Experience, 2, raw = TRUE)1  \n#                          23.0680                            2.2952  \n# poly(Experience, 2, raw = TRUE)2  \n#                          -0.0335\n\n\\[\n\\operatorname{\\widehat{Income}} = 23.07 + 2.3(\\operatorname{Experience}) - 0.03(\\operatorname{Experience}^2)\n\\] This says that income is predicted to be $23,068 with no experience, increase initially by $2295, but that yearly increase decreases by $330. Some further details of orthogonal polynomials are explained below.\n\n\n5.1.1.1 Factors\nFactor variables are treated specially in linear models, but have simple notations in R formulas. The following examples use A, B, C to represent discrete factors with two or more levels.\n\ny ~ A                 # one-way ANOVA\ny ~ A + B             # two-way, main effects only\ny ~ A * B             # full two-way, with interaction\ny ~ A + B + A:B       # same, in long-hand\ny ~ x + A             # one-way ANCOVA\ny ~ (A + B + C)^2     # three-way ANOVA, incl. all two-way interactions\n\n\n5.1.1.2 Crossing\nThe * operator has special meaning used to specify the crossing of variables and factors and : specifies interactions (products of variables). So, the model y ~ x1 * x2 is expanded to give y ~ x1 + x2 + x1:x2 and the interaction term x1:x2 is calculated as \\(x_1 \\times x_2\\). In algebraic notation (omitting the error term) this works out to the model,\n\\[\\begin{aligned}\ny & = & \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 +  \\beta_1 x_1 *  \\beta_2 x_2 \\\\\n  & = & \\beta_0 + (\\beta_1 + \\beta_2 x_2) x_1 + \\beta_2 x_2 \\:\\: ,\\\\\n\\end{aligned}\\]\nwhich means that the coefficient for \\(x_1\\) in the model is not constant for all values of \\(x_2\\), but rather changes with the value of \\(x_2\\). If \\(\\beta_2 &gt; 0\\), the slope for \\(x_1\\) increases with \\(x_2\\) and vice-versa.\nThe formula y ~ A * B for factors is similar, expanding to y ~ A + B + A:B, but the columns in the model matrix represent contrasts among the factor levels as described in detail below (Section 5.1.3). The main effects, A and B come from contrasts among the means of their factor levels and the interaction term A:B reflects differences among means of A across the levels of B (and vice-versa).\nThe model formula y ~ x + A specifies an ANCOVA model with different intercepts for the levels of A, but with a common slope for x. Adding an interaction of x:A in the model y ~ x * A allow separate slopes and intercepts for the groups.\n\n5.1.1.3 Powers\nThe ^ exponent operator indicates powers of a term expression to a specified degree. Thus the term (A + B)^2 is identical to (A + B) * (A + B) which expands to the main effects of A, B and their interaction, also identical to A * B. In general, the product of parenthesized terms expands as in ordinary algebra,\n\ny ~ (A + B) * (C + D) -&gt; A + B + C + D + A:C + A:D + B:C + B:D\n\nPowers get more interesting with more terms, so (A + B + C)^2 is the same as (A + B + C) * (A + B + C), which includes main effects of A, B and C as well as all two-way interactions, A:B, A:C and B:C. The model formula (A + B + C)^3 expands to include all two-way interactions and the three-way interaction A:B:C.\n\n(A + B + C)^3 -&gt; A + B + C + A:B + A:C + B:C + A:B:C\n\nIn this context - can be use to remove terms, as shown in the following examples\n\n(A + B + C)^2 &lt;-&gt; (A + B + C)^3 - A:B:C\n(A + B + C)^3 - B:C - A:B:C  &lt;-&gt; A + B + C + A:B + A:C \n\nFinally, the symbol . on the right-hand side specifies all terms in the current dataset other than the response. Thus if you have a data.frame containing y, x1, x2, ..., x6, you can specify a model with all variables except x6 as predictors as\n\ny ~ . - x6\n\nTo test what we’ve covered above,\n\nWhat do you think the model formula y ~ .^2 means in a data set containing variables x1, x2, x3, and x4?\nWhat about the formula with y ~ .^2 - A:B:C:D with factors A, B, C, D?\n\nYou can work out questions like these or explore model formulae using terms() for a \"formula\" object. The labels of these terms can then be concatenated to a string and turned back into a formula using as.formula():\n\nf &lt;- formula(y ~ (x1 + x2 + x3 + x4)^2)\nterms = attr(terms(f), \"term.labels\")\n\nterms |&gt; paste(collapse = \" + \")\n# [1] \"x1 + x2 + x3 + x4 + x1:x2 + x1:x3 + x1:x4 + x2:x3 + x2:x4 + x3:x4\"\n# convert back to a formula\nas.formula(sprintf(\"y ~ %s\", paste(terms, collapse=\" + \"))) \n# y ~ x1 + x2 + x3 + x4 + x1:x2 + x1:x3 + x1:x4 + x2:x3 + x2:x4 + \n#     x3:x4\n\n\n5.1.2 Model matrices\nAs noted above, a model formula is used to generate the \\(n \\times (p+1)\\) model matrix, \\(\\mathbf{X}\\), typically containing the column of 1s for the intercept \\(\\beta_0\\) in the model, followed by \\(p\\) columns representing the regressors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots , \\mathbf{x}_p\\). Internally, lm() uses stats::model.matrix() and you can use this to explore how factors, interactions and other model terms are represented in a model object.\nFor a small example, here are a few observations representing income (inc) and type of occupation, taking on values bc (blue colar), wc (white colar) and prof (professional). model.matrix() takes a one-sided formula with the terms on the right-hand side. The main effect model looks like this:\n\nset.seed(42)\ninc &lt;- round(runif(n=9, 20, 40))\ntype &lt;- rep(c(\"bc\", \"wc\", \"prof\"), each =3)\n\nmm &lt;- model.matrix(~ inc + type) \ndata.frame(type, mm)\n#   type X.Intercept. inc typeprof typewc\n# 1   bc            1  38        0      0\n# 2   bc            1  39        0      0\n# 3   bc            1  26        0      0\n# 4   wc            1  37        0      1\n# 5   wc            1  33        0      1\n# 6   wc            1  30        0      1\n# 7 prof            1  35        1      0\n# 8 prof            1  23        1      0\n# 9 prof            1  33        1      0\n\nAs you can see, type, with 2 degrees of freedom is represented by two dummy (0/1) variables, labeled typeprof and typewc here. Together, these represent treatment contrasts (comparisons) between the baseline group type==\"bc\", which is coded (0, 0) and each of the others: type==\"prof\", coded (1, 0) and type==\"wc\", codes (0, 1). Different coding schemes are described in the following section.\nIn a model with the interaction inc * type, additional columns are constructed as the product of inc with each of the columns for type. We will see below how this generalizes to an arbitrary number of predictor terms and their possible interactions.\n\nmodel.matrix(~ inc * type)\n#   (Intercept) inc typeprof typewc inc:typeprof inc:typewc\n# 1           1  38        0      0            0          0\n# 2           1  39        0      0            0          0\n# 3           1  26        0      0            0          0\n# 4           1  37        0      1            0         37\n# 5           1  33        0      1            0         33\n# 6           1  30        0      1            0         30\n# 7           1  35        1      0           35          0\n# 8           1  23        1      0           23          0\n# 9           1  33        1      0           33          0\n# attr(,\"assign\")\n# [1] 0 1 2 2 3 3\n# attr(,\"contrasts\")\n# attr(,\"contrasts\")$type\n# [1] \"contr.treatment\"\n\n\n5.1.3 Coding factors and contrasts\nDiscrete explanatory variables, such as race, type of occupation or level of education require special attention in linear models because, unlike continuous variables, they cannot by entered into the model equation just as they are. Instead, we need some way to code those variables numerically.\nA key insight is that your choice of a coding scheme changes the meaning of the model parameters, and allows you to perform different comparisons (test different statistical hypotheses) about the means of the category levels according to meaningful questions in your research design. For a more general discussion of coding schemes, see Fox & Weisberg (2018), sec. 4.7 and the vignette Coding Matrices, Contrast Matrices and Linear Models for the codingMatrices package.\nEach coding scheme for a factor represents the same model in terms of fitted values and overall significance for that term, but they differ in how the coefficients are parameterized and interpreted. This is crucial to understand, because tests of the coefficients can directly answer different research questions depending on the coding scheme used.\nIn R, categorical variables are called factors usually created by g &lt;- factor(g) or g &lt;- as.factor(g) for a discrete variable g. If levels of the variable g are ordered, such as type of occupation with levels \"bc\" &lt; \"wc\" &lt; \"prof\" or dose of a drug, \"low\" &lt; \"medium\" &lt; \"high\", you can use g &lt;- ordered(g) to reflect this.\nIn any case, a factor with \\(k\\) levels is reflected in an overall test with \\(k-1\\) degrees of freedom corresponding to the null hypothesis \\(\\mathcal{H}_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\). This can be represented as \\(k-1\\) comparisons among the factor level means, or \\(k-1\\) separate questions asking how the means differ.\nBase R provides several coding schemes via assignment to the contrasts() function for a factor, as in contrasts(df$Drug) &lt;- ... one of:\n\ncontr.treatment(): Compares each level to a reference level using \\(k-1\\) dummy (0, 1) variables. This is the default, and the reference level is taken as the first (in alphabetic or numerical order). You can change the reference level using relevel() or reorder() for the factor, or simply using factor(A, levels = ...).\ncontr.sum(): Compares each level to the grand mean.\ncontr.helmert(): Compares each level to the mean of the previous levels, which is useful for ordered categories such as type of occupation with levels \"bc\" &lt; \"wc\" &lt; \"prof\" or dose of a drug, \"low\" &lt; \"medium\" &lt; \"high\".\ncontr.poly(): For ordered factors with numeric levels, this creates orthogonal polynomial contrasts, representing the linear, quadratic, cubic … trends in the factor means, as if these appeared as \\(x, x^2, x^3, ...\\) terms in a model with \\(x\\) as a numeric variable.\n\nI take up some of the details of these coding schemes below. But first, it is useful to define exactly what I mean by a contrast. For a factor with \\(k\\) groups, a contrast is simply a comparison of the mean of one subset of groups against the mean of another subset. This is specified as a weighted sum, \\(L\\) of the means \\(\\boldsymbol{\\mu}\\) with weights \\(\\mathbf{c}\\) that sum to zero,\n\\[\nL = \\mathbf{c}^\\mathsf{T} \\boldsymbol{\\mu} = \\sum_i^k c_i \\mu_i \\quad\\text{such that}\\quad \\Sigma c_i = 0 \\; .\n\\tag{5.2}\\]\nTwo contrasts, \\(\\mathbf{c}_1\\) and \\(\\mathbf{c}_2\\) are orthogonal if the sum of products of their weights is zero, i.e., \\(\\mathbf{c}_1^\\top \\mathbf{c}_2 = \\Sigma c_{1i} \\times c_{2i} = 0\\). When contrasts are placed as columns of a matrix \\(\\mathbf{C}\\), they are all mutually orthogonal if each pair is orthogonal, which means \\(\\mathbf{C}^\\top \\mathbf{C}\\) is a diagonal matrix. If the columns of \\(\\mathbf{C}\\) are normalized to have sums of squares = 1, then \\(\\mathbf{C}^\\top \\mathbf{C} = \\mathbf{I}\\).\nOrthogonal contrasts correspond to statistically independent tests. This is nice because they reflect separate, non-overlapping research questions. Another consequence is that the sums of squares for the overall hypothesis of differences among the groups is exactly decomposed as the sum of the sum of squares accounted for by the \\(k-1\\) contrasts \\(L_i\\):\n\\[\n\\text{SS}_{\\text{group}} = \\sum_i^{k-1} \\text{SS} (L_i) \\; .\n\\]\nTreatment coding\nLet’s examine R’s default coding scheme, contr.treatment (also called dummy coding), for a factor with 4 levels: ‘a’, ‘b’, ‘c’, and ‘d’, with a view to understanding the relationship between the true population means, \\(\\mu_a\\), \\(\\mu_b\\), \\(\\mu_c\\), and \\(\\mu_d\\) and the parameters \\(\\boldsymbol{\\beta}\\) estimated in a linear model. We get the following:\n\nC &lt;- contr.treatment(letters[1:4]) |&gt; print()\n#   b c d\n# a 0 0 0\n# b 1 0 0\n# c 0 1 0\n# d 0 0 1\n\nHere, the columns of C correspond to three dummy variables for the levels b, c, d compared to the reference level a. If we denote these columns as \\(x_b\\), \\(x_c\\), and \\(x_d\\), then:\n\n\\[\nx_b =\n\\begin{cases}\n1 & \\! \\text{if factor=b} \\\\\n0 & \\! \\text{otherwise}\n\\end{cases}\n;\\quad\nX_c =\n\\begin{cases}\n1 & \\! \\text{if factor=c} \\\\\n0 & \\! \\text{otherwise}\n\\end{cases}\n;\\quad\nX_d =\n\\begin{cases}\n1 & \\! \\text{if factor=d} \\\\\n0 & \\! \\text{otherwise}\n\\end{cases}\n\\] \nThe design matrix \\(\\mathbf{X}_{(4 \\times 4)} = [\\mathbf{1}, \\mathbf{C}] = [\\mathbf{1}, \\mathbf{x}_b, \\mathbf{x}_c, \\mathbf{x}_c]\\) includes the constant column \\(\\mathbf{1}\\) representing the intercept, which averages over the factor levels when there are other terms in the model.\n\\[\n\\mathbf{X} =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1\n\\end{pmatrix}\n\\]\nWith this coding, the parameters \\(\\boldsymbol{\\beta}\\) in the model are related to the means \\(\\boldsymbol{\\mu}\\) as,\n\\[\n\\begin{pmatrix}\n\\mu_a \\\\\n\\mu_b \\\\\n\\mu_c \\\\\n\\mu_d \\\\\n\\end{pmatrix}\n= \\mathbf{X} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_b \\\\\n\\beta_c \\\\\n\\beta_d \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}  \n\\beta_0           \\\\\n\\beta_0 + \\beta_b \\\\\n\\beta_0 + \\beta_c \\\\\n\\beta_0 + \\beta_d \\\\\n\\end{pmatrix}\n\\]\nThus we have,\n\\[\n\\begin{pmatrix}\n\\mu_a \\\\\n\\mu_b \\\\\n\\mu_c \\\\\n\\mu_d \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}  \n\\beta_0           \\\\\n\\beta_0 + \\beta_b \\\\\n\\beta_0 + \\beta_c \\\\\n\\beta_0 + \\beta_d \\\\\n\\end{pmatrix}\n\\]\nNote that \\(\\mathbf{X}\\) is non-singular as long as the comparisons in \\(\\mathbf{C}\\) are linearly independent. It’s inverse, \\(\\mathbf{X}^{-1}\\) determines how the transformed parameters relate to the original class means, that is, it determines the interpretation of the parameters in terms of the means.\n\nX &lt;- cbind(1, C)\nXinv &lt;- solve(X) |&gt; print()\n#    a b c d\n#    1 0 0 0\n# b -1 1 0 0\n# c -1 0 1 0\n# d -1 0 0 1\n\nThus, you can solve for the parameters in terms of the means symbolically as follows\n\\[\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_b \\\\\n\\beta_c \\\\\n\\beta_d \\\\\n\\end{pmatrix}\n= \\mathbf{X}^{-1} \\boldsymbol{\\mu} = \\begin{bmatrix}\n1 &  0 &  0 &  0 \\\\\n-1 &  1 &  0 &  0 \\\\\n-1 &  0 &  1 &  0 \\\\\n-1 &  0 &  0 &  1 \\\\\n\\end{bmatrix}\n\\begin{pmatrix}\n\\mu_a \\\\\n\\mu_b \\\\\n\\mu_c \\\\\n\\mu_d \\\\\n\\end{pmatrix}\n= \\begin{pmatrix}  \n\\mu_a           \\\\\n\\mu_b - \\mu_a   \\\\\n\\mu_c - \\mu_a   \\\\\n\\mu_d - \\mu_a    \\\\\n\\end{pmatrix}\n\\]\n\nDeviation coding\nAnother common coding scheme, useful for unordered factors, is deviation coding given by contrast.sum(). This has the property that the parameters are constrained to sum to zero, \\(\\Sigma \\beta_i = 0\\).\n\nC &lt;- contr.sum(letters[1:4]) |&gt; print()\n#   [,1] [,2] [,3]\n# a    1    0    0\n# b    0    1    0\n# c    0    0    1\n# d   -1   -1   -1\n\nThe parameters estimated with this coding are: With this coding, the intercept is \\(\\beta_0 = \\bar{\\mu}\\), the grand mean across all levels and the parameters are the deviations from that,\n\n\\(\\beta_1 = \\mu_a - \\bar{\\mu}\\)\n\\(\\beta_2 = \\mu_b - \\bar{\\mu}\\)\n\\(\\beta_3 = \\mu_c - \\bar{\\mu}\\)\n\nA (redundant) coefficient for the last group is omitted, because with this coding a coefficient for that group would be equal to the negative of the sum of the others, \\(\\beta_d = \\mu_d - \\bar{\\mu} = -(\\beta_1 + \\beta_2 + \\beta_3)\\).\nHelmert contrasts\nFor ordered factors, it is sensible to take the ordering into account in interpreting the results. Helmert contrasts are designed for this purpose. The intercept is again the grand mean across all levels. Each contrast compares the mean of a given level to the average of all previous ones in the order; they contrast the second level with the first, the third with the average of the first two, and so on.\n\nC &lt;- contr.helmert(letters[1:4]) |&gt; print()\n#   [,1] [,2] [,3]\n# a   -1   -1   -1\n# b    1   -1   -1\n# c    0    2   -1\n# d    0    0    3\n\nIt is easier to understand these if the columns are normalized so that the largest value in each column is 1.\n\nC %*% diag(1/c(1, 2, 3)) |&gt; MASS::fractions()\n#   [,1] [,2] [,3]\n# a   -1 -1/2 -1/3\n# b    1 -1/2 -1/3\n# c    0    1 -1/3\n# d    0    0    1\n\nThen we would have the coefficients as:\n\n\\(\\beta_0 = \\bar{\\mu}\\)\n\\(\\beta_1 = \\mu_b - \\mu_a\\)\n\\(\\beta_2 = \\mu_c - \\frac{\\mu_a + \\mu_b}{2}\\)\n\\(\\beta_3 = \\mu_d - \\frac{\\mu_a + \\mu_b + \\mu_c}{3}\\)\n\nNote that you can easily reverse the ordering of the comparisons to contrast each of the first \\(k-1\\) means with the average of all those higher in the order.\n\nC.rev &lt;- C[4:1, 3:1]\nrow.names(C.rev) &lt;- letters[1:4]\nC.rev\n#   [,1] [,2] [,3]\n# a    3    0    0\n# b   -1    2    0\n# c   -1   -1    1\n# d   -1   -1   -1\n\nPolynomial contrasts\nFor ordered factors that are also numeric, like Age = c(8, 9, 10, 11) or those that can be considered equally spaced along some continuum, polynomial contrasts, given by contr.poly(), provide orthogonal (uncorrelated) contrasts that assess the degree to which the factor means vary linearly, quadratically, and so on with the factor levels.\ncontr.poly() scales each column so that it’s sum of squares is 1. Each pair of columns is orthogonal, \\(\\mathbf{c}_i^\\top \\mathbf{c}_j = 0\\), so that \\(\\mathbf{C}^\\top \\mathbf{C} = \\mathbf{I}\\).\n\nC &lt;- contr.poly(4) |&gt; print()\n#          .L   .Q     .C\n# [1,] -0.671  0.5 -0.224\n# [2,] -0.224 -0.5  0.671\n# [3,]  0.224 -0.5 -0.671\n# [4,]  0.671  0.5  0.224\n\n# show they are orthonormal\nt(C) %*% C |&gt; zapsmall()\n#    .L .Q .C\n# .L  1  0  0\n# .Q  0  1  0\n# .C  0  0  1\n\nWe can get a better sense of orthogonal polynomial contrasts by taking a numeric vector \\(x\\), and raising it to successive powers, 1, 2, 3. Here \\(x^0 = 1\\) is the constant term or intercept.\n\nM &lt;- outer(1:8, 0:3, `^`)\ncolnames(M) &lt;- c(\"int\", \"lin\", \"quad\", \"cubic\")\nrownames(M) &lt;- paste0(\"x\", 1:8)\nM\n#    int lin quad cubic\n# x1   1   1    1     1\n# x2   1   2    4     8\n# x3   1   3    9    27\n# x4   1   4   16    64\n# x5   1   5   25   125\n# x6   1   6   36   216\n# x7   1   7   49   343\n# x8   1   8   64   512\n\nThen we can make the columns of M orthogonal using the Gram-Schmidt method, where each successive column after the first is made orthogonal to all previous columns by subtracting their projections on that column. Plotting these, as in Figure 5.3 shows that the coefficients for a linear term plot as a line, those for \\(x^2\\) follow a quadratic, and so forth.\n\nop &lt;- par(mar = c(4, 4, 1, 1)+.1)\nM1 &lt;- matlib::GramSchmidt(M, normalize = FALSE)\nmatplot(M1, \n  type = \"b\",\n  pch = as.character(0:3),\n  cex = 1.5,\n  cex.lab = 1.5,\n  lty = 1,\n  lwd = 3,\n  xlab = \"X\",\n  ylab = \"Coefficient\")\n\n\n\n\n\n\nFigure 5.3: The coefficients for orthogonal polynomial contrasts for linear (1), quadratic (2) and cubic (3) terms for a numeric variable \\(X\\). The intercept or constant term is represented as 0. Orthogonality means that each pair of values is uncorrelated.\n\n\n\n\nCustom contrasts\nYou don’t have to be constrained by the kinds of comparisons available in contr.* functions. For a factor with \\(k\\) levels you are free to make up any \\(k-1\\) contrasts that correspond to \\(k-1\\) different questions or tests of hypotheses about the factor level means. Even better: if your contrasts are orthogonal, their tests are statistically independent and they account for non-overlapping portions of the variance for the factor.\nOne useful idea for defining orthogonal comparisons of substantive interest is the idea of nested dichotomies. Here you would start with contrasting one meaningful subset of the groups against all the others. Then, successive contrasts are defined by making dichotomies among the groups within each subset.\n\nAs one example, suppose we are looking at support on some issue among four political parties: A, B, C and D, where A and B are left-of-center and C and D are to the right of the political spectrum. The following comparisons might of interest:\n\nAB.CD: {A, B} vs. {C, D}\nA.B: {A} vs. {B}\nC.D: {C} vs. {D}\n\nYou could set up these comparisons as the following contrasts:\n\n# contrasts\nAB.CD &lt;- c(1,  1, -1, -1)\nA.B   &lt;- c(1, -1,  0,  0)\nC.D   &lt;- c(0,  0,  1, -1)\n\n# put them in a matrix\nC &lt;- cbind(AB.CD, A.B, C.D)\nrownames(C) &lt;- LETTERS[1:4]\nC\n#   AB.CD A.B C.D\n# A     1   1   0\n# B     1  -1   0\n# C    -1   0   1\n# D    -1   0  -1\n\nWith a data frame like df with the factor party, you would then use these contrasts in a model by assigning C to contrasts(df$party):\n\nset.seed(47)\ndf &lt;- data.frame(\n  party = factor(rep(LETTERS[1:4], each = 3)),\n  support = c(35, 25, 25, 15) + round(rnorm(12, 0, 1), 1)\n)\ncontrasts(df$party) &lt;- C\n\nThen, in a linear model, the coefficients estimate the mean difference between the averages of the subset of parties in each comparison. For example, parties A and B on average are 2.11 higher in support than parties C and D; support for party A is 2.37 greater than party B and so forth.\n\nparty.mod &lt;- lm(support ~ party, data = df)\ncoef(party.mod)\n# (Intercept)  partyAB.CD    partyA.B    partyC.D \n#       24.83        2.11        2.37        1.85\n\nFor another example, say we are examining differences among three psychiatric diagnostic patient groups, “bipolar”, “manic”, “depressed” and also have a matched normal group. One set of meaningful comparisons would be as follows:\n\nD1: {Normal} vs. {Bipolar, Depressed, Manic}\nD2: {Bipolar} vs. {Depressed, Manic}\nD3: {Depressed} vs. {Manic}\n\nWeights for these contrasts are assigned by making them positive values for the groups in one subset and negative for the other, and giving numbers that sum to zero for each one:\n\nD1 &lt;- c(3, -1, -1, -1)\nD2 &lt;- c(0,  2, -1, -1)\nD3 &lt;- c(0,  0,  1, -1)\n\nC &lt;- cbind(D1, D2, D3)\nrownames(C) &lt;- c(\"Normal\", \"Bipolar\", \"Depressed\", \"Manic\")\nC\n#           D1 D2 D3\n# Normal     3  0  0\n# Bipolar   -1  2  0\n# Depressed -1 -1  1\n# Manic     -1 -1 -1\n\nThese have the same form as the reversed Helmert contrasts considered earlier.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "06-linear_models.html#what-have-we-learned",
    "href": "06-linear_models.html#what-have-we-learned",
    "title": "5  Overview of Linear models",
    "section": "\n5.2 What have we learned?",
    "text": "5.2 What have we learned?\nThis chapter introduced the fundamental building blocks of linear modeling in R, focusing on the versatile lm() function as our primary tool for fitting and understanding linear relationships in data. The goal for the chapter is to help you understand the mechanics of translating between the algebraic formulation of a linear model and your research questions using the tools in the lm() family.\nHere are the essential takeaways:\n\n\nThe lm() function is your Swiss Army knife for linear modeling – Whether you’re fitting simple regression, multiple regression, ANOVA, or ANCOVA models, lm() provides a unified interface through R’s elegant formula syntax. The beauty lies in how a model formula like y ~ x1 + x2 + x1:x2 captures complex relationships with intuitive notation.\n\n\nModel objects contain a wealth of information – An \"lm\" object isn’t just coefficients; it’s a comprehensive container holding fitted values, residuals, the design matrix, and diagnostic information. Learning to extract and manipulate these components with functions like coef(), fitted(), residuals(), and model.matrix() unlocks the full power of linear modeling.\nModel matrices reveal the algebraic heart of your analysis – Understanding how R transforms your formula into a design matrix via model.matrix() is fundamental to grasping what linear models actually compute. The journey from a formula like y ~ treatment + block to a matrix of 0s and 1s illuminates how categorical predictors are used in mathematical operations to calculate fitted values.\nContrasts control how factors speak to your research questions – The choice between treatment, sum, or Helmert contrasts isn’t just technical housekeeping – it determines which comparisons your model coefficients represent. Using contrasts() and C() strategically means your model output directly answers your scientific questions rather than leaving you to decode cryptic parameter estimates.\n\nHowever fitting a model is just the first step. While summary() and anova() provide essential numerical summaries, diagnostic plots created with plot(lm_object) expose patterns in residuals. Other plots help to identify influential observations, and validate model assumptions. The interplay between numerical and graphical summaries is where true understanding emerges. This is the topic of Chapter 6.\n\n\n\n\nBashaw, W. L., & Findley, W. G. (Eds.). (1967). Symposium on general linear model approach to the analysis of experimental data in educational research, final report. https://files.eric.ed.gov/fulltext/ED026737.pdf\n\n\nBock, R. D. (1963). Programming univariate and multivariate analysis of variance. Technometrics, 5(1), 95–117. https://doi.org/10.1080/00401706.1963.10490061\n\n\nBock, R. D. (1964). A computer program forunivariate and multivariate analysis of variance. Proceedings of Scientific Symposium on Statistics.\n\n\nClyde, D. J., Cramer, E. M., & Sherin, R. J. (1966). Multivariate statistical programs. Biometric Laboratory,University of Miami.\n\n\nDixon, W. J. (1965). BMD biomedical computer programs. Health Sciences Computing Facility, School of Medicine, University of California; Health Sciences Computing Faculty.\n\n\nFinn, J. D. (1967). MULTIVARIANCE: Fortran program for univariate and multivariate analysis of variance and covariance. School of Education, State University of New York at Buffalo.\n\n\nFisher, R. A. (1923). Studies in crop variation. II. The manurial response of different potato varieties. The Journal of Agricultural Science, 13(2), 311–320. https://hdl.handle.net/2440/15179\n\n\nFisher, R. A. (1925). Statistical methods for research workers. Oliver & Boyd.\n\n\nFox, J., & Weisberg, S. (2018). An R companion to applied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGalton, F. (1889). Natural inheritance. Macmillan. http://galton.org/books/natural-inheritance/pdf/galton-nat-inh-1up-clean.pdf\n\n\nGraybill, F. A. (1961). An introduction to linear statistical models. McGraw-Hill.\n\n\nIBM. (1965). Proceedings of the IBM scientific computing symposium on statistics: Oct 21-23, 1963 (L. Robinson, Ed.). IBM. https://www.amazon.com/Proceedings-Scientific-Computing-Symposium-Statistics/dp/B000GL5RLU\n\n\nPearson, K. (1896). Contributions to the mathematical theory of evolution—III, regression, heredity and panmixia. Philosophical Transactions of the Royal Society of London, 187, 253–318.\n\n\nScheffé, H. A. (1960). The analysis of variance. Wiley.\n\n\nWilkinson, G. N., & Rogers, C. E. (1973). Symbolic description of factorial models for analysis of variance. Applied Statistics, 22(3), 392. https://doi.org/10.2307/2346786\n\n\nWiner, B. J. (1962). Statistical principles in experimental design. McGraw-Hill.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "06-linear_models.html#footnotes",
    "href": "06-linear_models.html#footnotes",
    "title": "5  Overview of Linear models",
    "section": "",
    "text": "Taking logarithms of both sides would yield the linear model, \\(log(y) = c + \\beta_1 x\\).↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "07-linear_models-plots.html",
    "href": "07-linear_models-plots.html",
    "title": "6  Plots for Univariate Response Models",
    "section": "",
    "text": "6.1 The “regression quartet”\nFor a univariate linear model fit using lm(), glm() and similar functions, the standard plot() method gives basic versions of diagnostic plots of residuals and other calculated quantities for assessing possible violations of the model assumptions. Some of these can be considerably enhanced using other packages.\nBeyond this,\nThe classic reference on regression diagnostics is Belsley et al. (1980). My favorite modern texts are the brief Fox (2020) and the more complete Fox & Weisberg (2018a), both of which are supported by the car package (Fox & Weisberg, 2019). Some of the examples in this chapter are inspired by Fox & Weisberg (2018a).\nPackages\nIn this chapter I use the following packages. Load them now:\nFor a fitted model, plotting the model object with plot(model) provides for any of six basic plots. Four of these are produced by default, giving rise to the term regression quartet for this collection. These are:\nOne key feature of these plots is providing reference lines or smoothed curves for ease of judging the extent to which a plot conforms to the expected pattern; another is the labeling of observations which deviate from an assumption.\nThe base-R plot(model) plots are done much better in a variety of packages. I illustrate some versions from the car (Fox & Weisberg, 2019) and performance (Lüdecke et al., 2021) packages, part of the easystats (Lüdecke et al., 2022) suite of packages.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for Univariate Response Models</span>"
    ]
  },
  {
    "objectID": "07-linear_models-plots.html#sec-regression-quartet",
    "href": "07-linear_models-plots.html#sec-regression-quartet",
    "title": "6  Plots for Univariate Response Models",
    "section": "",
    "text": "Residuals vs. Fitted: For well-behaved data, the points should hover around a horizontal line at residual = 0, with no obvious pattern or trend.\nNormal Q-Q plot: A plot of sorted standardized residuals \\(e_i\\) (obtained from rstudent(model)) against the theoretical values those values would have in a standard normal \\(\\mathcal{N}(0, 1)\\) distribution. Well-behaved residuals should appear as a line with slope=1 (when the aspect ratio is 1) in such plots. Departures from normality usually appear as systematic curvature above or below the line in the tails of the distribution.\nScale-Location: Plots the square-root of the absolute values of the standardized residuals \\(\\sqrt{| e_i |}\\) as a measure of “scale” against the fitted values \\(\\hat{y}_i\\) as a measure of “location”. This provides an assessment of homogeneity of variance. Violations appear as a tendency for scale (variability) to vary with location.\nResiduals vs. Leverage: Plots standardized residuals against leverage to help identify possibly influential observations. Leverage, or “hat” values (given by hat(model)) are proportional to the squared Mahalanobis distances of the predictor values \\(\\mathbf{x}_i\\) from the means, and measure the potential of an observation to change the fitted coefficients if that observation was deleted. Actual influence is measured by Cooks’s distance (cooks.distance(model)) and is proportional to the product of residual times leverage. Contours of constant Cook’s \\(D\\) are added to the plot.\n\n\n\n\n6.1.1 Example: Duncan’s occupational prestige\nIn a classic study in sociology, Duncan (1961) used data from the U.S. Census in 1950 to study how one could predict the prestige of occupational categories — which is hard to measure — from available information in the census for those occupations. His data is available in carData:Duncan, and contains\n\n\ntype: the category of occupation, one of prof (professional), wc (white collar) or bc (blue collar);\n\nincome: the percentage of occupational incumbents with a reported income &gt; 3500 (about 40,000 in current dollars);\n\neducation: the percentage of occupational incumbents who were high school graduates;\n\nprestige: the percentage of respondents in a social survey who rated the occupation as “good” or better in prestige.\n\nThese variables are a bit quirky in they are measured in percents, 0-100, rather dollars for income and years for education, but this common scale permitted Duncan to ask an interesting sociological question: Assuming that both income and education predict prestige, are they equally important, as might be assessed by testing the hypothesis \\(\\mathcal{H}_0: \\beta_{\\text{income}} = \\beta_{\\text{education}}\\). If so, this would provide a very simple theory for occupational prestige.\nA quick look at the data shows the variables and a selection of the occupational categories, which are the row.names() of the dataset.\n\ndata(Duncan, package = \"carData\")\nset.seed(42)\ncar::some(Duncan)\n#                  type income education prestige\n# accountant       prof     62        86       82\n# professor        prof     64        93       93\n# engineer         prof     72        86       88\n# factory.owner    prof     60        56       81\n# store.clerk        wc     29        50       16\n# carpenter          bc     21        23       33\n# machine.operator   bc     21        20       24\n# barber             bc     16        26       20\n# soda.clerk         bc     12        30        6\n# janitor            bc      7        20        8\n\nLet’s start by fitting a simple model using just income and education as predictors. The results look very good! Both income and education are highly significant and the \\(R^2 = 0.828\\) for the model indicates that prestige is very well predicted by just these variables.\n\nduncan.mod &lt;- lm(prestige ~ income + education, data=Duncan)\nsummary(duncan.mod)\n# \n# Call:\n# lm(formula = prestige ~ income + education, data = Duncan)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -29.54  -6.42   0.65   6.61  34.64 \n# \n# Coefficients:\n#             Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)  -6.0647     4.2719   -1.42     0.16    \n# income        0.5987     0.1197    5.00  1.1e-05 ***\n# education     0.5458     0.0983    5.56  1.7e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 13.4 on 42 degrees of freedom\n# Multiple R-squared:  0.828, Adjusted R-squared:  0.82 \n# F-statistic:  101 on 2 and 42 DF,  p-value: &lt;2e-16\n\nBeyond this, Duncan was interested in the coefficients and whether income and education could be said to have equal impacts on predicting occupational prestige. A nice display of model coefficients with confidence intervals is provided by parameters::model_parameters().\n\nparameters::model_parameters(duncan.mod)\n# Parameter   | Coefficient |   SE |         95% CI | t(42) |      p\n# ------------------------------------------------------------------\n# (Intercept) |       -6.06 | 4.27 | [-14.69, 2.56] | -1.42 | 0.163 \n# income      |        0.60 | 0.12 | [  0.36, 0.84] |  5.00 | &lt; .001\n# education   |        0.55 | 0.10 | [  0.35, 0.74] |  5.56 | &lt; .001\n\nWe can also test Duncan’s hypothesis that income and education have equal effects on prestige with car::linearHypothesis(). This is constructed as a test of a restricted model in which the two coefficients are forced to be equal against the unrestricted model. Duncan was very happy with this result.\n\ncar::linearHypothesis(duncan.mod, \"income = education\")\n# \n# Linear hypothesis test:\n# income - education = 0\n# \n# Model 1: restricted model\n# Model 2: prestige ~ income + education\n# \n#   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n# 1     43 7519                         \n# 2     42 7507  1      12.2 0.07    0.8\n\nEquivalently, the linear hypothesis that \\(\\beta_{\\text{Inc}} = \\beta_{\\text{Educ}}\\) can be tested with a Wald test1 for difference between these coefficients, expressed as \\(\\mathcal{H}_0 : \\mathbf{C} \\boldsymbol{\\beta} = 0\\), using \\(\\mathbf{C} = (0, -1, 1)\\). The estimated value, -0.053 has a confidence interval [-0.462, 0.356], consistent with Duncan’s hypothesis.\n\nwtest &lt;- spida2::wald(duncan.mod, c(0, -1, 1))[[1]]\nwtest$estimate\n#       \n#        Estimate Std.Error DF t-value p-value Lower 0.95 Upper 0.95\n#   Larg  -0.0529     0.203 42  -0.261   0.795     -0.462      0.356\n\nWe can visualize this test and confidence intervals using a joint confidence ellipse for the coefficients for income and education in the model duncan.mod. In Figure 6.1 the size of the ellipse is set to \\(\\sqrt{F^{0.95}_{1,\\nu}} = t^{0.95}_{\\nu}\\), so that its shadows on the horizontal and vertical axes correspond to 1D 95% confidence intervals. In this plot, the line through the origin with slope \\(= 1\\) corresponds to equal coefficients for income and education and the line with slope \\(= -1\\) corresponds to their difference, \\(\\beta_{\\text{Educ}} - \\beta_{\\text{Inc}}\\). The orthogonal projection of the coefficient vector \\((\\widehat{\\beta}_{\\text{Inc}}, \\widehat{\\beta}_{\\text{Educ}})\\) (the center of the ellipse) is the point estimate of \\(\\widehat{\\beta}_{\\text{Educ}} - \\widehat{\\beta}_{\\text{Inc}}\\) and the shadow of the ellipse along this axis is the 95% confidence interval for the difference in slopes.\n\n\nSee the codeconfidenceEllipse(duncan.mod, col = \"blue\",\n  levels = 0.95, dfn = 1,\n  fill = TRUE, fill.alpha = 0.2,\n  xlim = c(-.4, 1),\n  ylim = c(-.4, 1), asp = 1,\n  cex.lab = 1.3,\n  grid = FALSE,\n  xlab = expression(paste(\"Income coefficient, \", beta[Inc])),\n  ylab = expression(paste(\"Education coefficient, \", beta[Educ])))\n\nabline(h=0, v=0, lwd = 2)\n\n# confidence intervals for each coefficient\nbeta &lt;- coef( duncan.mod )[-1]\nCI &lt;- confint(duncan.mod)       # confidence intervals\nlines( y = c(0,0), x = CI[\"income\",] , lwd = 5, col = 'blue')\nlines( x = c(0,0), y = CI[\"education\",] , lwd = 5, col = 'blue')\npoints(rbind(beta), col = 'black', pch = 16, cex=1.5)\npoints(diag(beta) , col = 'black', pch = 16, cex=1.4)\narrows(beta[1], beta[2], beta[1], 0, angle=8, len=0.2)\narrows(beta[1], beta[2], 0, beta[2], angle=8, len=0.2)\n\n# add line for equal slopes\nabline(a=0, b = 1, lwd = 2, col = \"darkgreen\")\ntext(0.8, 0.8, expression(beta[Educ] == beta[Inc]), \n     srt=45, pos=3, cex = 1.5, col = \"darkgreen\")\n\n# add line for difference in slopes\ncol &lt;- \"darkred\"\nx &lt;- c(-1.5, .5)\nlines(x=x, y=-x)\ntext(-.15, -.15, expression(~beta[\"Educ\"] - ~beta[\"Inc\"]), \n     col=col, cex=1.5, srt=-45)\n\n# confidence interval for b1 - b2\nwtest &lt;- spida2::wald(duncan.mod, c(0, -1, 1))[[1]]\nlower &lt;- wtest$estimate$Lower /2\nupper &lt;- wtest$estimate$Upper / 2\nlines(-c(lower, upper), c(lower,upper), lwd=6, col=col)\n\n# projection of (b1, b2) on b1-b2 axis\nbeta &lt;- coef( duncan.mod )[-1]\nbdiff &lt;- beta %*% c(1, -1)/2\npoints(bdiff, -bdiff, pch=16, cex=1.3)\narrows(beta[1], beta[2], bdiff, -bdiff, \n       angle=8, len=0.2, col=col, lwd = 2)\n\n# calibrate the diff axis\nticks &lt;- seq(-0.3, 0.3, by=0.2)\nticklen &lt;- 0.02\nsegments(ticks, -ticks, ticks-sqrt(2)*ticklen, -ticks-sqrt(2)*ticklen)\ntext(ticks-2.4*ticklen, -ticks-2.4*ticklen, ticks, srt=-45)\n\n\n\n\n\n\nFigure 6.1: Joint 95% confidence ellipse for \\((\\beta_{\\text{Inc}}, \\beta_{\\text{Educ}})\\), together with their 1D shadows, which give 95% confidence intervals for the separate coefficients and the linear hypothesis that the coefficients are equal. Projecting the confidence ellipse along the line with unit slope gives a confidence interval for the difference between coefficients, shown by the dark red line.\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.2 Diagnostic plots\nBut, should Duncan be so happy? It is unlikely that he ran any model diagnostics or plotted his model; we do so now. Here is the regression quartet (Figure 6.2) for this model. Each plot shows some trend lines, and importantly, labels some observations that stand out and might deserve attention. \n\nplot(duncan.mod, lwd=2, pch=16,\n     cex.lab = 1.2, cex = 1.1, cex.id = 1.2)\n\n\n\n\n\n\nFigure 6.2: Regression quartet of diagnostic plots for the Duncan data. Several possibly unusual observations are labeled.\n\n\n\n\nSome points to note:\n\nA few observations (minister, reporter, conductor, contractor) are flagged in multiple panels. It turns out (Section 6.5.3) that the observations for minister and reporter noted in the residuals vs. leverage plot are highly influential and largely responsible for Duncan’s finding that the slopes for income and education could be considered equal.\nThe red trend line in the scale-location plot indicates that residual variance is not constant, but rather increases from both ends. This is a consequence of the fact that prestige is measured as a percentage, bounded at [0, 100], and the standard deviation of a percentage \\(p\\) is proportional to \\(\\sqrt{p \\times (1-p)}\\) which is maximal at \\(p = 0.5\\).\n\nSimilar, but nicer-looking diagnostic plots are provided by performance::check_model() which uses ggplot2 for graphics. These include helpful captions indicating what should be observed for each for a good-fitting model. However, they don’t have as good facilities for labeling unusual observations as the base R plot.lm() method or functions in the car package.\n\ncheck_model(duncan.mod, \n            check=c(\"linearity\", \"qq\", \n                    \"homogeneity\", \"outliers\"))\n\n\n\n\n\n\nFigure 6.3: Diagnostic plots for the Duncan data, using performance::check_model().\n\n\n\n\n\n6.1.3 Example: Canadian occupational prestige\n\nFollowing Duncan (1961), occupational prestige was studied in a Canadian context by Bernard Blishen and others at York University, giving the dataset Prestige which we looked at in Example 3.2. It differs from the Duncan dataset primarily in that the main variables—prestige, income and education were revamped to better reflect the underlying constructs in more meaningful units.\n\nprestige: Rather than a simple percentage of “good+” ratings, this uses a wider and more reliable scale from Pineo & Porter (1967) on a scale from 10–90.\nincome is measured as the average income of incumbents in each occupation, in 1971 dollars, rather than percent exceeding a given threshold ($3500)\neducation is measured as the average education of occupational incumbents, years.\n\nThe dataset again includes type of occupation with the same levels \"bc\" (blue collar), \"wc\" (white collar) and \"prof\" (professional)2, but in addition includes the percent of women in these occupational categories.\nOur interest again is in understanding how prestige is related to census measures of the average education, income, percent women of incumbents in those occupations, but with attention to the scales of measurement and possibly more complex relationships.\n\ndata(Prestige, package=\"carData\")\n# Reorder levels of type\nPrestige$type &lt;- factor(Prestige$type, \n                        levels=c(\"bc\", \"wc\", \"prof\")) \nstr(Prestige)\n# 'data.frame': 102 obs. of  6 variables:\n#  $ education: num  13.1 12.3 12.8 11.4 14.6 ...\n#  $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...\n#  $ women    : num  11.16 4.02 15.7 9.11 11.68 ...\n#  $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...\n#  $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...\n#  $ type     : Factor w/ 3 levels \"bc\",\"wc\",\"prof\": 3 3 3 3 3 3 3 3 3 3 ...\n\nWe fit a main-effects model using all predictors (ignoring census):\n\nprestige.mod &lt;- lm(prestige ~ education + income + women + type,\n                   data=Prestige)\n\nplot(model) produces four separate plots by default, prompting you to see the next plot. For a quick look, I like to arrange them in a single 2 x 2 figure, using par(mfrow = c(2,2))\n\nop &lt;- par(mfrow = c(2,2), \n          mar=c(4,4,3,1)+.1)\nplot(prestige.mod, lwd=2, cex.lab=1.4)\npar(op)\n\n\n\n\n\n\nFigure 6.4: Regression quartet of diagnostic plots for the Prestige data. Several possibly unusual observations are labeled in each plot.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for Univariate Response Models</span>"
    ]
  },
  {
    "objectID": "07-linear_models-plots.html#sec-coefficient-displays",
    "href": "07-linear_models-plots.html#sec-coefficient-displays",
    "title": "6  Plots for Univariate Response Models",
    "section": "\n6.2 Coefficient displays",
    "text": "6.2 Coefficient displays\nThe results of linear models are most often reported in tables and typically with “significance stars” (*, **, ***) to indicate the outcome of hypothesis tests. These are useful for looking up precise values and you can use this format to compare a small number of competing models side-by-side. However, as illustrated by Kastellec & Leoni (2007), plots of coefficients can increase the clarity of presentation and make it easier to draw correct conclusions. Yet, when you need to present tables, there is a variety of tools in R that can help make them attractive in publications.\nFor illustration, I’ll consider three models for the Prestige data of increasing complexity:\n\n\nmod1 fits the main effects of the three quantitative predictors;\n\nmod2 adds the categorical variable type of occupation;\n\nmod3 allows an interaction of income with type.\n\n\nmod1 &lt;- lm(prestige ~ education + income + women,\n           data=Prestige)\nmod2 &lt;- lm(prestige ~ education + women + income + type,\n           data=Prestige)\nmod3 &lt;- lm(prestige ~ education + women + income * type,\n           data=Prestige)\n\nFrom our earlier analyses (Example 3.2) we saw that the marginal relationship between income and prestige was nonlinear Figure 3.12), and was better represented in a linear model using log(income) (Section 3.2.4) shown in Figure 3.15. However, this possibly non-linear relationship could also be explained by stratifying (Section 3.2.5) the data by type of occupation (Figure 3.16).\n\n6.2.1 Displaying coefficients\nsummary() gives the complete precis of a fitted model, with information about the estimated coefficients, residuals and goodness-of fit statistics like \\(R^2\\). But if you only want to see the coefficients, standard errors, etc. lmtest::coeftest() gives these results in the familiar format for console output. broom::tidy() places these in a tidy format common to many modeling functions which is useful for futher processing (e.g., comparing models).\n\nlmtest::coeftest(mod1)\n# \n# t test of coefficients:\n# \n#              Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept) -6.794334   3.239089   -2.10    0.039 *  \n# education    4.186637   0.388701   10.77  &lt; 2e-16 ***\n# income       0.001314   0.000278    4.73  7.6e-06 ***\n# women       -0.008905   0.030407   -0.29    0.770    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nbroom::tidy(mod1)\n# # A tibble: 4 × 5\n#   term        estimate std.error statistic  p.value\n#   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n# 1 (Intercept) -6.79     3.24        -2.10  3.85e- 2\n# 2 education    4.19     0.389       10.8   2.59e-18\n# 3 income       0.00131  0.000278     4.73  7.58e- 6\n# 4 women       -0.00891  0.0304      -0.293 7.70e- 1\n\nThe modelsummary package (Arel-Bundock, 2022) is an easy to use, very general package to summarize data and statistical models in R. The main function modelsummary() can produce highly customizable tables of coefficients in a wide variety of output formats, including HTML, PDF, LaTeX, Markdown, and MS Word. You can select the statistics displayed for any model term with the estimate and statistic arguments.\n\n\nmodelsummary(list(\"Model1\" = mod1),\n  coef_omit = \"Intercept\",\n  shape = term ~ statistic,\n  estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n  statistic = c(\"std.error\", \"p.value\"),\n  fmt = fmt_statistic(\"estimate\" = 3, \"conf.low\" = 3, \"conf.high\" = 3),\n  gof_omit = \".\")\n\n\nTable 6.1: Table of coefficients for the main effects model.\n\n\n\n\n    \n\n\n      \n\n\n \nModel1\n\n\n \n                Est.\n                S.E.\n                p\n              \n\n\n\neducation\n                  4.187 [3.415, 4.958]\n                  0.389\n                  0.000\n                \n\nincome\n                  0.001 [0.001, 0.002]\n                  0.000\n                  0.000\n                \n\nwomen\n                  -0.009 [-0.069, 0.051]\n                  0.030\n                  0.770\n                \n\n\n\n\n\n\n\n\n\ngof_omit allows you to omit or select the goodness-of-fit statistics and other model information available from those listed by get_gof():\n\nget_gof(mod1)\n#   aic bic r.squared adj.r.squared rmse nobs   F logLik\n# 1 716 729     0.798         0.792 7.69  102 129   -353\n\n\n6.2.2 Visualizing coefficients\nmodelplot() is the companion function. It allows you to plot model estimates and confidence intervals. It makes it easy to subset, rename, reorder, and customize plots using same mechanics as in modelsummary().\n\ntheme_set(theme_minimal(base_size = 14))\n\nmodelplot(mod1, coef_omit=\"Intercept\", \n          color=\"red\", size=1, linewidth=2) +\n  labs(title=\"Raw coefficients for mod1\")\n\n\n\n\n\n\nFigure 6.5: Plot of coefficients and their standard error bars for the simple main effects model\n\n\n\n\nBut this plot is disappointing and misleading because it shows the raw coefficients, which are on different scales. From the plot, it looks like only education has a non-zero effect, but the effect of income is also highly significant. The problem is that the magnitude of the coefficient \\(\\hat{b}_{\\text{education}}\\) is more than 40,000 times that of the other coefficients, because education is measured years, while income is measured in dollars. The 95% confidence interval for \\(\\hat{b}_{\\text{income}} = [0.0008, 0.0019]\\), but this is invisible in the plot.\nBefore figuring out how to fix this issue, I show the comparable displays from modelsummary() and modelplot() for all three models. When you give modelsummary() a list of models, it displays their coefficients side-by-side as shown in Table 6.2.\n\nmodels &lt;- list(\"Model1\" = mod1, \"Model2\" = mod2, \"Model3\" = mod3)\nmodelsummary(models,\n     coef_omit = \"Intercept\",\n     fmt = 2,\n     stars = TRUE,\n     shape = term ~ statistic,\n     statistic = c(\"std.error\", \"p.value\"),\n     gof_map = c(\"rmse\", \"r.squared\")\n     )\n\n\nTable 6.2: Table of coefficients for three models.\n\n\n\n\n    \n\n\n      \n\n\n \nModel1\nModel2\nModel3\n\n\n \n                Est.\n                S.E.\n                p\n                Est.\n                S.E.\n                p\n                Est.\n                S.E.\n                p\n              \n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\neducation\n                  4.19***\n                  0.39\n                  &lt;0.01\n                  3.66***\n                  0.65\n                  &lt;0.01\n                  2.80***\n                  0.59\n                  &lt;0.01\n                \n\nincome\n                  0.00***\n                  0.00\n                  &lt;0.01\n                  0.00***\n                  0.00\n                  &lt;0.01\n                  0.00***\n                  0.00\n                  &lt;0.01\n                \n\nwomen\n                  -0.01\n                  0.03\n                  0.77\n                  0.01\n                  0.03\n                  0.83\n                  0.08*\n                  0.03\n                  0.02\n                \n\ntypewc\n                  \n                  \n                  \n                  -2.92\n                  2.67\n                  0.28\n                  3.43\n                  5.37\n                  0.52\n                \n\ntypeprof\n                  \n                  \n                  \n                  5.91\n                  3.94\n                  0.14\n                  27.55***\n                  5.41\n                  &lt;0.01\n                \n\nincome × typewc\n                  \n                  \n                  \n                  \n                  \n                  \n                  -0.00\n                  0.00\n                  0.21\n                \n\nincome × typeprof\n                  \n                  \n                  \n                  \n                  \n                  \n                  -0.00***\n                  0.00\n                  &lt;0.01\n                \n\nRMSE\n                  7.69\n                  \n                  \n                  6.91\n                  \n                  \n                  6.02\n                  \n                  \n                \n\nR2\n                  0.798\n                  \n                  \n                  0.835\n                  \n                  \n                  0.875\n                  \n                  \n                \n\n\n\n\n\n\n\n\n\nNote that a factor predictor (like type here) with \\(d\\) levels is represented by \\(d-1\\) coefficients in main effects and in interactions with quantitative variables. These levels are coded with treatment contrasts by default. Also by default, the first level is set as the reference level in alphabetical order. Here the reference level is blue collar (bc), so the coefficient typeprof = 5.91 indicates that professional occupations on average are rated 5.91 greater on the Prestige scale than blue collar workers.\nNote also that unlike the table, the coefficients in Figure 6.5 are ordered from bottom to top, because the Y axis starts at the lower left corner. In Figure 6.6 I use scale_y_discrete() to reverse the order. It is also useful to add a vertical reference line at \\(\\beta = 0\\).\n\nmodelplot(models, \n          coef_omit=\"Intercept\", \n          size=1.3, linewidth=2) +\n  ggtitle(\"Raw coefficients\") +\n  geom_vline(xintercept = 0, linewidth=1.5) +\n  scale_y_discrete(limits=rev) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.2))\n\n\n\n\n\n\nFigure 6.6: Plot of raw coefficients and their confidence intervals for all three models\n\n\n\n\n\n6.2.3 More useful coefficient plots\nThe problem with plots of raw coefficients shown in Figure 6.5 and Figure 6.6 is that the coefficients for different predictors are not directly comparable because they are measured in different units.\nOne alternative is to plot the standardized coefficients. Another way is to re-scale the predictors into more comparable and meaningful units. I illustrate these ideas below.\nStandardized coefficients\nThe simplest way to do this is to transform all variables to standardized (\\(z\\)) scores. The coefficients are then interpreted as the standardized change in prestige for a one standard deviation change in the predictors. The syntax below uses scale to transform all the numeric variables. Then, we re-fit the models using the standardized data.\n\nPrestige_std &lt;- Prestige |&gt;\n  as_tibble() |&gt;\n  mutate(across(where(is.numeric), scale))\n\nmod1_std &lt;- lm(prestige ~ education + income + women, \n               data=Prestige_std)\nmod2_std &lt;- lm(prestige ~ education + women + income + type, \n               data=Prestige_std)\nmod3_std &lt;- lm(prestige ~ education + women + income * type, \n               data=Prestige_std)\n\nThe plot in Figure 6.7 now shows the significant effect of income in all three models. As well, it offers a more sensitive comparison of the coefficients of other terms across models; for example women is not significant in models 1 and 2, but becomes significant in Model 3 when the interaction of income * type is included.\n\nmodels &lt;- list(\"Model1\" = mod1_std, \"Model2\" = mod2_std, \"Model3\" = mod3_std)\nmodelplot(models, \n          coef_omit=\"Intercept\", size=1.3) +\n  ggtitle(\"Standardized coefficients\") +\n  geom_vline(xintercept = 0, linewidth = 1.5) +\n  scale_y_discrete(limits=rev) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.2))\n\n\n\n\n\n\nFigure 6.7: Plot of standardized coefficients and their confidence intervals for all three models\n\n\n\n\nIt turns out there is an easier way to get plots of standardized coefficients. modelsummary() extracts coefficients from model objects using the parameters package, and that package offers several options for standardization: See model parameters documentation. We can pass the standardize=\"refit\" (or other) argument directly to modelsummary() or modelplot(), and that argument will be forwarded to parameters. The plot produced by the code below is identical to Figure 6.7 and is not shown.\n\nmodelplot(list(\"mod1\" = mod1, \"mod2\" = mod2, \"mod3\" = mod3),\n          standardize = \"refit\",\n          coef_omit=\"Intercept\", size=1.3) +\n  ggtitle(\"Standardized coefficients\") +\n  geom_vline(xintercept = 0, linewidth=1.5) +\n  scale_y_discrete(limits=rev) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.2))\n\nThe ggstats package (Larmarange, 2025) provides even nicer versions of coefficient plots that handle factors in a more reasonable way, as levels within the factor. ggcoef_model() plots a single model and ggcoef_compare() plots a list of models using sensible defaults. A small but nice feature is that it explicitly shows the 0 value for the reference level of a factor (type = \"bc\" here) and uses better labels for factors and their interactions.\n\nmodels &lt;- list(\n  \"Base model\"      = mod1_std,\n  \"Add type\"        = mod2_std,\n  \"Add interaction\" = mod3_std)\n\nggcoef_compare(models) +\n  labs(x = \"Standarized Coefficient\")\n\n\n\n\n\n\nFigure 6.8: Model comparison plot from ggcoef_compare()\n\n\n\n\nMore meaningful units\nStandardizing the variables makes the coefficients directly comparable, but it may be harder to understand what they mean in terms of the variables. For example, the coefficient of income in mod2_std is 0.25. A literal interpretation is that occupational prestige is expected to increase 0.25 standard deviations for each standard deviation increase in income, but it may be difficult to appreciate what this means.\nA better substantive comparison of the coefficients could use understandable scales for the predictors, e.g., months of education, $100,000 of income or 10% of women’s participation. Note that the effect of this is just to multiply the coefficients and their standard errors by a factor. The statistical conclusions of significance are unchanged.\nFor simplicity, I do this just for Model 1.\n\nPrestige_scaled &lt;- Prestige |&gt;\n  mutate(education = 12 * education,\n         income = income / 100,\n         women = women / 10)\n\nmod1_scaled &lt;- lm(prestige ~ education + income + women,\n                  data=Prestige_scaled)\n\nWhen we plot this with ggcoef_model(), there are many options to control how variables are labeled and other details.\n\nggcoef_model(mod1_scaled,\n  signif_stars = FALSE,\n  variable_labels = c(education = \"education\\n(months)\",\n                      income = \"income\\n(/$100K)\",\n                      women = \"women\\n(/10%)\")) +\n  xlab(\"Coefficients for prestige with scaled predictors\")\n\n\n\n\n\n\nFigure 6.9: Plot of coefficients for prestige with scaled predictors for Model 1.\n\n\n\n\nSo, on average, each additional month of education increases the prestige rating by 0.34 units, while an additional $100,000 of income increases it by 0.13 units. While these are significant effects, they are not large in relation to the scale of prestige which ranges 14.8—87.2.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for Univariate Response Models</span>"
    ]
  },
  {
    "objectID": "07-linear_models-plots.html#sec-avplots",
    "href": "07-linear_models-plots.html#sec-avplots",
    "title": "6  Plots for Univariate Response Models",
    "section": "\n6.3 Added-variable and related plots",
    "text": "6.3 Added-variable and related plots\nIn multiple regression problems, it is most often useful to construct a scatterplot matrix and examine the plot of the response vs. each of the predictors as well as those of the predictors against each other. However, these simple, marginal scatterplots of a response \\(y\\) against each of several predictors \\(x_1, x_2, \\dots\\) can be misleading because each one ignores the other predictors.\n\nTo see this, consider the dataset matlib::coffee, giving measures of Heart (\\(y\\)), an index of cardiac damage, Coffee (\\(x_1\\)), a measure of daily coffee consumption, and Stress (\\(x_2\\)), a measure of occupational stress, in a contrived sample of \\(n=20\\) university people.3 For the sake of the example we assume that the main goal is to determine whether or not coffee is good or bad for your heart, and stress represents one potential confounding variable among others (age, smoking, etc.) that might be useful to control statistically.\n\nset.seed(1234)\ndata(coffee, package=\"matlib\")\ncoffee |&gt; dplyr::sample_n(6)\n#          Group Coffee Stress Heart\n# 1 Grad_Student    104    117    92\n# 2      Student     52     86    63\n# 3 Grad_Student     76     92    58\n# 4      Student    100    123    92\n# 5      Student     64     74    63\n# 6    Professor    141    175   145\n\n\ndata(coffee, package=\"matlib\")\n\nscatterplotMatrix(~ Heart + Coffee + Stress, \n  data=coffee,\n  smooth = FALSE,\n  ellipse = list(levels=0.68, fill.alpha = 0.1),\n  pch = 19, cex.labels = 2.5)\n\n\n\n\n\n\nFigure 6.10: Scatterplot matrix showing pairwise relations among Heart damage (\\(y\\)), Coffee consumption (\\(x_1\\)) and Stress (\\(x_2\\)), with linear regression lines and 68% data ellipses for the bivariate relations\n\n\n\n\nThe message from these marginal plots in Figure 6.10 seems to be that coffee is bad for your heart, stress is bad for your heart, and stress is also strongly related to coffee consumption. Yet, when we fit a model with both variables together, we get the following results:\n\nfit.both   &lt;- lm(Heart ~ Coffee + Stress, data=coffee)\nlmtest::coeftest(fit.both)\n# \n# t test of coefficients:\n# \n#             Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)   -7.794      5.793   -1.35     0.20    \n# Coffee        -0.409      0.292   -1.40     0.18    \n# Stress         1.199      0.224    5.34  5.4e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe coefficients suggest that stress is indeed bad for your heart, but the negative (though non-significant) coefficient for coffee suggests that coffee is good for you. How can this be? Does that mean I should drink more coffee, while avoiding stress?\nThe reason for this apparent paradox is that the general linear model fit by lm() estimates all effects together and so the coefficients pertain to the partial effect of a given predictor, adjusting for the effects of all others. That is, the coefficient for coffee (\\(\\beta_{\\text{Coffee}} = -0.41\\)) estimates the effect of coffee for people with same level of stress. In the marginal scatterplot, the positive slope for coffee (1.10) ignores the correlation of coffee and stress.\nThis is an example of confounding in regression when an important predictor is omitted. Stress is positively associated with both coffee consumption and heart damage. When stress is omitted, the coefficient for coffee is biased because it “picks up” the relation with the omitted variable.\nA solution to this problem is the added-variable plot (“AV plot”, also called partial regression plot, Mosteller & Tukey (1977)). This is a multivariate analog of a simple marginal scatterplot, designed to visualize directly the partial relation between \\(y\\) and the predictors \\(x_1, x_2, \\dots\\) in a multiple regression model.\nYou can think of this as a magic window that hides the relations of all other variables with each of the \\(y\\) and \\(x_i\\) shown in a given added-variable plot. This gives an unobstructed view of the net relation between \\(y\\) and \\(x_i\\) with the effect of all other variables removed.\nIn effect, this reduces the problem of viewing the complete model in \\(p\\)-dimensional space to a sequence of \\(p\\) 2D plots, each of which tells the story of one predictor, unentangled from the others. This is essentially the same idea as the partial variables plot (Section 3.12.3) used to understand partial correlations.\nThe construction of an AV plot is conceptually very simple. For variable \\(x_i\\), imagine that we fit two supplementary regressions:\n\nRegress \\(\\mathbf{y}\\) on \\(\\mathbf{X_{(-i)}}\\), the model matrix of all of the regressors except \\(x_i\\). By definition, the residuals from this regression, \\(\\mathbf{y}^\\star \\equiv \\mathbf{y} \\,\\vert\\, \\text{others} = \\mathbf{y} - \\widehat{\\mathbf{y}} \\,\\vert\\, \\mathbf{X_{(-i)}}\\),  are the part of \\(\\mathbf{y}\\) that cannot be explained by all the other regression terms. These residuals are necessarily uncorrelated with the other predictors.\nRegress \\(x_i\\) on the other predictors, \\(\\mathbf{X_{(-i)}}\\) and again obtain the residuals. These residuals, \\(\\mathbf{x}_i^\\star \\equiv \\mathbf{x}_i \\,\\vert\\, \\text{others} = \\mathbf{x}_i - \\widehat{\\mathbf{x}}_i \\,\\vert\\, \\mathbf{X_{(-i)}}\\) give the part of \\(x_i\\) that cannot be explained by the others, and so are uncorrelated with them.\n\nThe AV plot is then just a simple scatterplot of these residuals, \\(\\mathbf{y}^\\star\\) on the vertical axis, and \\(\\mathbf{x}^\\star\\) on the horizontal. In practice, it is unnecessary to run the auxilliary regressions this way (Velleman & Welsh, 1981). Both can be calculated using stats::lsfit() roughly as follows:\n\nAVcalc &lt;- function(model, variable)\nX &lt;- model.matrix(model)\nresponse &lt;- model.response(model)\nx &lt;- X[, -variable]\ny &lt;- cbind(X[, variable], response)\nfit &lt;- lsfit(x, y, intercept = FALSE)\nresids &lt;- residuals(fit)\nreturn(resids)\n\nNote that y in the code here contains both the current predictor, \\(\\mathbf{x}_i\\) and the response \\(\\mathbf{y}\\), so the residuals resids have two columns, one for \\(x_i \\,\\vert\\, \\text{others}\\) and one for \\(y \\,\\vert\\, \\text{others}\\).\nThe avPlot() function\nAdded-variable plots are produced using car::avPlot() for one predictor or avPlots() for any number of model terms. The id argument controls which points are identified in the plots; n=2 labels the two points that are furthest from the mean on the horizontal axis and the two with the largest absolute residuals. For instance, in Figure 6.11, observations 5 and 13 are flagged because their conditional \\(\\mathbf{x}_i^\\star\\) values are extreme; observation 17 has a large absolute residual, \\(\\mathbf{y}^\\star = \\text{Heart} \\,\\vert\\, \\text{others}\\).\n\navPlots(fit.both,\n  ellipse = list(levels = 0.68, fill=TRUE, fill.alpha = 0.1),\n  pch = 19,\n  id = list(n = 2),\n  cex.lab = 1.5,\n  main = \"Added-variable plots for Coffee data\")\n\n\n\n\n\n\nFigure 6.11: Added-variable plots for Coffee and Stress in the multiple regression model\n\n\n\n\nThe data ellipses for \\(\\mathbf{x}_i^\\star\\) and \\(\\mathbf{y}^\\star\\) summarize the conditional (or partial) relations of the response to each predictor controlling for all other predictors in each plot. The essential idea is that the data ellipse for \\((\\mathbf{x}_i^\\star, \\mathbf{y}^\\star)\\) has the identical relation to the estimate \\(\\hat{b}_i\\) in a multiple regression as the data ellipse of \\((\\mathbf{x}, \\mathbf{y})\\) has to the slope in a simple regression.\n\n6.3.1 Properties of AV plots\nAV plots are particularly interesting and useful for the following noteworthy properties:\n\nThe slope of the simple regression in the AV plot for variable \\(x_i\\) is identical to the slope \\(b_i\\) for that variable in the full multiple regression model.\nThe residuals in this plot are the same as the residuals using all predictors. This means you can see the degree of fit for observations directly in relation to the various predictors, which is not the case for marginal scatterplots.\nConsequentially, the standard deviation of the (vertical) residuals in the AV plot is the same as \\(s = \\sqrt(MSE)\\) in the full model and the standard error of a coefficient is \\(\\text{SE}(b_i) = s / \\sqrt{\\Sigma (\\mathbf{x}_i^\\star)^2}\\). This is shown by the size of the shadow of the data ellipses on the vertical axis in Figure 6.11.\nThe horizontal positions, \\(\\mathbf{x}_i^\\star\\), of points adjust for all other predictors, and so we can see points at the extreme left and right as unusual in relation to the others. If these points are also badly fitted (large residuals), we can see their influence on the fitted relation in the full model. AV plots thus provide visual displays of (partial) leverage and influence on each of the regression coefficients.\nThe correlation of \\(\\mathbf{x}_i^\\star\\) and \\(\\mathbf{y}^\\star\\) shown by the shape of the data ellipses is the partial correlation between \\(\\mathbf{x}_i\\) and \\(\\mathbf{y}_i\\) with other predictors partialled out.\n\n6.3.2 Marginal - conditional plots\nThe relation of the conditional data ellipses in AV plots to those in marginal plots of the same variables provides further insight into what it means to “control for” other variables.\nFigure 6.12 shows the same added-variable plots for Heart disease on Stress and Coffee as in Figure 6.11 (with a zoomed-out scaling). But here I also overlay the marginal data ellipses for \\((\\mathbf{x}_i, \\mathbf{y})\\) (centered at the means), and marginal regression lines for Stress and Coffee separately. Drawing arrows connecting the original data points to their positions in the AV plot shows exactly what happens when we condition on or partial out the other variable.\nThese marginal - conditional plots are produced by car::mcPlot() (for one regressor) and car::mcPlots() (for several). The plots for the marginal and conditional relations can be compared separately using the same scales for both, or overlaid as shown here. The points labeled here are only those with large absolute residuals \\(\\mathbf{y}^\\star\\) in the vertical direction.\n\nmcPlots(fit.both, \n  ellipse = list(levels=0.68, fill=TRUE, fill.alpha=0.2),\n  id = list(n=2),\n  pch = c(16, 16),\n  col.marginal = \"red\", col.conditional = \"blue\",\n  col.arrows = \"black\",\n  cex.lab = 1.5)\n\n\n\n\n\n\nFigure 6.12: Marginal \\(+\\) conditional (added-variable) plots for Coffee and Stress in the multiple regression predicting Heart disease. Each panel shows the 68% conditional data ellipse for \\(x_i^\\star, y^\\star\\) residuals (shaded, blue) as well as the marginal 68% data ellipse for the \\((x_i, y)\\) variables, shifted to the origin. Arrows connect the mean-centered marginal points (red) to the residual points (blue).\n\n\n\n\nThe most obvious feature of Figure 6.12 is that Coffee has a negative slope in the conditional AV plot but a positive slope in the marginal plot. This is an example of Simpson’s paradox in a regression context: marginal and conditional relations can have opposite signs. \nLess obvious is the relation between the marginal and AVP ellipses. In 3D, the marginal data ellipse is the shadow (projection) of the ellipsoid for \\((\\mathbf{y}, \\mathbf{x}_1, \\mathbf{x}_2)\\) on one of the coordinate planes, whereas the AV plot is a slice through the ellipsoid where either \\(\\mathbf{x}_1\\) or \\(\\mathbf{x}_2\\) is held constant. Thus, the AVP ellipse must be contained in the marginal ellipse, as we can see in Figure 6.12. If there are only two \\(x\\)s, then the AVP ellipse must touch the marginal ellipse at two points.\nFinally, Figure 6.12 also shows how conditioning on other predictors works for individual observations, where each point of \\((\\mathbf{x}_i^\\star, \\mathbf{y}^\\star)\\) is the image of \\((\\mathbf{x}_i, \\mathbf{y})\\) along the path of the marginal regression. The variability in the response and in the focal predictor are both reduced, leaving only the uncontaminated relation of \\(\\mathbf{y}\\) with \\(\\mathbf{x}_i\\).\nThese plots are similar in spirit to the ARES plot (“Adding REgressors Smoothly”) proposed by Cook & Weisberg (1994), but their idea was an interactive animation, displaying a smooth transition between the fit of a marginal model and the fit of a larger model. They used linear interpolation, \\[\n(\\mathbf{x}_i, \\mathbf{y}_i)_{\\text{interp}} = (\\mathbf{x}_i, \\mathbf{y}_i) + \\lambda [(\\mathbf{x}_i^\\star, \\mathbf{y}_i^\\star) - (\\mathbf{x}_i, \\mathbf{y}_i)] \\:\\: ,\n\\] controlled by a slider whose value, \\(\\lambda \\in [0, 1]\\), was the weight given to the smaller marginal model. See an animated graphic (https://bit.ly/49XDHn0) for an example using the Duncan data.\n\n6.3.3 Prestige data\nFor a substantive example, let’s return to the model for income, education and women in the Prestige data. The plot in Figure 6.13 shows the strong positive relations of income and education to prestige in the full model, and the negligible relation of percent women. But, in the plot for income, two occupations (physicians and general managers) with high income strongly pull the regression line down from what can be seen in the orientation of the conditional data ellipse.\n\nprestige.mod1 &lt;- lm(prestige ~ education + income + women,\n           data=Prestige)\n\navPlots(prestige.mod1, \n  ellipse = list(levels = 0.68),\n  id = list(n = 2, cex = 1.2),\n  pch = 19,\n  cex.lab = 1.5,\n  main = \"Added-variable plots for prestige\")\n\n\n\n\n\n\nFigure 6.13: Added-variable plot for the quantitative predictors in the Prestige data.\n\n\n\n\nThe influential points for physicians and general managers could just be unusual, or suggest that the relation of income to prestige is nonlinear. A rough test of this is to fit a smoothed curve through the points in the AV plot as shown in Figure 6.14.\n\nres &lt;- avPlot(prestige.mod1, \"income\",\n              ellipse = list(levels = 0.68),\n              pch = 19,\n              cex.lab = 1.5)\nsmooth &lt;- loess.smooth(res[,1], res[,2])\nlines(smooth, col = \"red\", lwd = 2.5)\n\n\n\n\n\n\nFigure 6.14: Added-variable plot for income, with a loess smooth.\n\n\n\n\nHowever, this use of AV plots to diagnose nonlinearity or suggest transformations can be misleading (Cook, 1996). Curvature in these plots is an indication of some model deficiency, but unless the predictors are uncorrelated, they cannot determine the form of a possible transformation of the predictors.\n\n6.3.4 Component + Residual plots\nA related method, the component + residual plot (“C+R plot”, also called partial residual plot, Larsen & McCleary (1972); Cook (1993)) gives a plot more suited to detecting the need to transform a predictor \\(\\mathbf{x}_i\\) to a form \\(f(\\mathbf{x}_i)\\) to make it’s relationship with the response \\(\\mathbf{y}\\) more nearly linear. This plot displays the partial residual \\(\\mathbf{e} + \\hat{b}_i \\mathbf{x}_i\\) on the vertical axis against \\(\\mathbf{x}_i\\) on the horizontal, where \\(\\mathbf{e}\\) are the residuals from the full model. A smoothed curve through the points will often suggest the form of the transformation \\(f()\\). The fact that the horizontal axis is \\(\\mathbf{x}_i\\) itself rather than \\(\\mathbf{x}^\\star_i\\) makes it easier to see the functional form.\nThe C+R plot has the same desirable properties as the AV plot: The slope \\(\\hat{b}_i\\) and residuals \\(\\mathbf{e}\\) in this plot are the same as those in the full model.\nC+R plots are produced by car::crPlots() and car::crPlot(). Figure 6.15 shows this just for income in the model prestige.mod1. (These plots for education and women show no strong evidence of curvilinearity.) The dashed blue line is the linear partial fit, \\(\\hat{b}_i \\mathbf{x}_i\\), whose slope \\(\\hat{b}_2 = 0.0013\\) is the same as that for income in prestige.mod1. The solid red curve is the loess smooth through the points. The same points are identified as noteworthy as in AV plot in Figure 6.14.\n\ncrPlot(prestige.mod1, \"income\",\n       smooth = TRUE,\n       order = 2,\n       pch = 19,\n       col.lines = c(\"blue\", \"red\"),\n       id = list(n=2, cex = 1.2),\n       cex.lab = 1.5) \n\n\n\n\n\n\nFigure 6.15: Component + residual plot for income in the model for the quantitative predictors of prestige. The dashed blue line is the partial linear fit for income. The solid red curve is the loess smooth.\n\n\n\n\nThe partial relation between prestige and income is clearly curved, so it would be appropriate to transform income or to include a polynomial (quadratic) term and refit the model. As suggested earlier  (Example 3.2) it makes sense statistically and substantively to model the effect of income on a log scale, so then the slope for log(income) would measure the increment in prestige for a constant percentage increase in income.\nThe effect of percent women on prestige seen in Figure 6.13 appears very small and essentially linear. However, if we wished to examine this more closely, we could use the C+R plot in Figure 6.16.\n\ncrPlot(prestige.mod1, \"women\",\n       pch = 19,\n       col.lines = c(\"blue\", \"red\"),\n       id = list(n=2, cex = 1.2),\n       cex.lab = 1.5)\n\n\n\n\n\n\nFigure 6.16: Component + residual plot for women in the model for the quantitative predictors of prestige.\n\n\n\n\nThis shows a slight degree of curvature, with modestly larger values in the extremes. If we wished to test this statistically, we could fit a model with a quadratic effect of women, and compare that to the linear-only effect using anova().\n\nprestige.mod2 &lt;- lm(prestige ~ education + income + poly(women,2),\n           data=Prestige)\n\nanova(prestige.mod1, prestige.mod2)\n# Analysis of Variance Table\n# \n# Model 1: prestige ~ education + income + women\n# Model 2: prestige ~ education + income + poly(women, 2)\n#   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n# 1     98 6034                         \n# 2     97 5907  1       127 2.08   0.15\n\nThis model ignores the type of occupation (“bc”, “wc”, “prof”) as well as any possible interactions of type with other predictors. We examine this next, using effect displays.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for Univariate Response Models</span>"
    ]
  },
  {
    "objectID": "07-linear_models-plots.html#sec-effect-displays",
    "href": "07-linear_models-plots.html#sec-effect-displays",
    "title": "6  Plots for Univariate Response Models",
    "section": "\n6.4 Effect displays",
    "text": "6.4 Effect displays\nFor two predictors it is possible, even if awkward, to display the fitted response surface in a 3D plot or faceted 2D views in what I call a full model plot. For more than two predictors such displays become cumbersome if not impractical, particularly when there are interactions in the model, when some effects are curvilinear, or when the main substantive interest is focused understanding on one or more main effects or interaction terms in the presence of others. The method of effect displays, largely introduced by John Fox (Fox, 1987, 2003; Fox & Weisberg, 2018b) is a generally useful solution to this problem. 4 These plots are nearly always easier to understand than tables of coefficients.\nThe idea of effect displays is quite simple, but very general and handles models of arbitrary complexity. Imagine that in a model we have a particular subset of predictors (focal predictors) whose effects on the response variable we wish to visualize. The essence of an effect display is that we calculate the predicted values (and standard errors) of the response for the model term(s) involving the focal predictors (and all low-order relatives, e.g, main effects that are marginal to an interaction) as those predictors are allowed to vary over a grid covering their range.\nFor a given plot, the other, non-focal variables are “controlled” by being fixed at typical values. For example, a quantitative predictor could be fixed at it’s mean, median or some representative value. A factor could be fixed at equal proportions of its levels or its proportions in the data. The result, when plotted, shows the predicted effects of the focal variables, either with multiple lines or in a faceted display, but with all the other variables controlled, adjusted for or averaged over. For interaction effects all low-order relatives are typically included in the fitted values for the term being graphed.\nIn practical terms, a scoring matrix \\(\\mathbf{X}^\\bullet\\) is defined by the focal variables varied over their ranges and the other variables held fixed. The fitted values for a model term are then calculated as \\(\\widehat{\\mathbf{y}}^\\bullet = \\mathbf{X}^\\bullet \\; \\widehat{\\mathbf{b}}\\) using the equivalent of:\n\npredict(model, newdata = X, se.fit = TRUE) \n\nwhich also calculates the standard errors as the square roots of \\(\\mathrm{diag}\\, (\\mathbf{X}^\\bullet \\, \\widehat{\\mathbf{\\mathsf{Var}}} (\\mathbf{b}) \\, \\mathbf{X}^{\\bullet\\mathsf{T}} )\\) where \\(\\widehat{\\mathbf{\\mathsf{Var}}} (\\mathbf{b})\\) is the estimated covariance matrix of the coefficients. Consequently, predictor effect values can be obtained for any modelling function that has predict() and vcov() methods. To date, effect displays are available for over 100 different model types in various packages.\nTo illustrate the mechanics for the effect of education in the prestige.mod1 model, construct a data frame varying education, but fixing income and women at their means:\n\nX &lt;- expand.grid(\n      education = seq(8, 16, 2),\n      income = mean(Prestige$income),\n      women = mean(Prestige$women)) |&gt; \n  print(digits = 3)\n#   education income women\n# 1         8   6798    29\n# 2        10   6798    29\n# 3        12   6798    29\n# 4        14   6798    29\n# 5        16   6798    29\n\npredict() then gives the fitted values for a simple effect plot of prestige against education. predict.lm() returns list, so it is necessary to massage this to a data frame for graphing.\n\npred &lt;- predict(prestige.mod1, newdata=X, se.fit = TRUE)\ncbind(X, fit = pred$fit, se = pred$se.fit) |&gt; \n  print(digits=3)\n#   education income women  fit    se\n# 1         8   6798    29 35.4 1.318\n# 2        10   6798    29 43.7 0.828\n# 3        12   6798    29 52.1 0.919\n# 4        14   6798    29 60.5 1.487\n# 5        16   6798    29 68.9 2.188\n\nAs Fox & Weisberg (2018b) note, effect displays can be combined with partial residuals to visualize both fit and potential lack of fit simultaneously, by plotting residuals from a model around 2D slices of the fitted response surface. This adds the benefits of C+R plots, in that we can see the impact of unmodeled curvilinearity and interactions in addition to those of predictor effect displays.\nThere are several implementations of effect displays in R, whose details, terminology and ease of use vary. Among these,  ggeffects (Lüdecke, 2018) calculates adjusted predicted values under several methods for conditioning.  marginaleffects (Arel-Bundock et al., 2024) is similar and also provides estimation of marginal slopes, contrasts, odds ratios, etc. Both have plot() methods based on ggplot2. My favorite is the  effects (Fox et al., 2025) package, which alone provides partial residuals, and is somewhat easier to use, though it uses lattice graphics. See the vignette Predictor Effects Graphics Gallery for details of the computations for effect displays.\nThe main functions for computing fitted effects are predictorEffect() (for one predictor) and predictorEffects() (for one or more). For a model mod with formula y ~ x1 + x2 + x3 + x1:x2, the call to predictorEffects(mod, ~ x1) recognizes that an interaction is present and calculates the fitted values for combinations of x1 and x2, holding x3 fixed at its average value. This returns an object of class \"eff\" which can be graphed using the plot.eff() method.\nThe effect displays for several predictors can be plotted together, as with avplots() (Figure 6.13) by including them in the plot formula, e.g., predictorEffects(mod, ~ x1 + x3). Another function, allEffects() calculates the effects for each high-order term in the model, so allEffects(mod) |&gt; plot() is handy for getting a visual overview of a fitted model.\n\n6.4.1 Prestige data\nTo illustrate effect plots, I consider a more complex model, allowing a quadratic effect of women, representing income on a \\(\\log_{10}\\) scale, and allowing this to interact with type of occupation. Anova() provides the Type II tests of each of the model terms.\n\nprestige.mod3 &lt;- lm(prestige ~ education + poly(women,2) +\n                       log10(income)*type, data=Prestige)\n\n# test model terms\nAnova(prestige.mod3)\n# Anova Table (Type II tests)\n# \n# Response: prestige\n#                    Sum Sq Df F value  Pr(&gt;F)    \n# education             994  1   25.87 2.0e-06 ***\n# poly(women, 2)        414  2    5.38 0.00620 ** \n# log10(income)        1523  1   39.63 1.1e-08 ***\n# type                  589  2    7.66 0.00085 ***\n# log10(income):type    221  2    2.88 0.06133 .  \n# Residuals            3420 89                    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe fitted coefficients, standard errors and \\(t\\)-tests from coeftest() are shown below. The coefficient for education means that an increase of one year of education, holding other predictors fixed, gives an expected increase of 2.96 in prestige. The other coefficients are more difficult to understand. For example, the effect of women is represented by two coefficients for the linear and quadratic components of poly(women, 2).\nThe interpretation of coefficients of terms involving type depend on the contrasts used. Here, with the default treatment contrasts, they represent comparisons with type = \"bc\" as the reference level. It is not obvious how to understand the interaction effects like log10(income):typeprof.\n\n\n\n\n\n\n\n\n\n\nlmtest::coeftest(prestige.mod3)\n# \n# t test of coefficients:\n# \n#                        Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)            -137.500     23.522   -5.85  8.2e-08 ***\n# education                 2.959      0.582    5.09  2.0e-06 ***\n# poly(women, 2)1          28.339     10.190    2.78   0.0066 ** \n# poly(women, 2)2          12.566      7.095    1.77   0.0800 .  \n# log10(income)            40.326      6.714    6.01  4.1e-08 ***\n# typewc                    0.969     39.495    0.02   0.9805    \n# typeprof                 74.276     30.736    2.42   0.0177 *  \n# log10(income):typewc     -1.073     10.638   -0.10   0.9199    \n# log10(income):typeprof  -17.725      7.947   -2.23   0.0282 *  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt is easiest to produce effect displays for all terms in the model using allEffects(), accepting all defaults. This gives (Figure 6.17) effect plots for the main effects of education and income and the interaction of income with type, with the non-focal variables held fixed. Each plot shows the fitted regression relation and a default 95% pointwise confidence band using the standard errors. Rug plots at the bottom show the locations of observations for the horizontal focal variable, which is useful when the observations are not otherwise plotted.\n\nallEffects(prestige.mod3) |&gt;\n  plot()\n\n\n\n\n\n\nFigure 6.17: Predictor effect plot for all terms in the model with 95% confidence bands.\n\n\n\n\nThe effect for women, holding education, income and type constant looks to be quite strong and curved upwards. But note that these plots use different vertical scales for prestige in each plot and the range in the plot for women is much smaller than in the others. The interaction is graphed showing separate curves for the three levels of type.\nFor a more detailed look, it is useful to make separate plots for the predictors in the model, which allows customizing options for calculation and display. Partial residuals for the observations are computed by using residuals = TRUE in the call to predictorEffects(). The slope of the fitted line (in blue) is exactly coefficient for education in the full model. As with C+R plots, a smooth loess curve (in red) gives a visual assessment of linearity for a given predictor. A wide variety of graphing options are available in the call to plot(). Figure 6.18 shows the effect display for education with partial residuals and point identification of those points with the largest Mahalanobis distances from the centroid.\n\n\nlattice::trellis.par.set(par.xlab.text=list(cex=1.5),\n                         par.ylab.text=list(cex=1.5))\n\npredictorEffects(prestige.mod3, ~ education,\n                 residuals = TRUE) |&gt;\n  plot(partial.residuals = list(pch = 16, col=\"blue\"),\n       id=list(n=4, col=\"black\")) \n\n\n\n\n\n\nFigure 6.18: Predictor effect plot for education displaying partial residuals. The blue line shows the slice of the fitted regression surface where other variables are held fixed. The red curve shows a loess smooth of the partial residuals.\n\n\n\n\nThe effect plot for women in this model is shown in Figure 6.19. This uses the same vertical scale as in Figure 6.18, showing a more modest effect of percent women.\n\npredictorEffects(prestige.mod3, ~women,\n                 residuals = TRUE) |&gt;\n  plot(partial.residuals = list(pch = 16, col=\"blue\", cex=0.8),\n       id=list(n=4, col=\"black\"))\n\n\n\n\n\n\nFigure 6.19: Predictor effect plot for women with partial residuals\n\n\n\n\n\nBecause of the interaction with type, the fitted effects for income are calculated for the three types of occupation. It is easiest to compare these in the a single plot (using multiline = TRUE), rather than in separate panels as in Figure 6.17. Income is represented as log10(income) in the model prestige.mod3, and it is also easier to understand the interaction by plotting income on a log scale, using the axes argument to specify a transformation of the \\(x\\) axis. I use 68% confidence bands here to make the differences among type more apparent.\n\npredictorEffects(prestige.mod3, ~ income,\n                 confidence.level = 0.68) |&gt;\n  plot(lines=list(multiline=TRUE, lwd=3),\n       confint=list(style=\"bands\"),\n       axes=list(\n          x=list(income=list(transform=list(trans=log, inverse=exp)))),\n       key.args = list(x=.7, y=.35)) \n\n\n\n\n\n\nFigure 6.20: Predictor effect plot for income, plotted on a log scale.\n\n\n\n\nFigure 6.20 provides a clear interpretation of the interaction, represented by the coefficients shown above for log10(income):typewc and log10(income):typeprof in the model. Averaging over three occupation types, prestige increases linearly with log income with a coefficient of 40.33. This means that increasing income by 10% (say) gives an increase of \\(40.33 / 10 = 4.033\\) in prestige. The slope for professional workers is less steep: the coefficient for log10(income):typeprof is -17.725. For these workers compared with blue collar jobs, prestige increases 1.77 less with a 10% increase in income. The difference in slopes for blue collar and white collar jobs is negligible.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for Univariate Response Models</span>"
    ]
  },
  {
    "objectID": "07-linear_models-plots.html#sec-leverage",
    "href": "07-linear_models-plots.html#sec-leverage",
    "title": "6  Plots for Univariate Response Models",
    "section": "\n6.5 Outliers, leverage and influence",
    "text": "6.5 Outliers, leverage and influence\nIn small to moderate samples, “unusual” observations can have dramatic effects on a fitted regression model, as we saw in the analysis of Davis’s data on reported and measured weight (Section 2.1.2) where one erroneous observations hugely altered the fitted line. As well, it turns out that two observations in Duncan’s data are unusual enough that removing them alters his conclusion that income and education have nearly equal effects on occupational prestige.\nAn observation can be unusual in three archetypal ways, with different consequences:\n\nUnusual in the response \\(y\\), but typical in the predictor(s), \\(\\mathbf{x}\\) — a badly fitted case with a large absolute residual, but with \\(x\\) not far from the mean, as in Figure 2.4. This case does not do much harm to the fitted model.\nUnusual in the predictor(s) \\(\\mathbf{x}\\), but typical in \\(y\\) — an otherwise well-fitted point. This case also does litle harm, and in fact can be considered to improve precision, a “good leverage” point.\nUnusual in both \\(\\mathbf{x}\\) and \\(y\\) — This is the case, a “bad leverage” point, revealed in the analysis of Davis’s data, Figure 2.3, where the one erroneous point for women was highly influential, pulling the regression line towards it and affecting the estimated coefficient as well as all the fitted values. In addition, subsets of observations can be jointly influential, in that their effects combine, or can mask each other’s influence.\n\nInfluential cases are the ones that matter most. As suggested above, to be influential an observation must be unusual in both \\(\\mathbf{x}\\) and \\(y\\), and affects the estimated coefficients, thereby also altering the predicted values for all observations. A heuristic formula capturing the relations among leverage, “outlyingness” on \\(y\\) and influence is\n\\[\n\\text{Influence}_{\\text{coefficients}} \\;=\\; X_\\text{leverage} \\;\\times\\; Y_\\text{residual}\n\\] As described below, leverage is proportional to the squared distance \\((x_i - \\bar{x})^2\\) of an observation \\(x_i\\) from its mean in simple regression and to the squared Mahalanobis distance in the general case. The \\(Y_\\text{residual}\\) is best measured by a studentized residual, obtained by omitting each case \\(i\\) in turn and calculating its residual from the coefficients obtained from the remaining cases. \n\n6.5.1 The leverage-influence quartet\nThese ideas can be illustrated in the “leverage-influence quartet” by considering a standard simple linear regression for a sample and then adding one additional point reflecting the three situations described above. Below, I generate a sample of \\(N = 15\\) points with \\(x\\) uniformly distributed between (40, 60) and \\(y \\sim 10 + 0.75 x + \\mathcal{N}(0, 1.25^2)\\), duplicated four times.\n\nlibrary(tidyverse)\nlibrary(car)\nset.seed(42)\nN &lt;- 15\ncase_labels &lt;- paste(1:4, c(\"OK\", \"Outlier\", \"Leverage\", \"Influence\"))\nlevdemo &lt;- tibble(\n  case = rep(case_labels, \n             each = N),\n  x = rep(round(40 + 20 * runif(N), 1), 4),\n  y = rep(round(10 + .75 * x + rnorm(N, 0, 1.25), 4)),\n  id = \" \"\n)\n\nmod &lt;- lm(y ~ x, data=levdemo)\ncoef(mod)\n# (Intercept)           x \n#      13.332       0.697\n\nThe additional points, one for each situation are set to the values below.\n\n\nOutlier: (52, 60) a low leverage point, but an outlier (O) with a large residual\n\nLeverage: (75, 65) a “good” high leverage point (L) that fits well with the regression line\n\nInfluence: (70, 40) a “bad” high leverage point (OL) with a large residual.\n\n\nextra &lt;- tibble(\n  case = case_labels,\n  x  = c(65, 52, 75, 70),\n  y  = c(NA, 65, 65, 40),\n  id = c(\"  \", \"O\", \"L\", \"OL\")\n)\n\n#' Join these to the data\nboth &lt;- bind_rows(levdemo, extra) |&gt;\n  mutate(case = factor(case))\n\nWe can plot these four situations with ggplot2 in panels faceted by case as shown below. The standard version of this plot shows the regression line for the original data and that for the ammended data with the additional point. Note that we use the levdemo dataset in geom_smooth() for the regression line with the original data, but specify data = both for that with the additional point.\n\nggplot(levdemo, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(data = both, \n              method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"red\", linewidth = 1.3, linetype = 1) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"blue\", linewidth = 1, linetype = \"longdash\" ) +\n  stat_ellipse(data = both, level = 0.5, color=\"blue\", type=\"norm\", linewidth = 1.4) +\n  geom_point(data=extra, color = \"red\", size = 4) +\n  geom_text(data=extra, aes(label = id), nudge_x = -2, size = 5) +\n  facet_wrap(~case, labeller = label_both) +\n  theme_bw(base_size = 14)\n\n\n\n\n\n\nFigure 6.21: Leverage influence quartet with data 50% ellipses. Case (1) original data; (2) adding one low-leverage outlier, “O”; (3) adding one “good” leverage point, “L”; (4) adding one “bad” leverage point, “OL”. The dashed blue line is the fitted line for the original data, while the solid red line reflects the additional point. The data ellipses show the effect of the additional point on precision.\n\n\n\n\nThe standard version of this graph shows only the fitted regression lines in each panel. As can be seen, the fitted line doesn’t change very much in panels (2) and (3); only the bad leverage point, “OL” in panel (4) is harmful. Adding data ellipses to each panel immediately makes it clear that there is another part to this story— the effect of the unusual point on precision (standard errors) of our estimates of the coefficients.\nNow, we see directly that there is a big difference in impact between the low-leverage outlier [panel (2)] and the high-leverage, small-residual case [panel (3)], even though their effect on coefficient estimates is negligible. In panel (2), the single outlier inflates the estimate of residual variance (the size of the vertical slice of the data ellipse at \\(\\bar{x}\\)), while in panel (3) this is decreased.\nTo allow direct comparison and make the added value of the data ellipse more apparent, we overlay the data ellipses from Figure 6.21 in a single graph, shown in Figure 6.22. Here, we can also see why the high-leverage point “L” (added in panel (c) of Figure 6.21) is called a “good leverage” point. By increasing the standard deviation of \\(x\\), it makes the data ellipse somewhat more elongated, giving increased precision of our estimates of \\(\\mathbf{\\beta}\\).\n\nCodecolors &lt;- c(\"black\", \"blue\", \"darkgreen\", \"red\")\nwith(both,\n     {dataEllipse(x, y, groups = case, \n          levels = 0.68,\n          plot.points = FALSE, add = FALSE,\n          center.pch = \"+\",\n          col = colors,\n          fill = TRUE, fill.alpha = 0.1)\n     })\n\ncase1 &lt;- both |&gt; filter(case == \"1 OK\")\npoints(case1[, c(\"x\", \"y\")], cex=1)\n\npoints(extra[, c(\"x\", \"y\")], \n       col = colors,\n       pch = 16, cex = 2)\n\ntext(extra[, c(\"x\", \"y\")],\n     labels = extra$id,\n     col = colors, pos = 2, offset = 0.5)\n\n\n\n\n\n\nFigure 6.22: Data ellipses in the Leverage-influence quartet. This graph overlays the data ellipses and additional points from the four panels of Figure 6.22. It can be seen that only the OL point affects the slope, while the O and L points affect precision of the estimates in opposite directions.\n\n\n\n\n\n6.5.1.1 Measuring leverage\nLeverage is thus an index of the potential impact of an observation on the model due to its’ atypical value in the X space of the predictor(s). It is commonly measured by the “hat” value, \\(h_i\\), so called because it puts the hat \\(\\mathbf{\\widehat{(\\bullet)}}\\) on \\(\\mathbf{y}\\), i.e., the vector of fitted values can be expressed as\n\n\\[\n\\begin{aligned}\n\\mathbf{\\widehat{y}}\n   &= \\underbrace{\\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}}_\\mathbf{H}\\mathbf{y} \\\\[2ex]\n   &= \\mathbf{H\\:y} \\:\\: .\n\\end{aligned}\n\\]\nHere, \\(h_i \\equiv h_{ii}\\) are the diagonal elements of the Hat matrix \\(\\mathbf{H}\\). In simple regression, hat values are proportional to the squared distance of the observation \\(x_i\\) from the mean, \\(h_i \\propto (x_i - \\bar{x})^2\\), \\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\Sigma_i (x_i - \\bar{x})^2} \\; ,\n\\tag{6.1}\\]\nand range from \\(1/n\\) to 1, with an average value \\(\\bar{h} = 2/n\\). Consequently, observations with \\(h_i\\) greater than \\(2 \\bar{h}\\) or \\(3 \\bar{h}\\) are commonly considered to be of high leverage.\nWith \\(p \\ge 2\\) predictors, an analogous relationship holds, but the correlations among the predictors must be taken into account. It is demonstrated below that \\(h_i \\propto D^2 (\\mathbf{x} - \\bar{\\mathbf{x}})\\), the Mahalanobis squared distance of \\(\\mathbf{x}\\) from the centroid \\(\\bar{\\mathbf{x}}\\)5.\nThe generalized version of Equation 6.1 is\n\\[\nh_i = \\frac{1}{n} + \\frac{1}{n-1} D^2 (\\mathbf{x} - \\bar{\\mathbf{x}}) \\; ,\n\\tag{6.2}\\]\nwhere \\(D^2 (\\mathbf{x} - \\bar{\\mathbf{x}}) = (\\mathbf{x} - \\bar{\\mathbf{x}})^\\mathsf{T} \\mathbf{S}_X^{-1} (\\mathbf{x} - \\bar{\\mathbf{x}})\\). From Section 3.2, it follows that contours of constant leverage correspond to data ellipses or ellipsoids of the predictors in \\(\\mathbf{x}\\), whose boundaries, assuming normality, correspond to quantiles of the \\(\\chi^2_p\\) distribution\n\n\nExample 6.1 Illustrating leverage\n\nTo illustrate Equation 6.2, I generate \\(N = 100\\) points from a bivariate normal distribution with means \\(\\mu = (30, 30)\\), variances = 10, and a correlation \\(\\rho = 0.7\\). Then I add two noteworthy points that show an apparently paradoxical result.\n\nset.seed(421)\nN &lt;- 100\nr &lt;- 0.7\nmu &lt;- c(30, 30)\ncov &lt;- matrix(c(10,   10*r,\n                10*r, 10), ncol=2)\n\nX &lt;- MASS::mvrnorm(N, mu, cov) |&gt; as.data.frame()\ncolnames(X) &lt;- c(\"x1\", \"x2\")\n\n# add 2 points\nX &lt;- rbind(X,\n           data.frame(x1 = c(28, 38),\n                      x2 = c(42, 35)))\n\nThe Mahalanobis squared distances of these points can be calculated using heplots::Mahalanobis(), and their corresponding hatvalues found using hatvalues() for any linear model using both x1 and x2.\n\nX &lt;- X |&gt;\n  mutate(Dsq = heplots::Mahalanobis(X)) |&gt;\n  mutate(y = 2*x1 + 3*x2 + rnorm(nrow(X), 0, 5),\n         hat = hatvalues(lm(y ~ x1 + x2))) \n\nPlotting x1 and x2 with data ellipses shows the relation of leverage to squared distance from the mean in Figure 6.23. The blue point looks to be farther from the mean, but the red point is actually very much further by Mahalanobis squared distance, which takes the correlation into account; it thus has much greater leverage.\n\ndataEllipse(X$x1, X$x2, \n            levels = c(0.40, 0.68, 0.95),\n            fill = TRUE, fill.alpha = 0.05,\n            col = \"darkgreen\",\n            xlab = \"X1\", ylab = \"X2\")\npoints(X[1:nrow(X) &gt; N, 1:2], pch = 16, \n       col=c(\"red\", \"blue\"), cex = 2)\nX |&gt; slice_tail(n = 2) |&gt;      # last two rows\n  points(pch = 16, col=c(\"red\", \"blue\"), cex = 2)\n\n\n\n\n\n\nFigure 6.23: Data ellipses for a bivariate normal sample with correlation 0.7, and two additional noteworthy points. The blue point looks to be farther from the mean, but the red point is actually more than 5 times further by Mahalanobis squared distance, and thus has much greater leverage.\n\n\n\n\nThe fact that hatvalues are proportional to leverage can be seen by plotting one against the other. I highlight the two noteworthy points in their colors from Figure 6.23 to illustrate how much greater leverage the red point has compared to the blue point.\n\nplot(hat ~ Dsq, data = X,\n     cex = c(rep(1, N), rep(3, 3)), \n     col = c(rep(\"black\", N), \"red\", \"blue\"),\n     pch = 16,\n     cex.lab = 1.5,\n     ylab = \"Hatvalue\",\n     xlab = \"Mahalanobis Dsq\")\n\n\n\n\n\n\nFigure 6.24: Hat values are proportional to squared Mahalanobis distances from the mean.\n\n\n\n\nLook back at these two points in Figure 6.23. Can you guess how much further the red point is from the mean than the blue point? You might be surprised that its’ \\(D^2\\) and leverage are about five times as great!\n\nX |&gt; slice_tail(n=2)\n#   x1 x2   Dsq   y    hat\n# 1 28 42 25.65 179 0.2638\n# 2 38 35  4.95 175 0.0588\n\n\n\n6.5.1.2 Outliers: Measuring residuals\nFrom the discussion in Section 6.5, outliers for the response \\(y\\) are those observations for which the residual \\(e_i = y_i - \\hat{y}_i\\) are unusually large in magnitude. However, as demonstrated in Figure 6.21, a high-leverage point will pull the fitted line towards it, reducing its’ residual and thus making them look less unusual.\nThe standard approach (Cook & Weisberg, 1982; Hoaglin & Welsch, 1978) is to consider a deleted residual \\(e_{(-i)}\\), conceptually as that obtained by re-fitting the model with observation \\(i\\) omitted and obtaining the fitted value \\(\\hat{y}_{(-i)}\\) from the remaining \\(n-1\\) observations, \\[\ne_{(-i)} = y_i - \\hat{y}_{(-i)} \\; .\n\\] The (externally) studentized residual is then obtained by dividing \\(e_{(-i)}\\) by it’s estimated standard error, giving \\[\ne^\\star_{(-i)} = \\frac{e_{(-i)}}{\\text{sd}(e_{(-i)})} = \\frac{e_i}{\\sqrt{\\text{MSE}_{(-i)}\\; (1 - h_i)}} \\; .\n\\tag{6.3}\\]\nThis is just the ordinary residual \\(e_i\\) divided by a factor that increases with the residual variance but decreases with leverage. It can be shown that these studentized residuals follow a \\(t\\) distribution with \\(n - p -2\\) degrees of freedom, so a value \\(|e^\\star_{(-i)}| &gt; 2\\) can be considered large enough to pay attention to.\nIn practice for classical linear models, it is unnecessary to actually re-fit the model \\(n\\) times. Velleman & Welsh (1981) show that all these leave-one-out quantities can be calculated from the model fitted to the full data set and the hat (projection) matrix \\(\\mathbf{H} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\) from which \\(\\widehat{\\mathbf{b}} = \\mathbf{H} \\mathbf{y}\\).\n\n6.5.1.3 Measuring influence\nAs described at the start of this section, the actual influence of a given case depends multiplicatively on its’ leverage and residual. But how can we measure it?\nThe essential idea introduced above, is to delete the observations one at a time, each time refitting the regression model on the remaining \\(n–1\\) observations. Then, for observation \\(i\\) compare the results using all \\(n\\) observations to those with the \\(i^{th}\\) observation deleted to see how much influence the observation has on the analysis.\nThe simplest such measure, called DFFITS, compares the predicted value for case \\(i\\) with what would be obtained when that observation is excluded.\n\n\\[\n\\begin{aligned}\n\\text{DFFITS}_i & = & \\frac{\\hat{y}_i - \\hat{y}_{(-i)}}{\\sqrt{\\text{MSE}_{(-i)}\\; h_i}} \\\\\n   & = & e^\\star_{(-i)} \\times \\sqrt{\\frac{h_i}{1-h_i}} \\;\\; .\n\\end{aligned}\n\\tag{6.4}\\]\nThe first equation gives the signed difference in fitted values in units of the standard deviation of that difference weighted by leverage; the second version (Belsley et al., 1980) represents that as a product of residual and leverage. A rule of thumb is that an observation is deemed to be influential if \\(| \\text{DFFITS}_i | &gt; 2 \\sqrt{(p+1) / n}\\).\nInfluence can also be assessed in terms of the change in the estimated coefficients \\(\\mathbf{b} = \\widehat{\\boldsymbol{\\beta}}\\) versus their values \\(\\mathbf{b}_{(-i)}\\) when case \\(i\\) is removed. Cook’s distance, \\(D_i\\), summarizes the size of the difference as a weighted sum of squares of the differences \\(\\mathbf{d} =\\mathbf{b} - \\mathbf{b}_{(-i)}\\) (Cook, 1977).\n\\[\nD_i = \\mathbf{d}^\\mathsf{T}\\, (\\mathbf{X}^\\mathsf{T}\\mathbf{X}) \\,\\mathbf{d} / (p+1) \\hat{\\sigma}^2\n\\] This can be re-expressed in terms of the components of residual and leverage\n\\[\nD_i = \\frac{e^{\\star 2}_{(-i)}}{p+1} \\times \\frac{h_i}{(1- h_i)}\n\\tag{6.5}\\]\nCook’s distance is in the metric of an \\(F\\) distribution with \\(p\\) and \\(n − p\\) degrees of freedom, so values \\(D_i &gt; 4/n\\) are considered large.\n\n6.5.2 Influence plots\nThe most common plot to detect influence is a bubble plot of the studentized residuals versus hat values, with the size (area) of the plotting symbol proportional to Cook’s \\(D\\). These plots are constructed using car::influencePlot() which also fills the bubble symbols with color whose opacity is proportional to Cook’s \\(D\\).\nThis is shown in Figure 6.25 for the demonstration dataset constructed in Section 6.5.1. In this plot, notable cutoffs for hatvalues at \\(2 \\bar{h}\\) and \\(3 \\bar{h}\\) are shown by dashed vertical lines and horizontal cutoffs for studentized residuals are shown at values of \\(\\pm 2\\).\nThe demonstration data of Section 6.5.1 has four copies of the same \\((x, y)\\) data, three of which have an unusual observation. The influence plot in Figure 6.25 subsets the data to give the \\(19 = 15 + 4\\) unique observations, including the three unusual cases. As can be seen, the high “Leverage” point has has less influence than the point labeled “Influence”, which has moderate leverage but a large absolute residual.\n\nSee the codeonce &lt;- both[c(1:16, 62, 63, 64),]      # unique observations\nonce.mod &lt;- lm(y ~ x, data=once)\ninf &lt;- influencePlot(once.mod, \n                     id = list(cex = 0.01),\n                     fill.alpha = 0.5,\n                     cex.lab = 1.5)\n# custom labels\nunusual &lt;- bind_cols(once[17:19,], inf) |&gt; \n  print(digits=3)\n# # A tibble: 3 × 7\n#   case            x     y id    StudRes    Hat CookD\n# * &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n# 1 2 Outlier      52    65 O        3.11 0.0591 0.201\n# 2 3 Leverage     75    65 L        1.52 0.422  0.784\n# 3 4 Influence    70    40 OL      -4.93 0.262  1.82\nwith(unusual, {\n  casetype &lt;- gsub(\"\\\\d \", \"\", case)\n  text(Hat, StudRes, label = casetype,\n       pos = c(4, 2, 3), cex=1.5)\n})\n\n\n\n\n\n\nFigure 6.25: Influence plot for the demonstration data. The areas of the bubble symbols are proportional to Cook’s \\(D\\). The impact of the three unusual points on Cook’s \\(D\\) is clearly seen.\n\n\n\n\n\n6.5.3 Duncan data\nLet’s return to the Duncan data used as an example in Section 6.1.1 where a few points stood out as unusual in the basic diagnostic plots (Figure 6.2). The influence plot in Figure 6.26 helps to make sense of these noteworthy observations. The default method for identifying points in influencePlot() labels points with any of large studentized residuals, hat-values or Cook’s distances.\n\ninf &lt;- influencePlot(duncan.mod, id = list(n=3),\n                     cex.lab = 1.5)\n\n\n\n\n\n\nFigure 6.26: Influence plot for the model predicting occupational prestige in Duncan’s data. Cases with large studentized residuals, hat-values or Cook’s distances are labeled.\n\n\n\n\ninfluencePlot() returns (invisibly) the influence diagnostics for the cases identified in the plot. It is often useful to look at data values for these cases to understand why each of these was flagged.\n\nmerge(Duncan, inf, by=\"row.names\", all.x = FALSE) |&gt; \n  arrange(desc(CookD)) |&gt; \n  print(digits=3)\n#     Row.names type income education prestige StudRes    Hat  CookD\n# 1    minister prof     21        84       87   3.135 0.1731 0.5664\n# 2   conductor   wc     76        34       38  -1.704 0.1945 0.2236\n# 3    reporter   wc     67        87       52  -2.397 0.0544 0.0990\n# 4 RR.engineer   bc     81        28       67   0.809 0.2691 0.0810\n# 5  contractor prof     53        45       76   2.044 0.0433 0.0585\n\n\nminister has by far the largest influence, because it has an extremely positive residual and a large hat value. Looking at the data, we see that ministers have very low income, so their prestige is under-predicted. The large hat value reflects the fact that ministers have low income combined with very high education.\nconductor has the next largest Cook’s \\(D\\). It has a large hat value because its combination of relatively high income and low education is unusual in the data.\nAmong the others, reporter has a relatively large negative residual—its prestige is far less than the model predicts—but its low leverage make it not highly influential. railroad engineer has an extremely large hat value because its income is very high in relation to education. But this case is well-predicted and has a small residual, so its leverage is not large.\n\n6.5.4 Influence in added-variable plots\nThe properties of added-variable plots discussed in Section 6.3 make them also useful for understanding why cases are influential because they control for other predictors in each plot, and therefore show the partial contributions of each observation to hat values and residuals. As a consequence, we can see directly the how individual cases become individually or jointly influential.\nThe Duncan data provides a particularly instructive example of this. Figure 6.27 shows the AV plots for both income and education in the model duncan.mod, with some annotations added. I want to focus here on the joint influence of the occupations minister and conductor which were seen to be the most influential in Figure 6.26. The green vertical lines show their residuals in each panel and the red lines show the regressions when these two observations are deleted.\nThe basic AV plots are produced using the call to avPlots() below. To avoid clutter, I use the argument id = list(method = \"mahal\", n=3) so that only the three points with the greatest Mahalanobis distances from the centroid in each plot are labeled. These are the cases with the largest leverage seen in Figure 6.26.\nTODO: Fix up the code displayed here, from R/Duncan/Duncan-reg.R\n\navPlots(duncan.mod,\n  ellipse = list(levels = 0.68, fill = TRUE, fill.alpha = 0.1),\n  id = list(method = \"mahal\", n=3),\n  pch = 16, cex = 0.9,\n  cex.lab = 1.5)\n\n\n\n\n\n\n\n\nFigure 6.27: Added variable plots for the Duncan model, highlighting the impact of the observations for minister and conductor in each plot. The green lines show the residuals for these observations. The red line in each panel shows the regression line omitting these observations.\n\n\n\n\nThe two cases—minister and conductor—are the most highly influential, but as we can see in Figure 6.27 their influence combines because they are at opposite sides of the horizontal axis and their residuals are of opposite signs. They act together to decrease the slope for income and increase that for education.\n\nCode for income AV plotres &lt;- avPlot(duncan.mod, \"income\",\n              ellipse = list(levels = 0.68),\n              id = list(method = \"mahal\", n=3),\n              pch = 16,\n              cex.lab = 1.5) |&gt;\n  as.data.frame()\nfit &lt;- lm(prestige ~ income, data = res)\ninfo &lt;- cbind(res, fitted = fitted(fit), \n             resids = residuals(fit),\n             hat = hatvalues(fit),\n             cookd = cooks.distance(fit))\n\n# noteworthy points in this plot\nbig &lt;- which(info$cookd &gt; .20)\nwith(info, {\n  arrows(income[big], fitted[big], income[big], prestige[big], \n         angle = 12, length = .18, lwd = 2, col = \"darkgreen\")\n  })\n\n# line w/o the unusual points\nduncan.mod2 &lt;- update(duncan.mod, subset = -c(6, 16))\nbs &lt;- coef(duncan.mod2)[\"income\"]\nabline(a=0, b=bs, col = \"red\", lwd=2)\n\n\nDuncan’s hypothesis that the slopes for income and education were equal thus fails when these two observations are deleted. The slope for income then becomes 2.6 times that of education.\n\nduncan.mod2 &lt;- update(duncan.mod, subset = -c(6, 16))\ncoef(duncan.mod2)\n# (Intercept)      income   education \n#      -6.409       0.867       0.332",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for Univariate Response Models</span>"
    ]
  },
  {
    "objectID": "07-linear_models-plots.html#what-have-we-learned",
    "href": "07-linear_models-plots.html#what-have-we-learned",
    "title": "6  Plots for Univariate Response Models",
    "section": "\n6.6 What have we learned?",
    "text": "6.6 What have we learned?\nThis chapter unveils the sophisticated visual toolkit that transforms regression models from black boxes into transparent interpretable analyses. Here are the essential insights that will revolutionize how you understand and communicate model results:\n\nThe regression quartet reveals model reality beyond the summary statistics: The four diagnostic plots (residuals vs. fitted, Q-Q plot, scale-location, and leverage plots) form a comprehensive health check for your regression models. Like a medical examination, each plot diagnoses different potential problems—linearity violations, non-normality, heteroscedasticity, and influential observations. The enhanced versions in R packages like car and performance don’t just show problems; they label the troublemakers and guide you toward solutions.\nCoefficient plots make model comparisons effortless and honest: Raw coefficient tables hide the forest for the trees, especially when predictors have wildly different scales (years vs. dollars vs. percentages). Standardized coefficient plots and meaningful rescaling transform incomprehensible tables into intuitive visual comparisons. When Duncan’s income coefficient (0.0013) appears invisible next to education’s coefficient (4.19), the plot reveals they’re actually comparable forces shaping occupational prestige.\nAdded-variable plots expose the truth behind “controlling for other variables”: These magical plots perform visual surgery, removing the effects of all other predictors to show the pure relationship between response and focal predictor. They reveal Simpson’s paradox in action–—how coffee can appear harmful marginally but beneficial conditionally when stress is controlled. The relationship between marginal and conditional ellipses tells the complete story of confounding and adjustment.\nEffect displays translate complex models into understandable stories: When interactions, transformations, and multiple predictors make coefficient interpretation impossible, effect displays come to the rescue. They show exactly how prestige changes with income for different occupation types, or how the effects of education varies across meaningful ranges, controlling for other variables These plots transform “significant at p &lt; 0.05” into “here’s exactly what this means in practice.”\nInfluence diagnostics separate the signal from the statistical noise: The leverage-influence framework reveals which observations affect your model the most versus which are merely unusual. Through Cook’s distance, hat values, and studentized residuals, we learned that minister and conductor in Duncan’s data weren’t just outliers—they were the puppet masters controlling his key findings about the equal effects of income and education. Remove them, and the entire conclusion changes.\n\nThe chapter’s profound message is that regression modeling without visualization is like navigating without a compass. In our R-powered world of complex models and large datasets, sophisticated plotting techniques aren’t just helpful—they’re essential for understanding what your models actually say about the world.\n\n\n\n\n\n\nArel-Bundock, V. (2022). modelsummary: Data and model summaries in R. Journal of Statistical Software, 103(1), 1–23. https://doi.org/10.18637/jss.v103.i01\n\n\nArel-Bundock, V., Greifer, N., & Heiss, A. (2024). How to interpret statistical models using marginaleffects for R and Python. Journal of Statistical Software, 111(9), 1–32. https://doi.org/10.18637/jss.v111.i09\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley; Sons.\n\n\nCook, R. D. (1977). Detection of influential observation in linear regression. Technometrics, 19(1), 15–18. https://doi.org/10.1080/00401706.1977.10489493\n\n\nCook, R. D. (1993). Exploring partial residual plots. Technometrics, 35(4), 351–362.\n\n\nCook, R. D. (1996). Added-variable plots and curvature in linear regression. Technometrics, 38(3), 275–278. https://doi.org/10.1080/00401706.1996.10484507\n\n\nCook, R. D., & Weisberg, S. (1982). Residuals and influence in regression. Chapman; Hall.\n\n\nCook, R. D., & Weisberg, S. (1994). ARES plots for generalized linear models. Computational Statistics & Data Analysis, 17(3), 303–315. https://doi.org/10.1016/0167-9473(92)00075-3\n\n\nDuncan, O. D. (1961). A socioeconomic index for all occupations. In Jr. A. J. Reiss, P. K. H. O. D. Duncan, & C. C. North (Eds.), Occupations and social status. The Free Press.\n\n\nFisher, R. A. (1925). Statistical methods for research workers (6th ed.). Oliver & Boyd.\n\n\nFox, J. (1987). Effect displays for generalized linear models. In C. C. Clogg (Ed.), Sociological methodology, 1987 (pp. 347–361). Jossey-Bass.\n\n\nFox, J. (2003). Effect displays in R for generalized linear models. Journal of Statistical Software, 8(15), 1–27.\n\n\nFox, J. (2020). Regression diagnostics (2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878651\n\n\nFox, J., & Weisberg, S. (2018a). An R companion to applied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFox, J., & Weisberg, S. (2018b). Visualizing fit and lack of fit in complex regression models with predictor effect plots and partial residuals. Journal of Statistical Software, 87(9). https://doi.org/10.18637/jss.v087.i09\n\n\nFox, J., & Weisberg, S. (2019). An R companion to applied regression (Third). Sage. https://www.john-fox.ca/Companion/\n\n\nFox, J., Weisberg, S., Price, B., Friendly, M., & Hong, J. (2025). Effects: Effect displays for linear, generalized linear, and other models. https://cran.r-project.org/package=effects\n\n\nHoaglin, D. C., & Welsch, R. E. (1978). The hat matrix in regression and ANOVA. The American Statistician, 32(1), 17–22. https://doi.org/10.1080/00031305.1978.10479237\n\n\nKastellec, J. P., & Leoni, E. L. (2007). Using graphs instead of tables in political science. Perspectives on Politics, 5(04), 755–771. https://doi.org/10.1017/S1537592707072209\n\n\nLarmarange, J. (2025). Ggstats: Extension to ’ggplot2’ for plotting stats. https://doi.org/10.32614/CRAN.package.ggstats\n\n\nLarsen, W. A., & McCleary, S. J. (1972). The use of partial residual plots in regression analysis. Technometrics, 14, 781–790.\n\n\nLüdecke, D. (2018). Ggeffects: Tidy data frames of marginal effects from regression models. Journal of Open Source Software, 3(26), 772. https://doi.org/10.21105/joss.00772\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., & Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., Bacher, E., Thériault, R., & Makowski, D. (2022). Easystats: Framework for easy statistical modeling, visualization, and reporting. CRAN. https://doi.org/10.32614/CRAN.package.easystats\n\n\nMosteller, F., & Tukey, J. W. (1977). Data analysis and regression: A second course in statistics. Addison-Wesley Publishing Company.\n\n\nPineo, P. O., & Porter, J. (1967). Occupational prestige in canada*. Canadian Review of Sociology, 4(1), 24–40. https://doi.org/https://doi.org/10.1111/j.1755-618X.1967.tb00472.x\n\n\nSearle, S. R., Speed, F. M., & Milliken, G. A. (1980). Population marginal means in the linear model: An alternative to least squares means. The American Statistician, 34(4), 216–221.\n\n\nVelleman, P. F., & Welsh, R. E. (1981). Efficient computing of regression diagnostics. The American Statistician, 35(4), 234–242.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for Univariate Response Models</span>"
    ]
  },
  {
    "objectID": "07-linear_models-plots.html#footnotes",
    "href": "07-linear_models-plots.html#footnotes",
    "title": "6  Plots for Univariate Response Models",
    "section": "",
    "text": "A Wald test assesses constraints on the parameters in a model. It is based on the weighted distance between the unrestricted estimate and its hypothesized value under the null hypothesis, where the weight is the precision (inverse variance) of the estimate. ↩︎\nNote that the factor type in the dataset has its levels ordered alphabetically. For analysis and graphing it is useful to reorder the levels in the natural increasing order. An alternative is to make type an ordered factor, but this would represent it using polynomial contrasts for linear and quadratic trends, which seems useful in this context. ↩︎\nThis example was constructed by Georges Monette.↩︎\nEarlier, but less general expression of these ideas go back to the use of adjusted means in analysis of covariance (Fisher, 1925) or least squares means or population marginal means in analysis of variance (Searle et al., 1980)↩︎\nSee this Stats StackExchange discussion for a proof.↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for Univariate Response Models</span>"
    ]
  },
  {
    "objectID": "08-lin-mod-topics.html",
    "href": "08-lin-mod-topics.html",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "7.1 Ellipsoids in data space and \\(\\boldsymbol{\\beta}\\) space\nThe geometric and graphical approach of earlier chapters has already introduced some new ideas for thinking about multivariate data, models for explaining them, and graphical methods for understanding their results. These can be applied to better understand common problems that arise in data analysis.\nIn Section 7.1 I explore the geometric relationships between ellipses in data space and how these appear in the space of the estimated coefficients of linear models, called \\(\\beta\\) space. It turns out that points in one space correspond to lines in the other, a reflection that each one is in some sense the inverse and dual of the other.\nThis geometry can also be used to clarify the effect of measurement errors in the predictor variables, as illustrated in Section 7.2.\nPackages\nIn this chapter I use the following packages. Load them now:\nIt is most common to look at data and fitted models in our familiar “data space”. Here, axes correspond to variables, points represent observations, and fitted models can be plotted as lines (or planes) in this space. As we’ve suggested, data ellipsoids provide informative summaries of relationships in data space.\nFor linear models, particularly regression models with quantitative predictors, there is another space—“\\(\\boldsymbol{\\beta}\\) space”—that provides deeper views of models and the relationships among them. This discussion extends Friendly et al. (2013), Sec. 4.6.\nIn \\(\\boldsymbol{\\beta}\\) space, the axes pertain to coefficients, for example \\((\\beta_0, \\beta_1)\\) in a simple linear regression corresponds to the model \\(y = \\beta_0 + \\beta_1 x\\). Points in this space are models (true, hypothesized, fitted) whose coordinates represent values of these parameters. For example,",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "08-lin-mod-topics.html#sec-betaspace",
    "href": "08-lin-mod-topics.html#sec-betaspace",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "In simple regression, one point plotted with the coefficients \\(\\widehat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\hat{\\beta}_0, \\hat{\\beta}_1)\\) represents the least squares estimate.\nOther points, representing different fitting methods can be shown in the same plot. For instance, \\(\\widehat{\\boldsymbol{\\beta}}_{\\text{WLS}}\\) and \\(\\widehat{\\boldsymbol{\\beta}}_{\\text{ML}}\\) would give weighted least squares and maximum likelihood estimates.\nThe line \\(\\beta_1 = 0\\) represents the null hypothesis that the slope is zero and the point \\((0, 0)\\) corresponds to the joint hypothesis \\(\\mathcal{H}_0: \\beta_0 = 0, \\beta_1 = 0\\).\n\n\n7.1.1 Dual and inverse spaces\nAs illustrated below, the data space of \\(\\mathbf{X}\\) and that of \\(\\boldsymbol{\\beta}\\) space are each dual and inverse to the other. To make this explicit for simple linear regression:\n\neach line, like \\(\\mathbf{y} = \\beta_0 + \\beta_1 \\mathbf{x}\\) with intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) in data space corresponds to a point \\((\\beta_0,\\beta_1)\\) in \\(\\boldsymbol{\\beta}\\) space, and conversely;\nthe set of points on any line \\(\\beta_1 = x + y \\beta_0\\) in \\(\\boldsymbol{\\beta}\\) space corresponds to a set of lines through a given point \\((x, y)\\) in data space, and conversely;\nthe geometric proposition that “every pair of points defines a line in one space” corresponds to the proposition that “every two lines intersect in a point in the other space”.\n\n\nExample 7.1 Dual points and lines\nThis duality of points and lines is illustrated in Figure 7.1. The left panel shows three lines in data space, which can be expressed as linear equations in \\(\\mathbf{z} = (x, y)\\) of the form \\(\\mathbf{A} \\mathbf{z} = \\mathbf{d}\\). The function matlib::showEqn(A, d) prints these as equations in the data coordinates \\(x\\) and \\(y\\).\n\nA &lt;- matrix(c( 1, 1, 0,\n              -1, 1, 1), nrow = 3, ncol = 2) \nd &lt;- c(2, 1/2, 1)\nshowEqn(A, d, vars = c(\"x\", \"y\"), simplify = TRUE)\n#   x - 1*y  =    2 \n#   x   + y  =  0.5 \n# 0*x   + y  =    1\n\n\n\n\n\n\n\n\nFigure 7.1: Duality of \\((x, y)\\) lines in data space (left) and points in \\(\\beta\\)-space (right). Each line in data space corresponds to a point, whose intercept and slope are shown in \\(\\beta\\)-space. Labels in both plots identify the points and lines.\n\n\n\n\nThe first equation, \\(x - y = 2\\) can be expressed as the red line \\(y = x - 2\\) labeled in the left panel of Figure 7.1. This corresponds to the red point \\((\\beta_0, \\beta_1) = (-2, 1)\\) in \\(\\beta\\) space, and similarly for the other two equations shown in blue and green.\nThe second equation, \\(x + y = \\frac{1}{2}\\), or \\(y = 0.5 - x\\) intersects the first at the point \\((x, y) = (1.25, 0.75)\\). This corresponds to the line connecting \\((-2, 1)\\) and \\((0.5, -1)\\) in \\(\\beta\\) space. All solutions to this pair of equations lie along this line.\n\n\nExample 7.2 Inverses\nThis lovely interchange between points and lines is an example of an important general principle of duality in modern mathematics, which translates concepts and structures from one perspective to another and back again. We get two views of the same thing, whose dual nature provides greater insight from the combination of perspectives.\nWe have seen (Section 3.2) how ellipsoids in data space summarize variance (lack of precision) and correlation of our data. For the purpose of understanding linear models, ellipsoids in \\(\\beta\\) space do the same thing for the estimates of parameters. These ellipsoids are dual and inversely related to each other, a point first made clear by Dempster (1969, Ch. 6):\n\nIn data space, joint confidence intervals for the mean vector or joint prediction regions for the data are given by the ellipsoids \\((\\bar{x}_1, \\bar{x}_2)^\\mathsf{T} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}}\\), where the covariance matrix \\(\\mathbf{S}_{\\mathbf{X}}\\) depends on \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) (\\(\\oplus\\) here shifts the ellipsoid to one centered at \\((\\bar{x}_1, \\bar{x}_2)\\) here, as in Equation 3.2).\nIn the dual \\(\\mathbf{\\beta}\\) space, joint confidence regions for the coefficients of a response variable \\(y\\) on \\((x_1, x_2)\\) are given by ellipsoids of the form \\(\\widehat{\\boldsymbol{\\beta}} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}^{-1}}\\), and depend on \\(\\mathbf{(\\mathbf{X}^\\mathsf{T}\\mathbf{X})}^{-1}\\).\n\nIt is useful to understand the underlying geometry here connecting the ellipses for a matrix and its inverse. This can be seen in Figure 7.2, which shows an ellipse for a covariance matrix \\(\\mathbf{S}\\), whose axes, as we saw in Chapter 4 are the eigenvectors \\(\\mathbf{v}_i\\) of \\(\\mathbf{S}\\) and whose radii are the square roots \\(\\sqrt{\\lambda_i}\\) of the corresponding eigenvalues. The comparable ellipse for \\(2 \\mathbf{S}\\) has radii multiplied by \\(\\sqrt{2}\\).\n\n\n\n\n\n\n\nFigure 7.2: Geometric properties of an ellipse \\(\\mathbf{S}\\) and its inverse, \\(\\mathbf{S}^{-1}\\). The principal axes (dotted lines) are given by the eigenvectors, which are the same for \\(\\mathbf{S}\\) and \\(\\mathbf{S}^{-1}\\). Multiplying \\(\\mathbf{S}\\) by 2 makes it’s ellipse larger by \\(\\sqrt{2}\\), while the same factor makes the ellipse for \\((2 \\mathbf{S})^{-1}\\) smaller by the same factor.\n\n\n\n\nAs long as \\(\\mathbf{S}\\) is of full rank, the eigenvectors of \\(\\mathbf{S}^{-1}\\) are identical, while the eigenvalues are \\(1 / \\lambda_i\\), so the radii are the reciprocals \\(1 / \\sqrt{\\lambda_i}\\). The analogous ellipse for \\((2 \\mathbf{S}^{-1})\\) is smaller by a factor of \\(\\sqrt{2}\\).\nThus, in two dimensions, the ellipse for \\(\\mathbf{S}^{-1}\\) is a \\(90^o\\) rotation of that for \\(\\mathbf{S}\\). The ellipse for \\(\\mathbf{S}^{-1}\\) is small in directions where the ellipse for \\(\\mathbf{S}\\) is large, and vice-versa. In our statistical applications, this translates as:\n\nParameter estimates in \\(\\beta\\) space are more precise (have less variance) in the directions where the data are more widely dispersed, giving more information about the relationship.\n\n\nI illustrate these ideas in the example below.\n\n7.1.2 Data ellipse and confidence ellipse\nIn Section 6.3 I used the data (heplots::coffee) on coffee consumption, stress and heart disease to illustrate added variable plots. These explained the perplexing result that increased coffee seemed strongly positively related to heart damage when considered alone (see Figure 6.10), but had a negative coefficient (\\(\\beta_{\\text{Coffee}} = -0.41\\)) in the model (fit.both) that also included the effect of stress on heart damage. The AV plots (Figure 6.11) clarified this by showing the conditional relations of each predictor, when the other was controlled or adjusted for.\nHere, I use this dataset to illustrate the inverse relation between a data ellipse based on the covariance matrix \\(\\mathbf{S}_X\\) and the corresponding confidence ellipse for the coefficients based on \\(\\mathbf{S}_X^{-1}\\). This is shown in Figure 7.3 for the data space relation between the predictors coffee and stress, and the \\(\\beta\\) space of the confidence ellipse for their coefficients.\n\n\n\n\n\n\n\nFigure 7.3: Data space and \\(\\boldsymbol{\\beta}\\) space representations of Coffee and Stress. Left: 40% and 68% data ellipses. Right: Joint 95% confidence ellipse (blue) for (\\(\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}}\\)), confidence interval generating ellipse (red) with 95% univariate shadows. \\(H_0\\) marks the joint hypothesis that both coefficients equal zero.\n\n\n\n\n\nThe left panel in Figure 7.3 is the same as that in the (3,2) cell of Figure 6.10 for the relation Stress ~ Coffee but with data ellipses of 40% and 68% coverage. The shadows of the 40% ellipse on any axis give univariate intervals of the mean \\(\\bar{x} \\pm 1 s_x\\) (standard deviation) shown by the thick red lines; the shadow of the 68% ellipse corresponds to an interval \\(\\bar{x} \\pm 1.5 s_x\\).1\nThe right panel shows the joint 95% confidence region for the coefficients \\((\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}})\\) and individual confidence intervals in \\(\\boldsymbol{\\beta}\\) space. These are determined as\n\\[\n\\widehat{\\mathbf{\\beta}} \\oplus \\sqrt{d F^{.95}_{d, \\nu}} \\times s_e \\times \\mathbf{S}_X^{-1/2} \\:\\: .\n\\] where \\(d\\) is the number of dimensions for which we want coverage, \\(\\nu\\) is the residual degrees of freedom for \\(s_e\\), and \\(\\mathbf{S}_X\\) is the covariance matrix of the predictors.\nThus, the blue ellipse in Figure 7.3 (right) is the ellipse of joint 95% coverage, using the factor \\(\\sqrt{2 F^{.95}_{2, \\nu}}\\), which covers the true values of (\\(\\beta_{\\mathrm{Stress}}, \\beta_{\\mathrm{Coffee}}\\)) in 95% of samples. Moreover:\n\nAny joint hypothesis (e.g., \\(\\mathcal{H}_0:\\beta_{\\mathrm{Stress}}=0, \\beta_{\\mathrm{Coffee}}=0\\)) can be tested visually, simply by observing whether the hypothesized point, \\((0, 0)\\) here, lies inside or outside the joint confidence ellipse. That hypothesis is rejected\nThe shadows of this ellipse on the horizontal and vertical axes give Scheff'e joint 95% confidence intervals for the parameters, with protection for simultaneous inference (“fishing”) in a 2-dimensional space.\nSimilarly, using the factor \\(\\sqrt{F^{1-\\alpha/d}_{1, \\nu}} = t^{1-\\alpha/2d}_\\nu\\) would give an ellipse whose 1D shadows are \\(1-\\alpha\\) Bonferroni confidence intervals for \\(d\\) posterior hypotheses.\n\nVisual hypothesis tests and \\(d=1\\) confidence intervals for the parameters separately are obtained from the red ellipse in Figure 7.3, which is scaled by \\(\\sqrt{F^{.95}_{1, \\nu}} = t^{.975}_\\nu\\). We call this the confidence-interval generating ellipse (or, more compactly, the “confidence-interval ellipse”). The shadows of the confidence-interval ellipse on the axes (thick red lines) give the corresponding individual 95% confidence intervals, which are equivalent to the (partial, Type III) \\(t\\)-tests for each coefficient given in the standard multiple regression output shown above.\nThus, controlling for Stress, the confidence interval for the slope for Coffee includes 0, so we cannot reject the hypothesis that \\(\\beta_{\\mathrm{Coffee}}=0\\) in the multiple regression model, as we saw above in the numerical output. On the other hand, the interval for the slope for Stress excludes the origin, so we reject the null hypothesis that \\(\\beta_{\\mathrm{Stress}}=0\\), controlling for Coffee consumption.\nFinally, consider the relationship between the data ellipse and the confidence ellipse. These have exactly the same shape, but (with equal coordinate scaling of the axes), the confidence ellipse is exactly a \\(90^o\\) rotation and rescaling of the data ellipse. In directions in data space where the slice of the data ellipse is wide—where we have more information about the relationship between Coffee and Stress—the projection of the confidence ellipse is narrow, reflecting greater precision of the estimates of coefficients. Conversely, where slice of the the data ellipse is narrow (less information), the projection of the confidence ellipse is wide (less precision).\nTODO: Maybe include this code only in the HTML version?\nConfidence ellipses are drawn using car::confidenceEllipse(). Click the button to show the code.\n\nCode for confidence ellipsesconfidenceEllipse(coffee.mod, \n    grid = FALSE,\n    xlim = c(-2, 1), ylim = c(-0.5, 2.5),\n    xlab = expression(paste(\"Coffee coefficient,  \", beta[\"Coffee\"])),\n    ylab = expression(paste(\"Stress coefficient,  \", beta[\"Stress\"])),\n    cex.lab = 1.5)\nconfidenceEllipse(coffee.mod, add=TRUE, draw = TRUE,\n    col = \"red\", fill = TRUE, fill.alpha = 0.1,\n    dfn = 1)\nabline(h = 0, v = 0, lwd = 2)\n\n# confidence intervals\nbeta &lt;- coef( coffee.mod )[-1]\nCI &lt;- confint(coffee.mod)\nlines( y = c(0,0), x = CI[\"Coffee\",] , lwd = 6, col = 'red')\nlines( x = c(0,0), y = CI[\"Stress\",] , lwd = 6, col = 'red')\npoints( diag( beta ), col = 'black', pch = 16, cex=1.8)\n\nabline(v = CI[\"Coffee\",], col = \"red\", lty = 2)\nabline(h = CI[\"Stress\",], col = \"red\", lty = 2)\n\ntext(-2.1, 2.35, \"Beta space\", cex=2, pos = 4)\narrows(beta[1], beta[2], beta[1], 0, angle=8, len=0.2)\narrows(beta[1], beta[2], 0, beta[2], angle=8, len=0.2)\n\ntext( -1.5, 1.85, \"df = 2\", col = 'blue', adj = 0, cex=1.2)\ntext( 0.2, .85, \"df = 1\", col = 'red', adj = 0, cex=1.2)\n\nheplots::mark.H0(col = \"darkgreen\", pch = \"+\", lty = 0, pos = 4, cex = 3)",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "08-lin-mod-topics.html#sec-meas-error",
    "href": "08-lin-mod-topics.html#sec-meas-error",
    "title": "\n7  Topics in Linear Models\n",
    "section": "\n7.2 Measurement error",
    "text": "7.2 Measurement error\n\nIn classical linear models, the predictors are often considered to be fixed variables, statistical constants as in a designed experiment where the \\(x\\)s are set to given values. If they are random variables, they are to be measured without error and independent of the regression errors in \\(y\\). Either condition, along with the assumption of linearity, guarantees that the standard OLS estimators are unbiased.\nThat is, in a simple linear regression, \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), the estimated slope \\(\\hat{\\beta}_1\\) will have an average, expected value \\(\\mathcal{E} (\\hat{\\beta}_1)\\) equal to the true population value \\(\\beta_1\\) over repeated samples.\nNot only this, but the Gauss-Markov theorem (https://bit.ly/4njKnBo) guarantees that under these conditions the OLS estimator is also the most efficient because it has the least variance (most precise) among all linear and unbiased estimators. The classical OLS estimator is then said to be BLUE: It is the Best (lowest variance), Linear (among linear estimators), Unbiased, Estimator.\nBut these happy results may go out the window when the predictor variables are subject to measurement errors, for example with rating scales of less than perfect reliability. This section illustrates how error in predictors affects bias and precision using the graphical methods of this book.\n\n7.2.1 Errors in predictors\nErrors in the response \\(y\\) are accounted for in the model and measured by the mean squared error, \\(\\text{MSE} = \\hat{\\sigma}_\\epsilon^2\\). But in practice, of course, predictor variables are often only observed indicators, subject to their own error. Indeed, in the behavioral sciences it is rare that predictors are perfectly reliable and measured exactly. In economics, measures of employment, or cost of your groceries cannot be assumed to be error-free.\nThis fact that is recognized in errors-in-variables regression models (Fuller, 2006) and in more general structural equation models, but often ignored otherwise. Ellipsoids in data space and \\(\\beta\\) space are well suited to showing the effect of measurement error in predictors on OLS estimates.\nThe statistical facts are well known, though perhaps counter-intuitive in certain details:\n\nmeasurement error in a predictor biases regression coefficients (towards 0), while\nerror in the measurement in \\(y\\) increases the MSE and thus standard errors of the regression coefficients but does not introduce bias in the coefficients.\n\n7.2.2 Example: Measurement Error Quartet\nAn illuminating example can be constructed by starting with the simple linear regression\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\; ,\n\\] where \\(x_i\\) is the true, fully reliable predictor and \\(y\\) is the response, with error variance \\(\\sigma_\\epsilon^2\\). Now consider that we don’t measure \\(x_i\\) exactly, but instead observe \\(x^\\star_i\\).\n\\[\nx^\\star_i = x_i + \\eta_i \\; ,\n\\] where the measurement error \\(\\eta_i\\) is independent of the true \\(x_i\\) with variance \\(\\sigma^2_\\eta\\). We can extend this example to also consider the effect of adding additional, independent error variance to \\(y\\), so that instead of \\(y_i\\) we observe\n\\[\ny^\\star_i = y_i + \\nu_i\n\\] with variance \\(\\sigma^2_\\nu\\).\nLet’s simulate an example where the true relation is \\(y = 0.2 + 0.3 x\\) with error standard deviation \\(\\sigma = 0.5\\). I’ll take \\(x\\) to be uniformly distributed in [0, 10] and calculate \\(y\\) as normally distributed around that linear relation.\n\n\nset.seed(123)\nn &lt;- 300\n\na &lt;- 0.2    # true intercept\nb &lt;- 0.3    # true slope\nsigma &lt;- 0.5 # baseline error standard deviation\n\nx &lt;- runif(n, 0, 10)\ny &lt;- rnorm(n, a + b*x, sigma)\ndemo &lt;- data.frame(x,y)\n\nThen, generate alternative values \\(x^\\star\\) and \\(y^\\star\\) with additional error standard deviations around \\(x\\) given by \\(\\sigma_\\eta = 4\\) and around \\(y\\) given by \\(\\sigma_\\nu = 1\\).\n\nerr_y &lt;- 1   # additional error stdev for y\nerr_x &lt;- 4   # additional error stdev for x\ndemo  &lt;- demo |&gt;\n  mutate(y_star = rnorm(n, y, err_y),\n         x_star = rnorm(n, x, err_x))\n\nThere are four possible models we could fit and compare, using the combinations of \\((x, x^\\star)\\) and \\((y, y^\\star)\\)\n\nfit_1 &lt;- lm(y ~ x,           data = demo)   # no additional error\nfit_2 &lt;- lm(y_star ~ x,      data = demo)   # error in y\nfit_3 &lt;- lm(y ~ x_star,      data = demo)   # error in x\nfit_4 &lt;- lm(y_star ~ x_star, data = demo)   # error in x and y\n\nHowever, to show the differences visually, we can simply plot the data for each pair and show the regression lines (with confidence bands) and the data ellipses. To do this efficiently with ggplot2, it is necessary to transform the demo data to long format with columns x and y, distinguished by name for the four combinations.\n\n# make the demo dataset long, with names for the four conditions\ndf &lt;- bind_rows(\n  data.frame(x=demo$x,      y=demo$y,      name=\"No measurement error\"),\n  data.frame(x=demo$x,      y=demo$y_star, name=\"Measurement error on y\"),\n  data.frame(x=demo$x_star, y=demo$y,      name=\"Measurement error on x\"),\n  data.frame(x=demo$x_star, y=demo$y_star, name=\"Measurement error on x and y\")) |&gt;\n  mutate(name = fct_inorder(name)) \n\nThen, we can plot the data in df with points, regression lines and a data ellipse, faceting by name to give the measurement error quartet. \n\n\nggplot(df, aes(x, y)) +\n  geom_point(alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", \n               color = \"blue\",fill= \"blue\", \n               alpha=0.05, linewidth = 1.1) +\n  geom_smooth(method=\"lm\", formula = y~x, fullrange=TRUE, level=0.995,\n              color = \"red\", fill = \"red\", alpha = 0.2) +\n  facet_wrap(~name) \n\n\n\n\n\n\nFigure 7.4: The measurement error quartet: Each plot shows the linear regression of y on x, but where additional error variance has been added to y or x or both. The widths of the confidence bands and the vertical extent of the data ellipses show the effect on precision.\n\n\n\n\nComparing the plots in the first row, you can see that when additional error is added to \\(y\\), the regression slope remains essentially unchanged, illustrating that the estimate is unbiased. However, the confidence bounds on the regression line become wider, and the data ellipse becomes fatter in the \\(y\\) direction, illustrating the loss of precision.\nThe effect of error in \\(x\\) is less kind. Comparing the first row of plots with the second row, you can see that the estimated slope decreases when errors are added to \\(x\\). This is called attenuation bias, and it can be shown that \\[\n\\widehat{\\beta}_{x^\\star} \\longrightarrow \\frac{\\beta}{1+\\sigma^2_\\eta /\\sigma^2_x} \\; ,\n\\] where \\(\\beta\\) here refers to the regression slope and \\(\\longrightarrow\\) means “converges to”, as the sample size gets large. Thus, as \\(\\sigma^2_\\eta\\) increases, \\(\\widehat{\\beta}_{x^\\star}\\) becomes less than \\(\\beta\\).\nBeyond plots like Figure 7.4, we can see the effects of error in \\(x\\) or \\(y\\) on the model summary statistics such as the correlation \\(r_{xy}\\) or MSE by extracting these from the fitted models. This is easily done using dplyr::nest_by(name) and fitting the regression model to each subset, from which we can obtain the model statistics using sigma(), coef() and so forth. A bit of dplyr::mutate() magic is used to construct indicators errX and errY giving whether or not error was added to \\(x\\) and/or \\(y\\).\n\nmodel_stats &lt;- df |&gt;\n  dplyr::nest_by(name) |&gt;\n  mutate(model = list(lm(y ~ x, data = data)),\n         sigma = sigma(model),\n         intercept = coef(model)[1],\n         slope = coef(model)[2],\n         r = sqrt(summary(model)$r.squared)) |&gt;\n  mutate(errX = stringr::str_detect(name, \" x\"),\n         errY = stringr::str_detect(name, \" y\")) |&gt;\n  mutate(errX = factor(errX, levels = c(\"TRUE\", \"FALSE\")),\n         errY = factor(errY, levels = c(\"TRUE\", \"FALSE\"))) |&gt;\n  relocate(errX, errY, r, .after = name) |&gt;\n  select(-data) |&gt;\n  print()\n# # A tibble: 4 × 8\n# # Rowwise:  name\n#   name                errX  errY      r model sigma intercept  slope\n#   &lt;fct&gt;               &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lis&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n# 1 No measurement err… FALSE FALSE 0.858 &lt;lm&gt;  0.495    0.244  0.294 \n# 2 Measurement error … FALSE TRUE  0.648 &lt;lm&gt;  1.09     0.0838 0.329 \n# 3 Measurement error … TRUE  FALSE 0.481 &lt;lm&gt;  0.844    1.22   0.0946\n# 4 Measurement error … TRUE  TRUE  0.401 &lt;lm&gt;  1.31     1.12   0.117\n\nWe plot the model \\(R = r_{xy}\\) and the estimated residual standard error in Figure 7.5 below. The lines connecting the points are approximately parallel, indicating that errors of measurement in \\(x\\) and \\(y\\) have nearly additive effects on model summaries.\n\n\np1 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = r, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model R \",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") +\n  legend_inside(c(0.27, 0.8))\n\np2 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = sigma, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model residual standard error\",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\nFigure 7.5: Model statistics for the combinations of additional error variance in x or y or both. Left: model R; right: Residual standard error.\n\n\n\n\n\n7.2.3 Coffee data: Bias and precision\nIn multiple regression the effects of measurement error in a predictor become more complex, because error variance in one predictor, \\(x_1\\), say, can affect the coefficients of other terms in the model.\nConsider the marginal relation between Heart disease and Stress in the coffee data. Figure 7.6 shows this with data ellipses in data space and the corresponding confidence ellipses in \\(\\beta\\) space. Each panel starts with the observed data (the darkest ellipse, marked \\(0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0.75, 1.0, 1.5\\}\\), to the value of Stress, while keeping the mean of Stress the same. All of the data ellipses have the same vertical shadows (\\(\\text{SD}_{\\textrm{Heart}}\\)), while the horizontal shadows increase with \\(\\delta\\), driving the slope for Stress toward 0.\nIn \\(\\beta\\) space, it can be seen that the estimated coefficients, \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) vary along a line and approach \\(\\beta_{\\textrm{Stress}}=0\\) as \\(\\delta\\) gets sufficiently large. The shadows of ellipses for \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) along the \\(\\beta_{\\textrm{Stress}}\\) axis also demonstrate the effects of measurement error on the standard error of \\(\\beta_{\\textrm{Stress}}\\).\n\n\n\n\n\n\n\nFigure 7.6: Effects of measurement error in Stress on the marginal relationship between Heart disease and Stress. Each panel starts with the observed data (\\(\\delta = 0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\text{SD}_\\text{Stress})\\) with standard deviations multiplied by \\(\\delta\\) = 0.75, 1.0, 1.5, to the value of Stress. Increasing measurement error biases the slope for Stress toward 0. Left: 50% data ellipses; right: 50% confidence ellipses.\n\n\n\n\nPerhaps less well-known, but both more surprising and interesting, is the effect that measurement error in one variable, \\(x_1\\), has on the estimate of the coefficient for an other variable, \\(x_2\\), in a multiple regression model. Figure 7.7 shows the confidence ellipses for \\((\\beta_{\\textrm{Coffee}}, \\beta_{\\textrm{Stress}})\\) in the multiple regression predicting Heart disease, adding random normal error \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0, 0.2, 0.4, 0.8\\}\\), to the value of Stress alone.\nAs can be plainly seen, while this measurement error in Stress attenuates its coefficient, it also has the effect of biasing the coefficient for Coffee toward that in the marginal regression of Heart disease on Coffee alone.\n\n\n\n\n\n\n\nFigure 7.7: Biasing effect of measurement error in one variable (Stress) on on the coefficient of another variable (Coffee) in a multiple regression. The coefficient for Coffee is driven towards its value in the marginal model using Coffee alone, as measurement error in Stress makes it less informative in the joint model.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "08-lin-mod-topics.html#what-have-we-learned",
    "href": "08-lin-mod-topics.html#what-have-we-learned",
    "title": "\n7  Topics in Linear Models\n",
    "section": "\n7.3 What have we learned?",
    "text": "7.3 What have we learned?\n\nData space and \\(\\beta\\) space are dualities of each other - While we typically visualize regression models in data space (where points are observations), there’s a parallel \\(\\beta\\) space where points represent models and their coefficients. These spaces mirror each other in elegant ways: lines in one space become points in the other, and confidence ellipses in \\(\\beta\\) space are 90\\(^\\circ\\) rotations of data ellipses. This duality reveals that we gain precision in estimating coefficients precisely where our data spread the most. \nConfidence ellipses make hypothesis testing visual and intuitive - Instead of squinting at p-values in regression output, we can literally see whether hypotheses are supported by checking whether null hypothesis points fall inside or outside confidence ellipses. The shadows of these ellipses automatically give us individual confidence intervals, while the full ellipse captures joint uncertainty about multiple coefficients.\nMeasurement error in predictors is far more dangerous than measurement error in responses - While errors in your response variable (y) simply inflate standard errors without biasing coefficients, errors in predictors create attenuation bias that systematically pulls slope estimates toward zero. This “errors-in-variables” problem means that unreliable measurements of your predictors can make real effects appear weaker than they actually are. \nIn multiple regression, measurement error in one predictor contaminates estimates of other predictors - Perhaps most surprisingly, when one predictor in your model suffers from measurement error, it doesn’t just bias its own coefficient—it also distorts the coefficients of other variables in unpredictable ways. This distortion will cascade, meaning that measurement quality affects your entire model, not just individual variables.\nEllipses reveal the hidden geometry behind familiar statistical concepts - Data ellipses, confidence ellipses, and their mathematical relationships provide a geometric foundation for understanding correlation, regression coefficients, confidence intervals, and hypothesis tests. This visual approach transforms abstract statistical concepts into concrete geometric relationships that you can literally see and manipulate.\n\n\n\n\n\nDempster, A. P. (1969). Elements of continuous multivariate analysis. Addison-Wesley.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFuller, W. (2006). Measurement error models (2nd ed.). John Wiley & Sons.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "08-lin-mod-topics.html#footnotes",
    "href": "08-lin-mod-topics.html#footnotes",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "I use 40% and 68% for the data ellipses and 95% for the confidence ellipse only for ease of interpretation and convenience in plotting. What is important is the relative shape and orientation of the ellipses in the two panels of Figure 7.3.↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html",
    "href": "09-collinearity-ridge.html",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "8.1 What is collinearity?\nIn univariate multiple regression models, we usually hope to have high correlations between the outcome \\(y\\) and each of the predictors, \\(\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x_2}, \\dots]\\). But high correlations among the predictors can cause problems in estimating and testing their effects. Exactly the same problems can exist in multivariate response models, because they involve only the relations among the predictor variables, so the problems and solutions discussed here apply equally to MLMs.\nThe problem of high correlations among the predictors in a model is called collinearity (or multicollinearity), referring to the situation when two or more predictors are very nearly linearly related to each other (collinear). This chapter illustrates the nature of collinearity geometrically, using data and confidence ellipsoids (Section 8.1) It describes diagnostic measures to asses these effects (Section 8.2) and presents some novel visual tools for these purposes using the VisCollin package. These include tableplots (Section 8.3) and collinearity biplots (Section 8.4).\nOne class of solutions for collinearity involves regularization methods such as ridge regression (Section 8.6). Another collection of graphical methods, generalized ridge trace plots, implemented in the genridge package, sheds further light on what is accomplished by this technique. In Section 8.9 we see that, once again, PCA-related techniques like the biplot can be insightful—here, to understand the nature of collinearity and ridge regression. More generally, the methods of this chapter are further examples of how data and confidence ellipsoids can be used to visualize bias and precision of regression estimates.\nPackages\nIn this chapter I use the following packages. Load them now.\nResearchers who have studies standard treatments of linear models (e.g, Graybill (1961); Hocking (2013)) are often less than clear about what collinearity is, how to find its sources and how to take steps to resolve them. There are a number of important diagnostic measures that can help, but these are usually presented in a tabular display like Figure 8.1, which prompted this query on an online forum:\nFigure 8.1: Collinearity diagnostics for a multiple regression model from SPSS. Source: Arndt Regorz, How to interpret a Collinearity Diagnostics table in SPSS, https://bit.ly/3YRB82b\nThe trouble with displays like Figure 8.1 is that the important information is hidden in a sea of numbers, some of which are bad when large, others bad when they are small and a large bunch which are irrelevant to interpretation.\nIn Friendly & Kwan (2009), we liken this problem to that of the reader of Martin Hansford’s successful series of books, Where’s Waldo. These consist of a series of full-page illustrations of hundreds of people and things and a few Waldos— a character wearing a red and white striped shirt and hat, glasses, and carrying a walking stick or other paraphernalia. Waldo was never disguised, yet the complex arrangement of misleading visual cues in the pictures made him very hard to find. Collinearity diagnostics often provide a similar puzzle: where should you look in traditional tabular displays?1\nFigure 8.2: A scene from one of the Where’s Waldo books. Waldo wears a red-striped shirt, but far too many of the other figures in the scene have horizontal red stripes, making it very difficult to find him among all the distractors. This is often the problem with collinearity diagnostics. Source: Modified from https://bit.ly/48KPcOo\nRecall the standard classical linear model for a response variable \\(y\\) with a collection of predictors in \\(\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_p)\\)\n\\[\n\\begin{aligned}\n\\mathbf{y}  & =  \\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\cdots + \\beta_p \\mathbf{x}_p + \\boldsymbol{\\epsilon} \\\\\n            & =  \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\; ,\n\\end{aligned}\n\\]\nfor which the ordinary least squares solution is:\n\\[\n\\widehat{\\mathbf{b}} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\; \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\; .\n\\] The sampling variances and covariances of the estimated coefficients is \\(\\text{Var} (\\widehat{\\mathbf{b}}) = \\sigma_\\epsilon^2 \\times (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1}\\) and \\(\\sigma_\\epsilon^2\\) is the variance of the residuals \\(\\boldsymbol{\\epsilon}\\), estimated by the mean squared error (MSE).\nIn the limiting case, collinearity becomes particularly problematic when one \\(x_i\\) is perfectly predictable from the other \\(x\\)s, i.e., \\(R^2 (x_i | \\text{other }x) = 1\\). This is problematic because:\nThis extreme case reflects a situation when one or more predictors are effectively redundant, for example when you include two variables \\(x\\) and \\(y\\) and their sum \\(z = x + y\\) in a model. For instance, a dataset may include variables for income, expenses, and savings. But income is the sum of expenses and savings, so not all three should be used as predictors.\nA more subtle case is the use ipsatized, defined as scores that sum to a constant, such as proportions of a total. You might have scores on tests of reading, math, spelling and geography. With ipsatized scores, any one of these is necessarily 1 \\(-\\) sum of the others, i.e., if reading is 0.5, math and geography are both 0.15, then geography must be 0.2. Once thre of the four scores are known, the last provides no new information.\nMore generally, collinearity refers to the case when there are very high multiple correlations among the predictors, such as \\(R^2 (x_i | \\text{other }x) \\ge 0.9\\). Note that you can’t tell simply by looking at the simple correlations. A large correlation \\(r_{ij}\\) is sufficient for collinearity, but not necessary—you can have variables \\(x_1, x_2, x_3\\) for which the pairwise correlation are low, but the multiple correlation is high.\nThe consequences are:",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#sec-what-is-collin",
    "href": "09-collinearity-ridge.html#sec-what-is-collin",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "Some of my collinearity diagnostics have large values, or small values, or whatever they are not supposed to be.\n\nWhat is bad?\nIf bad, what can I do about it?\n\n\n\n\n\n\n\n\n\n\n\n\n\nthere is no unique solution for the regression coefficients \\(\\mathbf{b} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X} \\mathbf{y}\\);\nthe standard errors \\(s (b_i)\\) of the estimated coefficients are infinite and t statistics \\(t_i = b_i / s (b_i)\\) are 0.\n\n\n\n\n\n\nThe estimated coefficients have large standard errors, \\(s(\\hat{b}_j)\\). They are multiplied by the square root of the variance inflation factor, \\(\\sqrt{\\text{VIF}}\\), discussed below.\nThe large standard errors deflate the \\(t\\)-statistics, \\(t = \\hat{b}_j / s(\\hat{b}_j)\\), by the same factor, so a coefficient that would significant if the predictors were uncorrelated becomes insignificant when collinearity is present.\nThus you may find a situation where an overall model is highly significant (large \\(F\\)-statistic), while no (or few) of the individual predictors are. This is a puzzlement!\nBeyond this, the least squares solution may have poor numerical accuracy (Longley, 1967), because the solution depends inversely on the determinant \\(|\\,\\mathbf{X}^\\mathsf{T} \\mathbf{X}\\,|\\), which approaches 0 as multiple correlations increase.\nThere is an interpretive problem as well. Recall that the coefficients \\(\\hat{b}\\) are partial coefficients, meaning that they estimate change \\(\\Delta y\\) in \\(y\\) when \\(x\\) changes by one unit \\(\\Delta x\\), but holding all other variables constant. Then, the model may be trying to estimate something that does not occur in the data. (For example: predicting strength from the highly correlated height and weight)\n\n\n8.1.1 Visualizing collinearity\nCollinearity can be illustrated in data space for two predictors in terms of the stability of the regression plane for a linear model Y = X1 + X2. Figure 8.3 shows three cases as 3D plots of \\((X_1, X_2, Y)\\), where the correlation of predictors can be observed in the \\((X_1, X_2)\\) plane.\n\nshows a case where \\(X_1\\) and \\(X_2\\) are uncorrelated as can be seen in their scatter in the horizontal plane (+ symbols). The gray regression plane is well-supported; a small change in Y for one observation won’t make much difference.\nIn panel (b), \\(X_1\\) and \\(X_2\\) have a perfect correlation, \\(r (x_1, x_2) = 1.0\\). The regression plane is not unique; in fact there are an infinite number of planes that fit the data equally well. Note that, if all we care about is prediction (not the coefficients), we could use \\(X_1\\) or \\(X_2\\), or both, or any weighted sum of them in a model and get the same predicted values.\nShows a typical case where there is a strong correlation between \\(X_1\\) and \\(X_2\\). The regression plane here is unique, but is not well determined. A small change in Y can make quite a difference in the fitted value or coefficients, depending on the values of \\(X_1\\) and \\(X_2\\). Where \\(X_1\\) and \\(X_2\\) are far from their near linear relation in the botom plane, you can imagine that it is easy to tilt the plane substantially by a small change in \\(Y\\).\n\n\n\n\n\n\n\n\nFigure 8.3: Effect of collinearity on the least squares regression plane. (a) Small correlation between predictors; (b) Perfect correlation ; (c) Very strong correlation. The black points show the data Y values, white points are the fitted values in the regression plane, and + signs represent the values of X1 and X2. Source: Adapted from Fox (2016), Fig. 13.2\n\n\n\n\n\n8.1.2 Data space and \\(\\beta\\) space\nIt is also useful to visualize collinearity by comparing the representation in data space with the analogous view of the confidence ellipses for coefficients in beta space. To do so in this example, I generate data from a known model \\(y = 3 x_1 + 3 x_2 + \\epsilon\\) with \\(\\epsilon \\sim \\mathcal{N} (0, 100)\\) and various true correlations between \\(x_1\\) and \\(x_2\\), \\(\\rho_{12} = (0, 0.8, 0.97)\\) 2.\n\n\nFirst, I use MASS:mvrnorm() to construct a list of three data frames XY with the same means and standard deviations, but with different correlations. In each case, the variable \\(y\\) is generated with true coefficients beta \\(=(3, 3)\\), and the fitted model for that value of rho is added to a corresponding list of models, mods.\n\nCodelibrary(MASS)\nlibrary(car)\n\nset.seed(421)            # reproducibility\nN &lt;- 200                 # sample size\nmu &lt;- c(0, 0)            # means\ns &lt;- c(1, 1)             # standard deviations\nrho &lt;- c(0, 0.8, 0.97)   # correlations\nbeta &lt;- c(3, 3)          # true coefficients\n\n# Specify a covariance matrix, with standard deviations\n#   s[1], s[2] and correlation r\nCov &lt;- function(s, r){\n  matrix(c(s[1],        r * s[1]*s[2],\n         r * s[1]*s[2], s[2]), nrow = 2, ncol = 2)\n}\n\n# Generate a dataframe of X, y for each rho\n# Fit the model for each\nXY &lt;- vector(mode =\"list\", length = length(rho))\nmods &lt;- vector(mode =\"list\", length = length(rho))\nfor (i in seq_along(rho)) {\n  r &lt;- rho[i]\n  X &lt;- mvrnorm(N, mu, Sigma = Cov(s, r))\n  colnames(X) &lt;- c(\"x1\", \"x2\")\n  y &lt;- beta[1] * X[,1] + beta[2] * X[,2] + rnorm(N, 0, 10)\n\n  XY[[i]] &lt;- data.frame(X, y=y)\n  mods[[i]] &lt;- lm(y ~ x1 + x2, data=XY[[i]])\n}\n\n\nThe estimated coefficients can then be extracted using coef() applied to each model:\n\ncoefs &lt;- sapply(mods, coef)\ncolnames(coefs) &lt;- paste0(\"mod\", 1:3, \" (rho=\", rho, \")\")\ncoefs\n#             mod1 (rho=0) mod2 (rho=0.8) mod3 (rho=0.97)\n# (Intercept)         1.01        -0.0535           0.141\n# x1                  3.18         3.4719           3.053\n# x2                  1.68         2.9734           2.059\n\nThen, I define a function to plot the data ellipse (car::dataEllipse()) for each data frame and confidence ellipse (car::confidenceEllipse()) for the coefficients in the corresponding fitted model. In the plots in Figure 8.4, I specify the x, y limits for each plot so that the relative sizes of these ellipses are comparable, so that variance inflation can be assessed visually.\n\nCodedo_plots &lt;- function(XY, mod, r) {\n  X &lt;- as.matrix(XY[, 1:2])\n  dataEllipse(X,\n    levels= 0.95,\n    col = \"darkgreen\",\n    fill = TRUE, fill.alpha = 0.05,\n    xlim = c(-3, 3),\n    ylim = c(-3, 3), asp = 1)\n  text(0, 3, bquote(rho == .(r)), cex = 2, pos = NULL)\n\n  confidenceEllipse(mod,\n    col = \"red\",\n    fill = TRUE, fill.alpha = 0.1,\n    xlab = expression(paste(\"x1 coefficient, \", beta[1])),\n    ylab = expression(paste(\"x2 coefficient, \", beta[2])),\n    xlim = c(-5, 10),\n    ylim = c(-5, 10),\n    asp = 1)\n  points(beta[1], beta[2], pch = \"+\", cex=2)\n  abline(v=0, h=0, lwd=2)\n}\n\nop &lt;- par(mar = c(4,4,1,1)+0.1,\n          mfcol = c(2, 3),\n          cex.lab = 1.5)\nfor (i in seq_along(rho)) {\n  do_plots(XY[[i]], mods[[i]], rho[i])\n}\npar(op)\n\n\n\n\n\n\nFigure 8.4: 95% Data ellipses for x1, x2 and the corresponding 95% confidence ellipses for their coefficients in the model predicting y. In the confidence ellipse plots, reference lines show the value (0,0) for the null hypothesis and “+” marks the true values for the coefficients. This figure adapts an example by John Fox (2022).\n\n\n\n\nRecall (Section 7.1) that the confidence ellipse for \\((\\beta_1, \\beta_2)\\) is just a 90 degree rotation (and rescaling) of the data ellipse for \\((x_1, x_2)\\): it is wide (more variance) in any direction where the data ellipse is narrow.\nThe shadows of the confidence ellipses on the coordinate axes in Figure 8.4 represent the standard errors of the coefficients, and get larger with increasing \\(\\rho\\). This is the effect of variance inflation, described in the following section.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#sec-measure-collin",
    "href": "09-collinearity-ridge.html#sec-measure-collin",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.2 Measuring collinearity",
    "text": "8.2 Measuring collinearity\nThis section first describes the variance inflation factor (VIF) used to measure the effect of possible collinearity on each predictor and a collection of diagnostic measures designed to help interpret these. Then I describe some novel graphical methods to make these effects more readily understandable, to answer the “Where’s Waldo” question posed at the outset.\n\n8.2.1 Variance inflation factors\nHow can we measure the effect of collinearity? The essential idea is to compare, for each predictor the variance \\(s^2 (\\widehat{b_j})\\) that the coefficient that \\(x_j\\) would have if it was totally unrelated to the other predictors to the actual variance it has in the given model.\nFor two predictors such as shown in Figure 8.4 the sampling variance of \\(x_1\\) can be expressed as\n\\[\ns^2 (\\widehat{b_1}) = \\frac{MSE}{(n-1) \\; s^2(x_1)} \\; \\times \\; \\left[ \\frac{1}{1-r^2_{12}} \\right]\n\\] The first term here is the variance of \\(b_1\\) when the two predictors are uncorrelated. The term in brackets represents the variance inflation factor (Marquardt, 1970), the amount by which the variance of the coefficient is multiplied as a consequence of the correlation \\(r_{12}\\) of the predictors. As \\(r_{12} \\rightarrow 1\\), the variances approaches infinity.\nMore generally, with any number of predictors, this relation has a similar form, replacing the simple correlation \\(r_{12}\\) with the multiple correlation predicting \\(x_j\\) from all others,\n\\[\ns^2 (\\widehat{b_j}) = \\frac{MSE}{(n-1) \\; s^2(x_j)} \\; \\times \\; \\left[ \\frac{1}{1-R^2_{j | \\text{others}}} \\right]\n\\] So, we have that the variance inflation factors are:\n\\[\n\\text{VIF}_j = \\frac{1}{1-R^2_{j \\,|\\, \\text{others}}}\n\\] In practice, it is often easier to think in terms of the square root, \\(\\sqrt{\\text{VIF}_j}\\) as the multiplier of the standard errors. The denominator, \\(1-R^2_{j | \\text{others}}\\) is sometimes called tolerance, a term I don’t find particularly useful, but it is just the proportion of the variance of \\(x_j\\) that is not explainable from the others.3\nFor the cases shown in Figure 8.4 the VIFs and their square roots are:\n\nvifs &lt;- sapply(mods, car::vif)\ncolnames(vifs) &lt;- paste(\"rho:\", rho)\nvifs\n#    rho: 0 rho: 0.8 rho: 0.97\n# x1      1     3.09      18.6\n# x2      1     3.09      18.6\n\nsqrt(vifs)\n#    rho: 0 rho: 0.8 rho: 0.97\n# x1      1     1.76      4.31\n# x2      1     1.76      4.31\n\nGeneralized VIF\nNote that when there are terms in the model with more than one degree of freedom, such as education with four levels (and hence 3 df) or a polynomial term specified as poly(age, 3), that variable, education or age is represented by three separate \\(x\\)s in the model matrix, and the standard VIF calculation gives results that vary with how those terms are coded in the model.\nTo allow for these cases, Fox & Monette (1992) define generalized, GVIFs as the inflation in the squared area of the confidence ellipse for the coefficients of such terms, relative to what would be obtained with uncorrelated data. Visually, this can be seen by comparing the areas of the ellipses in the bottom row of Figure 8.4. Because the magnitude of the GVIF increases with the number of degrees of freedom for the set of parameters, Fox & Monette suggest the analog \\(\\sqrt{\\text{GVIF}^{1/2 \\text{df}}}\\) as the measure of impact on standard errors. This is what car::vif() calculates for a factor or other term with more than 1 df.\n\nExample 8.1 Cars data\n\nThis example uses the cars dataset in the VisCollin package which contains various measures of size and performance on 406 models of automobiles from 1982. Interest is focused on predicting gas mileage, mpg.\n\ndata(cars, package = \"VisCollin\")\nstr(cars)\n# 'data.frame': 406 obs. of  10 variables:\n#  $ make    : Factor w/ 30 levels \"amc\",\"audi\",\"bmw\",..: 6 4 22 1 12 12 6 22 23 1 ...\n#  $ model   : chr  \"chevelle\" \"skylark\" \"satellite\" \"rebel\" ...\n#  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...\n#  $ cylinder: int  8 8 8 8 8 8 8 8 8 8 ...\n#  $ engine  : num  307 350 318 304 302 429 454 440 455 390 ...\n#  $ horse   : int  130 165 150 150 140 198 220 215 225 190 ...\n#  $ weight  : int  3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ...\n#  $ accel   : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n#  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...\n#  $ origin  : Factor w/ 3 levels \"Amer\",\"Eur\",\"Japan\": 1 1 1 1 1 1 1 1 1 1 ...\n\nWe fit a model predicting gas mileage (mpg) from the number of cylinders, engine displacement, horsepower, weight, time to accelerate from 0 – 60 mph and model year (1970–1982). Perhaps surprisingly, only weight and year appear to significantly predict gas mileage. What’s going on here?\n\ncars.mod &lt;- lm (mpg ~ cylinder + engine + horse + \n                      weight + accel + year, \n                data=cars)\nAnova(cars.mod)\n# Anova Table (Type II tests)\n# \n# Response: mpg\n#           Sum Sq  Df F value Pr(&gt;F)    \n# cylinder      12   1    0.99   0.32    \n# engine        13   1    1.09   0.30    \n# horse          0   1    0.00   0.98    \n# weight      1214   1  102.84 &lt;2e-16 ***\n# accel          8   1    0.70   0.40    \n# year        2419   1  204.99 &lt;2e-16 ***\n# Residuals   4543 385                   \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe check the variance inflation factors, using car::vif(). We see that most predictors have very high VIFs, indicating moderately severe multicollinearity.\n\nvif(cars.mod)\n# cylinder   engine    horse   weight    accel     year \n#    10.63    19.64     9.40    10.73     2.63     1.24\n\nsqrt(vif(cars.mod))\n# cylinder   engine    horse   weight    accel     year \n#     3.26     4.43     3.07     3.28     1.62     1.12\n\nAccording to \\(\\sqrt{\\text{VIF}}\\), the standard error of cylinder has been multiplied by \\(\\sqrt{10.63} = 3.26\\) and it’s \\(t\\)-value is divided by this number, compared with the case when all predictors are uncorrelated. engine, horse and weight suffer a similar fate.\nIf we also included the factor origin in the models, we would get the generalized GVIF:\n\ncars.mod2 &lt;- lm (mpg ~ cylinder + engine + horse + \n                       weight + accel + year + origin, \n                 data=cars)\nvif(cars.mod2)\n#           GVIF Df GVIF^(1/(2*Df))\n# cylinder 10.74  1            3.28\n# engine   22.94  1            4.79\n# horse     9.96  1            3.16\n# weight   11.07  1            3.33\n# accel     2.63  1            1.62\n# year      1.30  1            1.14\n# origin    2.10  2            1.20\n\n\n\n\n\n\n\n\nConnection with inverse of correlation matrix\n\n\n\nIn the linear regression model with standardized predictors, the covariance matrix of the estimated intercept-excluded parameter vector \\(\\mathbf{b}^\\star\\) has the simpler form, \\[\n\\mathcal{V} (\\mathbf{b}^\\star) = \\frac{\\sigma^2}{n-1} \\mathbf{R}^{-1}_{X} \\; .\n\\] where \\(\\mathbf{R}_{X}\\) is the correlation matrix among the predictors. It can then be seen that the VIF\\(_j\\) are just the diagonal entries of \\(\\mathbf{R}^{-1}_{X}\\).\nMore generally, the matrix \\(\\mathbf{R}^{-1}_{X} = (r^{ij})\\), when standardized to a correlation matrix as \\(-r^{ij} / \\sqrt{r^{ii} \\; r^{jj}}\\) gives the matrix of all partial correlations, \\(r_{ij} \\,|\\, \\text{others}\\).\nThis inverse connection is analogous to the dual relationship (Section 8.1.2) between ellipses in data space, based on \\(\\mathbf{X}^\\top \\mathbf{X}\\) and in \\(\\beta\\)-space, based on \\((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\).\n\n\n\n8.2.2 VIF displays\nBeyond the console output from car::vif(), the easystats suite of packages has some useful functions for displaying VIFs in helpful tables and plots. performance::check_collinearity() calculates VIFs and their standard errors and returns a \"check_collinearity\" data frame. The plot method for this uses a log scale for VIF because it is multiples of variance that matter here. It uses intervals of 1-5, 5-10, 10+ to highlight low, medium and high variance inflation with colored backgrounds.\n\ncars.collin &lt;- check_collinearity(cars.mod)\n\nplot(cars.collin, \n     linewidth = 1.1,\n     size_point = 5, size_title = 16, base_size = 14)\n\n\n\n\n\n\nFigure 8.5: Variance inflation plot. VIF is plotted on a log scale. Colored bands show regions of low, medium and high variance inflation.\n\n\n\n\nThe graphic properties here help to make the problematic variables more apparent than in a table of numbers, though the underlying message is the same. Number of cylinders, engine displacement and weight are the collinearity bad boys.\nKnowing this helps to pin Waldo down a bit, but to really find him, we need a few more diagnostics and better graphical methods.\n\n8.2.3 Collinearity diagnostics\nOK, we now know that large VIF\\(_j\\) indicate predictor coefficients whose estimation is degraded due to large \\(R^2_{j \\,|\\, \\text{others}}\\). But for this to be useful, we need to determine:\n\nhow many dimensions in the space of the predictors are associated with nearly collinear relations?\nwhich predictors are most strongly implicated in each of these?\n\nAnswers to these questions are provided using measures developed by Belsley and colleagues (Belsley et al., 1980; Belsley, 1991). These measures are based on the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots \\lambda_p\\) of the correlation matrix \\(R_{X}\\) of the predictors (preferably centered and scaled, and not including the constant term for the intercept), and the corresponding eigenvectors in the columns of \\(\\mathbf{V}_{p \\times p}\\), given by the the eigen decomposition\n\\[\n\\mathbf{R}_{X} = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^\\mathsf{T} \\; .\n\\]\nBy elementary matrix algebra, the eigen decomposition of \\(\\mathbf{R}_{XX}^{-1}\\) is then\n\\[\n\\mathbf{R}_{X}^{-1} = \\mathbf{V} \\boldsymbol{\\Lambda}^{-1} \\mathbf{V}^\\mathsf{T} \\; ,\n\\tag{8.1}\\]\nso, \\(\\mathbf{R}_{X}\\) and \\(\\mathbf{R}_{XX}^{-1}\\) have the same eigenvectors, and the eigenvalues of \\(\\mathbf{R}_{X}^{-1}\\) are just \\(\\lambda_i^{-1}\\). Using Equation 8.1, the variance inflation factors may be expressed as\n\\[\n\\text{VIF}_j = \\sum_{k=1}^p \\frac{V^2_{jk}}{\\lambda_k} \\; .\n\\tag{8.2}\\]\nThis shows that (a) only the small eigenvalues contribute to variance inflation, but (b) only for those predictors that have large eigenvector coefficients \\(V_{jk}\\) on those small components.\nThese facts lead to the following diagnostic statistics for collinearity:\n\n\nCondition indices (\\(\\kappa\\)): The smallest of the eigenvalues, those for which \\(\\lambda_j \\approx 0\\), indicate collinearity and the number of small values indicates the number of near collinear relations. Because the sum of the eigenvalues, \\(\\Sigma \\lambda_i = p\\) increases with the number of predictors \\(p\\), it is useful to scale them all inversely in relation to the largest so that larger numbers are worse. This leads to condition indices, defined as \\(\\kappa_j = \\sqrt{ \\lambda_1 / \\lambda_j}\\). These have the property that the resulting numbers have common interpretations regardless of the number of predictors.\n\nFor completely uncorrelated predictors, all \\(\\kappa_j = 1\\).\nAs any \\(\\lambda_k \\rightarrow 0\\) the corresponding \\(\\kappa_j \\rightarrow \\infty\\).\nAs a rule of thumb, Belsley (1991) suggests that values \\(\\kappa_j &gt; 10\\) reflect a moderate problem, while \\(\\kappa_j &gt; 30\\) indicates severe collinearity. Even worse values use bounds of 100, 300, … as collinearity becomes more extreme.\n\n\nVariance decomposition proportions: Large VIFs indicate variables that are involved in some nearly collinear relations, but they don’t indicate which other variable(s) each is involved with. For this purpose, Belsley et al. (1980) and Belsley (1991) proposed calculation of the proportions of variance of each variable associated with each principal component as a decomposition of the coefficient variance for each dimension. These are simply the the terms in Equation 8.2 divided by their sum.\n\nThese measures can be calculated using VisCollin::colldiag(). For the current model, the usual display contains both the condition indices and variance proportions. However, even for a small example, it is often difficult to know what numbers to pay attention to.\n\n(cd &lt;- colldiag(cars.mod, center=TRUE))\n# Condition\n# Index Variance Decomposition Proportions\n#           cylinder engine horse weight accel year \n# 1   1.000 0.005    0.003  0.005 0.004  0.009 0.010\n# 2   2.252 0.004    0.002  0.000 0.007  0.022 0.787\n# 3   2.515 0.004    0.001  0.002 0.010  0.423 0.142\n# 4   5.660 0.309    0.014  0.306 0.087  0.063 0.005\n# 5   8.342 0.115    0.000  0.654 0.715  0.469 0.052\n# 6  10.818 0.563    0.981  0.032 0.176  0.013 0.004\n\nBelsley (1991) recommends that the sources of collinearity be diagnosed (a) only for those components with large \\(\\kappa_j\\), and (b) for those components for which the variance proportion is large (say, \\(\\ge 0.5\\)) on two or more predictors. The print method for \"colldiag\" objects has a fuzz argument controlling this. The descending = TRUE argument puts the rows with the largest condition indices at the top.\n\nprint(cd, fuzz = 0.5, descending = TRUE)\n# Condition\n# Index Variance Decomposition Proportions\n#           cylinder engine horse weight accel year \n# 1   1.000  .        .      .     .      .     .   \n# 2   2.252  .        .      .     .      .    0.787\n# 3   2.515  .        .      .     .      .     .   \n# 4   5.660  .        .      .     .      .     .   \n# 5   8.342  .        .     0.654 0.715   .     .   \n# 6  10.818 0.563    0.981   .     .      .     .\n\nThe Waldo mystery is nearly solved, if you can read that table with these recommendations in mind. There are two nearly collinear relations among the predictors, corresponding to the two smallest dimensions with largest condition indices.\n\nDimension 6 reflects the high correlation between number of cylinders and engine displacement.\nDimension 5 reflects the high correlation between horsepower and weight,\n\nNote that the high variance proportion for year (0.787) on the second component creates no problem and should be ignored because (a) the condition index is low and (b) it shares nothing with other predictors.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#sec-tableplot",
    "href": "09-collinearity-ridge.html#sec-tableplot",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.3 Tableplots",
    "text": "8.3 Tableplots\nThe default tabular display of condition indices and variance proportions from colldiag() is what triggered the comparison to “Where’s Waldo”. It suffers from the fact that the important information — (a) how many Waldos? (b) where are they hiding — is disguised by being embedded in a sea of mostly irrelevant numbers, just as Waldo is hiding in Figure 8.2 in a field of stripy things. The simple option of using a principled fuzz factor helps considerably, but not entirely.\nThe simplified tabular display above can be improved to make the patterns of collinearity more visually apparent and to signify warnings directly to the eyes. A tableplot (Kwan et al., 2009) is a semi-graphic display that presents numerical information in a table using shapes proportional to the value in a cell and other visual attributes (shape type, color fill, and so forth) to encode other information.\nFor collinearity diagnostics, these show:\n\nthe condition indices, using squares whose background color is red for condition indices &gt; 10, brown for values &gt; 5 and green otherwise, reflecting danger, warning and OK respectively. The value of the condition index is encoded within this using a white square whose side is proportional to the value (up to some maximum value, cond.max that fills the cell).\nVariance decomposition proportions are shown by filled circles whose radius is proportional to those values and are filled (by default) with shades ranging from white through pink to red. Rounded values of those diagnostics are printed in the cells.\n\nThe tableplot below (Figure 8.6) encodes all the information from the values of colldiag() printed above. To aid perception, it uses prop.col color breaks such that variance proportions &lt; 0.3 are shaded white. The visual message is that one should attend to collinearities with large condition indices and large variance proportions implicating two or more predictors.\n\n\n\n\n\ntableplot(cd, title = \"Tableplot of cars data\", \n          cond.max = 30 )\n\n\n\n\n\n\nFigure 8.6: Tableplot of condition indices and variance proportions for the cars data. In column 1, the square symbols are scaled relative to a maximum condition index of 30. In the remaining columns, variance proportions (times 100) are shown as circles scaled relative to a maximum of 100.\n\n\n\n\nThe information in Figure 8.6 is essentially the same as the fuzzed version of the printed output from colldiag() shown above; however the graphic encoding of the tableplot makes the pattern of the numbers and their",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#sec-collin-biplots",
    "href": "09-collinearity-ridge.html#sec-collin-biplots",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.4 Collinearity biplots",
    "text": "8.4 Collinearity biplots\nAs we have just seen, the collinearity diagnostics are all functions of the eigenvalues and eigenvectors of the correlation matrix of the predictors in the regression model, or alternatively, the SVD of the \\(\\mathbf{X}\\) matrix in the linear model (excluding the constant). We can use our trusty multivariate juicer the biplot (Section 4.3) to see where the problems lie in a space that relates to observations and variables together.\nA standard biplot (Gabriel, 1971; Gower & Hand, 1996) showing the 2 (or 3) largest dimensions in the data is what we usually want to use. By projecting multivariate data into a low-D space, we can see the main variation in the data an how this related to the variables.\nHowever the standard biplot of the largest dimensions is less useful for visualizing the relations among the predictors that lead to nearly collinear relations. Instead, biplots of the smallest dimensions show these relations directly, and can show other features of the data as well, such as outliers and leverage points. I use prcomp(X, scale.=TRUE) to obtain the PCA of the correlation matrix of the predictors in the cars dataset:\n\ncars.X &lt;- cars |&gt;\n  select(where(is.numeric)) |&gt;\n  select(-mpg) |&gt;\n  tidyr::drop_na()\ncars.pca &lt;- prcomp(cars.X, scale. = TRUE)\ncars.pca\n# Standard deviations (1, .., p=6):\n# [1] 2.070 0.911 0.809 0.367 0.245 0.189\n# \n# Rotation (n x k) = (6 x 6):\n#             PC1     PC2    PC3    PC4     PC5     PC6\n# cylinder -0.454 -0.1869  0.168 -0.659 -0.2711 -0.4725\n# engine   -0.467 -0.1628  0.134 -0.193 -0.0109  0.8364\n# horse    -0.462 -0.0177 -0.123  0.620 -0.6123 -0.1067\n# weight   -0.444 -0.2598  0.278  0.350  0.6860 -0.2539\n# accel     0.330 -0.2098  0.865  0.143 -0.2774  0.0337\n# year      0.237 -0.9092 -0.335  0.025 -0.0624  0.0142\n\nThe standard deviations above are the square roots \\(\\sqrt{\\lambda_j}\\) of the eigenvalues of the correlation matrix; these are returned in the sdev component of the \"prcomp\" object. The eigenvectors are returned in the rotation component. Their orientations are arbitrary and can be reversed for ease of interpretation. Because we are interested in seeing the relative magnitude of variable vectors, we are also free to multiply them all by any constant to make them to zoom in or out, making them more visible in relation to the scores for the cars.\nI use factoextra::fviz_pca_biplot() for the biplot in Figure 8.7 because I want to illustrate identification of noteworthy points with geom_text_repel().\n\ncars.pca$rotation &lt;- -2.5 * cars.pca$rotation    # reflect & scale var vectors\n\nggp &lt;- fviz_pca_biplot(\n  cars.pca,\n  axes = 6:5,\n  geom = \"point\",\n  col.var = \"blue\",\n  labelsize = 5,\n  pointsize = 1.5,\n  arrowsize = 1.5,\n  addEllipses = TRUE,\n  ggtheme = ggplot2::theme_bw(base_size = 14),\n  title = \"Collinearity biplot for cars data\")\n\n# add point labels for outlying points\ndsq &lt;- heplots::Mahalanobis(cars.pca$x[, 6:5])\nscores &lt;- as.data.frame(cars.pca$x[, 6:5])\nscores$name &lt;- rownames(scores)\n\nggp + geom_text_repel(data = scores[dsq &gt; qchisq(0.95, df = 6),],\n                aes(x = PC6,\n                    y = PC5,\n                    label = name),\n                vjust = -0.5,\n                size = 5)\n\n\n\n\n\n\nFigure 8.7: Collinearity biplot of the Cars data, showing the last two dimensions. The projections of the variable vectors on the coordinate axes are proportional to their variance proportions. To reduce graphic clutter, only the most outlying observations in predictor space are identified by case labels. An extreme outlier (case 20) appears in the lower right corner.\n\n\n\n\nAs with the tabular display of variance proportions, Waldo is hiding in the dimensions associated with the smallest eigenvalues (largest condition indices). As well, it turns out that outliers in the predictor space (also high leverage observations) can often be seen as observations far from the centroid in the space of the smallest principal components.\nThe projections of the variable vectors in Figure 8.7 on the Dimension 5 and Dimension 6 axes are proportional to their variance proportions shown above. The relative lengths of these variable vectors can be considered to indicate the extent to which each variable contributes to collinearity for these two near-singular dimensions.\nThus, we see again that Dimension 6 is largely determined by engine size, with a substantial (negative) relation to cylinder. Dimension 5 has its strongest relations to weight and horse.\nMoreover, there is one observation, #20, that stands out as an outlier in predictor space, far from the centroid. It turns out that this vehicle, a Buick Estate wagon, is an early-year (1970) American behemoth. It had an 8-cylinder, 455 cu. in, 225 horse-power engine, and able to go from 0 to 60 mph in 10 sec. (Its MPG is only slightly under-predicted from the regression model, however.) \nWith PCA and the biplot, we are used to looking at the dimensions that account for the most variation, but the answer to Where’s Waldo? is that he is hiding in the smallest data dimensions, just as he does in Figure 8.2 where the weak signals of his stripped shirt, hat and glasses are embedded in a visual field of noise. As we just saw, outliers hide there also, hoping to escape detection. These small dimensions are also implicated in ridge regression as we will see shortly (Section 8.6).",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#sec-remedies",
    "href": "09-collinearity-ridge.html#sec-remedies",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.5 Remedies for collinearity: What can I do?",
    "text": "8.5 Remedies for collinearity: What can I do?\nCollinearity is often a data problem, for which there is no magic cure. Nevertheless there are some general guidelines and useful techniques to address this problem.\n\nPure prediction: If we are only interested in predicting / explaining an outcome, and not the model coefficients or which are “significant”, collinearity can be largely ignored. The fitted values are unaffected by collinearity, even in the case of perfect collinearity as shown in Figure 8.3 (b).\n\nStructural collinearity: Sometimes collinearity results from structural relations among the variables that relate to how they have been defined.\n\nFor example, polynomial terms, like \\(x, x^2, x^3\\) or interaction terms like \\(x_1, x_2, x_1 * x_2\\) are necessarily correlated. A simple cure is to center the predictors at their means, using \\(x - \\bar{x}, (x - \\bar{x})^2, (x - \\bar{x})^3\\) or \\((x_1 - \\bar{x}_1), (x_2 - \\bar{x}_2), (x_1 - \\bar{x}_1) * (x_2 - \\bar{x}_2)\\). Centering removes the spurious ill-conditioning, thus reducing the VIFs. Note that in polynomial models, using y ~ poly(x, 3) to specify a cubic model generates orthogonal (uncorrelated) regressors, whereas in y ~ x + I(x^2) + I(x^3) the terms have built-in correlations.\nWhen some predictors share a common cause, as in GNP or population in time-series or cross-national data, you can reduce collinearity by re-defining predictors to reflect per capita measures. In a related example with sports data, when you have cumulative totals (e.g., runs, hits, homeruns in baseball) for players over years, expressing these measures as per year will reduce the common effect of longevity on these measures.\n\n\n\nModel re-specification:\n\nDrop one or more regressors that have a high VIF, if they are not deemed to be essential to understanding the model. Care must be taken here to not omit variables which should be controlled or accounted for in interpretation.\nReplace highly correlated regressors with less correlated linear combination(s) of them. For example, two related variables, \\(x_1\\) and \\(x_2\\) can be replaced without any loss of information by replacing them with their sum and difference, \\(z_1 = x_1 + x_2\\) and \\(z_2 = x_1 - x_2\\). For instance, in a dataset on fitness, we may have correlated predictors of resting pulse rate and pulse rate while running. Transforming these to average pulse rate and their difference gives new variables which are interpretable and less correlated.\n\n\n\nStatistical remedies:\n\nTransform the predictors \\(\\mathbf{X}\\) to uncorrelated principal component scores \\(\\mathbf{Z} = \\mathbf{X} \\mathbf{V}\\), and regress \\(\\mathbf{y}\\) on \\(\\mathbf{Z}\\). These will have the identical overall model fit without loss of information. A related technique is incomplete principal components regression, where some of the smallest dimensions (those causing collinearity) are omitted from the model. The trade-off is that it may be more difficult to interpret what the model means, but this can be countered with a biplot, showing the projections of the original variables into the reduced space of the principal components.\nUse regularization methods such as ridge regression and lasso, which correct for collinearity by introducing shrinking coefficients towards 0, but inducing a small amount of bias. I illustrate ridge regression below (Section 8.6) using the genridge package for visualization methods.\nUse Bayesian regression. If multicollinearity prevents a regression coefficient from being estimated precisely, Bayesian regression (e.g., Pesaran & Smith (2019)) can reduce collinearity by imposing shrinkage priors; these incorporate prior information to regularize the model, making it less sensitive to correlated predictors and reducing its posterior varioance.\n\n\n\n\nExample 8.2 Centering \n\nTo illustrate the effect of centering a predictor in a polynomial model, I generate data with a perfect quadratic relationship, \\(y = x^2\\) and consider the correlations of \\(y\\) with \\(x\\) and with \\((x - \\bar{x})^2\\). The correlation of \\(y\\) with \\(x\\) is 0.97, while the correlation of \\(y\\) with \\((x - \\bar{x})^2\\) is zero.\n\nx &lt;- 1:20\ny1 &lt;- x^2\ny2 &lt;- (x - mean(x))^2\nXY &lt;- data.frame(x, y1, y2)\n\n(R &lt;- cor(XY))\n#        x    y1    y2\n# x  1.000 0.971 0.000\n# y1 0.971 1.000 0.238\n# y2 0.000 0.238 1.000\n\nThe effect of centering here is remove the linear association in what is a purely quadratic relationship. This can be seen in Figure 8.8 by plotting y1 and y2 against x.\n\nr1 &lt;- R[1, 2]\nr2 &lt;- R[1, 3]\n\ngg1 &lt;-\nggplot(XY, aes(x = x, y = y1)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y~x, \n              linewidth = 2, se = FALSE) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_bw(base_size = 16) +\n  annotate(\"text\", x = 5, y = 350, size = 6,\n           label = paste(\"X Uncentered\\nr =\", round(r1, 3)))\n\ngg2 &lt;-\n  ggplot(XY, aes(x = x, y = y2)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y~x, \n              linewidth = 2, se = FALSE) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_bw(base_size = 16) +\n  annotate(\"text\", x = 5, y = 80, size = 6,\n           label = paste(\"X Centered\\nr =\", round(r2, 3)))\n\ngg1 + gg2         # show plots side-by-side\n\n\n\n\n\n\nFigure 8.8: Centering a predictor removes the nessessary correlation in a quadratic regression. Left: linear relatioship fitting \\(y\\) to the uncentered \\(x\\). Right: fitting to the centered \\((x - \\bar{x}\\).\n\n\n\n\nInterpretation\nCentering of predictors has an added benefit: the fitted coefficients are easier to interpret*, particularly the intercept in this example. In the left panel of Figure 8.8, the fitted blue line has an intercept of -77 and slope of 21. But x = 0 is outside the range of the data and it is hard to understand what -77 means.4\n\nlm(y1 ~ x, data = XY) |&gt; coef()\n# (Intercept)           x \n#         -77          21\n\nContrast this with the coefficients for the model using the centered x and shown by the blue line in the right panel. The slope of the line is clearly zero and the intercept 33.25 is the average value of y.\n\nlm(y2 ~ x, data = XY) |&gt; coef() |&gt; zapsmall()\n# (Intercept)           x \n#        33.3         0.0\n\nThis ease of interpretation is more pronounced in polynomial models. For example, in the quadratic model \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) with \\(x\\) uncentered, the slope coefficient \\(\\beta_1\\) gives the slope of the curve at the value \\(x = 0\\), which may be well-outside the range of data. With a centered predictor \\(x^\\star = x - \\bar{x}\\), the analogous coefficient \\(\\beta_1^\\star\\) in the model \\(y = \\beta_0^\\star + \\beta_1^\\star x^\\star + \\beta_2^\\star (x^\\star)^2\\) is gives the slope at the mean value of \\(x\\).\n\n\n\nExample 8.3 Interactions and response surface models\nCentering of numeric predictors becomes even more important in polynomial models that also include interaction effects, such as response surface models this include all quadratic terms of the predictors and all possible pairwise interactions. The simple notation in an R formula5 is y ~ (x1 + x2 + ...)^2. Spelled out for two predictors with uncentered x1 and x2, this would be:\ny ~ x1 + x2 + I(x1^2) + I(x2^2)+ x1:x2\nHere, the product term is necessarily correlated with each of the predictors involved. This gives more opportunities for collinearity and more places for Waldo to hide.\nTo illustrate, I use the the genridge::Acetylene dataset, which gives results from a manufacturing experiment to study the yield of acetylene in relation to reactor temperature (temp), the ratio of two components and the contact time in the reactor. A naive response surface model might suggest that yield is quadratic in time and there are potential interactions among all pairs of predictors. Without centering time the quadratic effect is fit as a term I(time^2), which simply squares the value of time.6\n\n\ndata(Acetylene, package = \"genridge\")\nacetyl.mod0 &lt;- lm(\n  yield ~ temp + ratio + time + I(time^2) + \n          temp:time + temp:ratio + time:ratio,\n  data=Acetylene)\n\n(acetyl.vif0 &lt;- vif(acetyl.mod0))\n#       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#        383      10555      18080        564       9719       9693 \n# ratio:time \n#        225\n\nThese results are horrible! How much does centering help? I first center all three predictors and then use update() to re-fit the same model using the centered data.\n\nAcetylene.centered &lt;-\n  Acetylene |&gt;\n  mutate(temp = temp - mean(temp),\n         time = time - mean(time),\n         ratio = ratio - mean(ratio))\n\nacetyl.mod1 &lt;- update(acetyl.mod0, \n                      data=Acetylene.centered)\n\n(acetyl.vif1 &lt;- vif(acetyl.mod1))\n#       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#      57.09       1.09      81.57      51.49      44.67      30.69 \n# ratio:time \n#      33.33\n\nThis is far better, although still not great in terms of VIF. But, how much have we improved the situation by the simple act of centering the predictors? The square roots of the ratios of VIFs tell us the impact of centering on the standard errors.\n\nsqrt(acetyl.vif0 / acetyl.vif1)\n#       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#       2.59      98.24      14.89       3.31      14.75      17.77 \n# ratio:time \n#       2.60\n\nFinally, I use poly(time, 2) in the model for the centered data. Because this polynomial term has 2 degree of freedom, car::vif() calculates GVIFs here. The final column gives \\(\\sqrt{\\text{GVIF}^{1/2 \\text{df}}}\\), the remaining effect of collinearity on the standard errors of terms in this model.\n\nacetyl.mod2 &lt;- lm(yield ~ temp + ratio + poly(time, 2) + \n                          temp:time + temp:ratio + time:ratio,\n                  data=Acetylene.centered)\n\nvif(acetyl.mod2, type = \"term\")\n#                  GVIF Df GVIF^(1/(2*Df))\n# temp            57.09  1            7.56\n# ratio            1.09  1            1.05\n# poly(time, 2) 1733.56  2            6.45\n# temp:time       44.67  1            6.68\n# temp:ratio      30.69  1            5.54\n# ratio:time      33.33  1            5.77",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#sec-ridge",
    "href": "09-collinearity-ridge.html#sec-ridge",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.6 Ridge regression",
    "text": "8.6 Ridge regression\nWhen the goals of your analysis are thwarted by the constraints of the assumptions and goals of a model, some trade-offs may help you stay closer to your goals. Ridge regression is a simple instance of a class of techniques designed to obtain more favorable predictions at the expense of some increase in bias in the coefficients, compared to ordinary least squares (OLS) estimation. These methods began as a way of solving collinearity problems in OLS regression with highly correlated predictors (Hoerl & Kennard, 1970). \nMore recently, the ideas of ridge regression spawned a larger class of model selection methods, of which the LASSO method of Tibshirani (1996) and LAR method of Efron et al. (2004) are well-known instances. See, for example, the reviews in Vinod (1978) and McDonald (2009) for details and context omitted here. \nThe case of ridge regression has also been extended to the multivariate case of two or more response variables (Brown & Zidek, 1980; Haitovsky, 1987), but there no implementations of these methods in R. \nAn essential idea behind these methods is that the OLS estimates are constrained in some way, shrinking them, on average, toward zero, to achieve increased predictive accuracy at the expense of some increase in bias. Another common characteristic is that they involve some tuning parameter (\\(k\\)) or criterion to quantify the tradeoff between bias and variance. In many cases, analytical or computationally intensive methods have been developed to choose an optimal value of the tuning parameter, for example using generalized cross validation, bootstrap methods.\nVisualization\nA common means to visualize the effects of shrinkage in these problems is to make what are called univariate ridge trace plots (Section 8.7) showing how the estimated coefficients \\(\\widehat{\\boldsymbol{\\beta}}_k\\) change as the shrinkage criterion \\(k\\) increases. (An example is shown in Figure 8.10 below.) But this only provides a view of bias. It is the wrong graphic form for a multivariate problem where we want to visualize bias in the coefficients \\(\\widehat{\\boldsymbol{\\beta}}_k\\) vs. their precision, as reflected in their estimated variances, \\(\\widehat{\\textsf{Var}} (\\widehat{\\boldsymbol{\\beta}}_k)\\). A more useful graphic plots the confidence ellipses for the coefficients, showing both bias and precision (Section 8.8). Some of the material below borrows from Friendly (2011) and Friendly (2013). \n\n8.6.1 Properties of ridge regression\nTo provide some context, I summarize the properties of ridge regression below, comparing the OLS estimates with their ridge counterparts. To avoid unnecessary details related to the intercept, assume the predictors have been centered at their means and the unit vector is omitted from \\(\\mathbf{X}\\). Further, to avoid scaling issues, we standardize the columns of \\(\\mathbf{X}\\) to unit length, so that \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is a also correlation matrix.\nThe ordinary least squares estimates of coefficients and their estimated variance covariance matrix take the (hopefully now) familiar form\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} = &\n    (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\mathbf{y} \\:\\: ,\\\\\n\\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}) = &\n    \\widehat{\\sigma}_{\\epsilon}^2 (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}.\n\\end{aligned}\n\\tag{8.3}\\]\nAs we saw earlier, one signal of the problem of collinearity is that the determinant \\(\\mathrm{det}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})\\) approaches zero as the predictors become more collinear. The inverse \\((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\) then becomes numerically unstable, or worse—does not exist, if the determinant becomes zero as in the case of exact dependency of one variable on the others. You just can’t divide by zero!\nRidge regression uses a cool matrix trick to avoid this. It simply adds a constant, \\(k\\) to the diagonal elements, thus replacing \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) with \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I}\\) in Equation 8.3. This drives the determinant away from zero as \\(k\\) increases. The ridge regression estimates then become,\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k = &\n    (\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T}\\mathbf{y}  \\\\\n                                    = & \\mathbf{G}_k \\, \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} \\:\\: ,\\\\\n\\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k) = &\n     \\widehat{\\sigma}^2  \\mathbf{G}_k (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{G}_k^\\mathsf{T}\\:\\: ,\n\\end{aligned}\n\\tag{8.4}\\]\nwhere \\(\\mathbf{G}_k = \\left[\\mathbf{I} + k (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\right] ^{-1}\\) is the \\((p \\times p)\\) shrinkage matrix. Thus, as \\(k\\) increases, \\(\\mathbf{G}_k\\) decreases, and drives \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k\\) toward \\(\\mathbf{0}\\) (Hoerl & Kennard, 1970).\nAnother insight, from the shrinkage literature, is that ridge regression can be formulated as least squares regression, minimizing a residual sum of squares, \\(\\text{RSS}(k)\\), which adds a penalty for large coefficients,\n\\[\n\\text{RSS}(k) = (\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\beta}) ^\\mathsf{T}(\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\beta}) + k \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} \\quad\\quad (k \\ge 0)\n\\:\\: ,\n\\tag{8.5}\\] where the penalty restrict the coefficients to some squared length \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} = \\Sigma \\beta_i \\le t(k)\\).\nGeometry The geometry of ridge regession is illustrated in Figure 8.9 for two coefficients \\(\\boldsymbol{\\beta} = (\\beta_1, \\beta_2)\\). The blue circles at the origin, having radii \\(\\sqrt{t_k}\\), show the constraint that the sum of squares of coefficients, \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} = \\beta_1^2 + \\beta_2^2\\) be less than \\(k\\). The red ellipses show contours of the covariance ellipse of \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}\\). As the shrinkage constant \\(k\\) increases, the center of these ellipses travel along the path illustrated toward \\(\\boldsymbol{\\beta} = \\mathbf{0}\\) This path is called the locus of osculation, the path along which circles or ellipses first kiss as they expand, like the pattern of ripples from rocks dropped into a pond (Friendly et al., 2013). \n\n\n\n\n\n\n\nFigure 8.9: Geometric interpretation of ridge regression, using elliptical contours of the \\(\\text{RSS}(k)\\) function. The blue circles at the origin show the constraint that the sum of squares of coefficients, \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta}\\) be less than \\(k\\). The red ellipses show the covariance ellipse of two coefficients \\(\\boldsymbol{\\beta}\\). Ridge regression finds the point \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k\\) where the OLS contours just kiss the constraint region. Source: Friendly et al. (2013).\n\n\n\n\n\nEquation 8.4 is computationally expensive, potentially numerically unstable for small \\(k\\), and it is conceptually opaque, in that it sheds little light on the underlying geometry of the data in the column space of \\(\\mathbf{X}\\).\nOnce again, an alternative formulation, highlighting the role of shrinkage here, can be given in terms of the singular value decomposition (SVD) of \\(\\mathbf{X}\\) (Section 4.3.1), \n\\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\mathsf{T}\n\\]\nwhere \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are respectively \\(n\\times p\\) and \\(p\\times p\\) orthonormal matrices, so that \\(\\mathbf{U}^\\mathsf{T}\\mathbf{U} = \\mathbf{V}^\\mathsf{T}\\mathbf{V} = \\mathbf{I}\\), and \\(\\mathbf{D} = \\mathrm{diag}\\, (d_1, d_2, \\dots d_p)\\) is the diagonal matrix of ordered singular values, with entries \\(d_1 \\ge d_2 \\ge \\cdots \\ge d_p \\ge 0\\).\nBecause \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X} = \\mathbf{V} \\mathbf{D}^2 \\mathbf{V}^\\mathsf{T}\\), the eigenvalues of \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) are given by \\(\\mathbf{D}^2\\) and therefore the eigenvalues of \\(\\mathbf{G}_k\\) can be shown (Hoerl & Kennard, 1970) to be the diagonal elements of\n\\[\n\\mathbf{D}(\\mathbf{D}^2 + k \\mathbf{I} )^{-1} \\mathbf{D} = \\mathrm{diag}\\,  \\left(\\frac{d_i^2}{d_i^2 + k}\\right) \\:\\: .\n\\]\nNoting that the eigenvectors, \\(\\mathbf{V}\\) are the principal component vectors, and that \\(\\mathbf{X} \\mathbf{V} = \\mathbf{U} \\mathbf{D}\\), the ridge estimates can be calculated more simply in terms of \\(\\mathbf{U}\\) and \\(\\mathbf{D}\\) as\n\\[\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k = (\\mathbf{D}^2 + k \\mathbf{I})^{-1} \\mathbf{D} \\mathbf{U}^\\mathsf{T}\\mathbf{y} = \\left( \\frac{d_i}{d_i^2 + k}\\right) \\: \\mathbf{u}_i^\\mathsf{T}\\mathbf{y}, \\quad i=1, \\dots p \\:\\: .\n\\]\nThe terms \\(d^2_i / (d_i^2 + k) \\le 1\\) are thus the factors by which the coordinates of \\(\\mathbf{u}_i^\\mathsf{T}\\mathbf{y}\\) are shrunk with respect to the orthonormal basis for the column space of \\(\\mathbf{X}\\). The small singular values \\(d_i\\) correspond to the directions which ridge regression shrinks the most. These are the directions which contribute most to collinearity, discussed earlier.\nThis analysis also provides an alternative and more intuitive characterization of the ridge tuning constant. By analogy with OLS, where the hat matrix, \\(\\mathbf{H} = \\mathbf{X} (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\) reflects degrees of freedom \\(\\text{df} = \\mathrm{tr} (\\mathbf{H}) = p\\) corresponding to the \\(p\\) parameters, the effective degrees of freedom for ridge regression (Hastie et al., 2009) is\n\\[\n\\begin{aligned}\n\\text{df}_k\n    = & \\text{tr}[\\mathbf{X} (\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T}] \\\\\n    = & \\sum_i^p \\text{df}_k(i) = \\sum_i^p \\left( \\frac{d_i^2}{d_i^2 + k} \\right) \\:\\: .\n\\end{aligned}\n\\tag{8.6}\\]\n\\(\\text{df}_k\\) is a monotone decreasing function of \\(k\\), and hence any set of ridge constants can be specified in terms of equivalent \\(\\text{df}_k\\). Greater shrinkage corresponds to fewer coefficients being estimated.\nThere is a close connection with principal components regression mentioned in Section 8.5. Ridge regression shrinks all dimensions in proportion to \\(\\text{df}_k(i)\\), so the low variance dimensions are shrunk more. Principal components regression discards the low variance dimensions and leaves the high variance dimensions unchanged.\n\n8.6.2 The genridge package\nRidge regression and other shrinkage methods are available in several packages including MASS (the lm.ridge() function), glmnet (Friedman et al., 2025), and penalized (Goeman et al., 2022), but none of these provides insightful graphical displays. glmnet::glmnet() also implements a method for multivariate responses with a `family=“mgaussian”.\nHere, I focus in the genridge package (Friendly, 2024), where the ridge() function is the workhorse and pca.ridge() transforms these results to PCA/SVD space. vif.ridge() calculates VIFs for class \"ridge\" objects and precision() calculates precision and shrinkage measures.\nA variety of plotting functions is available for univariate, bivariate and 3D plots:\n\n\ntraceplot() Traditional univariate ridge trace plots\n\nplot.ridge() Bivariate 2D ridge trace plots, showing the covariance ellipse of the estimated coefficients\n\npairs.ridge() All pairwise bivariate ridge trace plots\n\nplot3d.ridge() 3D ridge trace plots with ellipsoids\n\nbiplot.ridge() ridge trace plots in PCA/SVD space\n\nIn addition, the pca() method for \"ridge\" objects transforms the coefficients and covariance matrices of a ridge object from predictor space to the equivalent, but more interesting space of the PCA of \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) or the SVD of \\(\\mathbf{X}\\). biplot.pcaridge() adds variable vectors to the bivariate plots of coefficients in PCA space",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#sec-ridge-univar",
    "href": "09-collinearity-ridge.html#sec-ridge-univar",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.7 Univariate ridge trace plots",
    "text": "8.7 Univariate ridge trace plots\nThe usual idea to visualize the effects of shrinkage of the coefficients in ridge regression is a simple set of line plots showing how the coefficient of each predictor decreases as the ridge constant increases, as shown below in Figure 8.10 and Figure 8.11.\n\nExample 8.4 A classic example for ridge regression is Longley’s (1967) data, consisting of 7 economic variables, observed yearly from 1947 to 1962 (n=16), in the dataset longley. The goal is to predict Employed from GNP, Unemployed, Armed.Forces, Population, Year, and GNP.deflator.\n\n\n\ndata(longley, package=\"datasets\")\nstr(longley)\n# 'data.frame': 16 obs. of  7 variables:\n#  $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...\n#  $ GNP         : num  234 259 258 285 329 ...\n#  $ Unemployed  : num  236 232 368 335 210 ...\n#  $ Armed.Forces: num  159 146 162 165 310 ...\n#  $ Population  : num  108 109 110 111 112 ...\n#  $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...\n#  $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...\n\nThese data were constructed to illustrate numerical problems in least squares software at the time, and they are (purposely) perverse, in that:\n\nEach variable is a time series so that there is clearly a lack of independence among predictors. Year is at least implicitly correlated with most of the others.\nWorse, there is also some structural collinearity among the variables GNP, Year, GNP.deflator, and Population; for example, GNP.deflator is a multiplicative factor to account for inflation.\n\nWe fit the regression model, and sure enough, there are some extremely large VIFs. The largest, for GNP represents a multiplier of \\(\\sqrt{1788.5} = 42.3\\) on the standard errors.\n\nlongley.lm &lt;- lm(Employed ~ GNP + Unemployed + Armed.Forces + \n                            Population + Year + GNP.deflator, \n                 data=longley)\nvif(longley.lm)\n#          GNP   Unemployed Armed.Forces   Population         Year \n#      1788.51        33.62         3.59       399.15       758.98 \n# GNP.deflator \n#       135.53\n\nShrinkage values can be specified using \\(k\\) (where \\(k = 0\\) corresponds to OLS) or the equivalent degrees of freedom \\(\\text{df}_k\\) (Equation 8.6). (The function uses argument lambda, \\(\\lambda \\equiv k\\) for the shrinkage constant.) Among other quantities, ridge() returns a matrix containing the coefficients for each predictor for each shrinkage value and other quantities.\n\nlambda &lt;- c(0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08)\nlridge &lt;- ridge(Employed ~ GNP + Unemployed + Armed.Forces + \n                           Population + Year + GNP.deflator, \n    data=longley, lambda=lambda)\nprint(lridge, digits = 2)\n# Ridge Coefficients:\n#        GNP     Unemployed  Armed.Forces  Population  Year  \n# 0.000  -3.447  -1.828      -0.696        -0.344       8.432\n# 0.002  -2.114  -1.644      -0.658        -0.713       7.466\n# 0.005  -1.042  -1.491      -0.623        -0.936       6.567\n# 0.010  -0.180  -1.361      -0.588        -1.003       5.656\n# 0.020   0.499  -1.245      -0.548        -0.868       4.626\n# 0.040   0.906  -1.155      -0.504        -0.523       3.577\n# 0.080   1.091  -1.086      -0.458        -0.086       2.642\n#        GNP.deflator\n# 0.000   0.157      \n# 0.002   0.022      \n# 0.005  -0.042      \n# 0.010  -0.026      \n# 0.020   0.098      \n# 0.040   0.321      \n# 0.080   0.570\n\nThe standard univariate plot, given by traceplot(), simply plots the estimated coefficients for each predictor against the shrinkage factor \\(k\\).\n\ntraceplot(lridge, \n          X = \"lambda\",\n          xlab = \"Ridge constant (k)\",\n          xlim = c(-0.02, 0.08), cex.lab=1.25)\n\n\n\n\n\n\nFigure 8.10: Univariate ridge trace plot for the coefficients of predictors of Employment in Longley’s data via ridge regression, with ridge constants k = (0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08). The dotted lines show optimal values for shrinkage by two criteria (HKB, LW).\n\n\n\n\nYou can see that the coefficients for Year and GNP are shrunk considerably. Differences from the \\(\\beta\\) value at \\(k =0\\) represent the bias (smaller \\(\\mid \\beta \\mid\\)) needed to achieve more stable estimates.\nThe dotted lines in Figure 8.10 show choices for the ridge constant by two commonly used criteria to balance bias against precision due to Hoerl et al. (1975) (HKB) and Lawless & Wang (1976) (LW). These values (along with a generalized cross-validation value GCV) are also stored in the “ridge” object as a vector criteria.\n\nlridge$criteria\n#    kHKB     kLW    kGCV \n# 0.00428 0.03230 0.00200\n\n\nThe shrinkage constant \\(k\\) doesn’t have much intrinsic meaning, so it is often easier to interpret the plot when coefficients are plotted against the equivalent degrees of freedom, \\(\\text{df}_k\\). OLS corresponds to \\(\\text{df}_k = 6\\) degrees of freedom in the space of six parameters, and the effect of shrinkage is to decrease the degrees of freedom, as if estimating fewer parameters.7 This more natural scale also makes the changes in coefficient with shrinkage more nearly linear.\n\ntraceplot(lridge, \n          X = \"df\",\n          xlim = c(4, 6.2), cex.lab=1.25)\n\n\n\n\n\n\nFigure 8.11: Univariate ridge trace plot using equivalent degrees of freedom, \\(\\text{df}_k\\) to specify shrinkage. This scale is easier to understand and makes the traces of prarameters more nearly linear.\n\n\n\n\n\n\n8.7.1 What’s not to like?\nThe bigger problem is that these univariate plots are the wrong kind of plot! They show the trends in increased bias (toward smaller \\(\\mid \\beta \\mid\\)) associated with larger \\(k\\), but they do not show the accompanying increase in precision (decrease in variance) achieved by allowing a bit of bias.\nFor that, we need to consider the variances and covariances of the estimated coefficients. The univariate trace plot is simply the wrong graphic form for what is essentially a multivariate problem, where we would like to visualize how both coefficients and their variances change with \\(k\\).",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#sec-ridge-bivar",
    "href": "09-collinearity-ridge.html#sec-ridge-bivar",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.8 Bivariate ridge trace plots",
    "text": "8.8 Bivariate ridge trace plots\nThe bivariate analog of the trace plot suggested by Friendly (2013) plots bivariate confidence ellipses for pairs of coefficients. Their centers, \\((\\widehat{\\beta}_i, \\widehat{\\beta}_j)\\) compared to the OLS values show the bias induced for each coefficient, and also how the change in the ridge estimate for one parameter is related to changes for other parameters.\nThe size and shapes of the covariance ellipses show directly the effect on precision of the estimates as a function of the ridge tuning constant; their size and shape indicate sampling variance and covariance, given by \\(\\widehat{\\text{Var}} (\\boldsymbol{\\widehat{\\beta}}_{ij})\\).\nHere (Figure 8.12), I plot those for GNP against four of the other predictors. The plot() method for \"ridge\" objects plots these ellipses for a pair of variables.\n\nclr &lt;-  c(\"black\", \"red\", \"brown\", \"darkgreen\",\"blue\", \"cyan4\", \"magenta\")\npch &lt;- c(15:18, 7, 9, 12)\nlambdaf &lt;- c(expression(~widehat(beta)^OLS), as.character(lambda[-1]))\n\nfor (i in 2:5) {\n  plot(lridge, variables=c(1,i), \n       radius=0.5, cex.lab=1.5, col=clr, \n       labels=NULL, fill=TRUE, fill.alpha=0.2)\n  text(lridge$coef[1,1], lridge$coef[1,i], \n       expression(~widehat(beta)^OLS), \n       cex=1.5, pos=4, offset=.1)\n  text(lridge$coef[-1,c(1,i)], lambdaf[-1], pos=3, cex=1.3)\n}\n\n\n\n\n\n\nFigure 8.12: Bivariate ridge trace plots for the coefficients of four predictors against the coefficient for GNP in Longley’s data, with k = 0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08. In most cases, the coefficients are driven toward zero, but the bivariate plot also makes clear the reduction in variance, as well as the bivariate path of shrinkage.\n\n\n\n\nAs can be seen, the coefficients for each pair of predictors trace a graceful path generally toward the origin (0,0), and the covariance ellipses get smaller, indicating increased precision. Most often these paths are rather direct, but it takes a peculiar curvilinear route in the case of population and GNP here.\nThe pairs() method for \"ridge\" objects shows all pairwise views in scatterplot matrix form. radius sets the base size of the ellipse-generating circle for the covariance ellipses.\n\npairs(lridge, radius=0.5, diag.cex = 2, \n      fill = TRUE, fill.alpha = 0.1)\n\n\n\n\n\n\nFigure 8.13: Scatterplot matrix of bivariate ridge trace plots. Each panel shows the effect of shrinkage on the covariance ellipse for a given pair of predictors.\n\n\n\n\nMost of the shrinkage paths in Figure 8.13 are regular, but those involving population are curvilinear, reflecting more complex behavior in the ridge method.\n\n8.8.1 Visualizing the bias-variance tradeoff\nThe function precision() calculates a number of measures of the effect of shrinkage of the coefficients in relation to the “size” of the covariance matrix \\(\\boldsymbol{\\mathcal{V}}_k \\equiv \\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k)\\). Larger shrinkage \\(k\\) should lead to a smaller ellipsoid for \\(\\boldsymbol{\\mathcal{V}}_k\\), indicating increased precision.\n\npdat &lt;- precision(lridge) |&gt; print()\n#       lambda   df   det  trace max.eig norm.beta norm.diff\n# 0.000  0.000 6.00 -12.9 18.119  15.419     1.000     0.000\n# 0.002  0.002 5.70 -13.6 11.179   8.693     0.857     0.695\n# 0.005  0.005 5.42 -14.4  6.821   4.606     0.741     1.276\n# 0.010  0.010 5.14 -15.4  4.042   2.181     0.637     1.783\n# 0.020  0.020 4.82 -16.8  2.218   1.025     0.528     2.262\n# 0.040  0.040 4.48 -18.7  1.165   0.581     0.423     2.679\n# 0.080  0.080 4.13 -21.1  0.587   0.260     0.337     3.027\n\nHere, the first three terms described below are (inverse) measures of precision; the last two quantify shrinkage:\n\ndet \\(=\\log{| \\mathcal{V}_k |}\\) is an overall measure of variance of the coefficients. It is the (linearized) volume of the covariance ellipsoid and corresponds conceptually to Wilks’ Lambda criterion.\ntrace \\(=\\text{trace} (\\boldsymbol{\\mathcal{V}}_k)\\) is the sum of the variances and also the sum of the eigenvalues of \\(\\boldsymbol{\\mathcal{V}}_k\\), conceptually similar to Pillai’s trace criterion.\nmax.eig is the largest eigenvalue measure of size, an analog of Roy’s maximum root test.\nnorm.beta \\(= \\left \\Vert \\boldsymbol{\\beta}\\right \\Vert / \\max{\\left \\Vert \\boldsymbol{\\beta}\\right \\Vert}\\) is a summary measure of shrinkage, the normalized root mean square of the estimated coefficients. It starts at 1.0 for \\(k=0\\) and decreases with the penalty for large coefficients.\ndiff.beta is the root mean square of the difference from the OLS estimate \\(\\lVert \\boldsymbol{\\beta}_{\\text{OLS}} - \\boldsymbol{\\beta}_k \\rVert\\). This measure is inversely related to norm.beta.\n\nPlotting shrinkage against a measure of variance gives a direct view of the tradeoff between bias and precision. In Figure 8.14 I use the plot() method for \"precision\" objects. By default, this plots norm.beta against det, joins the points with a smooth spline curve and adds labels for the optimum values according to the different criteria. The points are labelled with their equivalent degrees of freedom.\n\n\n\nShow the codepridge &lt;- precision(lridge)\ncriteria &lt;- lridge$criteria\nnames(criteria) &lt;- sub(\"k\", \"\", names(criteria))\nplot(pridge, criteria = criteria, \n     labels=\"df\", label.prefix=\"df:\",\n     cex.lab = 1.5,\n     xlab ='shrinkage: ||b|| / max(||b||)',\n     ylab='variance: log |Var(b)|'\n     )\nwith(pdat, {\n    text(min(norm.beta), max(det), \n       labels = \"log |Variance| vs. Shrinkage\", \n       cex=1.5, pos=4)\n  })\n\n\n\n\n\n\nFigure 8.14: Precision plot showing the tradeoff between bias and precision. Bias increases as we move away from the OLS solution, but precision increases. The slope of the curve indicates the rate at which variance decreases with shrinkage.\n\n\n\n\n\nYou can see that in this example the HKB and CGV criteria prefer a smaller degree of shrinkage, but achieves only a modest decrease in variance. But variance decreases more sharply thereafter and the LW choice achieves greater precision.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#sec-ridge-low-rank",
    "href": "09-collinearity-ridge.html#sec-ridge-low-rank",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.9 Low-rank views",
    "text": "8.9 Low-rank views\nJust as principal components analysis gives low-dimensional views of a data set, PCA can be useful to understand ridge regression, just as it did for the problem of collinearity.\nThe visCollin::pca() method transforms a \"ridge\" object from parameter space, where the estimated coefficients are \\(\\beta_k\\) with covariance matrices \\(\\boldsymbol{\\mathcal{V}}_k\\), to the principal component space defined by the right singular vectors, \\(\\mathbf{V}\\), of the singular value decomposition \\(\\mathbf{U} \\mathbf{D} \\mathbf{V}^\\mathsf{T}\\) of the scaled predictor matrix, \\(\\mathbf{X}\\). \nIn PCA space the total variance of the predictors remains the same, but it is distributed among the linear combinations that account for successively greatest variance.\n\nplridge &lt;- pca(lridge) |&gt;\n  print()\n# Ridge Coefficients:\n#        dim1     dim2     dim3     dim4     dim5     dim6   \n# 0.000  1.51541  0.37939  1.80131  0.34595  5.97391  6.74225\n# 0.002  1.51537  0.37935  1.80021  0.34308  5.69497  5.06243\n# 0.005  1.51531  0.37928  1.79855  0.33886  5.32221  3.68519\n# 0.010  1.51521  0.37918  1.79579  0.33205  4.79871  2.53553\n# 0.020  1.51500  0.37898  1.79031  0.31922  4.00988  1.56135\n# 0.040  1.51459  0.37858  1.77944  0.29633  3.01774  0.88291\n# 0.080  1.51377  0.37778  1.75810  0.25915  2.01876  0.47238\n\nTraceplot\nThen, a traceplot() of the resulting \"pcaridge\" object shows how the dimensions are affected by shrinkage, shown on the scale of degrees of freedom in Figure 8.15.\n\ntraceplot(plridge, X=\"df\", \n          cex.lab = 1.2, lwd=2)\n\n\n\n\n\n\nFigure 8.15: Ridge traceplot for the longley regression viewed in PCA space. The dimensions are the linear combinations of the predictors which account for greatest variance.\n\n\n\n\nWhat may be surprising at first is that the coefficients for the first 4 components are not shrunk at all. These large dimensions are immune to ridge tuning. Rather, the effect of shrinkage is seen only on the last two dimensions. But those also are the directions that contribute most to collinearity as we saw earlier.\n\npairs() plot\nA pairs() plot gives a dramatic representation bivariate effects of shrinkage in PCA space: the principal components of X are uncorrelated, so the ellipses are all aligned with the coordinate axes. The ellipses largely coincide for dimensions 1 to 4 where there is little effect of shrinkage. You can see them shrink in one direction in the last two columns and rows, and in both for the combination of (dim5, dim6).\n\npairs(plridge)\n\n\n\n\n\n\nFigure 8.16: pairs method: All pairwise bivariate ridge plots shown in PCA space.\n\n\n\n\nIf we focus on the plot of dimensions dim5:dim6, we can see where all the shrinkage action is in this representation. Generally, the predictors that are related to the smallest dimension (6) are shrunk quickly at first.\n\nplot(plridge, variables=5:6, \n     fill = TRUE, fill.alpha=0.15, cex.lab = 1.5)\ntext(plridge$coef[, 5:6], \n     label = lambdaf, \n     cex=1.5, pos=4, offset=.1)\n\n\n\n\n\n\nFigure 8.17: Bivariate ridge trace plot for the smallest two dimensions. The coefficients for these two dimensions head smoothly toward zero and their variance also shrinks.\n\n\n\n\n\n8.9.1 Biplot view\nThe question arises how to relate this view of shrinkage in PCA space to the original predictors. The biplot is again your friend. In effect, it adds vectors showing the contributions of the predictors to a plot like Figure 8.17. You can project variable vectors for the predictor variables into the PCA space of the smallest dimensions, where the shrinkage action mostly occurs to see how the predictor variables relate to these dimensions.\nbiplot.pcaridge() supplements the standard display of the covariance ellipsoids for a ridge regression problem in PCA/SVD space with labeled arrows showing the contributions of the original variables to the dimensions plotted. Recall from Section 4.3 that these reflect the correlations of the variables with the PCA dimensions. The lengths of the arrows reflect the proportion of variance that each predictors shares with the components.\n\nbiplot(plridge, radius=0.5, \n       ref=FALSE, asp=1, \n       var.cex=1.15, cex.lab=1.3, col=clr,\n       fill=TRUE, fill.alpha=0.15, \n       prefix=\"Dimension \")\n# Vector scale factor set to  5.25\ntext(plridge$coef[,5:6], lambdaf, pos=2, cex=1.3)\n\n\n\n\n\n\nFigure 8.18: Biplot view of the ridge trace plot for the smallest two dimensions, where the effects of shrinkage are most apparent.\n\n\n\n\nThe biplot view in Figure 8.18 showing the two smallest dimensions is particularly useful for understanding how the predictors contribute to shrinkage in ridge regression. Here, Year and Population largely contribute to dimension 5; a contrast between (Year, Population) and GNP contributes to dimension 6.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#what-have-we-learned",
    "href": "09-collinearity-ridge.html#what-have-we-learned",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.10 What have we learned?",
    "text": "8.10 What have we learned?\nTODO: Consider replacing this with bullet point take-aways.\nThis chapter has considered the problems in regression models which stem from high correlations among the predictors. We saw that collinearity results in unstable estimates of coefficients with larger uncertainty, often dramatically more so than would be the case if the predictors were uncorrelated. \nCollinearity can be seen as merely a “data problem” which can safely be ignored if we are only interested in prediction. When we want to understand a model, ridge regression can tame the collinearity beast by shrinking the coefficients slightly to gain greater precision in the estimates.\nBeyond these statistical considerations, the methods of this chapter highlight the roles of multivariate thinking and visualization in understanding these phenomena and the methods developed for solving them. Data ellipses and confidence ellipses for coefficients again provide tools for visualizing what is concealed in numerical summaries. A perhaps surprising feature of both collinearity and ridge regression is that the important information usually resides in the smallest PCA dimensions and biplots help again to understand these dimensions.\n\n\n\n\n\nBelsley, D. A. (1991). Conditioning diagnostics: Collinearity and weak data in regression. Wiley.\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley; Sons.\n\n\nBrown, P. J., & Zidek, J. V. (1980). Adaptive multivariate ridge regression. The Annals of Statistics, 8(1), 64–74. http://www.jstor.org/stable/2240743\n\n\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. The Annals of Statistics, 32(2), 407–499.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (Third edition.). SAGE.\n\n\nFox, J., & Monette, G. (1992). Generalized collinearity diagnostics. Journal of the American Statistical Association, 87(417), 178–183.\n\n\nFriedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K., Simon, N., & Yang, J. (2025). Glmnet: Lasso and elastic-net regularized generalized linear models. https://glmnet.stanford.edu\n\n\nFriendly, M. (2011). Generalized ridge trace plots: Visualizing bias and precision with the genridge R package. SCS Seminar.\n\n\nFriendly, M. (2013). The generalized ridge trace plot: Visualizing bias and precision. Journal of Computational and Graphical Statistics, 22(1), 50–68. https://doi.org/10.1080/10618600.2012.681237\n\n\nFriendly, M. (2024). Genridge: Generalized ridge trace plots for ridge regression. https://doi.org/10.32614/CRAN.package.genridge\n\n\nFriendly, M., & Kwan, E. (2009). Where’s Waldo: Visualizing collinearity diagnostics. The American Statistician, 63(1), 56–65. https://doi.org/10.1198/tast.2009.0012\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with application to principal components analysis. Biometrics, 58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGoeman, J., Meijer, R., Chaturvedi, N., & Lueder, M. (2022). Penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the cox model. https://doi.org/10.32614/CRAN.package.penalized\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman & Hall.\n\n\nGraybill, F. A. (1961). An introduction to linear statistical models. McGraw-Hill.\n\n\nHaitovsky, Y. (1987). On multivariate ridge regression. Biometrika, 74(3), 563–570. https://doi.org/10.1093/biomet/74.3.563\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference and prediction (2nd ed.). Springer. http://www-stat.stanford.edu/~tibs/ElemStatLearn/\n\n\nHocking, R. R. (2013). Methods and applications of linear models: Regression and the analysis of variance. Wiley. https://books.google.ca/books?id=iq2J-1iS6HcC\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12, 55–67.\n\n\nHoerl, A. E., Kennard, R. W., & Baldwin, K. F. (1975). Ridge regression: Some simulations. Communications in Statistics, 4(2), 105–123. https://doi.org/10.1080/03610927508827232\n\n\nKwan, E., Lu, I. R. R., & Friendly, M. (2009). Tableplot: A new tool for assessing precise predictions. Zeitschrift für Psychologie / Journal of Psychology, 217(1), 38–48. https://doi.org/10.1027/0044-3409.217.1.38\n\n\nLawless, J. F., & Wang, P. (1976). A simulation study of ridge and other regression estimators. Communications in Statistics, 5, 307–323.\n\n\nLongley, J. W. (1967). An appraisal of least squares programs for the electronic computer from the point of view of the user. Journal of the American Statistical Association, 62, 819–841. https://doi.org/https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896\n\n\nMarquardt, D. W. (1970). Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation. Technometrics, 12, 591–612.\n\n\nMcDonald, G. C. (2009). Ridge regression. Wiley Interdisciplinary Reviews: Computational Statistics, 1(1), 93–100. https://doi.org/10.1002/wics.14\n\n\nPesaran, M. H., & Smith, R. P. (2019). A bayesian analysis of linear regression models with highly collinear regressors. Econometrics and Statistics, 11, 1–21. https://doi.org/10.1016/j.ecosta.2018.10.001\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B: Methodological, 58, 267–288.\n\n\nVinod, H. D. (1978). A survey of ridge regression and related techniques for improvements over ordinary least squares. The Review of Economics and Statistics, 60(1), 121–131. http://www.jstor.org/stable/1924340",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-collinearity-ridge.html#footnotes",
    "href": "09-collinearity-ridge.html#footnotes",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "The “Where’s Waldo” problem has attracted attention in machine learning, AI and computational image analysis circles. One approach uses convolutional neural networks. FindWaldo is one example implemented in Python.↩︎\nThis example is adapted from one by John Fox (2022), Collinearity Diagnostics↩︎\nRecall that in an added-variable plot (Section 6.3), the horizontal axis for predictor \\(x_j\\) is \\(x^\\star_j = x_j  \\,|\\, \\text{others}\\) … TODO complete this thought↩︎\nWell, -77 is just a bit beyond left-most point on the fitted line in this panel of Figure 8.8.↩︎\nThe rsm package provides convenient shortcuts for specifying response surface models. For instance, the SO() shortcut, y ~ SO(x1, x2) automatically generates all linear, interaction, and quadratic terms for the specified variables x1 and x2.↩︎\nThe I() here is the identity function. It is needed because time^2 has a different interpretation in a model formula than in algebra.↩︎\nA related shrinkage method, LASSO (Least Absolute Shrinkage and Selection Operator) (Tibshirani, 1996) uses a penalty term of the sum of absolute values of the coefficients, \\(\\Sigma \\lvert \\beta_i \\rvert \\le t(k)\\) rather than the sum of squares in Equation 8.5. The effect of this change is to shrink some coefficients exactly to zero, effectively eliminating them from the model. This makes LASSO a model selection method, similar in aim to other best subset regression methods. This is widely used in machine learning methods, where interpretation less important than prediction accuracy. ↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html",
    "href": "10-hotelling.html",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "",
    "text": "9.1 \\(T^2\\) as a generalized \\(t\\)-test\nJust as the one- and two- sample univariate \\(t\\)-test is the gateway drug for understanding analysis of variance, so too Hotelling’s \\(T^2\\) test provides an entry point to multivariate analysis of variance. This simple case provides a simple springboard to understanding the collection of methods I call the HE plot framework for visualizing effects in multivariate linear models, which are a main focus of this book.\nThe essential idea (Section 9.1) is that Hotelling’s \\(T^2\\) provides a test of the difference in means between two groups on a collection of variables, \\(\\mathbf{x} = x_1, x_2, \\dots x_p\\) simultaneously, rather than one by one. This has the advantages that it:\nAfter describing it’s features, I use an example of a two-group \\(T^2\\) test to illustrate the basic ideas behind multivariate tests and hypothesis error plots in Section 9.3. Then, we’ll dip our toes into the visual ideas behind discriminant analysis (Section 9.4) and how this compares with biplots based on PCA (Section 9.5.1).\nPackages\nIn this chapter I use the following packages. Load them now.\nHotelling’s \\(T^2\\) (Hotelling, 1931) is an analog of the square of a univariate \\(t\\) statistic, extended to the case of two or more response variables tested together.\nConsider the basic one-sample \\(t\\)-test, where we wish to test the hypothesis that the mean \\(\\bar{x}\\) of a set of \\(N\\) measures on a test of basic math, with standard deviation \\(s\\) does not differ from an assumed mean \\(\\mu_0 = 150\\) for a population. The \\(t\\) statistic for testing \\(\\mathcal{H}_0 : \\mu = \\mu_0\\) against the two-sided alternative, \\(\\mathcal{H}_0 : \\mu \\ne \\mu_0\\) is\n\\[\nt = \\frac{(\\bar{x} - \\mu_0)}{s / \\sqrt{N}} = \\frac{(\\bar{x} - \\mu_0)\\sqrt{N}}{s} \\; .\n\\]\nSquaring this gives:\n\\[\nt^2 = \\frac{N (\\bar{x} - \\mu_0)^2}{s^2} = N (\\bar{x} - \\mu_0)(s^2)^{-1} (\\bar{x} - \\mu_0)\n\\tag{9.1}\\]\nNow consider we also have measures on a test of solving word problems for the same sample. Then, a hypothesis test for the means on basic math (BM) and word problems (WP) is the test of the means of these two variables jointly equal some specified values, say, \\((\\mu_{0,BM}=150,\\; \\mu_{0,WP} =100)\\):\n\\[\n\\mathcal{H}_0 : \\boldsymbol{\\mu} = \\boldsymbol{\\mu_0} =\n  \\begin{pmatrix}\n    \\mu_{0, \\text{BM}} \\\\ \\mu_{0,\\text{WP}}\n  \\end{pmatrix}\n  =\n  \\begin{pmatrix}\n    150 \\\\ 100\n  \\end{pmatrix}\n\\]\nHotelling’s \\(T^2\\) is then the analog of \\(t^2\\), with the variance-covariance matrix \\(\\mathbf{S}\\) of the scores on (BM, WP) replacing the variance of a single score.\n\\[\n\\begin{aligned}\nT^2 &= N (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0)^\\mathsf{T} \\; \\mathbf{S}^{-1} \\; (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0) \\\\\n    &= N D^2_M (\\bar{\\mathbf{x}}, \\boldsymbol{\\mu}_0)\n\\end{aligned}\n\\tag{9.2}\\]\nThis is nothing more than the squared Mahalanobis \\(D^2_M\\) distance between the sample mean vector \\((\\bar{x}_\\text{BM}, \\bar{x}_\\text{WP})^\\mathsf{T}\\) and the hypothesized means \\(\\boldsymbol{\\mu}_0\\), in the metric of \\(\\mathbf{S}\\), as shown in Figure 9.1.\nFigure 9.1: Hotelling’s T^2 statistic as the squared distance between the sample means and hypothesized means relative to the variance-covariance matrix. Source: Author",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html#sec-t2-properties",
    "href": "10-hotelling.html#sec-t2-properties",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.2 \\(T^2\\) properties",
    "text": "9.2 \\(T^2\\) properties\nAside from it’s elegant geometric interpretation, Hotelling’s \\(T^2\\) has simple properties that aid in understanding the extension to more complex multivariate tests.\n\n\nMaximum \\(t^2\\) : Consider constructing a new variable \\(w\\) as a linear combination of the scores in a matrix \\(\\mathbf{X} = [ \\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}]\\) with weights \\(\\mathbf{a}\\),\n\n\\[\nw = a_1 \\mathbf{x_1} + a_2 \\mathbf{x_2} + \\dots + a_p \\mathbf{x_p} = \\mathbf{X} \\mathbf{a}\n\\]\nHotelling’s \\(T^2\\) is then the maximum value of a univariate \\(t^2 (\\mathbf{a})\\) over all possible choices of the weights in \\(\\mathbf{a}\\). In this way, Hotellings test reduces a multivariate problem to a univariate one. The weights in \\(\\mathbf{a}\\) show how the variables contribute to significance: larger absolute values \\(\\vert a_j \\vert\\) variables have greater impact.\n\n\nEigenvalue : Hotelling showed that \\(T^2\\) is the one non-zero eigenvalue (latent root) \\(\\lambda\\) of the matrix \\(\\mathbf{Q}_H = N (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0)^\\mathsf{T}  (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0)\\) relative to \\(\\mathbf{Q}_E = \\mathbf{S}\\). The “relative to” means that the eigenvalue problem is expressed as finding \\(\\lambda\\) and \\(\\mathbf{a}\\) that solve the equation,\n\n\\[\n(\\mathbf{Q}_H - \\lambda \\mathbf{Q}_E) \\; \\mathbf{a} \\; = \\; 0 \\:\\: .\n\\tag{9.3}\\]\nIn more complex MANOVA problems, there are more than one non-zero latent roots, \\(\\lambda_1, \\lambda_2, \\dots \\lambda_s\\), and test statistics (Wilks’ \\(\\Lambda\\), Pillai and Hotelling-Lawley trace criteria, Roy’s maximum root test) are functions of these as shown in Section 10.3.\n\nEigenvector : The corresponding eigenvector is \\(\\mathbf{a} = \\mathbf{S}^{-1} (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0)\\). These are the (raw) discriminant coefficients, giving the relative contribution of each variable to \\(T^2\\).\nCritical values : For a single response, the square of a \\(t\\) statistic with \\(N-1\\) degrees of freedom is an \\(F (1, N-1)\\) statistic. But we chose \\(\\mathbf{a}\\) to give the maximum \\(t^2 (\\mathbf{a})\\); this can be taken into account with a transformation of \\(T^2\\) to give an exact \\(F\\) test with the correct sampling distribution:1\n\n\\[\nF^* = \\frac{N - p}{p (N-1)} T^2 \\; \\sim \\; F (p, N - p) \\:\\: .\n\\tag{9.4}\\]\n\n\nInvariance under linear transformation : Just as a univariate \\(t\\)-test is unchanged if we apply a linear transformation to the variable, \\(x \\rightarrow a x + b\\), so too \\(T^2\\) is invariant under all linear (affine) transformations, multiplying \\(\\mathbf{x}\\) by a matrix \\(\\mathbf{A}\\) and adding a vector \\(\\mathbf{b}\\),\n\n\\[\n\\mathbf{x}_{p \\times 1} \\rightarrow \\mathbf{A}_{p \\times p} \\; \\mathbf{x} + \\mathbf{b}_{p \\times 1} \\:\\: .\n\\]\nSo, with the Penguins data, you would get the same results if you convert flipper lengths, bill length and depth, from millimeters to centimeters or inches. The same is true for all MANOVA tests.\n\n\nTwo-sample tests : With minor variations in notation, everything above applies to the more usual test of equality of multivariate means in a two sample test of \\(\\mathcal{H}_0 : \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\).\n\n\\[\nT^2 = N (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)^\\mathsf{T} \\; \\mathbf{S}_p^{-1} \\; (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2) \\; ,\n\\tag{9.5}\\]\nwhere \\(\\mathbf{S}_p\\) is the pooled within-sample variance covariance matrix.\n\n9.2.1 Example: Mathscore data\nAs a running example, I’ll use the data set heplots::mathscore, which gives (fictitious) scores on a test of basic math skills (BM) and solving word problems (WP) for two groups of \\(N=6\\) students in an algebra course. Each group was taught by a different instructors, and the questions are: (a) Do the groups differ on both variables, considered jointly? (b) If so, how do they differ?\nThe null hypothesis is that the means are equal for both variables, \\(\\mathcal{H}_0: \\mu_{\\text{BM}} = \\mu_{\\text{WP}}\\), or, equivalently, that the difference between means (a contrast!) is zero: \\(\\mathcal{H}_0: \\mu_{\\text{BM}} - \\mu_{\\text{WP}} = \\mathbf{0}\\).\n\ndata(mathscore, package = \"heplots\")\nstr(mathscore)\n# 'data.frame': 12 obs. of  3 variables:\n#  $ group: Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 2 2 2 2 ...\n#  $ BM   : int  190 170 180 200 150 180 160 190 150 160 ...\n#  $ WP   : int  90 80 80 120 60 70 120 150 90 130 ...\n\nYou can carry out the test that the means for both variables are jointly equal across groups using either Hotelling::hotelling.test() (Curran & Hersh, 2021) or car::Anova(), but the latter is more generally useful because it applies to any multivariate linear model.\n\nhotelling.test(cbind(BM, WP) ~ group, data=mathscore) |&gt; print()\n# Test stat:  64.174 \n# Numerator df:  2 \n# Denominator df:  9 \n# P-value:  0.0001213\n\nmath.mod &lt;- lm(cbind(BM, WP) ~ group, data=mathscore)\nAnova(math.mod)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#       Df test stat approx F num Df den Df  Pr(&gt;F)    \n# group  1     0.865     28.9      2      9 0.00012 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWhat’s wrong with just doing the two \\(t\\)-tests (or equivalent \\(F\\)-test with lm())? Let’s see.\n\nAnova(mod1 &lt;- lm(BM ~ group, data=mathscore))\n# Anova Table (Type II tests)\n# \n# Response: BM\n#           Sum Sq Df F value Pr(&gt;F)  \n# group       1302  1    4.24  0.066 .\n# Residuals   3071 10                 \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(mod2 &lt;- lm(WP ~ group, data=mathscore))\n# Anova Table (Type II tests)\n# \n# Response: WP\n#           Sum Sq Df F value Pr(&gt;F)   \n# group       4408  1    10.4  0.009 **\n# Residuals   4217 10                  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFrom this, we might conclude that the two groups do not differ significantly on Basic Math but strongly differ on Word problems. But this interpretation would be wrong! The two univariate tests do not take the correlation among the mean differences into account. In this case, the means are negatively related across the groups, which heightens the significance test, as we will shortly see (Figure 9.4).\nIf you want to just extract the \\(t\\)-tests, here’s a handy trick using broom::tidy.mlm(), which summarizes the test statistics for each response and each term in a MLM. The mean difference shown below is that for group 2 - group 1.\n\ntidy(math.mod) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  select(-term) |&gt;\n  rename(Mean_diff = estimate,\n         t = statistic) |&gt;\n  mutate(signif = noquote(gtools::stars.pval(p.value)))\n# # A tibble: 2 × 6\n#   response Mean_diff std.error     t p.value signif   \n#   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;noquote&gt;\n# 1 BM           -20.8      10.1 -2.06 0.0665  .        \n# 2 WP            38.3      11.9  3.23 0.00897 **\n\nTo see the differences between the groups on both variables together, we draw their data (68%) ellipses, using heplots::covEllipses(). (Setting pooled=FALSE here omits drawing the the ellipse for the pooled covariance matrix \\(\\mathbf{S}_p\\).)\n\n\ncolors &lt;- c(\"darkgreen\", \"blue\")\ncovEllipses(mathscore[,c(\"BM\", \"WP\")], mathscore$group,\n            pooled = FALSE, \n            col = colors,\n            fill = TRUE, \n            fill.alpha = 0.05,\n            cex = 2, cex.lab = 1.5,\n            asp = 1,\n            xlab=\"Basic math\", ylab=\"Word problems\")\n# plot points\npch &lt;- ifelse(mathscore$group==1, 15, 16)\ncol &lt;- ifelse(mathscore$group==1, colors[1], colors[2])\npoints(mathscore[,2:3], pch=pch, col=col, cex=1.25)\n\n\n\n\n\n\nFigure 9.2: Data ellipses for the mathscore data, enclosing approximately 68% of the observations in each group.\n\n\n\n\nIn Figure 9.2 you can see that:\n\nGroup 1 &gt; Group 2 on Basic Math, but worse on Word Problems\nGroup 2 &gt; Group 1 on Word Problems, but worse on Basic Math\nWithin each group, those who do better on Basic Math also do better on Word Problems\n\nWe can also see why the univariate test, at least for Basic math is non-significant: the scores for the two groups overlap considerably on the horizontal axis. They are slightly better separated along the vertical axis for word problems. The plot also reveals why Hotelling’s \\(T^2\\) reveals such a strongly significant result: the two groups are very widely separated along an approximately 45\\(^o\\) line between them.\nA relatively simple interpretation is that the groups don’t really differ in overall math ability, but perhaps the instructor in Group 1 put more focus on basic math skills, while the instructor for Group 2 placed greater emphasis on solving word problems.\nIn Hotelling’s \\(T^2\\), the “size” of the difference between the means (labeled “1” and “2”) is assessed relative to the pooled within-group covariance matrix \\(\\mathbf{S}_p\\), which is just a size-weighted average of the two within-sample matrices, \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\),\n\\[\n\\mathbf{S}_p = [ (n_1 - 1) \\mathbf{S}_1 + (n_2 - 1) \\mathbf{S}_2 ] / (n_1 + n_2 - 2) \\:\\: .\n\\]\nVisually, imagine sliding the the separate data ellipses to the grand mean, \\((\\bar{x}_{\\text{BM}}, \\bar{x}_{\\text{WP}})\\) and finding their combined data ellipse. This is just the data ellipse of the sample of deviations of the scores from their group means, or that of the residuals from the model lm(cbind(BM, WP) ~ group, data=mathscore)\nTo see this, we plot \\(\\mathbf{S}_1\\), \\(\\mathbf{S}_2\\) and \\(\\mathbf{S}_p\\) together,\n\ncovEllipses(mathscore[,c(\"BM\", \"WP\")], mathscore$group,\n            col = c(colors, \"red\"),\n            fill = c(FALSE, FALSE, TRUE), \n            fill.alpha = 0.3,\n            cex = 2, cex.lab = 1.5,\n            asp = 1,\n            xlab=\"Basic math\", ylab=\"Word problems\")\n\n\n\n\n\n\nFigure 9.3: Data ellipses and the pooled covariance matrix mathscore data.\n\n\n\n\nOne of the assumptions of the \\(T^2\\) test (and of MANOVA) is that the within-group variance covariance matrices, \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\), are the same. In Figure 9.3, you can see how the shapes of \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\) are very similar, differing in that the variance of word Problems is slightly greater for group 2. In Chapter 12 we take of the topic of visualizing tests of this assumption, based on Box’s \\(M\\)-test.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html#sec-t2-heplot",
    "href": "10-hotelling.html#sec-t2-heplot",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.3 HE plot and discriminant axis",
    "text": "9.3 HE plot and discriminant axis\nAs we describe in detail in Chapter 11, all the information relevant to the \\(T^2\\) test and MANOVA can be captured in the remarkably simple Hypothesis Error plot, which shows the relative size of two data ellipses,\n\n\n\\(\\mathbf{H}\\): the data ellipse of the fitted values, which are just the group means on the two variables, \\(\\bar{\\mathbf{x}}\\), corresponding to \\(\\mathbf{Q}_H\\) in Equation 9.3. In case of \\(T^2\\), the \\(\\mathbf{H}\\) matrix is of rank 1, so the “ellipse” plots as a line.\n\n\n# calculate H directly\nfit &lt;- fitted(math.mod)\nxbar &lt;- colMeans(mathscore[,2:3])\nN &lt;- nrow(mathscore)\ncrossprod(fit) - N * outer(xbar, xbar)\n#       BM    WP\n# BM  1302 -2396\n# WP -2396  4408\n\n# same as: SSP for group effect from Anova\nmath.aov &lt;- Anova(math.mod)\n(H &lt;- math.aov$SSP)\n# $group\n#       BM    WP\n# BM  1302 -2396\n# WP -2396  4408\n\n\n\n\\(\\mathbf{E}\\): the data ellipse of the residuals, the deviations of the scores from the group means, \\(\\mathbf{x} - \\bar{\\mathbf{x}}\\), corresponding to \\(\\mathbf{Q}_E\\).\n\n\n# calculate E directly\nresids &lt;- residuals(math.mod)\ncrossprod(resids)\n#      BM   WP\n# BM 3071 2808\n# WP 2808 4217\n\n# same as: SSPE from Anova\n(E &lt;- math.aov$SSPE)\n#      BM   WP\n# BM 3071 2808\n# WP 2808 4217\n\n\n9.3.1 The heplot() function\nThe function heplots::heplot() takes the model object, extracts the \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) matrices (from summary(Anova(math.mod))) and plots them as their data ellipses. There are many options to control the details. See help(heplot).\n\nheplot(math.mod, \n       fill=TRUE, lwd = 3,\n       asp = 1,\n       cex=2, cex.lab=1.8,\n       xlab=\"Basic math\", ylab=\"Word problems\")\n\n\n\n\n\n\nFigure 9.4: Hypothesis error plot of the mathscore data. The line through the group means is the H ellipse, which plots as a line here. The red ellipse labeled ‘Error’ represents the pooled within-group covariance matrix.\n\n\n\n\nBut the HE plot offers more:\n\nA visual test of significance: the \\(\\mathbf{H}\\) ellipse is scaled so that it projects anywhere outside the \\(\\mathbf{E}\\) ellipse, if and only if the test is significant at a given \\(\\alpha\\) level (\\(\\alpha = 0.05\\) by default)\nThe \\(\\mathbf{H}\\) ellipse, which appears as a line, goes through the means of the two groups. This is also the discriminant axis, the direction in the space of the variables which maximally discriminates between the groups. That is, if we project the data points onto this line, we get the linear combination \\(w\\) which has the maximum possible univariate \\(t^2\\).\n\n9.3.2 HE plot, data ellipses and discriminant axis\nYou can see how the HE plot relates to the plots of the separate data ellipses by overlaying them in a single figure. In Figure 9.5 below I also plot the scores on the discriminant axis, by using this small function project_on() to find the orthogonal projection of a point \\(\\mathbf{a}\\) on the line joining two points, \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\), which in math is \\(\\mathbf{p}_1 + \\frac{\\mathbf{d}^\\mathsf{T} (\\mathbf{a} - \\mathbf{p}_1)} {\\mathbf{d}^\\mathsf{T} \\mathbf{d}}\\), letting \\(\\mathbf{d} = \\mathbf{p}_1 - \\mathbf{p}_2\\).\n\ndot &lt;- function(x, y) sum(x*y)       # dot product of two vectors\nproject_on &lt;- function(a, p1, p2) {\n  a &lt;- as.numeric(a)\n  p1 &lt;- as.numeric(p1)\n  p2 &lt;- as.numeric(p2)\n  t &lt;- dot(p2-p1, a-p1) / dot(p2-p1, p2-p1)\n  C &lt;- p1 + t*(p2-p1)\n  C\n}\n\nThen, I run the same code as before (for Figure 9.3) to plot the data ellipses, and follow this with a call to heplot() using the option add=TRUE which adds to an existing plot. Following this, I find the group means and draw lines projecting the points on the line between them.\n\ncovEllipses(mathscore[,c(\"BM\", \"WP\")], mathscore$group,\n            pooled=FALSE, \n            col = colors,\n            cex=2, cex.lab=1.5,\n            asp=1, \n            xlab=\"Basic math\", ylab=\"Word problems\"\n            )\npch &lt;- ifelse(mathscore$group==1, 15, 16)\ncol &lt;- ifelse(mathscore$group==1, \"red\", \"blue\")\npoints(mathscore[,2:3], pch=pch, col=col, cex=1.25)\n\n# overlay with HEplot (add = TRUE)\nheplot(math.mod, \n       fill=TRUE, \n       cex=2, cex.lab=1.8, \n       fill.alpha=0.2, lwd=c(1,3),\n       add = TRUE, \n       error.ellipse=TRUE)\n\n# find group means\nmeans &lt;- mathscore |&gt;\n  group_by(group) |&gt;\n  summarize(BM = mean(BM), WP = mean(WP))\n\nfor(i in 1:nrow(mathscore)) {\n  gp &lt;- mathscore$group[i]\n  pt &lt;- project_on( mathscore[i, 2:3], means[1, 2:3], means[2, 2:3]) \n  segments(mathscore[i, \"BM\"], mathscore[i, \"WP\"], pt[1], pt[2], lwd = 1.2)\n}\n\n\n\n\n\n\nFigure 9.5: HE plot overlaid on top of the within-group data ellipses, with lines showing the projection of each point on the line joining the means, which is the discriminant axis.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html#sec-t2-discrim",
    "href": "10-hotelling.html#sec-t2-discrim",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.4 Discriminant analysis",
    "text": "9.4 Discriminant analysis\nDiscriminant analysis for two-group designs or for one-way MANOVA essentially turns the problem around: Instead of asking whether the mean vectors for two or more groups are equal, discriminant analysis tries to find the linear combination \\(w\\) of the response variables that has the greatest separation among the groups, allowing cases to be best classified.2\n\nFor the mathscore data, you can perform the discriminant analysis as follows, using the MASS function lda():\n\n(math.lda &lt;- MASS::lda(group ~ ., data=mathscore))\n# Call:\n# lda(group ~ ., data = mathscore)\n# \n# Prior probabilities of groups:\n#   1   2 \n# 0.5 0.5 \n# \n# Group means:\n#    BM    WP\n# 1 178  83.3\n# 2 158 121.7\n# \n# Coefficients of linear discriminants:\n#        LD1\n# BM -0.0835\n# WP  0.0753\n\nThe coefficients give \\(w = -0.084 \\;\\text{BM} + 0.075 \\;\\text{WP}\\). This is exactly the direction given by the line for the \\(\\mathbf{H}\\) ellipse in Figure 9.5.\nTo round this out, we can calculate the discriminant scores by multiplying the matrix \\(\\mathbf{X}\\) by the vector \\(\\mathbf{a} = \\mathbf{S}^{-1} (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)\\) of the discriminant weights, given by the scaling component of the result of lda(). These were shown in Figure 9.5 as the projections of the data points on the line joining the group means,\n\nmath.lda$scaling\n#        LD1\n# BM -0.0835\n# WP  0.0753\n\nLD1_scores &lt;- as.matrix(mathscore[, 2:3]) %*% math.lda$scaling\nscores &lt;- cbind(group = mathscore$group,\n                LD1 = LD1_scores) |&gt;\n  as.data.frame()\nscores |&gt;\n  group_by(group) |&gt;\n  slice(1:3)\n# # A tibble: 6 × 2\n# # Groups:   group [2]\n#   group   LD1\n#   &lt;dbl&gt; &lt;dbl&gt;\n# 1     1 -9.09\n# 2     1 -8.17\n# 3     1 -9.01\n# 4     2 -4.33\n# 5     2 -4.58\n# 6     2 -5.75\n\nThen a \\(t\\)-test on these scores gives the same value as Hotelling’s \\(T\\); it is accessed via the statistic component of t.test()\n\nt &lt;- t.test(LD1 ~ group, data=scores)$statistic\nc(t, T2 = t^2)\n#     t  T2.t \n# -8.01 64.17\n\nFinally, it is instructive to compare violin plots for the three measures, BM, WP and LD1. To do this with ggplot2 requires reshaping the data from wide to long format so the plots can be faceted.\n\nscores &lt;- mathscore |&gt;\n  bind_cols(LD1 = scores[, \"LD1\"]) \n\nscores |&gt;\n  tidyr::gather(key = \"measure\", value =\"Score\", BM:LD1) |&gt;\n  mutate(measure = factor(measure, levels = c(\"BM\", \"WP\", \"LD1\"))) |&gt;\n  ggplot(aes(x = group, y = Score, color = group, fill = group)) +\n    geom_violin(alpha = 0.2) +\n    geom_jitter(width = .2, size = 2) +\n    facet_wrap( ~ measure, scales = \"free\", labeller = label_both) +\n    scale_fill_manual(values = c(\"darkgreen\", \"blue\")) +\n    scale_color_manual(values = c(\"darkgreen\", \"blue\")) +\n    theme_bw(base_size = 14) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 9.6: Violin plots comparing group 1 and 2 for the two observed measures and the linear discriminant score.\n\n\n\n\nYou can readily see how well the groups are separated on the discriminant axes, relative to the two individual variables—the groups are much further apart in on the discriminant axis.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html#sec-t2-more-variables",
    "href": "10-hotelling.html#sec-t2-more-variables",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.5 More variables",
    "text": "9.5 More variables\nThe mathscore data gave a simple example with two outcomes to explain the essential ideas behind Hotelling’s \\(T^2\\) and multivariate tests. Multivariate methods become increasingly useful as the number of response variables increases because it is harder to show them all together and see how they relate to differences between groups.\n\nExample 9.1 Real and fake banknotes\nA classic example is the dataset mclust::banknote, containing six size measures made on 100 genuine and 100 counterfeit old-Swiss 1000-franc bank notes (Flury & Riedwyl, 1988). The goal is to see how well the real and fake banknotes can be distinguished. The measures are the Length and Diagonal lengths of a banknote and the Left, Right, Top and Bottom edge margins in mm.\nBefore considering hypothesis tests, let’s look at some exploratory graphics. Figure 9.7 shows univariate violin and boxplots of each of the measures. To make this plot, faceted by measure, I first reshape the data from wide to long and make measure a factor with levels in the order of the variables in the data set.\n\n\ndata(banknote, package= \"mclust\")\nbanknote |&gt;\n  tidyr::gather(key = \"measure\", \n                value = \"Size\", \n                Length:Diagonal) |&gt; \n  mutate(measure = factor(measure, \n                          levels = c(names(banknote)[-1]))) |&gt; \n\n  ggplot(aes(x = Status, y = Size, color = Status)) +\n  geom_violin(aes(fill = Status), alpha = 0.2) +           # (1)\n  geom_jitter(width = .2, size = 1.2) +                    # (2)\n  geom_boxplot(width = 0.25,                               # (3)\n               linewidth = 1.1, \n               color = \"black\", \n               alpha = 0.5) +\n  labs(y = \"Size (mm)\") +\n  facet_wrap( ~ measure, scales = \"free\", labeller = label_both) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nFigure 9.7: Overlaid violin and boxplots of the banknote variables. The violin plots give a sense of the shapes of the distributions, while the boxplots highlight the center and spread.\n\n\n\n\nA quick glance at Figure 9.7 shows that the counterfeit and genuine bills differ in their means on most of the measures, with the counterfeit ones slightly larger on Left, Right, Bottom and Top margins. But univariate plots don’t give an overall sense of how these variables are related to one another.\n\n\n\n\n\n\n\nGraph craft: Layers and transparency\n\n\n\nFigure 9.7 is somewhat complex, so it is useful to understand the steps needed to make this figure show what I wanted. The plot in each panel contains three layers noted by comments in the code:\n\nthe violin plot based on a density estimate, showing the shape of each distribution;\nthe data points, but they are jittered horizontally using geom_jiter() because otherwise they would all overlap on the X axis;\nthe boxplot, showing the center (median) and spread (IQR) of each distribution.\n\nIn composing graphs with layers, plotting order matters, and also does the alpha transparency, because each layer adds data ink on top of earlier ones. I plotted these in the order shown because I wanted the violin plot to provide the background, and the boxplot to show a simple univariate summary, not obscured by the other layers. The alpha values allow the data ink to be blended for each layer, and in this case, alpha = 0.5 for the boxplot let the earlier layers show through.\n\n\n\n9.5.1 Biplots\nMultivariate relations among these six variables could be explored in data space using scatterplots or other methods, but I turn to my trusty multivariate juicer, a PCA and biplot, to give a 2D summary. This differs from the analysis above in that it does not take the Status grouping into account.\nTwo dimensions account for 70% of the total variance of all the banknotes, while three would give 85%.\n\nbanknote.pca &lt;- prcomp(banknote[, -1], scale = TRUE)\nsummary(banknote.pca)\n# Importance of components:\n#                          PC1   PC2   PC3   PC4    PC5    PC6\n# Standard deviation     1.716 1.131 0.932 0.671 0.5183 0.4346\n# Proportion of Variance 0.491 0.213 0.145 0.075 0.0448 0.0315\n# Cumulative Proportion  0.491 0.704 0.849 0.924 0.9685 1.0000\n\nThe biplot in Figure 9.8 gives a nicely coherent overview, at least in two dimensions. The first component shows the positive correlations among the measures of the margins, where the counterfeit bills are larger than the real ones and a negative correlation of the Diagonal with the other measures. The length of bills only distinguishes the types of banknotes on the second dimension.\n\nbanknote.pca &lt;- ggbiplot::reflect(banknote.pca)\nggbiplot(banknote.pca,\n   obs.scale = 1, var.scale = 1,\n   groups = banknote$Status,\n   ellipse = TRUE, \n   ellipse.level = 0.5, \n   ellipse.alpha = 0.1, \n   ellipse.linewidth = 0,\n   varname.size = 4,\n   varname.color = \"black\") +\n  labs(fill = \"Status\", \n       color = \"Status\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = 'top')\n\n\n\n\n\n\nFigure 9.8: Biplot of the banknote variables, showing how the size measurements are related to each other. The points and data ellipses for the component scores are colored by Status, showing how the counterfeit and genuine bills are distinguished by these measures.\n\n\n\n\n\n9.5.2 Testing mean differences\nAs noted above, Hotelling’s \\(T^2\\) is equivalent to a one-way MANOVA, fitting the size measures to the Status of the banknotes. Anova() reports only the \\(F\\)-statistic based on Pillai’s trace criterion.\n\nbanknote.mlm &lt;- lm(cbind(Length, Left, Right, Bottom, Top, Diagonal) ~ Status,\n                    data = banknote)\nAnova(banknote.mlm)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#        Df test stat approx F num Df den Df Pr(&gt;F)    \n# Status  1     0.924      392      6    193 &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nYou can see all the multivariate test statistics with the summary() method for \"Anova.mlm\" objects. With two groups, and hence a 1 df test, these all translate into identical \\(F\\)-statistics.\n\nsummary(Anova(banknote.mlm)) |&gt; \n  print(SSP = FALSE)\n# \n# Type II MANOVA Tests:\n# \n# ------------------------------------------\n#  \n# Term: Status \n# \n# Multivariate Tests: Status\n#                  Df test stat approx F num Df den Df Pr(&gt;F)    \n# Pillai            1      0.92      392      6    193 &lt;2e-16 ***\n# Wilks             1      0.08      392      6    193 &lt;2e-16 ***\n# Hotelling-Lawley  1     12.18      392      6    193 &lt;2e-16 ***\n# Roy               1     12.18      392      6    193 &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIf you wish, you can extract the univariate \\(t\\)-tests or equivalent \\(F = t^2\\) statistics from the \"mlm\" object using broom::tidy.mlm().3 What is given as the estimate is the difference in the mean for the genuine banknotes relative to the counterfeit ones.\n\nbroom::tidy(banknote.mlm) |&gt; \n  filter(term != \"(Intercept)\") |&gt;\n  select(-term) |&gt;\n  rename(t = statistic) |&gt;\n  mutate(F = t^2) |&gt;\n  relocate(F, .after = t)\n# # A tibble: 6 × 6\n#   response estimate std.error      t      F  p.value\n#   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n# 1 Length      0.146    0.0524   2.79   7.77 5.82e- 3\n# 2 Left       -0.357    0.0445  -8.03  64.5  8.50e-14\n# 3 Right      -0.473    0.0464 -10.2  104.   6.84e-20\n# 4 Bottom     -2.22     0.130  -17.1  292.   7.78e-41\n# 5 Top        -0.965    0.0909 -10.6  113.   3.85e-21\n# 6 Diagonal    2.07     0.0715  28.9  836.   5.35e-73\n\nThe individual \\(F_{(1, 198)}\\) statistics can be compared to the \\(F_{(6, 193)} = 392\\) value for the overall multivariate test. While all of the individual tests are highly significant, the average of the univariate \\(F\\)s is only 236. The multivariate test gains power by taking the correlations of the size measures into account.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html#variance-accounted-for-eta2",
    "href": "10-hotelling.html#variance-accounted-for-eta2",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.6 Variance accounted for: \\(\\eta^2\\)\n",
    "text": "9.6 Variance accounted for: \\(\\eta^2\\)\n\nIn a univariate multiple regression model, the coefficient of determination \\(R^2 = \\text{SS}_H / \\text{SS}_\\text{Total}\\) gives the proportion of variance accounted for by hypothesized terms in \\(H\\) relative to the total variance. An analog for ANOVA-type models with categorical, group factors as predictors is \\(\\eta^2\\) (Pearson, 1903), defined as:\n\\[\n\\eta^2 = \\frac{\\text{SS}_\\text{Between groups}}{\\text{SS}_\\text{Total}} \\:\\: .\n\\]\nFor multivariate response models, the generalization of \\(\\eta^2\\) uses multivariate analogs of these sums of squares, \\(\\mathbf{Q}_H\\) and \\(\\mathbf{Q}_T = \\mathbf{Q}_H + \\mathbf{Q}_E\\). In larger designs, there are different calculations for a single measure corresponding to the various test statistics (Wilks’ \\(\\Lambda\\), etc.), as described in Chapter 10.\nLet’s calculate the \\(\\eta^2\\) for the multivariate model banknote.mlm with Status as the only predictor. The function heplots::etasq() gives \\(\\eta^2 = 0.92\\), or 92% of the total variance.4\n\nheplots::etasq(banknote.mlm)\n#        eta^2\n# Status 0.924\n\nThis can be compared to the principal components analysis and the biplot in Figure 9.8, where two components (less favorably) accounted for 70% of total variance and it took four PCA dimensions to account for over 90%.\nThe goals of PCA and MANOVA are different, of course, but they are both concerned with accounting for variance of multivariate data. We will meet another multivariate juicer, canonical discriminant analysis in Chapter 11.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html#the-grand-scheme",
    "href": "10-hotelling.html#the-grand-scheme",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.7 The Grand Scheme",
    "text": "9.7 The Grand Scheme\n\n\nFigure 9.9 summarizes what was shown in Section 9.3 and Section 9.4. The data ellipses for the two groups in the mathscore data summarize the information about means and within-group variances. In the HE plot, the difference between the means is itself summarized by the line through them, which represents the \\(\\mathbf{H} =\\mathbf{Q}_H\\) matrix and within-group variation is represented by the “Error” ellipse which is the \\(\\mathbf{E} = \\mathbf{S}_p = \\mathbf{Q}_E\\) matrix.\n\n\n\n\n\n\n\nFigure 9.9: The Hypothesis Error plot framework for a two-group design. Above: Data ellipses can be summarized in an HE plot showing the pooled within-group error (\\(\\mathbf{E}\\)) ellipse and the \\(\\mathbf{H}\\) ‘ellipse’ for the group means. Below: Observations projected on the line joining the means give discriminant scores which correpond to a one-dimensional canonical space, represented by a boxplot of their scores and arrows reflecting the variable weights.\n\n\n\n\nAs we will see later (Chapter 11), the \\(\\mathbf{H}\\) ellipse is scaled so that it provides a visual test of significance: it projects somewhere outside the \\(\\mathbf{E}\\) ellipse if and only if the means differ significantly.\nThe direction of the line between the means is also the discriminant axis. Scores on this axis are shown in Figure 9.9 as the projection of the points on the line joining the group means. These are the weighted sum of the responses that have the greatest possible mean difference. A plot of these scores using boxplots, together with vectors for the variable weights is shown in the bottom right in Figure 9.9. I call this “Canonical space”, described more generally in Section 11.7.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html#what-have-we-learned",
    "href": "10-hotelling.html#what-have-we-learned",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.8 What have we learned?",
    "text": "9.8 What have we learned?\nThis chapter introduced Hotelling’s \\(T^2\\) as the gateway to understanding multivariate analysis of variance and the HE plot framework for visualizing multivariate tests. Here are the essential takeaways:\n\nHotelling’s \\(T^2\\) transforms multivariate complexity into univariate clarity. Rather than running separate \\(t\\)-tests on each variable (with multiple comparison problems), \\(T^2\\) finds the single linear combination of variables that maximally separates the groups. This elegant approach reduces a \\(p\\)-dimensional problem to testing differences on one optimally-weighted composite variable.\nThe geometric interpretation reveals the statistical essence. \\(T^2\\) is simply the squared Mahalanobis distance between group means, scaled by sample size. This geometric view helps us understand why \\(T^2\\) can detect significant differences even when individual variables show weak or non-significant effects —the groups may be well-separated along some direction in the \\(p\\)-dimensional multivariate space.\nHE plots provide visual hypothesis testing. The key insight is that all information needed for multivariate tests can be captured in two data ellipses: the \\(\\mathbf{H}\\) ellipse (representing group differences) and the \\(\\mathbf{E}\\) ellipse (representing within-group variation). When the \\(\\mathbf{H}\\) ellipse projects outside the \\(\\mathbf{E}\\) ellipse, the test is significant—making statistical significance immediately visible.\nThe discriminant axis is the line of maximum separation. The direction connecting group means in an HE plot is the axis that achieves the maximum possible univariate \\(t^2\\). This direction shows exactly how the variables combine to best distinguish between groups, providing both statistical power and interpretive insight.\nMultivariate tests gain power through correlation structure. While individual variables might show modest effects, the multivariate test leverages the correlations among variables to accumulate evidence. As we saw with the banknote data, the average univariate F was 236, but the multivariate F reached 392 by exploiting the relationships among the six size measures.\nThis framework scales directly to more complex designs. Though we focused on two-group comparisons, the HE plot approach extends naturally to full MANOVA and regression designs with multiple factors and interactions. The visual principles remain the same: hypothesis effects as ellipses, error as ellipses, and significance determined by their relative sizes and positions.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html#exercises",
    "href": "10-hotelling.html#exercises",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.9 Exercises",
    "text": "9.9 Exercises\n\nExercise 9.1 The value of Hotelling’s \\(T^2\\) found by hotelling.test() is 64.17. The value of the equivalent \\(F\\) statistic found by Anova() is 28.9. Verify that Equation 9.4 gives this result.\n\n\n\n\n\n\nCurran, J., & Hersh, T. (2021). Hotelling: Hotelling’s t^2 test and variants. https://doi.org/10.32614/CRAN.package.Hotelling\n\n\nFlury, B., & Riedwyl, H. (1988). Multivariate statistics: A practical approach. Chapman & Hall.\n\n\nHotelling, H. (1931). The generalization of Student’s ratio. The Annals of Mathematical Statistics, 2(3), 360–378. https://doi.org/10.1214/aoms/1177732979\n\n\nPearson, K. (1903). I. Mathematical contributions to the theory of evolution. —XI. On the influence of natural selection on the variability and correlation of organs. Philosophical Transactions of the Royal Society of London, 200(321–330), 1–66. https://doi.org/10.1098/rsta.1903.0001",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-hotelling.html#footnotes",
    "href": "10-hotelling.html#footnotes",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "",
    "text": "An exact test in statistics is a hypothesis test a gives a true, accurate significance level (\\(p\\)-value) without relying on approximations or large-sample assumptions. For example, the complete sampling distribution of the statistic \\(t = (\\bar{x}_1 - \\bar{x}_2) / s_p\\) for a two-sample \\(t\\) test can be generated in a permutation test that generates all possible random assignments of \\(n_1\\) and \\(n_2\\) observations to the two groups. This trades computation for the use of any assumptions.↩︎\nOne important difference between MANOVA and linear discriminant analysis (LDA) is the role of prior probabilities of the groups in the latter. LDA treats this as a decision problem, so prior weights for the groups effectively shift the decision boundary or classification threshold. If one group is more common, its prior probability will be higher, making it “easier” for a new observation to be classified into that group.↩︎\nThe function heplots::uniStats() does this more compactly and displays the \\(R^2\\) and \\(F\\) statistics for each response variable.↩︎\nheplots::etasq calculates overall \\(\\eta^2\\), across all response variables, for each term in a MLM. You can obtain these values for each of the response variables separately using effectsize::eta_squared().↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html",
    "href": "11-mlm-review.html",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "10.1 Structure of the MLM\nChapter 9 introduced the essential ideas of multivariate models in the context of a two-group design using Hotelling’s \\(T^2\\). Through the magical power of multivariate thinking, I extend this here to a “Holy of Holies”, inner sanctuary of the Tabernacle, the awed general Multivariate Linear Model (MLM).1\nThis can be understood as a simple extension of the univariate linear model, with the main difference being that there are multiple response variables considered together, instead of just one, analysed alone. (Or, from my perspective, the univariate version is a restricted form of the MLM.) These outcomes might reflect several different ways or scales for measuring an underlying theoretical construct, or they might represent different aspects of some phenomenon that we hope to better understand when they are studied jointly.\nFor example, in the case of different measures, there are numerous psychological scales used to assess depression or anxiety and it may be important to include more than one measure to ensure that the construct has been measured adequately. It would add considerably to our understanding to know if the different outcome measures all had essentially the same relations to the predictor variables, or if they differ across measures.\nIn the second case of various aspects, student “aptitude” or “achievement” reflects competency in different various subjects (reading, math, history, science, …) that are better studied together. We get a better understanding of the factors that influence each of aspects by testing them jointly.\nJust as in univariate analysis there are variously named techniques (ANOVA, regression) that can be applied to several outcomes, depending on the structure of the predictors at hand. For instance, with one or more continuous predictors and multiple response variables, you could use multivariate multiple regression (MMRA) to obtain estimates useful for prediction.\nInstead, if the predictors are categorical factors, multivariate analysis of variance (MANOVA) can be applied to test for differences between groups. Again, this is akin to ANOVA in the univariate context— the same underlying model is utilized, but the tests for terms in the model are multivariate ones for the collection of all response variables, rather than univariate ones for a single response.\nThe main goal of this chapter is to describe the details of the extension of univariate linear models to the case of multiple outcome measures. But the larger goal is to set the stage for the visualization methods using HE plots and low-D views discussed separately in Chapter 11. Some of the example datasets used here will re-appear there, and also in Chapter 12 which concerns some model-diagnostic graphical methods.\nHowever, before considering the details and examples that apply separately to MANOVA and MMRA, it is useful to consider the general features of the multivariate linear model of which these cases are examples.\nPackages\nIn this chapter I use the following packages. Load them now:\nWith \\(p\\) response variables, the multivariate linear model is most easily appreciated as the collection of \\(p\\) linear models, one for each response. We have \\(p\\) outcomes, so why not just consider a separate model for each?\n\\[\n\\begin{aligned}\n\\mathbf{y}_1 =& \\mathbf{X}\\boldsymbol{\\beta}_1 + \\boldsymbol{\\epsilon}_1 \\\\\n\\mathbf{y}_2 =& \\mathbf{X}\\boldsymbol{\\beta}_2 + \\boldsymbol{\\epsilon}_2 \\\\\n  \\vdots      &  \\\\\n\\mathbf{y}_p =& \\mathbf{X}\\boldsymbol{\\beta}_p + \\boldsymbol{\\epsilon}_p \\\\\n\\end{aligned}\n\\tag{10.1}\\]\nBut the problems with fitting separate univariate models are that:\nThe model matrix \\(\\mathbf{X}\\) in Equation 10.1 is the same for all responses, but each one gets its own vector \\(\\boldsymbol{\\beta}_j\\) of coefficients for how the predictors in \\(\\mathbf{X}\\) fit a given response \\(\\mathbf{y}_j\\).\nAmong the beauties of multivariate thinking is that we can put these separate equations together in single equation by joining the responses \\(\\mathbf{y}_j\\) as columns in a matrix \\(\\mathbf{Y}\\) and similarly arranging the vectors of coefficients \\(\\boldsymbol{\\beta}_j\\) as columns in a matrix \\(\\mathbf{B}\\).2\nTODO: Revise notation here, to be explicit and consistent about inclusion of \\(\\boldsymbol{\\beta}_0\\) and size of \\(\\mathbf{B}\\).\nThe MLM then becomes:\n\\[\n\\mathord{\\mathop{\\mathbf{Y}}\\limits_{n \\times p}} =\n\\mathord{\\mathop{\\mathbf{X}}\\limits_{n \\times (q+1)}} \\, \\mathord{\\mathop{\\mathbf{B}}\\limits_{(q+1) \\times p}} + \\mathord{\\mathop{\\mathbf{\\boldsymbol{\\Large\\varepsilon}}}\\limits_{n \\times p}} \\:\\: ,\n\\tag{10.2}\\]\nwhere:\nWriting Equation 10.2 in terms of its elements, we have\n\\[\\begin{aligned}\n\\overset{\\mathbf{Y}}\n  {\\begin{bmatrix}\n  y_{11} & y_{12} & \\cdots & y_{1p} \\\\\n  y_{21} & y_{22} & \\cdots & y_{2p} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  y_{n1} & y_{n2} & \\cdots & y_{np}\n  \\end{bmatrix}\n  }\n& =\n\\overset{\\mathbf{X}}\n  {\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1q} \\\\\n  1 & x_{21} & \\cdots & x_{2q} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  1 & x_{n1} & \\cdots & x_{nq}\n  \\end{bmatrix}\n  }\n\\overset{\\mathbf{B}}\n  {\\begin{bmatrix}\n  \\beta_{01} & \\beta_{02} & \\cdots & \\beta_{0p} \\\\\n  \\beta_{11} & \\beta_{12} & \\cdots & \\beta_{1p} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\beta_{q1} & \\beta_{q2} & \\cdots & \\beta_{qp}\n  \\end{bmatrix}\n  } \\\\\n& + \\quad\\quad\n\\overset{\\mathcal{\\boldsymbol{\\Large\\varepsilon}}}\n  {\\begin{bmatrix}\n  \\epsilon_{11} & \\epsilon_{12} & \\cdots & \\epsilon_{1p} \\\\\n  \\epsilon_{21} & \\epsilon_{22} & \\cdots & \\epsilon_{2p} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\epsilon_{n1} & \\epsilon_{n2} & \\cdots & \\epsilon_{np}\n  \\end{bmatrix}\n  }\n\\end{aligned}\\]\nThe structure of the model matrix \\(\\mathbf{X}\\) is exactly the same as the univariate linear model, and may therefore contain,",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#sec-mlm-structure",
    "href": "11-mlm-review.html#sec-mlm-structure",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "They don’t give simultaneous tests for all regressions. The situation is similar to that in a one-way ANOVA, where an overall test for group differences is usually applied before testing individual comparisons to avoid problems of multiple testing: \\(g\\) groups gives \\(g \\times (g-1)/2\\) pairwise tests.\n\nMore importantly, fitting separate univariate models does not take correlations among the \\(\\mathbf{y}\\)s into account.\n\nIt might be the case that the response variables are all essentially measuring the same thing, but perhaps weakly. If so, the multivariate approach can pool strength across the outcomes to detect their common relations to the predictors, giving greater power.\nOn the other hand, perhaps the responses are related in different ways to the predictors. A multivariate approach can help you understand how many different ways there are, and characterize each.\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{Y} = (\\mathbf{y}_1 , \\mathbf{y}_2, \\dots ,  \\mathbf{y}_p )\\) is the matrix of \\(n\\) observations on \\(p\\) responses, with typical column \\(\\mathbf{y}_j\\);\n\n\\(\\mathbf{X}\\) is the model matrix with columns \\(\\mathbf{x}_i\\) for \\(q\\) regressors, which typically includes an initial column \\(\\mathbf{x}_0\\) of 1s for the intercept;\n\n\\(\\mathbf{B} = ( \\mathbf{b}_1 , \\mathbf{b}_2 , \\dots,  \\mathbf{b}_p )\\) is a matrix of regression coefficients, one column \\(\\mathbf{b}_j\\) for each response variable;\n\n\\(\\boldsymbol{\\Large\\varepsilon}\\) is a matrix of errors in predicting \\(\\mathbf{Y}\\).\n\n\n\n\n\n\nquantitative predictors, such as age, income, years of education\n\n\ntransformed predictors like \\(\\sqrt{\\text{age}}\\) or \\(\\log{(\\text{income})}\\)\n\n\npolynomial terms: \\(\\text{age}^2\\), \\(\\text{age}^3, \\dots\\) (using poly(age, k) in R)\n\ncategorical predictors (“factors”), such as treatment (Control, Drug A, drug B), or sex; internally a factor with k levels is transformed to k-1 dummy (0, 1) variables, representing comparisons with a reference level, typically the first.\n\ninteraction terms, involving either quantitative or categorical predictors, e.g., age * sex, treatment * sex.\n\n\n10.1.1 Assumptions\nJust as in univariate models, the assumptions of the multivariate linear model almost entirely concern the behavior of the errors (residuals). Let \\(\\boldsymbol{\\epsilon}_{i}^{\\prime}\\) represent the \\(i\\)th row of \\(\\boldsymbol{\\Large\\varepsilon}\\). Then it is assumed that:\n\n\nNormality: The residuals, \\(\\boldsymbol{\\epsilon}_{i}^{\\prime}\\) are distributed as multivariate normal, \\(\\mathcal{N}_{p}(\\mathbf{0},\\boldsymbol{\\Sigma})\\), where \\(\\boldsymbol{\\Sigma}\\) is a non-singular error-covariance matrix.\n\nStatistical tests of multivariate normality of the residuals include the Shapiro-Wilk (Shapiro & Wilk, 1965) and Mardia (1970, 1974) tests (and others, in the MVN package package). \nAs in univariate models, the MLM is relatively robust against non-normality. However this is often better assessed visually using a \\(\\chi^2\\) QQ plot of Mahalanobis squared distance against their corresponding \\(\\chi^2_p\\) values for \\(p\\) degrees of freedom using heplots::cqplot().\n\n\nHomoscedasticity: The error-covariance matrix \\(\\boldsymbol{\\Sigma}\\) is constant across all observations and grouping factors. Graphical methods to show if this assumption is met are illustrated in Chapter 12.\nIndependence: \\(\\boldsymbol{\\epsilon}_{i}^{\\prime}\\) and \\(\\boldsymbol{\\epsilon}_{j}^{\\prime}\\) are independent for \\(i\\neq j\\), so knowing the data for case \\(i\\) gives no information about case \\(j\\). This assumption would be violated if the observations consisted of pairs of husbands and wives, or clusters of students from different schools and this grouping was ignored in the model.\nError-free \\(\\mathbf{X}\\): The predictors, \\(\\mathbf{X}\\), are fixed and measured without error. The discussion in Section 7.2 illustrates how measurement error biases (toward 0) the estimated effects of predictors. A weaker form, called exogeneity3 merely asserts that the predictors are independent of (uncorrelated with) the errors, \\(\\boldsymbol{\\Large\\varepsilon}\\). 4 \n\nThese statements are simply the multivariate analogs of the assumptions of normality, constant variance and independence of the errors in univariate models. Note that it is unnecessary to assume that the predictors (regressors, columns of \\(\\mathbf{X}\\)) are normally distributed.\nImplicit in the above is perhaps the most important assumption—that the model has been correctly specified. This means:\n\nLinearity: The form of the relations between each \\(\\mathbf{y}\\) and the \\(\\mathbf{x}\\)s is correct. Typically this means that the relations are linear, but if not, we have specified a correct transformation of \\(\\mathbf{y}\\) and/or \\(\\mathbf{x}\\), such as modeling \\(\\log(\\mathbf{y})\\), or using polynomial terms in one or more of the \\(\\mathbf{x}\\)s.\nCompleteness: No relevant predictors have been omitted from the model. For example in the coffee, stress example (Section 6.3, Section 7.1.2), omitting stress from the model biases the effect of coffee on heart disease.\nAdditive effects: The combined effect of different terms in the model is the sum of their individual effects, so the impact of one predictor on the outcome variable is the same regardless of the values of other predictors in the model. When this is not the case, you can add an interaction term like \\(x_1 \\times x_2\\) to account for this.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#sec-mlm-fitting",
    "href": "11-mlm-review.html#sec-mlm-fitting",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.2 Fitting the model",
    "text": "10.2 Fitting the model\nThe least squares (and also maximum likelihood) solution for the coefficients \\(\\mathbf{B}\\) is given by\n\n\n\n\\[\n\\widehat{\\mathbf{B}} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{Y} \\:\\: .\n\\]\nThe coefficients are precisely the same as fitting the separate responses \\(\\mathbf{y}_1 , \\mathbf{y}_2 , \\dots , \\mathbf{y}_p\\), and placing the estimated coefficients \\(\\widehat{\\mathbf{b}}_i\\) as columns in \\(\\widehat{\\mathbf{B}}\\)\n\\[\n\\widehat{\\mathbf{B}} = [ \\widehat{\\mathbf{b}}_1, \\widehat{\\mathbf{b}}_2, \\dots , \\widehat{\\mathbf{b}}_p] \\:\\: .\n\\] In R, we fit the multivariate linear model with lm() simply by giving a collection of response variables y1, y2, ... on the left-hand side of the model formula, wrapped in cbind() which combines them to form a matrix response.\n\nlm(cbind(y1, y2, y3) ~ x1 + x2 + ..., data=)\n\nIn the presence of possible outliers, robust methods are available for univariate linear models (e.g., MASS::rlm()). So too, heplots::robmlm() provides robust estimation in the multivariate case as illustrated in Section 13.5.\n\n10.2.1 Example: Dog food data\nAs a toy example to make these ideas concrete, consider the dataset dogfood. Here, a dogfood manufacturer wanted to study preference for different dogfood formulas, two of their own (“Old”, “New”) and two from other manufacturers (“Major”, “Alps”).\nIn a between-dog design, each of \\(n=4\\) dogs were presented with a bowl of one formula and the time to start eating and amount eaten were recorded. Greater preference would be seen in a shorter delay to start eating and a greater amount, so these responses are expected to be negatively correlated.\n\ndata(dogfood, package = \"heplots\")\nstr(dogfood)\n# 'data.frame': 16 obs. of  3 variables:\n#  $ formula: Factor w/ 4 levels \"Old\",\"New\",\"Major\",..: 1 1 1 1 2 2 2 2 3 3 ...\n#  $ start  : int  0 1 1 0 0 1 2 3 1 5 ...\n#  $ amount : int  100 97 88 92 95 85 82 89 77 84 ...\n\nFor this data, boxplots for the two responses provide an initial look, shown in Figure 10.1. Putting these side-by-side makes it easy to see the inverse relation between the medians on the two response variables.\n\nCodedog_long &lt;- dogfood |&gt;\n  pivot_longer(c(start, amount),\n               names_to = \"variable\")\nggplot(data = dog_long, \n       aes(x=formula, y = value, fill = formula)) +\n  geom_boxplot(alpha = 0.2) +\n  geom_point(size = 2.5) +\n  facet_wrap(~ variable, scales = \"free\") +\n  theme_bw(base_size = 14) + \n  theme(legend.position=\"none\")\n\n\n\n\n\n\nFigure 10.1: Boxplots for time to start eating and amount eaten by dogs given one of four dogfood formulas.\n\n\n\n\nAs suggested above, the multivariate model for testing mean differences due to the dogfood formula is fit using lm() on the matrix \\(\\mathbf{Y}\\) constructed with cbind(start, amount).\n\ndogfood.mod &lt;- lm(cbind(start, amount) ~ formula, \n                  data=dogfood) |&gt; \n  print()\n# \n# Call:\n# lm(formula = cbind(start, amount) ~ formula, data = dogfood)\n# \n# Coefficients:\n#               start   amount\n# (Intercept)     0.50   94.25\n# formulaNew      1.00   -6.50\n# formulaMajor    2.00  -12.25\n# formulaAlps     1.75  -16.00\n\nBy default, the factor formula is represented by three columns in the \\(\\mathbf{X}\\) matrix that correspond to treatment contrasts, which are comparisons of the Old formula (a baseline level) with each of the others. The coefficients, for example formulaNEW, are the difference in means from those for Old.\nThen, the overall multivariate test that means on both variables do not differ is carried out using car::Anova().\n\ndogfood.aov &lt;- Anova(dogfood.mod) |&gt;\n  print()\n# \n# Type II MANOVA Tests: Pillai test statistic\n#         Df test stat approx F num Df den Df Pr(&gt;F)  \n# formula  3     0.702     2.16      6     24  0.083 .\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe details of these analysis steps are explained below.\n\n10.2.2 Sums of squares\nIn univariate response models, statistical tests and model summaries (like \\(R^2\\)) are based on the familiar decomposition of the total sum of squares \\(\\text{SS}_T\\) into regression or hypothesis (\\(\\text{SS}_H\\)) and error (\\(\\text{SS}_E\\)) sums of squares. For example the \\(R^2\\) is just \\(\\text{SS}_H / \\text{SS}_T\\).\nThe multivariate linear model has a simple, direct extension: Each of these sum of squares becomes a \\(p \\times p\\) matrix \\(SSP\\) containing sums of squares for the \\(p\\) responses on the diagonal and sums of cross products in the off-diagonal. elements. For the MLM this is expressed as:\n\\[\n\\begin{aligned}\n\\underset{(p\\times p)}{\\mathbf{SSP}_{T}}\n& =  \\mathbf{Y}^{\\top} \\mathbf{Y} - n \\overline{\\mathbf{y}}\\,\\overline{\\mathbf{y}}^{\\top} \\\\\n& =  \\left(\\widehat {\\mathbf{Y}}^{\\top}\\widehat{\\mathbf{Y}} - n\\overline{\\mathbf{y}}\\,\\overline{\\mathbf{y}}^{\\top} \\right) + \\widehat{\\boldsymbol{\\Large\\varepsilon}}^{\\top}\\widehat{\\boldsymbol{\\Large\\varepsilon}} \\\\\n& =   \\mathbf{SSP}_{H} + \\mathbf{SSP}_{E} \\\\\n& \\equiv  \\mathbf{H} + \\mathbf{E} \\:\\: ,\n\\end{aligned}\n\\tag{10.3}\\]\nwhere,\n\n\n\\(\\overline{\\mathbf{y}}\\) is the \\((p\\times 1)\\) vector of means for the response variables;\n\n\\(\\widehat{\\mathbf{Y}} = \\mathbf{X}\\widehat{\\mathbf{B}}\\) is the matrix of fitted values; and\n\n\\(\\widehat{\\boldsymbol{\\Large\\varepsilon}} = \\mathbf{Y} -\\widehat{\\mathbf{Y}}\\) is the matrix of residuals.\n\nWe can visualize this decomposition in the simple case of a two-group design (using the mathscore data from Section 9.2) as shown in Figure 10.2. Let \\(\\mathbf{y}_{ij}\\) be the vector of \\(p\\) responses for subject \\(j\\) in group \\(i, i=1,\\dots g\\) for \\(j = 1, \\dots n_i\\). Then, using “\\(.\\)” to represent a subscript averaged over, Equation 10.3 comes from the identity\n\\[\n\\underbrace{(\\mathbf{y}_{ij} - \\mathbf{y}_{\\cdot \\cdot})}_T =\n\\underbrace{(\\overline{\\mathbf{y}}_{i \\cdot} - \\mathbf{y}_{\\cdot \\cdot})}_H +\n\\underbrace{(\\mathbf{y}_{ij} - \\overline{\\mathbf{y}}_{i \\cdot})}_E\n\\tag{10.4}\\]\nwhere each side of Equation 10.4 is squared and summed over observations to give Equation 10.3. In Figure 10.2,\n\nThe total variance \\(\\mathbf{SSP}_T\\) reflects the deviations of the observations \\(\\mathbf{y}_{ij}\\) from the grand mean \\(\\overline{\\mathbf{y}}_{. .}\\) and has the data ellipse shown in gray.\nIn the middle \\(\\mathbf{SSP}_H\\) panel, all the observations are represented at their group means, \\(\\overline{\\mathbf{y}}_{i .}\\), the fitted values. Their variance and covariance is then reflected by deviations of the group means (weighted for the number of observations per group) around the grand mean.\nThe right \\(\\mathbf{SSP}_E\\) panel then shows the residual variance, which is the variation of the observations \\(\\mathbf{y}_{ij}\\) around their group means, \\(\\overline{\\mathbf{y}}_{i .}\\). Centering the two data ellipses at the centroid \\(\\overline{\\mathbf{y}}_{. .}\\) then gives the ellipse for the \\(\\mathbf{SSP}_E\\), also called the pooled within-group covariance matrix.\n\n\n\n\n\n\n\n\nFigure 10.2: Breakdown of the total \\(\\mathbf{SSP}_{T}\\) into sums of squares and products for between-group hypothesis variance (\\(\\mathbf{SSP}_{H}\\)) and within-group, error variance (\\(\\mathbf{SSP}_{E}\\)).\n\n\n\n\nThe formulas for these sum of squares and products matrices can be shown explicitly as follows, where the notation \\(\\mathbf{z} \\mathbf{z}^\\top\\) generates the \\(p \\times p\\) outer product of a vector \\(\\mathbf{z}\\), giving \\(z_k \\times z_\\ell\\) for all pairs of elements. \\[\n\\mathbf{SSP}_T =\n\\sum_{i=1}^{g} \\sum_{j=1}^{n_{i}}\\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)\\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)^{\\top}\n\\tag{10.5}\\]\n\\[\n\\mathbf{SSP}_H =\n\\sum_{i=1}^{g} \\mathbf{n}_{i}\\left(\\overline{\\mathbf{y}}_{i \\cdot}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)\\left(\\overline{\\mathbf{y}}_{i \\cdot}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)^{\\top}\n\\tag{10.6}\\]\n\\[\n\\mathbf{SSP}_E =\n\\sum_{i=1}^{g} \\sum_{j=1}^{n_{i}} \\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{i \\cdot}\\right) \\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{i \\cdot}\\right)^{\\top}\n\\tag{10.7}\\]\nThis is the decomposition that we visualize in HE plots, where the size and direction of \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) can be represented as ellipsoids.\nBut first, let’s find these results for the example. The easy way is to get them from the result returned by car::Anova(), where the hypothesis \\(\\mathbf{SSP}_{H}\\) for each term in the model is returned as an element in a named list SSP and the error \\(\\mathbf{SSP}_{E}\\) is returned as the matrix SSPE.\n\nSSP_H &lt;- dogfood.aov$SSP |&gt; print()\n# $formula\n#         start amount\n# start    9.69  -70.9\n# amount -70.94  585.7\n\nSSP_E &lt;- dogfood.aov$SSPE |&gt; print()\n#        start amount\n# start   25.8   11.8\n# amount  11.8  390.3\n\nYou can calculate these directly as shown below. sweep() is used to subtract the colMeans() from \\(\\mathbf{Y}\\) and \\(\\widehat{\\mathbf{Y}}\\) and crossprod() premultiplies a matrix by its’ transpose.\n\nY &lt;- dogfood[, c(\"start\", \"amount\")]\nYdev &lt;- sweep(Y, 2, colMeans(Y)) |&gt; as.matrix()\nSSP_T &lt;- crossprod(as.matrix(Ydev)) |&gt; print()\n#        start amount\n# start   35.4  -59.2\n# amount -59.2  975.9\n\nfitted &lt;- fitted(dogfood.mod)\nYfit &lt;- sweep(fitted, 2, colMeans(fitted)) |&gt; as.matrix()\nSSP_H &lt;- crossprod(Yfit) |&gt; print()\n#         start amount\n# start    9.69  -70.9\n# amount -70.94  585.7\n\nresiduals &lt;- residuals(dogfood.mod)\nSSP_E &lt;- crossprod(residuals) |&gt; print()\n#        start amount\n# start   25.8   11.8\n# amount  11.8  390.3\n\nThe decomposition of the total sum of squares and products in Equation 10.3 can be shown as:\n\\[\n\\overset{\\mathbf{SSP}_T}\n  {\\begin{pmatrix}\n   35.4 & -59.2 \\\\\n  -59.2 & 975.9 \\\\\n  \\end{pmatrix}}\n=\n\\overset{\\mathbf{SSP}_H}\n  {\\begin{pmatrix}\n    9.69 & -70.94 \\\\\n  -70.94 & 585.69 \\\\\n  \\end{pmatrix}}\n+\n\\overset{\\mathbf{SSP}_E}\n  {\\begin{pmatrix}\n   25.8 &  11.8 \\\\\n   11.8 & 390.3 \\\\\n  \\end{pmatrix}}\n\\] These numbers are the variances in the diagonal and covariance between start and amount in the off-diagonal. The sign is important: In \\(\\text{SSP}_H\\) the negative covariance reflects the fact that for these brands larger time to start eating is negatively related to the amount eaten. In \\(\\text{SSP}_E\\) the the covariance is slightly positive. This might reflect a mild hunger factor: for a given brand, dogs who lunge for their bowls sooner also eat more. But, they’re all good boys, right?\n\n10.2.3 How big is \\(SS_H\\) compared to \\(SS_E\\)?\nIn a univariate response model, \\(SS_H\\) and \\(SS_E\\) are both scalar numbers and the univariate \\(F\\) test statistic,\n\\[F = \\frac{\\text{SS}_H/\\text{df}_h}{\\text{SS}_E/\\text{df}_e} = \\frac{\\mathsf{Var}(H)}{\\mathsf{Var}(E)} \\:\\: ,\n\\tag{10.8}\\]\nassesses “how big” \\(\\text{SS}_H\\) is, relative to \\(\\text{SS}_E\\), the variance accounted for by a hypothesized model or model terms relative to error variance. The measure \\(R^2 = SS_H / (SS_H + SS_E) = SS_H / SS_T\\) gives the proportion of total variance accounted for by the model terms.\nIn the multivariate analog \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) are both \\(p \\times p\\) matrices, and \\(\\mathbf{H}\\) “divided by” \\(\\mathbf{E}\\) becomes \\(\\mathbf{H}\\mathbf{E}^{-1}\\). The answer, “how big” \\(\\text{SS}_H\\) is compared to \\(\\text{SS}_E\\) is expressed in terms of the \\(p\\) eigenvalues \\(\\lambda_i, i = 1, 2, \\dots p\\) of \\(\\mathbf{H}\\mathbf{E}^{-1}\\). These are the \\(\\lambda_i\\) which solve the determinant equation\n\\[\n\\text{det}{(\\mathbf{H}\\mathbf{E}^{-1} - \\lambda \\mathbf{I}}) = 0 \\:\\: .\n\\]\nThe solution gives also the vectors \\(\\mathbf{v}_i\\) as the corresponding eigenvectors, which satisfy\n\\[\n\\mathbf{H}\\mathbf{E}^{-1} \\; \\lambda_i = \\lambda_i \\mathbf{v}_i \\:\\: .\n\\tag{10.9}\\]\nThis can also be expressed in terms of the size of \\(\\mathbf{H}\\) relative to total variation \\((\\mathbf{T} =\\mathbf{H}+ \\mathbf{E})\\) as\n\\[\n\\mathbf{H}(\\mathbf{H}+\\mathbf{E})^{-1} \\; \\rho_i = \\rho_i \\mathbf{v}_i \\:\\: ,\n\\tag{10.10}\\]\nwhich has the same eigenvectors as Equation 10.9 and the eigenvalues are \\(\\rho_i = \\lambda_i / (1 + \\lambda_i)\\).\nHowever, when the hypothesized model terms have \\(\\text{df}_h\\) degrees of freedom (columns of the \\(\\mathbf{X}\\) matrix for that term), \\(\\mathbf{H}\\) is of rank \\(\\text{df}_h\\), so only \\(s=\\min(p, \\text{df}_h)\\) eigenvalues can be non-zero. For example, a test for a hypothesis about a single quantitative predictor \\(\\mathbf{x}\\), has \\(\\text{df}_h = 1\\) degree of freedom and \\(\\mathrm{rank} (\\mathbf{H}) = 1\\); for a factor with \\(g\\) groups, \\(\\text{df}_h = \\mathrm{rank} (\\mathbf{H}) = g-1\\).\nFor example, we get the following results for the dogfood data:\n\nHEinv &lt;- SSP_H %*% solve(SSP_E) |&gt; \n  print()\n#         start amount\n# start   0.466 -0.196\n# amount -3.488  1.606\n\neig &lt;- eigen(HEinv)\neig$values\n# [1] 2.0396 0.0317\n\n# as proportions\neig$values / sum(eig$values)\n# [1] 0.9847 0.0153\n\nThe factor formula has four levels and therefore \\(\\text{df}_h = 3\\) degrees of freedom. But there are only \\(p = 2\\) responses, so there are \\(s=\\min(p, \\text{df}_h) = 2\\) non-zero eigenvalues (and corresponding eigenvectors). The eigenvalues tell us that 98.5% of the hypothesis variance due to formula can be accounted for by a single dimension.\n\n\n\n\n\n\n\n\nThe overall multivariate test for the model in Equation 10.2 is essentially a test of the hypothesis \\(\\mathcal{H}_0: \\mathbf{B} = 0\\) (excluding the row for the intercept). Equivalently, this is a test based on the incremental \\(\\mathbf{SSP}_{H}\\) for the hypothesized terms in the model—that is, the difference between the \\(\\mathbf{SSP}_{H}\\) for the full model and the null, intercept-only model. The same idea can be applied to test the difference between any pair of nested models—the added contribution of terms in a larger model relative to a smaller model containing a subset of terms.\nThe eigenvectors \\(\\mathbf{v}_i\\) in Equation 10.9 are also important. These are the weights for the variables in a linear combination \\(v_{i1} \\mathbf{y}_1 + v_{i2} \\mathbf{y}_2 + \\cdots + v_{ip} \\mathbf{y}_p\\) which produces the largest univariate \\(F\\) statistic for the \\(i\\)-th dimension. We exploit this in canonical discriminant analysis and the corresponding canonical HE plots (Section 11.7).\nThe eigenvectors of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) for the dogfood model are shown below:\n\nrownames(eig$vectors) &lt;- rownames(HEinv)\ncolnames(eig$vectors) &lt;- paste(\"Dim\", 1:2)\neig$vectors\n#         Dim 1  Dim 2\n# start   0.123 -0.411\n# amount -0.992 -0.911\n\nThe first column corresponds to the weighted sum \\(0.12 \\times\\text{start} - 0.99 \\times \\text{amount}\\), which as we saw above accounts for 95.5% of the differences in the group means.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#sec-multivar-tests",
    "href": "11-mlm-review.html#sec-multivar-tests",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.3 Multivariate test statistics",
    "text": "10.3 Multivariate test statistics\nIn the univariate case, the overall \\(F\\)-test of \\(\\mathcal{H}_0: \\boldsymbol{\\beta} = \\mathbf{0}\\) is the uniformly most powerful invariant test when the assumptions are met. There is nothing better. This is not the case in the MLM.\nThe reason is that when there are \\(p &gt; 1\\) response variables, and we are testing a hypothesis comprising \\(\\text{df}_h &gt;1\\) coefficients or degrees of freedom, there are \\(s = \\min(p, \\text{df}_h) &gt; 1\\) possible dimensions in which \\(\\mathbf{H}\\) can be large relative to \\(\\mathbf{E}\\). The size of each dimension is measured by the its eigenvalue \\(\\lambda_i\\). There are several test statistics that combine these into a single measure, shown in Table 10.1.\n\n\nTable 10.1: How test statistics for multivariate tests combine the size of dimensions of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) into a single measure.\n\n\n\n\nCriterion\nFormula\nPartial \\(\\eta^2\\)\n\n\n\n\n\nWilks’s \\(\\Lambda\\)\n\n\\(\\Lambda = \\prod^s_i \\frac{1}{1+\\lambda_i}\\)\n\\(\\eta^2 = 1-\\Lambda^{1/s}\\)\n\n\n\nPillai trace\n\\(V = \\sum^s_i \\frac{\\lambda_i}{1+\\lambda_i}\\)\n\\(\\eta^2 = \\frac{V}{s}\\)\n\n\n\nHotelling-Lawley trace\n\\(H = \\sum^s_i \\lambda_i\\)\n\\(\\eta^2 = \\frac{H}{H+s}\\)\n\n\n\nRoy maximum root\n\\(R = \\lambda_1\\)\n\\(\\eta^2 = \\frac{\\lambda_1}{1+\\lambda_1}\\)\n\n\n\n\n\n\n\n\nThese correspond to different kinds of “means” of the \\(\\lambda_i\\): geometric (Wilks), arithmetic (Pillai), harmonic (Hotelling-Lawley) and supremum (Roy). See Friendly et al. (2013) for the geometry behind these measures.\nEach of these statistics have different sampling distributions under the null hypothesis. But conveniently they can all be converted to \\(F\\) statistics. These conversions are exact when the hypothesis has \\(s \\le 2\\) degrees of freedom, and approximations otherwise.\n\n\n\n\n\n\nWhich multivariate test statistic is “best”?\n\n\n\nThe answer is that it depends on the structure of the size of dimensions of an effect reflected in the eigenvalues \\(\\lambda_i\\) and what possible violation of assumptions you want to protect against.\n\nSchatzoff (1966) compared the power and sensitivity of the four multivariate statistics and showed that Roy’s largest-latent root statistic was most sensitive when population centroids in MANOVA differed in a single dimension, but less sensitive when the group differences were more diffuse.\nIn addition to power to detect differences, Olson (1974) also examined the robustness of these tests against nonnormality and heterogeneity of covariance matrices. He found that Pillai’s trace was most robust. Under diffuse differences, Pillai’s trace had the greatest power, while Roy’s test was worse. Not surprisingly, when differences were largely one-dimensional, this ordering was reversed.\n\nThese differences among the test statistics disappear as your sample size grows large, because these tests become asymptotically equivalent. The default choice in car::Anova() of test.statistic = \"Pillai\" reflects the view that Pillai’s test is preferred over a wider range of circumstances. The summary() method for \"Anova.mlm\" objects permits the specification of more than one multivariate test statistic, and the default is to report all four.\n\n\nAs well, each test statistic has an analog of the \\(R^2\\)-like partial \\(\\eta^2\\) measure, giving the partial association accounted for by each term in the MLM. These reflect the proportion of total variation attributable to a given model term, partialling out (excluding) other factors from the total non-error variation. They can be used as a measure of effect size in a MLM and are calculated using heplots::etasq(). \n\n10.3.1 Testing contrasts and linear hypotheses\nEven more generally, these multivariate tests apply to every linear hypothesis concerning the coefficients in \\(\\mathbf{B}\\). Suppose we want to test the hypothesis that a subset of rows (predictors) and/or columns (responses) simultaneously have null effects. This can be expressed in the general linear test,\n\\[\n\\mathcal{H}_0 : \\mathbf{C}_{h \\times q} \\, \\mathbf{B}_{q \\times p} = \\mathbf{0}_{h \\times p} \\:\\: ,\n\\]\nwhere \\(\\mathbf{C}\\) is a full rank \\(h \\le q\\) hypothesis matrix of constants, that selects subsets or linear combinations (contrasts) of the coefficients in \\(\\mathbf{B}\\) to be tested in a \\(h\\) degree-of-freedom hypothesis.\nIn this case, the SSP matrix for the hypothesis has the form\n\\[\n\\mathbf{H}  =\n(\\mathbf{C} \\widehat{\\mathbf{B}})^\\mathsf{T}\\,\n[\\mathbf{C} (\\mathbf{X}^\\mathsf{T}\\mathbf{X} )^{-1} \\mathbf{C}^\\mathsf{T}]^{-1} \\,\n(\\mathbf{C} \\widehat{\\mathbf{B}}) \\:\\: ,\n\\tag{10.11}\\]\nwhere there are \\(s = \\min(h, p)\\) non-zero eigenvalues of \\(\\mathbf{H}\\mathbf{E}^{-1}\\). In Equation 10.11, \\(\\mathbf{H}\\) measures the (Mahalanobis) squared distances (and cross products) among the linear combinations \\(\\mathbf{C} \\widehat{\\mathbf{B}}\\) from the origin under the null hypothesis.  \nFor example, with three responses \\(y_1, y_2, y_3\\) and three predictors \\(x_1, x_2, x_3\\), we can test the hypothesis that neither \\(x_1\\) nor \\(x_2\\) contribute at all to predicting the \\(y\\)s in terms of the hypothesis that the coefficients for the corresponding rows of \\(\\mathbf{B}\\) are zero using a 1-row \\(\\mathbf{C}\\) matrix that simply selects those rows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\mathcal{H}_0 : \\mathbf{C} \\mathbf{B} & =\n\\begin{bmatrix}\n0 & 1 & 1 & 0\n\\end{bmatrix}\n\\begin{pmatrix}\n  \\beta_{0,y_1} & \\beta_{0,y_2} & \\beta_{0,y_3} \\\\\n  \\beta_{1,y_1} & \\beta_{1,y_2} & \\beta_{1,y_3} \\\\\n  \\beta_{2,y_1} & \\beta_{2,y_2} & \\beta_{2,y_3} \\\\\n  \\beta_{3,y_1} & \\beta_{3,y_2} & \\beta_{3,y_3}\n\\end{pmatrix} \\\\ \\\\\n& =\n\\begin{bmatrix}\n  \\beta_{1,y_1} & \\beta_{1,y_2} & \\beta_{1,y_3} \\\\\n  \\beta_{2,y_1} & \\beta_{2,y_2} & \\beta_{2,y_3} \\\\\n\\end{bmatrix}\n=\n\\mathbf{0}_{(2 \\times 3)}\n\\end{aligned}\\]\nIn MANOVA designs, it is often desirable to follow up a significant effect for a factor with subsequent tests to determine which groups differ. While you can simply test for all pairwise differences among groups (using Bonferonni or other corrections for multiplicity), a more substantively-driven approach uses planned comparisons or contrasts among the factor levels as described in Section 5.1.3.\n\nFor a factor with \\(g\\) groups, a contrast is simply a comparison of the mean of one subset of groups against the mean of another subset. This is specified as a weighted sum, \\(L\\) of the means with weights \\(\\mathbf{c}\\) that sum to zero,\n\\[\nL = \\mathbf{c}^\\mathsf{T} \\boldsymbol{\\mu} = \\sum_i c_i \\mu_i \\quad\\text{such that}\\quad \\Sigma c_i = 0\n\\] Two contrasts, \\(\\mathbf{c}_1\\) and \\(\\mathbf{c}_2\\) are orthogonal if the sum of products of their weights is zero, i.e., \\(\\mathbf{c}_1^\\mathsf{T} \\mathbf{c}_2 = \\Sigma c_{1i} \\times c_{2i} = 0\\). When contrasts are placed as columns of a matrix \\(\\mathbf{C}\\), they are all mutually orthogonal if each pair is orthogonal, which means \\(\\mathbf{C}^\\top \\mathbf{C}\\) is a diagonal matrix. Orthogonal contrasts correspond to statistically independent tests.\nThis is nice because orthogonal contrasts reflect separate, non-overlapping research questions. When these questions are posed apriori, in advance of analysis, there is no need to correct for multiple testing, on the grounds that you shouldn’t be penalized for having more ideas!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor example, with the \\(g=4\\) groups for the dogfood data, the company might want to test the following comparisons among the formulas Old, New, Major and Alps: (a) Ours vs. Theirs: The average of (Old, New) compared to (Major, Alps); (b) Old vs. New; (c) Major vs. Alps. The contrasts that do this are:\n\\[\\begin{aligned}\nL_1 & = \\textstyle{\\frac12} (\\mu_O + \\mu_N) -\n        \\textstyle{\\frac12} (\\mu_M + \\mu_A) & \\rightarrow\\: & \\mathbf{c}_1 =\n\\textstyle{\\frac12}\n     \\begin{pmatrix}\n      1 &  1 & -1 & -1\n     \\end{pmatrix} \\\\\nL_2 & = \\mu_O - \\mu_N                     & \\rightarrow\\: & \\mathbf{c}_2 =\n    \\begin{pmatrix}\n     1 &  -1 & 0 & 0\n    \\end{pmatrix} \\\\\nL_3 & = \\mu_M - \\mu_A                     & \\rightarrow\\: & \\mathbf{c}_3 =\n    \\begin{pmatrix}\n     0 &  0 & 1 & -1\n    \\end{pmatrix}\n\\end{aligned}\\]\nNote that these correspond to nested dichotomies among the four groups: first we compare groups (Old and New) against groups (Major and Alps), then subsequently within each of these sets. Nested dichotomy contrasts are always orthogonal, and therefore correspond to statistically independent tests. We are effectively taking a three degree-of-freedom question, “do the means differ?” and breaking it down into three separate 1 df tests that answer specific parts of that overall question. \nIn R, contrasts for a factor are specified as columns of matrix, each of which sums to zero. For this example, we can set this up by creating each as a vector and joining them as columns using cbind():\n\nc1 &lt;- c(1,  1, -1, -1)/2    # Old,New vs. Major,Alps\nc2 &lt;- c(1, -1,  0,  0)      # Old vs. New\nc3 &lt;- c(0,  0,  1, -1)      # Major vs. Alps\nC &lt;- cbind(c1,c2,c3) \nrownames(C) &lt;- levels(dogfood$formula)\n\nC\n#         c1 c2 c3\n# Old    0.5  1  0\n# New    0.5 -1  0\n# Major -0.5  0  1\n# Alps  -0.5  0 -1\n\n# show they are mutually orthogonal\nt(C) %*% C\n#    c1 c2 c3\n# c1  1  0  0\n# c2  0  2  0\n# c3  0  0  2\n\nFor the dogfood data, with formula as the group factor, you can set up the analyses to use these contrasts by assigning the matrix C to contrasts() for that factor in the dataset itself. The estimated coefficients then become the estimated mean differences for the contrasts. When the contrasts are changed, it is necessary to refit the model.\n\ncontrasts(dogfood$formula) &lt;- C\ndogfood.mod &lt;- lm(cbind(start, amount) ~ formula, \n                  data=dogfood)\ncoef(dogfood.mod)\n#              start amount\n# (Intercept)  1.688  85.56\n# formulac1   -1.375  10.88\n# formulac2   -0.500   3.25\n# formulac3    0.125   1.88\n\nFor example, the contrast “Ours vs. Theirs” estimated by formulac1 takes 1.375 less time to start eating and eats 10.88 more on average.\nFor multivariate tests, when all contrasts are pairwise orthogonal, the overall test of a factor with \\(\\text{df}_h = g-1\\) degrees of freedom can be broken down into \\(g-1\\) separate 1 df tests. This gives rise to a set of \\(\\text{df}_h\\) rank 1 \\(\\mathbf{H}\\) matrices that additively decompose the overall hypothesis SSCP matrix,\n\\[\n\\mathbf{H} = \\mathbf{H}_1 + \\mathbf{H}_2 + \\cdots + \\mathbf{H}_{\\text{df}_h} \\:\\: ,\n\\tag{10.12}\\]\nexactly as the univariate \\(\\text{SS}_H\\) can be decomposed using orthogonal contrasts in an ANOVA (Section 5.1.3)\nYou can test such contrasts or any other hypotheses involving linear combinations of the coefficients using car::linearHypothesis(). Here, \"formulac1\" refers to the contrast c1 for the difference between Ours and Theirs. Note that because this is a 1 df test, all four test statistics yield the same \\(F\\) values.\n\nhyp &lt;- rownames(coef(dogfood.mod))[-1] |&gt; \n  print()\n# [1] \"formulac1\" \"formulac2\" \"formulac3\"\nH1 &lt;- linearHypothesis(dogfood.mod, hyp[1], \n                       title=\"Ours vs. Theirs\") |&gt; \n  print()\n# \n# Sum of squares and products for the hypothesis:\n#         start amount\n# start    7.56  -59.8\n# amount -59.81  473.1\n# \n# Sum of squares and products for error:\n#        start amount\n# start   25.8   11.7\n# amount  11.7  390.3\n# \n# Multivariate Tests: Ours vs. Theirs\n#                  Df test stat approx F num Df den Df Pr(&gt;F)   \n# Pillai            1     0.625     9.18      2     11 0.0045 **\n# Wilks             1     0.375     9.18      2     11 0.0045 **\n# Hotelling-Lawley  1     1.669     9.18      2     11 0.0045 **\n# Roy               1     1.669     9.18      2     11 0.0045 **\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSimilarly we can test the other two contrasts within these each of these two subsets, but I don’t print the results.\n\nH2 &lt;- linearHypothesis(dogfood.mod, hyp[2], \n                       title=\"Old vs. New\")\nH3 &lt;- linearHypothesis(dogfood.mod, hyp[3], \n                       title=\"Alps vs. Major\")\n\nThen, we can illustrate Equation 10.12 by extracting the 1 df \\(\\mathbf{H}\\) matrices (SSPH) from the results of linearHypothesis.\n\n\\[\n\\overset{\\mathbf{H}}\n{\\begin{pmatrix}\n  9.7 & -70.9 \\\\\n-70.9 & 585.7 \\\\\n\\end{pmatrix}}\n=\n\\overset{\\mathbf{H}_1}\n{\\begin{pmatrix}\n  7.6 & -59.8 \\\\\n-59.8 & 473.1 \\\\\n\\end{pmatrix}}\n+\n\\overset{\\mathbf{H}_2}\n{\\begin{pmatrix}\n0.13 &  1.88 \\\\\n1.88 & 28.12 \\\\\n\\end{pmatrix}}\n+\n\\overset{\\mathbf{H}_3}\n{\\begin{pmatrix}\n  2 & -13 \\\\\n-13 &  84 \\\\\n\\end{pmatrix}}\n\\]",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#anova-rightarrow-manova",
    "href": "11-mlm-review.html#anova-rightarrow-manova",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.4 ANOVA \\(\\rightarrow\\) MANOVA",
    "text": "10.4 ANOVA \\(\\rightarrow\\) MANOVA\nMultivariate analysis of variance (MANOVA) generalizes the familiar ANOVA model to situations where there are two or more response variables. Unlike ANOVA, which focuses on discerning statistical differences in one continuous dependent variable influenced by an independent variable (or grouping variable), MANOVA considers several dependent variables at once. It integrates these variables into a single, composite variable through a weighted linear combination, allowing for a comprehensive analysis of how these dependent variables collectively vary with respect to the levels of the independent variable. Essentially, MANOVA investigates whether the grouping variable explains significant variations in the combined dependent variables.\nThe situation is illustrated in Figure 10.3 where there are two response measures, \\(Y_1\\) and \\(Y_2\\) with data collected for three groups. For concreteness, \\(Y_1\\) might be a score on a math test and \\(Y_2\\) might be a reading score. Let’s also say that group 1 has been studying Shakespeare, while group 2 has concentrated on physics, but group 3 has done nothing beyond the normal curriculum.\n\n\n\n\n\n\n\nFigure 10.3: Diagram of data from simple MANOVA design involving three groups and two response measures, \\(Y_1\\) and \\(Y_2\\), summarized by their data ellipses.\n\n\n\n\nAs shown in the figure, the centroids, \\((\\mu_{1g}, \\mu_{2g})\\), clearly differ—the data ellipses barely overlap. A multivariate analysis would show a highly difference among groups. From a rough visual inspection, it seems that means differ on the math test \\(Y_1\\), with the physics group out-performing the other two. On the reading test \\(Y_2\\) however it might turn out that the three group means don’t differ significantly in an ANOVA, but the Shakespeare and physics groups appear to outperform the normal curriculum group. Doing separate ANOVAs on these variables would miss what is so obvious from Figure 10.3: there is wide separation among the groups in the two tests considered jointly.\nFigure 10.4 illustrates a second important advantage of performing a multivariate analysis over separate ANOVAS: that of determining the number of dimensions or aspects along which groups differ. In the panel on the left, the means of the three groups increase nearly linearly on the combination of \\(Y_1\\) and \\(Y_2\\), so their differences can be ascribed to a single dimension, which simplifies the interpretation: both memory and attention scores decrease together with increasing degree of schizophrenia.\nFor example, the groups here might be patients diagnosed as normal, mild schizophrenia and profound schizophrenia, and the measures could be tests of memory and attention. The obvious multivariate interpretation from the figure is that of increasing impairment of cognitive functioning across the groups, comprised by memory and attention. Note also the positive association within each group: those who perform better on the memory task also do better on attention.\n\n\n\n\n\n\n\nFigure 10.4: A simple MANOVA design involving three groups and two response measures, \\(Y_1\\) and \\(Y_2\\), but with different patterns of the differences among the group means. The red arrows suggest interpretations in terms of dimensions or aspects of the response variables.\n\n\n\n\nIn contrast, the right panel of Figure 10.4 shows a situation where the group means have a low correlation. Data like this might arise in a study of parental competency, where there are measures of both the degree of caring (\\(Y_1\\)) and time spent in play (\\(Y_2\\)) by fathers and groups consisting of fathers of children with no disability, or a physical disability or a mental ability.\nAs can be seen in Figure 10.4 fathers of the disabled children differ from those of the not disabled group in two different directions corresponding to being higher on either \\(Y_1\\) or \\(Y_2\\). The red arrows suggest that the differences among groups could be interpreted in terms of two uncorrelated dimensions, perhaps labeled overall competency and emphasis on physical activity. (The pattern in Figure 10.4 (right) is contrived for the sake of illustration; it does not reflect the data analyzed in the example below.)\n\n10.4.1 Example: Father parenting data\nI use a simple example of a three-group multivariate design to illustrate the basic ideas of fitting MLMs in R and testing hypotheses. Visualization methods using HE plots are discussed in Chapter 11.\nThe dataset Parenting come from an exercise (10B) in Meyers et al. (2006) and are probably contrived, but are modeled on a real study in which fathers were assessed on three subscales of a Perceived Parenting Competence Scale,\n\n\ncaring, caretaking responsibilities;\n\nemotion, emotional support provided to the child; and\n\nplay, recreational time spent with the child.\n\nThe dataset Parenting comprises 60 fathers selected from three groups of \\(n = 20\\) each: (a) fathers of a child with no disabilities (\"Normal\"); (b) fathers with a physically disabled child; (c) fathers with a mentally disabled child. The design is thus a three-group MANOVA, with three response variables.\n\nThe main questions concern whether the group means differ on these scales, and the nature of these differences. That is, do the means differ significantly on all three measures? Is there a consistent order of groups across these three aspects of parenting?\nMore specific questions are: (a) Do the fathers of typical children differ from the other two groups on average? (b) Do the physical and mental groups differ on these measures?\nThese questions can be tested using contrasts, and are specified by assigning a matrix to contrasts(Parenting$group); each column is a contrast whose values sum to zero. They are given labels \"group1\" (normal vs. other) and \"group2\" (physical vs. mental) in some output.\n\ndata(Parenting, package=\"heplots\")\nC &lt;- matrix(c(1, -.5, -.5,\n              0,  1,  -1), \n            nrow = 3, ncol = 2) |&gt; print()\n#      [,1] [,2]\n# [1,]  1.0    0\n# [2,] -0.5    1\n# [3,] -0.5   -1\ncontrasts(Parenting$group) &lt;- C\n\nExploratory plots\nBefore setting up a model and testing, it is well-advised to examine the data graphically. The simplest plots are side-by-side boxplots (or violin plots) for the three responses. With ggplot2, this is easily done by reshaping the data to long format and using faceting. In Figure 10.5, I’ve also plotted the group means with white dots.\n\nSee the ggplot codeparenting_long &lt;- Parenting |&gt;\n  tidyr::pivot_longer(cols=caring:play, \n                      names_to = \"variable\")\n\nggplot(parenting_long, \n       aes(x=group, y=value, fill=group)) +\n  geom_boxplot(outlier.size=2.5, \n               alpha=.5, \n               outlier.alpha = 0.9) + \n  stat_summary(fun=mean, \n               color=\"white\", \n               geom=\"point\", \n               size=2) +\n  scale_fill_hue(direction = -1) +     # reverse default colors\n  labs(y = \"Scale value\", x = \"Group\") +\n  facet_wrap(~ variable) +\n  theme_bw(base_size = 14) + \n  theme(legend.position=\"top\") +\n  theme(axis.text.x = element_text(angle = 15,\n                                   hjust = 1)) \n\n\n\n\n\n\nFigure 10.5: Faceted boxplots of scores on the three parenting scales, showing also the mean for each.\n\n\n\n\nIn this figure, differences among the groups on play are most apparent, with fathers of non-disabled children scoring highest. Differences among the groups on emotion are very small, but one high outlier for the fathers of mentally disabled children is apparent. On caring, fathers of children with a physical disability stand out as highest.\nFor exploratory purposes, you might also make a scatterplot matrix. Here, because the MLM assumes homogeneity of the variances and covariance matrices \\(\\mathbf{S}_i\\), I show only the data ellipses in scatterplot matrix format, using heplots:covEllipses() (with 50% coverage, for clarity):\n\ncolors &lt;- scales::hue_pal()(3) |&gt; rev()  # match color use in ggplot\ncovEllipses(cbind(caring, play, emotion) ~ group, \n  data=Parenting,\n  variables = 1:3,\n  fill = TRUE, fill.alpha = 0.2,\n  pooled = FALSE,\n  level = 0.50, \n  col = colors)\n\n\n\n\n\n\nFigure 10.6: Bivariate data ellipses for pairs of the three responses, showing the means, correlations and variances for the three groups.\n\n\n\n\nIf the covariance matrices were all the same, the data ellipses would have roughly the same size and orientation, but that is not the case in Figure 10.6. The normal group shows greater variability overall and the correlations among the measures differ somewhat from group to group. We’ll assess later whether this makes a difference in the conclusions that can be drawn (Chapter 12). The group centroids also differ, but the pattern is not particularly clear. We’ll see an easier to understand view in HE plots (Section 11.3) and their canonical discriminant cousins (Section 11.7).\nTesting the model\nLet’s proceed to fit the multivariate model predicting all three scales from the group factor. lm() for a multivariate response returns an object of class \"mlm\", for which there are many methods (use methods(class=\"mlm\") to find them).\n\nparenting.mlm &lt;- lm(cbind(caring, play, emotion) ~ group, \n                    data=Parenting) |&gt; print()\n# \n# Call:\n# lm(formula = cbind(caring, play, emotion) ~ group, data = Parenting)\n# \n# Coefficients:\n#              caring   play     emotion\n# (Intercept)   5.8833   4.6333   5.9167\n# group1       -0.3833   2.4167  -0.0667\n# group2        1.7750  -0.2250  -0.6000\n\nThe coefficients in this model are the values of the contrasts set up above. group1 is the mean of the typical group minus the average of the other two, which is negative on caring and emotion but positive for play. group2 is the difference in means for the physical vs. mental groups.\nBefore doing multivariate tests, it is useful to see what would happen if we ran univariate ANOVAs on each of the responses. These can be extracted from an MLM using stats::summary.aov() and they give tests of the model terms for each response variable separately:\n\nsummary.aov(parenting.mlm)\n#  Response caring :\n#             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n# group        2    130    65.2    18.6  6e-07 ***\n# Residuals   57    200     3.5                   \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n#  Response play :\n#             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n# group        2    177    88.6    27.6  4e-09 ***\n# Residuals   57    183     3.2                   \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n#  Response emotion :\n#             Df Sum Sq Mean Sq F value Pr(&gt;F)\n# group        2     15    7.27    1.02   0.37\n# Residuals   57    408    7.16\n\nFor a more condensed summary, you can instead extract the univariate model fit statistics from the \"mlm\" object using the heplots::glance() method for a multivariate model object. The code below selects just the \\(R^2\\) and \\(F\\)-statistic for the overall model for each response, together with the associated \\(p\\)-value.\n\nglance(parenting.mlm) |&gt;\n  select(response, r.squared, fstatistic, p.value)\n# # A tibble: 3 × 4\n#   response r.squared fstatistic       p.value\n#   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n# 1 caring      0.395       18.6  0.000000602  \n# 2 play        0.492       27.6  0.00000000405\n# 3 emotion     0.0344       1.02 0.369\n\nFrom this, one might conclude that there are differences only in caring and play and therefore ignore emotion, but this would be short-sighted. car::Anova(), shown below, gives the overall multivariate test \\(\\mathcal{H}_0: \\mathbf{B} = 0\\) of the group effect, that the groups don’t differ on any of the response variables. Note that this has a much smaller \\(p\\)-value than any of the univariate \\(F\\) tests.\n\nAnova(parenting.mlm)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#       Df test stat approx F num Df den Df Pr(&gt;F)    \n# group  2     0.948     16.8      6    112  9e-14 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova() returns an object of class \"Anova.mlm\" which has various methods. The summary() method for this gives more details, including all four test statistics. With \\(p=3\\) responses, these tests have \\(s = \\min(p, \\text{df}_h) = \\min(3,2) = 2\\) dimensions and the \\(F\\) approximations are not equivalent here. All four tests are highly significant, with Roy’s test giving the largest \\(F\\) statistic.\n\nparenting.summary &lt;- Anova(parenting.mlm) |&gt;  summary() \nprint(parenting.summary, SSP=FALSE)\n# \n# Type II MANOVA Tests:\n# \n# ------------------------------------------\n#  \n# Term: group \n# \n# Multivariate Tests: group\n#                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n# Pillai            2     0.948     16.8      6    112 9.0e-14 ***\n# Wilks             2     0.274     16.7      6    110 1.3e-13 ***\n# Hotelling-Lawley  2     1.840     16.6      6    108 1.8e-13 ***\n# Roy               2     1.108     20.7      3     56 3.8e-09 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe summary() method by default prints the SSH = \\(\\mathbf{H}\\) and SSE = \\(\\mathbf{E}\\) matrices, but I suppressed them above. The data structure returned contains nested elements which can be extracted more easily from the object using purrr::pluck():\n\nH &lt;- parenting.summary |&gt; \n  purrr::pluck(\"multivariate.tests\", \"group\", \"SSPH\") |&gt; \n  print()\n#         caring    play emotion\n# caring   130.4 -43.767 -41.833\n# play     -43.8 177.233   0.567\n# emotion  -41.8   0.567  14.533\nE &lt;- parenting.summary |&gt; \n  purrr::pluck(\"multivariate.tests\", \"group\", \"SSPE\") |&gt; \n  print()\n#         caring  play emotion\n# caring   199.8 -45.8    35.2\n# play     -45.8 182.7    80.6\n# emotion   35.2  80.6   408.0\n\nLinear hypotheses & contrasts\nWith three or more groups or with a more complex MANOVA design, contrasts provide a way of testing questions of substantive interest regarding differences among group means.\nThe test of the contrast comparing the typical group to the average of the others is the test using the contrast \\(c_1 = (1, -\\frac12, -\\frac12)\\) which produces the coefficients labeled \"group1\". The function car::linearHypothesis() carries out the multivariate test that this difference is zero. This is a 1 df test, so all four test statistics produce the same \\(F\\) and \\(p\\)-values.\n\ncoef(parenting.mlm)[\"group1\",]\n#  caring    play emotion \n# -0.3833  2.4167 -0.0667\nlinearHypothesis(parenting.mlm, \"group1\") |&gt; \n  print(SSP=FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n# Pillai            1     0.521     19.9      3     55 7.1e-09 ***\n# Wilks             1     0.479     19.9      3     55 7.1e-09 ***\n# Hotelling-Lawley  1     1.088     19.9      3     55 7.1e-09 ***\n# Roy               1     1.088     19.9      3     55 7.1e-09 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSimilarly, the difference between the physical and mental groups uses the contrast \\(c_2 = (0, 1, -1)\\) and the test that these means are equal is given by linearHypothesis() applied to group2.\n\ncoef(parenting.mlm)[\"group2\",]\n#  caring    play emotion \n#   1.775  -0.225  -0.600\nlinearHypothesis(parenting.mlm, \"group2\") |&gt; \n  print(SSP=FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df Pr(&gt;F)    \n# Pillai            1     0.429     13.8      3     55  8e-07 ***\n# Wilks             1     0.571     13.8      3     55  8e-07 ***\n# Hotelling-Lawley  1     0.752     13.8      3     55  8e-07 ***\n# Roy               1     0.752     13.8      3     55  8e-07 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlinearHypothesis() is very general. The second argument (hypothesis.matrix) corresponds to \\(\\mathbf{C}\\), and can be specified as numeric matrix giving the linear combinations of coefficients by rows to be tested, or a character vector giving the hypothesis in symbolic form; \"group1\" is equivalent to \"group1 = 0\".\nBecause the two contrasts used here are orthogonal, they add together to give the overall test of \\(\\mathbf{B} = \\mathbf{0}\\), which implies that the means of the three groups are all equal. The test below gives the same results as Anova(parenting.mlm).\n\nlinearHypothesis(parenting.mlm, c(\"group1\", \"group2\")) |&gt; \n  print(SSP=FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n# Pillai            2     0.948     16.8      6    112 9.0e-14 ***\n# Wilks             2     0.274     16.7      6    110 1.3e-13 ***\n# Hotelling-Lawley  2     1.840     16.6      6    108 1.8e-13 ***\n# Roy               2     1.108     20.7      3     56 3.8e-09 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n10.4.2 Ordered factors\nWhen groups are defined by an ordered factor, such as level of physical fitness (rated 1–5) or grade in school, it is tempting to treat that as a numeric variable and use a multivariate regression model. This would assume that the effect of that factor is linear and if not, we might consider adding polynomial terms.\nA different strategy, often preferable, is to make the group variable an ordered factor, for which R assigns polynomial contrasts. This gives separate tests of the linear, quadratic, cubic, … trends of the response, without the need to specify them separately in the model.\n\n10.4.3 Example: Adolescent mental health\nThe dataset AddHealth contains a large cross-sectional sample of participants from grades 7–12 from the National Longitudinal Study of Adolescent Health, described by Warne (2014). It contains responses to two Likert-scale (1–5) items, anxiety and depression. grade is an ordered factor, which means that the default contrasts are taken as orthogonal polynomials with linear (grade.L), quadratic (grade.Q), up to 5th degree (grade^5) trends, which decompose the total effect of grade.\n\ndata(AddHealth, package=\"heplots\")\nstr(AddHealth)\n# 'data.frame': 4344 obs. of  3 variables:\n#  $ grade     : Ord.factor w/ 6 levels \"7\"&lt;\"8\"&lt;\"9\"&lt;\"10\"&lt;..: 5 4 6 1 2 2 2 3 3 3 ...\n#  $ depression: int  0 0 0 0 0 0 0 0 1 2 ...\n#  $ anxiety   : int  0 0 0 1 1 0 0 1 1 0 ...\n\nThe research questions are:\n\nHow do the means for anxiety and depression vary separately with grade? Is there evidence for linear and nonlinear trends?\nHow do anxiety and depression vary jointly with grade?\nHow does the association (correlation, $R^2) of anxiety and depression vary with age?\n\nThe first question can be answered by fitting separate linear models for each response (e.g., lm(anxiety ~ grade))). However the second question is more interesting because it considers the two responses together and takes their correlation into account. This would be fit as the MLM:\n\\[\n\\mathbf{y} = \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1 x + \\boldsymbol{\\beta}_2 x^2 + \\cdots \\boldsymbol{\\beta}_5 x^5\n\\tag{10.13}\\]\nor, expressed in terms of the variables,\n\\[\\begin{aligned}\n\\begin{bmatrix} y_{\\text{anx}} \\\\y_{\\text{dep}} \\end{bmatrix} & =\n\\begin{bmatrix} \\beta_{0,\\text{anx}} \\\\ \\beta_{0,\\text{dep}} \\end{bmatrix} +\n\\begin{bmatrix} \\beta_{1,\\text{anx}} \\\\ \\beta_{1,\\text{dep}} \\end{bmatrix} \\text{grade} +\n\\begin{bmatrix} \\beta_{2,\\text{anx}} \\\\ \\beta_{2,\\text{dep}} \\end{bmatrix} \\text{grade}^2 \\\\\n& + \\cdots +\n\\begin{bmatrix} \\beta_{5,\\text{anx}} \\\\ \\beta_{5,\\text{dep}} \\end{bmatrix} \\text{grade}^5\n\\end{aligned}\\] \nWith grade represented as an ordered factor, the values of \\(x\\) in Equation 10.13 are those of the orthogonal polynomials given by poly(grade,5).\nExploratory plots\nSome exploratory analysis is useful before fitting and visualizing models. As a first step, we find the means, standard deviations, and standard errors of the means.\n\nmeans &lt;- AddHealth |&gt;\n  group_by(grade) |&gt;\n  summarise(\n    n = n(),\n    dep_sd = sd(depression, na.rm = TRUE),\n    anx_sd = sd(anxiety, na.rm = TRUE),\n    dep_se = dep_sd / sqrt(n),\n    anx_se = anx_sd / sqrt(n),\n    depression = mean(depression),\n    anxiety = mean(anxiety) ) |&gt; \n  relocate(depression, anxiety, .after = grade) |&gt;\n  print()\n# # A tibble: 6 × 8\n#   grade depression anxiety     n dep_sd anx_sd dep_se anx_se\n#   &lt;ord&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n# 1 7          0.881   0.751   622   1.11   1.05 0.0447 0.0420\n# 2 8          1.08    0.804   664   1.19   1.06 0.0461 0.0411\n# 3 9          1.17    0.934   778   1.19   1.08 0.0426 0.0387\n# 4 10         1.27    0.956   817   1.23   1.11 0.0431 0.0388\n# 5 11         1.37    1.12    790   1.20   1.16 0.0428 0.0411\n# 6 12         1.34    1.10    673   1.14   1.11 0.0439 0.0426\n\nNow, plot the means with \\(\\pm 1\\) error bars. It appears that average level of both depression and anxiety increase steadily with grade, except for grades 11 and 12 which don’t differ much. Alternatively, we could describe this as relationships that seem largely linear, with a hint of curvature at the upper end.\n\np1 &lt;-ggplot(data = means, aes(x = grade, y = anxiety)) +\n  geom_point(size = 4) +\n  geom_line(aes(group = 1), linewidth = 1.2) +\n  geom_errorbar(aes(ymin = anxiety - anx_se, \n                   ymax = anxiety + anx_se),\n                width = .2) \n\np2 &lt;-ggplot(data = means, aes(x = grade, y = depression)) +\n  geom_point(size = 4) +\n  geom_line(aes(group = 1), linewidth = 1.2) +\n  geom_errorbar(aes(ymin = depression - dep_se, \n                    ymax = depression + dep_se),\n                width = .2) \n\np1 + p2\n\n\n\n\n\n\nFigure 10.7: Means of anxiety and depression by grade, with \\(\\pm 1\\) standard error bars.\n\n\n\n\nIt is also useful to within-group correlations using covEllipses(), as shown in Figure 10.8. This also plots the bivariate means showing the form of the association , treating anxiety and depression as multivariate outcomes. (Because the variability of the scores within groups is so large compared to the range of the means, I show the data ellipses with coverage of only 10%.)\n\ncovEllipses(AddHealth[, 3:2], group = AddHealth$grade,\n            pooled = FALSE, level = 0.1,\n            center.cex = 2.5, cex = 1.5, cex.lab = 1.5,\n            fill = TRUE, fill.alpha = 0.05)\n\n\n\n\n\n\nFigure 10.8: Within-group covariance ellipses for the grade groups.\n\n\n\n\nThe bivariate means in Figure 10.8 increase with grade, but the relationship is curved rather than strictly linear.\nFit the MLM\nNow, let’s fit the MLM for both responses jointly in relation to grade. The null hypothesis is that the means for anxiety and depression are the same at all six grades,\n\\[\n\\mathcal{H}_0: \\boldsymbol{\\mu}_7 = \\boldsymbol{\\mu}_8 = \\cdots = \\boldsymbol{\\mu}_{12} \\; ,\n\\] or equivalently, that all coefficients except the intercept in the model Equation 10.13 are zero,\n\\[\n\\mathcal{H}_0: \\boldsymbol{\\beta}_1 =  \\boldsymbol{\\beta}_2  = \\cdots =  \\boldsymbol{\\beta}_5 = \\boldsymbol{0} \\; .\n\\]\nWe fit the MANOVA model, and test the grade effect using car::Anova(). The effect of grade is highly significant, as we could tell from Figure 10.7.\n\nAH.mlm &lt;- lm(cbind(anxiety, depression) ~ grade, data = AddHealth)\n\n# overall test of `grade`\nAnova(AH.mlm)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#       Df test stat approx F num Df den Df Pr(&gt;F)    \n# grade  5    0.0224     9.83     10   8676 &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHowever, the overall test, with 5 degrees of freedom is diffuse, in that it can be rejected if any pair of means differ. Given that grade is an ordered factor, it makes sense to examine narrower hypotheses of linear and nonlinear trends, car::linearHypothesis() on the coefficients of model AH.mlm.\n\ncoef(AH.mlm) |&gt; rownames()\n# [1] \"(Intercept)\" \"grade.L\"     \"grade.Q\"     \"grade.C\"    \n# [5] \"grade^4\"     \"grade^5\"\n\nThe joint test of the linear coefficients \\(\\boldsymbol{\\beta}_1 = (\\beta_{1,\\text{anx}},  \\beta_{1,\\text{dep}})^\\mathsf{T}\\) for anxiety and depression, \\(\\mathcal{H}_0 : \\boldsymbol{\\beta}_1 = \\boldsymbol{0}\\) is highly significant,\n\n## linear effect\nlinearHypothesis(AH.mlm, \"grade.L\") |&gt; print(SSP = FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df Pr(&gt;F)    \n# Pillai            1     0.019     42.5      2   4337 &lt;2e-16 ***\n# Wilks             1     0.981     42.5      2   4337 &lt;2e-16 ***\n# Hotelling-Lawley  1     0.020     42.5      2   4337 &lt;2e-16 ***\n# Roy               1     0.020     42.5      2   4337 &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe test of the quadratic coefficients \\(\\mathcal{H}_0 : \\boldsymbol{\\beta}_2 = \\boldsymbol{0}\\) indicates significant curvature in trends across grade, as we saw in the plots of their means in Figure 10.7. One interpretation might be that depression and anxiety after increasing steadily up to grade eleven could level off thereafter.\n\n## quadratic effect\nlinearHypothesis(AH.mlm, \"grade.Q\") |&gt; print(SSP = FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df Pr(&gt;F)  \n# Pillai            1     0.002     4.24      2   4337  0.014 *\n# Wilks             1     0.998     4.24      2   4337  0.014 *\n# Hotelling-Lawley  1     0.002     4.24      2   4337  0.014 *\n# Roy               1     0.002     4.24      2   4337  0.014 *\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAn advantage of linear hypotheses is that we can test several terms jointly. Of interest here is the hypothesis that all higher order terms beyond the quadratic are zero, \\(\\mathcal{H}_0 : \\boldsymbol{\\beta}_3 =  \\boldsymbol{\\beta}_4 =  \\boldsymbol{\\beta}_5 = \\boldsymbol{0}\\). Using linearHypothesis you can supply a vector of coefficient names to be tested for their joint effect when dropped from the model.\n\ncoefs &lt;- rownames(coef(AH.mlm)) |&gt; print()\n# [1] \"(Intercept)\" \"grade.L\"     \"grade.Q\"     \"grade.C\"    \n# [5] \"grade^4\"     \"grade^5\"\n## joint test of all higher terms\nlinearHypothesis(AH.mlm, coefs[3:5],\n                 title = \"Higher-order terms\") |&gt; \n  print(SSP = FALSE)\n# \n# Multivariate Tests: Higher-order terms\n#                  Df test stat approx F num Df den Df Pr(&gt;F)  \n# Pillai            3     0.002     1.70      6   8676   0.12  \n# Wilks             3     0.998     1.70      6   8674   0.12  \n# Hotelling-Lawley  3     0.002     1.70      6   8672   0.12  \n# Roy               3     0.002     2.98      3   4338   0.03 *\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#sec-factorial-manova",
    "href": "11-mlm-review.html#sec-factorial-manova",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.5 Factorial MANOVA",
    "text": "10.5 Factorial MANOVA\nWhen there are two or more categorical factors, the general linear model provides a way to investigate the effects (differences in means) of each simultaneously. More importantly, this allows you to determine if factors interact, so the effect of one factor varies depending on the levels of another factor. For instance, the effect of a weight loss drug treatment may vary with the patient’s ethnicity or age group.\nIn such situations, when an interaction is significant it is unwise to interpret the overall main effects of treatment and the other factor. Instead, you would look at the “simple effects”– how the treatment works at specific levels of the other factor separately.\n\n\n\nExample 10.1 Plastic film data\nAn industrial experiment was conducted to determine the optimal conditions for extruding plastic film. Of interest were three responses: resistance to tear, film gloss and the opacity of the film. Two factors were manipulated, both at two levels, labeled High and Low: change in rate of extrusion (-10%, +10%) and amount of some additive (1%, 1.5%), with \\(n=5\\) runs at each combination of the factor levels. The dataset heplots::Plastic comes from Johnson & Wichern (1998), Example 6.12.\n\ndata(Plastic, package=\"heplots\")\nstr(Plastic)\n# 'data.frame': 20 obs. of  5 variables:\n#  $ tear    : num  6.5 6.2 5.8 6.5 6.5 6.9 7.2 6.9 6.1 6.3 ...\n#  $ gloss   : num  9.5 9.9 9.6 9.6 9.2 9.1 10 9.9 9.5 9.4 ...\n#  $ opacity : num  4.4 6.4 3 4.1 0.8 5.7 2 3.9 1.9 5.7 ...\n#  $ rate    : Factor w/ 2 levels \"Low\",\"High\": 1 1 1 1 1 1 1 1 1 1 ...\n#  $ additive: Factor w/ 2 levels \"Low\",\"High\": 1 1 1 1 1 2 2 2 2 2 ...\n\nMultivariate tests\nThe MANOVA model plastic.mod fits the main effects of rate and additive and the interaction rate:additive. The Anova() summary shows a strong effect for rate of extrusions for the three responses jointly, a lesser, but still significant effect of additive and a non-significant interaction.\n\nplastic.mod &lt;- lm(cbind(tear, gloss, opacity) ~ rate*additive, \n                  data=Plastic)\nAnova(plastic.mod)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#               Df test stat approx F num Df den Df Pr(&gt;F)   \n# rate           1     0.618     7.55      3     14  0.003 **\n# additive       1     0.477     4.26      3     14  0.025 * \n# rate:additive  1     0.223     1.34      3     14  0.302   \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAs a reminder, if you want to see the results of univariate tests for the responses separately, heplots::uniStats() (or heplots::glance.mlm()) gives some answers, including \\(R^2\\) and univariate \\(F\\) tests.\n\nuniStats(plastic.mod)\n# Univariate tests for responses in the multivariate linear model plastic.mod \n# \n#           R^2    F df1 df2 Pr(&gt;F)   \n# tear    0.586 7.56   3  16 0.0023 **\n# gloss   0.483 4.99   3  16 0.0125 * \n# opacity 0.125 0.76   3  16 0.5315   \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNote that these two summaries are complementary. Anova() collapses over the responses to give overall multivariate tests for each model term. uniStats() shows only the overall statistics for each response variable, combining the effects of all terms, and testing against the null model that none of them contribute to that response.\nPlotting main effects and interactions\nTo understand main effects and interactions in a simple two-way MANOVA design, the simplest thing to do is to plot the cell means and error bars in a traditional line plot with one factor on the horizontal axis and the other as separate lines within the panel and with one panel for each response variable.\nThis is slightly awkward using ggplot2 directly: you have to plot the points and lines, but to plot error bars with geom_errorbar() you must calculate the standard errors and work out the upper and lower limits. In Figure 10.9 I use ggpubr::ggline(), which simplifies such plots, though at the expense of a bit of control for ggplot2.5 The argument add = c(\"mean_se\") draws error bars6 showing \\(\\pm 1\\) standard error around the mean.\n\nlegend_inside &lt;- function(position) {     # simplify legend placement\n  theme(legend.position = \"inside\",\n        legend.position.inside = position)\n}\n\np1 &lt;- ggline(Plastic, \n  x = \"rate\", y = \"tear\",\n  color = \"additive\", shape = \"additive\", linetype = \"additive\",\n  add = c(\"mean_se\"), position = position_dodge(width = .1),\n  point.size = 5, linewidth = 1.5,\n  palette = c(\"red\", \"blue\"),\n  ggtheme = theme_pubr(base_size = 16)\n  ) +\n  xlab(\"Rate of extrusion\") +\n  ylab(\"Tear resistance\") +\n  legend_inside(c(.25, .8)) \n\np2 &lt;- ggline(Plastic, \n  x = \"rate\", y = \"gloss\",\n  color = \"additive\", shape = \"additive\", linetype = \"additive\",\n  add = c(\"mean_se\"), position = position_dodge(width = .1),\n  palette = c(\"red\", \"blue\"),\n  point.size = 5, linewidth = 1.5,\n  ggtheme = theme_pubr(base_size = 16)\n  ) +\n  xlab(\"Rate of extrusion\") +\n  ylab(\"Film gloss\") +\n  theme(legend.position = \"none\")\n\np1 + p2 \n\n\n\n\n\n\nFigure 10.9: Line plots of means and their standard errors for the response tear (left) and gloss (right) in the plastic film data.\n\n\n\n\nResistance to tear is greater with the high rate of extrusion and high level of the additive. The lines are parallel, so there is no interaction. In the panel for gloss, the means for additive don’t differ at the high rate, but do so substantially at the low extrusion rate, giving rise to the interaction for this outcome.\nSuch univariate plots are certainly useful for this simple \\(2 \\times 2\\) design with two response variables, but you can imagine that they get more complicated, both to construct and to understand with larger designs and more response variables. As well, plotting the responses separately give no information on how the outcomes vary jointly. I return to this data in Example 11.2, where I show how HE plots can give greater insight in this situation.\n\n\nExample 10.2 Does defendant physical attractiveness affect jury decisions?\nIn a social psychology study of influences on jury decisions by Plaster (1989), male participants (prison inmates) were shown a picture of one of three young women. Pilot work had indicated that one woman’s photo was “beautiful”, another of “average” physical attractiveness, and the third “unattractive”. Participants rated the woman they saw on each of twelve attributes on scales of 1–9. These measures were used to check on the manipulation of “attractiveness” by the photo.\nIn the main part of the study, other participants were told that the person in the photo had committed a non-violent crime, either burglary or swindling. They were asked to rate the seriousness of the crime and recommend a prison sentence, in Years.\nThe data are contained in the data frame heplots::MockJury. Only the first four are analyzed here: Attr, Crime, Year and Serious. The remaining twelve, exciting … ownPA relate to validity tests of whether the attractiveness classification Attr of the photo captures the essence of the attribute ratings.\n\ndata(MockJury, package = \"heplots\")\nnames(MockJury)\n#  [1] \"Attr\"          \"Crime\"         \"Years\"         \"Serious\"      \n#  [5] \"exciting\"      \"calm\"          \"independent\"   \"sincere\"      \n#  [9] \"warm\"          \"phyattr\"       \"sociable\"      \"kind\"         \n# [13] \"intelligent\"   \"strong\"        \"sophisticated\" \"happy\"        \n# [17] \"ownPA\"\n\nSample sizes were roughly balanced for the independent variables in the three conditions of the attractiveness of the photo, and the two combinations of this with Crime, which was burglary or swindling, giving a \\(3 \\times 2\\) factorial design.\n\ntable(MockJury$Attr)\n# \n#    Beautiful      Average Unattractive \n#           39           38           37\ntable(MockJury$Attr, MockJury$Crime)\n#               \n#                Burglary Swindle\n#   Beautiful          21      18\n#   Average            18      20\n#   Unattractive       20      17\n\nThe main questions of interest were:\n\nDoes attractiveness of the “defendant” influence the sentence or perceived seriousness of the crime?\n\nDoes attractiveness interact with the nature of the crime? That is, does attractiveness have the same pattern of means for both crimes?\n\nSo, I carry out a two-way MANOVA of the responses Years and Serious in relation to the independent variables Attr and Crime.\n\n# influence of Attr of photo and nature of crime on Serious and Years\njury.mod &lt;- lm( cbind(Serious, Years) ~ Attr * Crime, data=MockJury)\nAnova(jury.mod, test = \"Roy\")\n# \n# Type II MANOVA Tests: Roy test statistic\n#            Df test stat approx F num Df den Df Pr(&gt;F)  \n# Attr        2    0.0756     4.08      2    108  0.020 *\n# Crime       1    0.0047     0.25      2    107  0.778  \n# Attr:Crime  2    0.0501     2.71      2    108  0.071 .\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe see that there is a strong main effect of Attr, no overall effect of Crime, and a nearly significant interaction between Attr and Crime. To probe these multivariate tests, you can also examine the univariate results for each response.\n\nuniStats(jury.mod)\n# Univariate tests for responses in the multivariate linear model jury.mod \n# \n#            R^2    F df1 df2 Pr(&gt;F)  \n# Serious 0.0117 0.26   5 108  0.936  \n# Years   0.0843 1.99   5 108  0.086 .\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAgain, you can interpret these results more easily from line plots of the means and standard error bars. In Figure 10.10, I assign attractiveness to the horizontal axis and plot separate lines for the types of crime.\n\np1 &lt;- ggline(MockJury, \n  x = \"Attr\", y = \"Years\",\n  color = \"Crime\", shape = \"Crime\", linetype = \"Crime\",\n  add = c(\"mean_se\"), position = position_dodge(width = .1),\n  point.size = 5, linewidth = 1.5,\n  palette = c(\"blue\", \"darkorange2\"),\n  ggtheme = theme_pubr(base_size = 16)\n  ) +\n  xlab(\"Physical attractiveness of photo\") +\n  ylab(\"Recommended years of sentence\") +\n  legend_inside(c(.25, .9))\n\np2 &lt;- ggline(MockJury, \n  x = \"Attr\", y = \"Serious\",\n  color = \"Crime\", shape = \"Crime\", linetype = \"Crime\",\n  add = c(\"mean_se\"), position = position_dodge(width = .1),\n  point.size = 5,  linewidth = 1.5,\n  palette = c(\"blue\", \"darkorange2\"),\n  ggtheme = theme_pubr(base_size = 16) \n  ) +\n  xlab(\"Physical attractiveness of photo\") +\n  ylab(\"Seriousness of crime\") +\n  legend_inside(c(.75, .9))\n\np1 + p2\n\n\n\n\n\n\nFigure 10.10: Line plots of means and their standard errors for the response Years (left) and Serious (right) in the Mock Jury data.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#sec-MRA-to-MMRA",
    "href": "11-mlm-review.html#sec-MRA-to-MMRA",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.6 MRA \\(\\rightarrow\\) MMRA",
    "text": "10.6 MRA \\(\\rightarrow\\) MMRA\nWhen all predictor variables are quantitative, the MLM Equation 10.2 becomes the extension of univariate multiple regression analysis (MRA) to the situation where there are \\(p\\) response variables (MMRA). Just as in univariate models, we might want to test hypotheses about subsets of the predictors, for example when some predictors are meant as controls or things you might want to adjust for in assessing the effects of predictors of main interest.\nBut first, there are a couple of aspects of statistical practice that should be mentioned.\nModel selection is one topic where univariate and multivariate approaches differ. When there are more than a few predictors, approaches like hierarchical regression, LASSO (Tibshirani, 1996) and stepwise selection can be used to eliminate uninformative predictors for each response.7 But this gives a different models for each response, based on the predictors included, each with its own interpretation. In contrast, the multivariate approach considers the outcome variables collectively. You can eliminate predictors that are unimportant, but the mechanics are geared toward removing them from the models for all responses.\nOverall tests In a one-way ANOVA, to control for multiple testing, it is common practice to carry out an overall \\(F\\)-test to see if the group means differ collectively before testing comparison between specific groups. Similarly, in univariate multiple regression, researchers sometimes report an overall \\(F\\)-test or test of the \\(R^2\\) so they can reject the hypothesis that all predictors have no effect, before considering them individually.\nSimilarly, the the case of multivariate linear models, some consider it necessary to reject the multivariate null hypothesis for a predictor term before considering how it contributes to each of the response variables. Some further suggest that the individual univariate models be tested after an overall significant effect. I believe the first of these is wise, but the second might be too much to require when the general goal is to understand the data.\n\n10.6.1 Example: NLSY data\nThe dataset NLSY comes from a small part of the National Longitudinal Survey of Youth, a series of annual surveys conducted by the U.S. Department of Labor to examine the transition of young people into the labor force. This particular subset gives measures of 243 children on mathematics and reading achievement and also measures of behavioral problems (antisocial, hyperactivity). Also available are the yearly income and education of the child’s father.\nIn this analysis the math and read scores are taken at the outcome variables.8 Among the remaining predictors, income and educ might be considered as background variables necessary to control for. Interest might then be focused on whether the behavioral variables antisoc and hyperact contribute beyond that.\n\ndata(NLSY, package = \"heplots\")\nstr(NLSY)\n# 'data.frame': 243 obs. of  6 variables:\n#  $ math    : num  50 28.6 50 32.1 21.4 ...\n#  $ read    : num  45.2 28.6 53.6 34.5 22.6 ...\n#  $ antisoc : int  4 0 2 0 0 1 0 1 1 4 ...\n#  $ hyperact: int  3 0 2 2 2 0 1 4 3 5 ...\n#  $ income  : num  52.52 42.6 50 6.08 7.41 ...\n#  $ educ    : int  14 12 12 12 14 12 12 12 12 9 ...\n\nExploratory plots\nTo begin, I would examine some scatterplots and univariate displays. I’ll start with density plots for all the variables to see the shapes of their distributions, wikh rug plots at the bottom to show where the observations are located. From Figure 10.11 we see that math and reading scores are positively skewed, anti-social and hyperactivity have distributions highly concentrated in the lower scores. As we would suspect, father’s income is quite positively skewed. Father’s education is reasonably symmetric, but highly peaked at 12 years of schooling in this sample. The spikes reflect the fact that education is measured in discrete years.\n\nNLSY_long &lt;- NLSY |&gt; \n  tidyr::pivot_longer(math:educ, names_to = \"variable\") |&gt;\n  dplyr::mutate(variable = forcats::fct_inorder(variable))\n\nggplot(NLSY_long, aes(x=value, fill=variable)) +\n  geom_density(alpha = 0.5) +\n  geom_rug() +\n  facet_wrap(~variable, scales=\"free\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\nFigure 10.11: Density plots for the variables in the NLSY dataset.\n\n\n\n\nIn terms of an analysis focused on math and read as outcomes, a scatterplot of one against the other is useful, as is collection of scatterplots of each against the remaining variables. The second of these is left as an exercise to the reader.\n\nset.seed(47)\nggplot(NLSY, aes(x = read, y = math)) +\n  geom_jitter()+\n  geom_smooth(method = lm, formula = y~x, fill = \"blue\", alpha = 0.2) +\n  geom_smooth(method = loess, se = FALSE, color = \"red\", linewidth = 2)\n\n\n\n\n\n\nFigure 10.12: Scatterplot of mathematics score against reading score in the NLSY data\n\n\n\n\nThe non-linear trend in Figure 10.12 may be due to the sparsity of data in the upper range of reading, and there are also a few unusual points shown in this plot. The function heplots::noteworthy() provides a variety of methods to identify such noteworthy points in scatterplots. The default method uses Mahalanobis \\(D^2\\). The plot below labels the five largest observations.\n\nids &lt;- heplots::noteworthy(NLSY[, 1:2], method = \"mahal\", n=5)\nggplot(NLSY, aes(x = read, y = math)) +\n  geom_jitter()+\n  geom_smooth(method = lm, formula = y~x, fill = \"blue\", alpha = 0.2) +\n  geom_text(data = NLSY[ids, ], label = ids, size = 5, nudge_y = 2) \n\n\n\n\n\n\nFigure 10.13: Scatterplot of mathematics score against reading score in the NLSY data, highlighting noteworthy points\n\n\n\n\nFitting models\nWe could of course include all of the predictors in a single model, and perhaps be done with it. To develop some model-thinking, it is more useful to proceed in smaller steps to see what we can learn from each. If we view parents’ income and education as the most obvious predictors of reading and mathematics scores, those are the variables to fit first.\n\n\n\n\n\nNLSY.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY)\n\nAnova(NLSY.mod1)  # Type II, partial test\n# \n# Type II MANOVA Tests: Pillai test statistic\n#        Df test stat approx F num Df den Df Pr(&gt;F)   \n# income  1    0.0345     4.27      2    239 0.0151 * \n# educ    1    0.0515     6.49      2    239 0.0018 **\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nOverall test\nThe Anova() results above give multivariate tests of the contributions of each predictor separately to explaining reading and math and how they vary together. To get an overall test of the global null hypothesis \\(\\mathcal{H}_0 : \\mathbf{B} =(\\boldsymbol{\\beta}_{\\text{inc}}, \\boldsymbol{\\beta}_{\\text{educ}}) =\\mathbf{0}\\) for all predictors together, you can use linearHypothesis():\n\ncoefs &lt;- rownames(coef(NLSY.mod1))[-1]\nlinearHypothesis(NLSY.mod1, coefs, title = \"income, educ = 0\") |&gt; \n  print(SSP = FALSE)\n# \n# Multivariate Tests: income, educ = 0\n#                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n# Pillai            2     0.117     7.44      4    480 8.1e-06 ***\n# Wilks             2     0.884     7.59      4    478 6.2e-06 ***\n# Hotelling-Lawley  2     0.130     7.75      4    476 4.7e-06 ***\n# Roy               2     0.123    14.79      2    240 8.7e-07 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis joint multivariate test is more highly significant than either of those for the separate effects of the predictors, again because it pools strength.\nCoefficient plots\nAs usual, you can display the coefficients using coef(). The tidy method for \"mlm\" objects defined in heplots shows these in a tidy format with \\(t\\)-tests for each coefficient., arranged by the response variable.\n\ncoef(NLSY.mod1)\n#                read   math\n# (Intercept) 15.8848 8.7829\n# income       0.0137 0.0893\n# educ         0.9495 1.2755\n\ntidy(NLSY.mod1)\n# # A tibble: 6 × 6\n#   response term        estimate std.error statistic  p.value\n#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n# 1 read     (Intercept)  15.9       4.25       3.74  0.000233\n# 2 read     income        0.0137    0.0325     0.420 0.675   \n# 3 read     educ          0.949     0.360      2.64  0.00894 \n# 4 math     (Intercept)   8.78      4.33       2.03  0.0437  \n# 5 math     income        0.0893    0.0331     2.70  0.00749 \n# 6 math     educ          1.28      0.367      3.47  0.000607\n\nHowever, a bivariate plot of these coefficients is more useful, because it provides visual tests of multivariate hypotheses. heplots::coefplot.mlm() gives displays of the coefficients for a given pair of response variables. For interpretation, it adds the bivariate confidence ellipse for the coefficients, as well as univariate confidence intervals for each response. The univariate intervals are simply the horizontal and vertical shadows of the ellipses on the response variable axes.\nA wrinkle here, as in Section 6.2, is that the coefficients are measured in different units and so coefficient plots for different predictors are more easily compared for standardized variables. To do this, I first re-fit the model using scale(NLSY) …\n\nNLSY_std &lt;- scale(NLSY) |&gt;\n  as.data.frame()\n\nNLSY_std.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY_std)\n\n\ncoefplot(NLSY_std.mod1, fill = TRUE,\n   col = c(\"darkgreen\", \"brown\"),\n   lwd = 2,\n   cex.lab = 1.5,\n   ylim = c(-0.1, 0.5),\n   xlab = \"read coefficient (std)\",\n   ylab = \"math coefficient (std)\")\n\n\n\n\n\n\nFigure 10.14: Bivariate coefficient plot for reading and math with 95% confidence ellipses. The variables have been standardized to make their units comparable.\n\n\n\n\nIn Figure 10.14, the confidence ellipses for income and educ both exclude the origin, which represents the multivariate hypothesis \\(\\mathcal{H}_0 : ( \\beta_\\textrm{read}, \\beta_\\textrm{math} ) = (0, \\, 0)\\), so this hypothesis is rejected. Note that if we only examined the univariate tests for each of the four parameters, we would conclude that for reading, income is not a significant predictor. The orientation of the confidence ellipses indicates the positive correlation between reading and mathematics scores.\n\n10.6.1.1 Behavioral measures\nGiven that the parental background variables are highly predictive of student performance, we might want to know if the behavioral measures antisoc and hyperact add importantly to this. One way to do this is to add these predictors to the model and test for their additional contributions over and above the baseline model.\nYou can do this using update(). In the model formula, “.” on the left hand side corresponds to the previous \\(y\\) variables; on the right-hand side it refers to the \\(x\\)s in the previous model, so I just add the new predictors to that.\n\nNLSY.mod2 &lt;- update(NLSY.mod1, . ~ . + antisoc + hyperact)\nAnova(NLSY.mod2)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#          Df test stat approx F num Df den Df Pr(&gt;F)   \n# income    1    0.0383     4.72      2    237 0.0098 **\n# educ      1    0.0532     6.65      2    237 0.0015 **\n# antisoc   1    0.0193     2.34      2    237 0.0988 . \n# hyperact  1    0.0144     1.74      2    237 0.1784   \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nEach of these new predictors are individually non-significant according to the Type II tests. Using linearHypothesis() you can test them jointly:\n\ncoefs &lt;- rownames(coef(NLSY.mod2))[-1] |&gt; print()\n# [1] \"income\"   \"educ\"     \"antisoc\"  \"hyperact\"\n\nlinearHypothesis(NLSY.mod2, coefs[3:4], \n                 title = \"NLSY.mod2 | NLSY.mod1\") |&gt;\n  print(SSP = FALSE)\n# \n# Multivariate Tests: NLSY.mod2 | NLSY.mod1\n#                  Df test stat approx F num Df den Df Pr(&gt;F)  \n# Pillai            2     0.024     1.45      4    476  0.218  \n# Wilks             2     0.976     1.44      4    474  0.218  \n# Hotelling-Lawley  2     0.024     1.44      4    472  0.218  \n# Roy               2     0.022     2.64      2    238  0.073 .\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n10.6.2 Example: School data\nCharnes et al. (1981) describe a large scale “social experiment” in public school education. Seventy school sites across the U.S. participated and a number of variables related to attributes of parents and teachers were used to predict aspects of students’ success in academic indicators (reading, mathematics), but also in their self-esteem. It was conceived in the late 1960’s in relation to a federally sponsored program charged with providing remedial assistance to educationally disadvantaged early primary school students.\nThe study was focused on the management styles used to guide educational planning across schools. In particular, it was primarily designed to compare schools using Program Follow Through (PFT) management methods of taking actions to achieve goals with those of Non Follow Through (NFT).\nHere, I simply focus on the relations between outcome scores on tests of reading, mathematics and self-esteem in relation to five explanatory variables related to parents and teachers:\n\n\neducation level of mother as measured by the percentage of high school graduates among female parents.\n\noccupation, highest occupation of a family member on a rating scale.\n\nvisit, an index of the number of parental visits to the school site.\n\ncounseling, a measure calculated from data on time spent with child on school-related topics such as reading together, etc.\n\nteacher, number of teachers at the given site.\n\nThe dataset, given in schooldata contains observations for 70 schools.9\nExploratory plots\nThere are eight variables in this example, so a scatterplot matrix or even a corrgram might not be sufficiently revealing. As usual, I tried a number of different methods and found a couple that were interesting and useful.\nMultivariate normality is not required for all the variables in \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\)— it is only required for the residuals, \\(\\boldsymbol{\\Large\\varepsilon}= \\mathbf{Y} - \\widehat{\\mathbf{Y}}\\). Yet, for MMRA problems, sometimes an initial \\(\\chi^2\\) QQ plot provides a handy way to flag possibly unusual values to pay attention to as the analysis proceeds. In Figure 10.15 we see five cases outside the 95% confidence envelope.\n\ndata(schooldata, package = \"heplots\")\nres &lt;- cqplot(schooldata, id.n = 5) |&gt; print()\n#     DSQ quantile       p\n# 59 44.6     21.0 0.00714\n# 44 38.8     18.0 0.02143\n# 33 27.9     16.5 0.03571\n# 66 24.0     15.5 0.05000\n# 35 21.7     14.7 0.06429\n\n# save the case ID numbers\noutliers &lt;- rownames(res) |&gt; as.numeric() |&gt; print()\n# [1] 59 44 33 66 35\n\n\n\n\n\n\nFigure 10.15: \\(\\chi^2\\) QQ plot of the schooldata variables.\n\n\n\n\nRather than a complete \\(8 \\times 8\\) scatterplot matrix, it is useful here to examine the scatterplots only for each \\(y\\) variable against each of the predictors in \\(\\mathbf{X}\\).10 I’ll take steps to flag some of these possibly unusual cases to see where they appear in these pairwise relations.\nTo prepare for this with ggplot2, it is necessary to reshape the data to long format twice—once for the (\\(q=5\\)) \\(x\\) variables and again for the (\\(p=3\\)) \\(y\\) responses to get all of their \\(q \\times p\\) combinations. That way, we get a data set with variables x and y whose variable names are by xvar and yvar.\n\n# plot predictors vs each response\nxvars &lt;- names(schooldata)[1:5]\nyvars &lt;- names(schooldata)[6:8]\n\nschool_long &lt;- schooldata |&gt;\n  tibble::rownames_to_column(var = \"site\") |&gt;\n  pivot_longer(cols = all_of(xvars), \n               names_to = \"xvar\", values_to = \"x\") |&gt;\n  pivot_longer(cols = all_of(yvars), \n               names_to = \"yvar\", values_to = \"y\") |&gt;\n  mutate(xvar = factor(xvar, xvars), \n         yvar = factor(yvar, yvars))\n\ncar::some(school_long, n=8)\n# # A tibble: 8 × 5\n#   site  xvar           x yvar            y\n#   &lt;chr&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n# 1 1     teacher     9    reading     54.5 \n# 2 18    education  28    mathematics 38.2 \n# 3 26    teacher     7    selfesteem  31.2 \n# 4 49    occupation  5.29 reading     12.2 \n# 5 53    counseling 26.3  mathematics 22.0 \n# 6 61    occupation  2.59 mathematics  7.1 \n# 7 62    visit       9.89 reading      9.35\n# 8 64    occupation  8.91 mathematics 24.5\n\nWith this data structure, each scatterplot is a plot of a y against and x, and we can facet this using facet_grid(yvar ~ xvar), giving Figure 10.16.\n\np1 &lt;- ggplot(school_long, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x) +\n  stat_ellipse(geom = \"polygon\", \n               level = 0.95, fill = \"blue\", alpha = 0.2) +\n  facet_grid(yvar ~ xvar, scales = \"free\") +\n  labs(x = \"predictor\", y = \"response\") +\n  theme_bw(base_size = 16)\n\n# label the 3 most unusual points in each panel\np1 + geom_text_repel(data = school_long |&gt; \n                       filter(site %in% outliers[1:3]), \n                     aes(label = site))\n\n\n\n\n\n\nFigure 10.16: Scatterplots of each of the three response variables against each of the five predictors in the schooldata dataset. Three of the points identified as possible multivariate outliers are labeled.\n\n\n\n\nAll of the predictors except for number of teachers show very strong linear relations with the outcome scores. Among the identified points, cases 44 and 59 stand out in all the plots, case 59 being particularly high on all the measures. As well, there is a small cluster of unusual points in the plots for number of teachers.\nFitting models\nLet’s proceed to fit the multivariate regression model. Here “.” on the right-hand side of the model formula means all the other variables in the dataset.\n\nschool.mod &lt;- lm(cbind(reading, mathematics, selfesteem) ~ ., \n                 data=schooldata)\ncar::Anova(school.mod)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#            Df test stat approx F num Df den Df  Pr(&gt;F)    \n# education   1     0.376    12.43      3     62 1.8e-06 ***\n# occupation  1     0.567    27.02      3     62 2.7e-11 ***\n# visit       1     0.260     7.27      3     62 0.00029 ***\n# counseling  1     0.065     1.43      3     62 0.24297    \n# teacher     1     0.049     1.07      3     62 0.37003    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThese multivariate tests have a seemingly simple interpretation: parent’s education and occupation and their visits to the schools are highly predictive of student’s outcomes; their counseling efforts and the number of teachers in the schools, not so much.\nYou can get an assessment of the strength of multivariate association from the \\(R^2\\) for each of the responses using glance() for the MLM. All of these are very high.\n\nglance(school.mod)\n# # A tibble: 3 × 8\n#   response    r.squared sigma fstatistic numdf dendf  p.value  nobs\n#   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n# 1 reading         0.929  4.83       167.     5    64 2.34e-35    70\n# 2 mathematics     0.917  6.16       141.     5    64 3.37e-33    70\n# 3 selfesteem      0.993  1.17      1852.     5    64 8.47e-68    70\n\nSimilarly etasq() for an MLM gives an \\(R^2\\)-like measure called \\(\\eta^2\\) of the partial association accounted for each of the predictor terms in the model. These are analogous to the Type II tests from Anova(), which test the additional contribution of each term in the model beyond all the others.\nIn spite of the overwhelming significance of the first three predictors, their variance accounted for is more modest. It is highest for parent’s occupation, followed by education. Parent counseling and teachers contribute very little.\n\netasq(school.mod)\n#             eta^2\n# education  0.3756\n# occupation 0.5666\n# visit      0.2603\n# counseling 0.0647\n# teacher    0.0491",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#sec-model-diagnostics-MLM",
    "href": "11-mlm-review.html#sec-model-diagnostics-MLM",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.7 Model diagnostics for MLMs",
    "text": "10.7 Model diagnostics for MLMs\nModel building, visualization and interpretation is often an iterative process. You fit a model and calculate some goodness of fit measures (\\(R^2\\) for responses, \\(\\eta^2\\) for predictors). If these are reasonably strong, you feel happy and proceed to graphical displays to help you understand what you’ve found and explain it to others.\nBut wait: did you check the assumptions of the MLM? As in univariate models, diagnostic plots can help you spot problems in the data (unusual cases) or in the model (nonlinear relations, omitted predictors or interactions). You sometimes need to go circle back and fit a revised model, starting the process again.\n\n\nFor multivariate regression models, I consider the assumed multivariate normality of residuals and multivariate influence here. For MANOVA models, the question of homogeneity of covariance matrices is deferred until Chapter 12.\n\n10.7.1 Multivariate normality of residuals\nOne easy thing to do is to check for multivariate normality of the residuals. Given that we found a few noteworthy points in Figure 10.15, a \\(\\chi^2\\) QQ plot of the residuals in the model will tell us if any of these are really problematic. The pattern of points relative to the confidence band gives a rough indication of overall multivariate normality.\n\ncqplot(school.mod, id.n = 5)\n\n\n\n\n\n\nFigure 10.17: \\(\\chi^2\\) QQ plot of the residuals in the schooldat multivariate regression model.\n\n\n\n\nSo, you can see that among the cases that stood out in the cqplot() of the observed variables (Figure 10.15), only case 35 attracts attention here, and it is well within the confidence band. Case 59, which was the largest in all the pairwise scatterplots (Figure 10.16) seems not unusual in the fitted model. It is a high-leverage point, but appeared to be well-fitted in all the simple regressions, except in those for teacher.\nIt is useful to contrast this with what we get from formal tests that the residuals are strictly multivariate normal. The MVN package (Korkmaz et al., 2014) provides mvn() for this, which performs a wide variety of normality tests. The most widely used of these is due to Mardia (1974), which gives multivariate tests of skewness (lack of symmetry) and kurtosis (length of the tails).\nApplying this to the residuals from the schools multivariate regression shows that multivariate normality is rejected here. Based on other evidence, this doesn’t seem particularly troubling.\n\nschool.mvn &lt;- mvn(residuals, mvn_test = \"mardia\")\n# print multivariate and univariate tests\nsummary(school.mvn, select = \"mvn\")\n#              Test Statistic p.value     Method      MVN\n# 1 Mardia Skewness      1.92   0.750 asymptotic ✓ Normal\n# 2 Mardia Kurtosis     -1.16   0.244 asymptotic ✓ Normal\nsummary(school.mvn, select = \"univariate\")\n#               Test Variable Statistic p.value Normality\n# 1 Anderson-Darling    start     0.307   0.524  ✓ Normal\n# 2 Anderson-Darling   amount     0.625   0.085  ✓ Normal\n\nmvn() also provides a variety of tests for univariate normality for each of the response variables. These are all OK.\n\n10.7.2 Distance plot\nAnother useful screening plot (suggested by Rousseeuw et al. (2004)) is a plot of Mahalanobis distances of the predictors against the Mahalanobis distances of the corresponding residuals for a fitted model. This diagnostic plot combines the information on leverage points and regression outliers in an interesting way. \nIt is much more useful than plotting them individually (against the theoretical values) as in a \\(\\chi^2\\) QQ plot. Moreover, plotting them against each other is visually informative, because it places the leverage points (unusual in \\(\\mathbf{X}\\)}) and the outliers (unusual in \\(\\mathbf{Y}\\)}) in different regions of the plot. To judge notably “large” values, they suggest using cutoffs of \\(\\sqrt{\\chi^2_{p}(0.975)}\\) for the predictor distances and \\(\\sqrt{\\chi^2_{q}(0.975)}\\) for the residuals.\nSuch plots are produced by heplots::distPlot(). Running this on the school.mod model gives Figure 10.18. Points beyond the horizontal and vertical cutoff values are labeled with their case numbers.\n\ndistancePlot(school.mod, cex = 1.5, cex.lab = 1.2)\n# 0.975 X, Y distance cutoffs: 3.58 3.06\n\n\n\n\n\n\nFigure 10.18: Plot of Mahalanobis distances of the least squares residuals vs. Mahalanobis distances of the predictors in the model\n\n\n\n\nMost of the points identified in the \\(\\chi^2\\) QQ plot Figure 10.15 are labeled here. Cases 44 and 59 are high leverage points with large \\(\\mathbf{X}\\) distances. Case 35 is the only one beyond the cutoff for residuals. Interestingly, case 66 appeared near 35 in Figure 10.15, but is unusual for a different reason as we can see in Figure 10.18.\nRousseeuw et al. (2004) also suggested methods of robust multivariate regression using robust estimates of location and scatter rather than the classical \\(\\overline{\\mathbf{y}}\\) and \\(\\mathbf{S}\\). The minimum covariance determinant (MCD) estimator is a robust estimator with high breakdown value and bounded influence. It looks for the subset of size \\(h\\), whose covariance matrix has the smallest determinant, where \\(h : \\lceil n/2 \\rceil &lt; h &lt; n\\) controls the robustness. The distancePlot() function implements this, as well as a minimum variance ellipse (MVE) method. Some robust methods are illustrated in Section 13.5. \n\n10.7.3 Multivariate influence\nTODO Sort out coverage here vs. Chapter 13\nAgain, what we see in simple scatterplots can be misleading because they ignore all the other variables in a model. But looking back after fitting a model and examining diagnostic plots can often be illuminating. Among the ideas we inherit from univariate models, the influence of particular observations on the results of analysis should be high on your list.\nThe multivariate extension of the diagnostic measures of leverage and influence, and influence plots (Section 6.5) is provided by the mvinfluence package (Friendly, 2025). The theory behind this is due to Barrett & Ling (1992) and better illustrated in Barrett (2003). Mathematical details of this generalization are given in help(\"mvinfluence-package\").\nFigure 10.19 shows one form of an influence plot for the school.mod model. Because multiple response variables are involved, this plots a measure of the squared studentized residuals for each observation against a generalized version of hat values, so potentially “bad” observations appear in the upper left corner. The size of the bubble symbol is proportional to a generalization of Cook’s distance, the measure of influence based on the change in all coefficients if each case was deleted from the analysis.\n\ninfluencePlot(school.mod, id.n=4, \n              type=\"stres\",\n              cex.lab = 1.5)\n#        H      Q  CookD     L      R\n# 33 0.328 0.2017 0.7054 0.488 0.3001\n# 35 0.101 0.2825 0.3038 0.112 0.3142\n# 44 0.503 0.2984 1.6022 1.014 0.6009\n# 59 0.568 0.4937 2.9938 1.317 1.1441\n# 66 0.355 0.0174 0.0657 0.551 0.0269\n\n\n\n\n\n\nFigure 10.19: Influence plot for the schooldat multivariate regression model. Five cases are labeled as “noteworthy” on either axis.\n\n\n\n\nThat does not look good! You can see that cases 44 and 59 are actually quite troublesome here, but it turns out for different reasons. Take another look at Figure 10.16. You can see that case 59 is the most extreme on all the predictors, giving it very high leverage and therefore pulling the regression lines toward it in most of the plots, except those for number of teachers, where it also has large residuals. Case 44, on the other hand, stands out as high-leverage on only a few of the predictor–response combinations, but enough to give it a large multivariate hat value. It is also a point that is furthest from the regression lines.\nWhat should be done? An appropriate action would be to re-fit the model, reducing the impact of these cases, in what I call a sensitivity test:\n\nDo the main conclusions change with those cases removed? That is, do any model terms change in significance tests or do coefficients change sign?\nDo the relative sizes of effects for predictors change enough to affect interpretation? That is, how much do the coefficients change?\n\nThe easiest solution is to just omit these troubling cases. You can do this using update(), specifying the data argument to be the subset of rows without the bad boys.\n\nbad &lt;- c(44, 59)\nOK &lt;- (1:nrow(schooldata)) |&gt; setdiff(bad)\nschool.mod2 &lt;- update(school.mod, data = schooldata[OK,])\nAnova(school.mod2)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#            Df test stat approx F num Df den Df  Pr(&gt;F)    \n# education   1     0.211     5.35      3     60  0.0025 ** \n# occupation  1     0.422    14.62      3     60 2.9e-07 ***\n# visit       1     0.191     4.73      3     60  0.0050 ** \n# counseling  1     0.053     1.13      3     60  0.3459    \n# teacher     1     0.064     1.37      3     60  0.2618    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe results of Anova() on this model tell us that the three significant predictors— occupation, education and visit— are still so, but slightly less so for the last two of these. To compare the coefficients in the new model compared to the old you can calculate the relative difference, \\(| \\mathbf{B}_1 - \\mathbf{B}_2 | / \\mathbf{B}_1\\), which are\n\nreldiff &lt;- function(x, y, pct=TRUE) {\n  res &lt;- abs(x - y) / x\n  if (pct) res &lt;- 100 * res\n  res\n}\n\nreldiff(coef(school.mod)[-1,], coef(school.mod2)[-1,]) |&gt;\n  round(1)\n#            reading mathematics selfesteem\n# education     10.1        19.8      -75.7\n# occupation    11.0        10.2       19.4\n# visit       -103.6       -76.8        2.8\n# counseling  -332.2       305.3     -115.4\n# teacher       -7.5        -5.6        7.0\n\nAs you can see, the effects on the coefficients for visit and counseling` are dramatic.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#sec-ANCOVA-MANCOVA",
    "href": "11-mlm-review.html#sec-ANCOVA-MANCOVA",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.8 ANCOVA \\(\\rightarrow\\) MANCOVA",
    "text": "10.8 ANCOVA \\(\\rightarrow\\) MANCOVA\nTODO: Consider moving this to Chapter 11 and use much of the heplots MMRA vignette.\nIn univariate linear models, analysis of covariance (ANCOVA) is most often used in a situation where we want to compare the mean response, \\(\\bar{y}_j\\) for different groups (defined by one or more factors), but where there are one or more quantitative predictors \\(\\mathbf{x}_1, \\dots\\) that should be taken into account for our comparisons to make sense. The simplest case is when \\(\\mathbf{x}\\) is a pre-test score on the same measure, or when it is a background measure like age or level of education that we want to control for, to adjust for differences among the groups.\nMore generally, ANCOVA and its’ multivariate MANCOVA brother are used for situations where the model matrix \\(\\mathbf{X}\\) contains a mixture of factor variables and quantitative predictors, called “covariates”. In this wider context, there are two flavors of analysis with different emphasis on the factors or the covariates:\n\ntrue ANCOVA/MANOVA: Attention is centered on the differences among group means, but controlling for any difference in the covariate(s). This requires assuming that the slopes for the groups are all the same.\nhomogeneity of regression: Here the focus is on the regression relations between the \\(\\mathbf{y}\\)s and the predictor \\(\\mathbf{x}\\)s, but we might also want to determine if the regression slopes are the same for all groups defined by the factors.\n\nIn the ANCOVA flavor, the model fits additive effects of the group factor(s) and the covariate(s), while the homogeneity of regression flavor adds interaction terms between groups and the \\(\\mathbf{x}\\)s. The test for homogeneity of regression is the added effect of the interaction terms:\nmod1 &lt;- lm(y ~ Group + x)            # ANCOVA model\nmod2 &lt;- lm(y ~ Group + x + Group:x)  # allow separate slopes\nmod2 &lt;- lm(y ~ Group * x)            # same as above\n\nanova(mod1, mod2)            # test homogeneity of regression\n\nFigure 10.20 illustrates these cases for a hypothetical two-group design studying the the effect of an exercise program treatment on weight, recorded pre- (\\(x\\)) and post- (\\(y\\)) compared to a control group given no treatment. In panel (a) the slopes for the two groups are approximately equal, so the effect of treatment can be estimated by the difference in the fitted values of \\(\\hat{y}_i\\) at the average value of \\(x\\). In panel (b), the slope for the treated group is considerably greater than that for the control group, so the difference between the groups varies with \\(x\\).\n\n\n\n\n\n\n\nFigure 10.20: Two possible outcome patterns for a two-group design assessing the effect of a treatment on weight, measured pre- and post-treatment. (a) Additive effects of Group and \\(x\\); (b) Different slopes for the two groups. Plus signs show the means \\((\\bar{x}_i, \\bar{y}_i)\\) for the two groups.\n\n\n\n\n\n\n10.8.1 Example: Paired-associate tasks and academic performance\nTo what extent can simple tests of paired-associate learning11 predict measures of aptitude and achievement in kindergarten children? This was the question behind an experiment by William Rohwer at the University of California, Berkeley.\nThere were three outcome measures, one verbal and two visually-based:\n\nA student achievement test (SAT),\nPeabody Picture Vocabulary test (PPVT),\nRaven Progressive Matrices test (Raven).\n\nFour paired-associate tasks were used, which differed in the syntactic and semantic relationship between the stimulus and response terms in each pair. These are called named (n), still (s), named still (ns), named action (na), and sentence still (ss).\nRohwer’s data, taken from Timm (1975), is given in Rohwer. But there’s a MANCOVA wrinkle: Performance on the academic tasks is well-known to vary with socioeconomic status of the parents or the school they attend. A simple design was to collect data from children in two schools, one in a low SES neighborhood (\\(n=37\\)) and the other an upper-class high SES one (\\(n=32\\)). The data look like this:\n\n\ndata(Rohwer, package = \"heplots\")\nset.seed(42)\nRohwer |&gt; dplyr::sample_n(6)\n#    group SES SAT PPVT Raven n  s ns na ss\n# 49     2  Hi  88  105    21 2 11 10 26 22\n# 65     2  Hi  50   96    13 5  8 20 28 26\n# 25     1  Lo   6   57    10 0  1 16 15 17\n# 18     1  Lo  45   54    10 0  6  6 14 16\n# 69     2  Hi  50   78    19 5 10 18 27 26\n# 64     2  Hi  24  102    16 4 17 21 27 31\n\nFollowing the scheme for reshaping the data used in Figure 10.16, a set of scatterplots of each predictor against each response will give a useful initial look at the data. There’s a lot to see here, so the plot in Figure 10.21 focuses attention on the regression lines for the two groups and their data ellipses.\n\nCodeyvars &lt;- c(\"SAT\", \"PPVT\", \"Raven\" )      # outcome variables\nxvars &lt;- c(\"n\", \"s\", \"ns\", \"na\", \"ss\")   # predictors\n\nRohwer_long &lt;- Rohwer %&gt;%\n  dplyr::select(-group) |&gt;\n  tidyr::pivot_longer(cols = all_of(xvars), \n                      names_to = \"xvar\", values_to = \"x\") |&gt;\n  tidyr::pivot_longer(cols = all_of(yvars), \n                      names_to = \"yvar\", values_to = \"y\") |&gt;\n  dplyr::mutate(xvar = factor(xvar, levels = xvars),\n                yvar = factor(yvar, levels = yvars))\n\nggplot(Rohwer_long, aes(x, y, color = SES, shape = SES, fill = SES)) +\n  geom_jitter(size=0.8) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              formula = y ~ x, \n              linewidth = 1.5) +\n  stat_ellipse(geom = \"polygon\", alpha = 0.1) +\n  labs(x = \"Predictor (PA task)\",\n       y = \"Response (Academic)\") +\n  facet_grid(yvar ~ xvar,            # plot matrix of Y by X\n             scales = \"free\") +\n  theme_bw(base_size = 16) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 10.21: Scatterplots of each of the three response variables against each of the five predictors in the Rohwer dataset.\n\n\n\n\nYou can see here that the high-SES group generally performs better than the low group. The regression lines have similar slopes in some of the panels, but not all. The low SES group also appears to have larger variance on most of the PA tasks.\nMANCOVA model\nNevertheless, I fit the MANCOVA model that allows a test of different means for the two SES groups on the responses, but constrains the slopes for the PA covariates to be equal. Only two of the PA tasks (na and ns) show individually significant effects in the multivariate tests.\n\n# Make SES == 'Lo' the reference category\nRohwer$SES &lt;- relevel(Rohwer$SES, ref = \"Lo\")\n\nRohwer.mod1 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES + n + s + ns + na + ss, \n                  data=Rohwer)\nAnova(Rohwer.mod1)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#     Df test stat approx F num Df den Df  Pr(&gt;F)    \n# SES  1     0.379    12.18      3     60 2.5e-06 ***\n# n    1     0.040     0.84      3     60  0.4773    \n# s    1     0.093     2.04      3     60  0.1173    \n# ns   1     0.193     4.78      3     60  0.0047 ** \n# na   1     0.231     6.02      3     60  0.0012 ** \n# ss   1     0.050     1.05      3     60  0.3770    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nYou can also examine the tests of the univariate ANCOVA models for each of the responses using glance() or heplots::uniStats(). All are significantly related, but the PPVT measure has the largest \\(R^2\\) by far.\n\nuniStats(Rohwer.mod1)\n# Univariate tests for responses in the multivariate linear model Rohwer.mod1 \n# \n#         R^2     F df1 df2 Pr(&gt;F)    \n# SAT   0.295  4.33   6  62  0.001 ** \n# PPVT  0.628 17.47   6  62  1e-11 ***\n# Raven 0.211  2.76   6  62  0.019 *  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTo help interpret these effects, bivariate coefficient plots for the paired associate tasks are shown in Figure 10.22. (The coefficients for the group variable SES are on a different scale and so are omitted here.) From this you can see that the named still and named action tasks have opposite signs: contrary to expectations, ns is negatively associated with the measures of aptitude and achievement (when the other predictors are adjusted for).\n\ncoefplot(Rohwer.mod1, parm = 2:6,\n         fill = TRUE,\n         level = 0.68,\n         cex.lab = 1.5)\ncoefplot(Rohwer.mod1, parm = 2:6, variables = c(1,3),\n         fill = TRUE,\n         level = 0.68,\n         cex.lab = 1.5)\n\n\n\n\n\n\nFigure 10.22: Bivariate coefficient plots for the MANCOVA model with confidence ellipses of 68% coverage.\n\n\n\n\nTODO: For interpretation, it would be nice to know how many items were used for each of the PA tasks. The range of na goes up to 35, but others are less.\nAdjusted means\nFrom the analysis of covariance perspective, interest is often centered on estimating the differences between the group means, but adjusting or controlling for differences on the covariates. From the table of means below, you can see that the high SES group performs better on all three response variables, but this group also has higher scores on the paired associate tasks.\n\nmeans &lt;- Rohwer |&gt;\n  group_by(SES) |&gt;\n  summarise_all(mean) |&gt;\n  print()\n# # A tibble: 2 × 10\n#   SES   group   SAT  PPVT Raven     n     s    ns    na    ss\n#   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n# 1 Lo        1  31.3  62.6  13.2  2.54  6.92  13.5  22.4  18.4\n# 2 Hi        2  47.7  83.1  15    4.59  7.25  14.5  24.3  21.5\n\nThe adjusted mean differences are simply the values estimated by the coefficients for SES in the model. These are smaller than the differences between the observed means.\n\nmeans[2, 3:5] - means[1, 3:5]  \n#    SAT PPVT Raven\n# 1 16.4 20.4  1.76\n\n# adjusted means\ncoef(Rohwer.mod1)[2,]\n#   SAT  PPVT Raven \n#  8.80 16.88  1.59\n\nTODO: do this with a CI for the effects\nHomogeneity of regression\nThe MANCOVA model, Rohwer.mod1, has relatively simple interpretations (a large effect of SES, with ns and na as the major predictors) but the test of the SES effect relies on the assumption of homogeneity of slopes for the predictors. We can test this assumption as follows, by adding interactions of SES with each of the covariates:\n\nRohwer.mod2 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES * (n + s + ns + na + ss),\n                  data=Rohwer)\nAnova(Rohwer.mod2)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#        Df test stat approx F num Df den Df  Pr(&gt;F)    \n# SES     1     0.391    11.78      3     55 4.5e-06 ***\n# n       1     0.079     1.57      3     55 0.20638    \n# s       1     0.125     2.62      3     55 0.05952 .  \n# ns      1     0.254     6.25      3     55 0.00100 ***\n# na      1     0.307     8.11      3     55 0.00015 ***\n# ss      1     0.060     1.17      3     55 0.32813    \n# SES:n   1     0.072     1.43      3     55 0.24417    \n# SES:s   1     0.099     2.02      3     55 0.12117    \n# SES:ns  1     0.118     2.44      3     55 0.07383 .  \n# SES:na  1     0.148     3.18      3     55 0.03081 *  \n# SES:ss  1     0.057     1.12      3     55 0.35094    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt appears from the above that there is only weak evidence of unequal slopes from the separate SES: terms; only that for SES:na is individually significant. The evidence for heterogeneity is stronger, however, when these terms are tested collectively using linearHypothesis(). I use a small grep() trick here to find the interaction terms, which have a “:” in their names.\n\n# test interaction terms jointly\ncoefs &lt;- rownames(coef(Rohwer.mod2)) \ninteractions &lt;- coefs[grep(\":\", coefs)]\n\nprint(linearHypothesis(Rohwer.mod2, interactions), SSP=FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df Pr(&gt;F)   \n# Pillai            5     0.418     1.85     15    171 0.0321 * \n# Wilks             5     0.624     1.89     15    152 0.0277 * \n# Hotelling-Lawley  5     0.539     1.93     15    161 0.0240 * \n# Roy               5     0.385     4.38      5     57 0.0019 **\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSeparate models\nModel Rohwer.mod2 with all interaction terms essentially fits a separate slope for each of the low and high SES groups for all responses with each of the predictor PA tasks. This is similar in spirit to what we would get if we fit a separate multivariate regression model for each of the groups, but parameterized differently: The heterogeneous regression model gives, for the interaction terms estimates of the difference in slopes between groups, while the separate-regressions approach gives separate slope estimates for each of the groups. These are equivalent, in the sense that the estimates for each approach can be derived from the other.\nThey are not equivalent in testing however, because the full model uses a combined pooled within-group error covariance, allows hypotheses about equality of slopes and intercepts to be tested directly and has greater power because it uses the total sample size. Here, I simply illustrate the mechanics of fitting separate models using the subset argument to lm().\n\nRohwer.sesHi &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Hi\")\nAnova(Rohwer.sesHi)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#    Df test stat approx F num Df den Df Pr(&gt;F)   \n# n   1     0.202     2.02      3     24 0.1376   \n# s   1     0.310     3.59      3     24 0.0284 * \n# ns  1     0.358     4.46      3     24 0.0126 * \n# na  1     0.465     6.96      3     24 0.0016 **\n# ss  1     0.089     0.78      3     24 0.5173   \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRohwer.sesLo &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Lo\")\nAnova(Rohwer.sesLo)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#    Df test stat approx F num Df den Df Pr(&gt;F)  \n# n   1    0.0384     0.39      3     29  0.764  \n# s   1    0.1118     1.22      3     29  0.321  \n# ns  1    0.2252     2.81      3     29  0.057 .\n# na  1    0.2675     3.53      3     29  0.027 *\n# ss  1    0.1390     1.56      3     29  0.220  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe strength of evidence for the predictors na and ns is weaker here than when tested in the full heterogeneous model.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#what-have-we-learned",
    "href": "11-mlm-review.html#what-have-we-learned",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.9 What have we learned?",
    "text": "10.9 What have we learned?\n\nThe multivariate linear model is an extension of the (univariate) general linear model. Three major forms of the multivariate linear model are the MANOVA, MMRA and MANCOVA, all of which have their equivalents in ANOVA, MRA and ANCOVA.\nWhat the multivariate linear model offers over the univariate linear model is not just more powerful statistical tests, but also examinations of how the associations amongst the response variables varies with the explanatory variables.\nJust as with the general linear model, model diagnostics transfer to the multivariate linear model and are important for identifying potential issues in the validity of the estimated statistical model.\nGiven that the multivariate linear model can do everything that the general linear model does, there is no reason to not consider the multivariate linear model. Confining discussions of multivariate response models to structural equation modeling may pedagogically create a cognitive blind spot that the association amongst response variables even can vary with the explanatory variables. Rather than something separate, for the purposes of causal inference, it would be better to use structural equation modeling as a more confirmatory statistical model than the multivariate linear model, which is perfectly well suited for exploratory purposes.\nOverall, we also see a glimpse of the importance of data visualization in the analysis of data using the multivariate linear model. The following chapter will explore this issue in more depth.\n\n\n\n\n\nBarrett, B. E. (2003). Understanding influence in multivariate regression. Communications in Statistics - Theory and Methods, 32(3), 667–680. https://doi.org/10.1081/STA-120018557\n\n\nBarrett, B. E., & Ling, R. F. (1992). General classes of influence measures for multivariate regression. Journal of the American Statistical Association, 87(417), 184–191. https://www.jstor.org/stable/i314301\n\n\nCharnes, A., Cooper, W. W., & Rhodes, E. (1981). Evaluating program and managerial efficiency: An application of data envelopment analysis to program follow through. Management Science, 27(6), 668–697. http://www.jstor.org/stable/2631155\n\n\nFriendly, M. (2025). Mvinfluence: Influence measures and diagnostic plots for multivariate linear models. https://github.com/friendly/mvinfluence\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nHarrell, F. E. (2015). Regression modeling strategies: With applications to linear models, logistic and ordinal regression, and survival analysis. Springer International Publishing. https://books.google.ca/books?id=sQ90rgEACAAJ\n\n\nJohnson, R., & Wichern, D. (1998). Applied multivariate statistical analysis (4th ed.). Prentice Hall.\n\n\nKay, M. (2025). ggdist: Visualizations of distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKorkmaz, S., Goksuluk, D., & Zararsiz, G. (2014). MVN: An r package for assessing multivariate normality. The R Journal, 6(2), 151–162. https://journal.r-project.org/archive/2014-2/korkmaz-goksuluk-zararsiz.pdf\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis with applications. Biometrika, 57(3), 519–530. https://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMardia, K. V. (1974). Applications of some measures of multivariate skewness and kurtosis in testing normality and robustness studies. Sankhya: The Indian Journal of Statistics, Series B, 36(2), 115–128. http://www.jstor.org/stable/25051892\n\n\nMeyers, L. S., Gamst, G., & Guarino, A. J. (2006). Applied multivariate research: Design and interpretation. SAGE Publications.\n\n\nOlson, C. L. (1974). Comparative robustness of six tests in multivariate analysis of variance. Journal of the American Statistical Association, 69(348), 894–908. https://doi.org/10.1080/01621459.1974.10480224\n\n\nPlaster, M. E. (1989). The effect of defendent physical attractiveness on juridic decisions using felon inmates as mock jurors [Unpublished master's thesis]. East Carolina University.\n\n\nRousseeuw, P. J., Van Aelst, S., Van Driessen, K., & Gulló, J. A. (2004). Robust multivariate regression. Technometrics, 46(3), 293–305. https://doi.org/10.1198/004017004000000329\n\n\nSchatzoff, M. (1966). Sensitivity comparisons among tests of the general linear hypothesis. Journal of the American Statistical Association, 61(314), 415–435. https://doi.org/10.1080/01621459.1966.10480878\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52(3–4), 591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B: Methodological, 58, 267–288.\n\n\nTimm, N. H. (1975). Multivariate analysis with applications in education and psychology. Wadsworth (Brooks/Cole).\n\n\nWarne, F. T. (2014). A primer on multivariate analysis of variance(MANOVA) for behavioral scientists. Practical Assessment, Research & Evaluation, 19(1). https://scholarworks.umass.edu/pare/vol19/iss1/17/\n\n\nYee, T. W. (2015). Vector generalized linear and additive models: With an implementation in r. Springer.\n\n\nYee, T. W. (2025). VGAM: Vector generalized linear and additive models. https://CRAN.R-project.org/package=VGAM",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-review.html#footnotes",
    "href": "11-mlm-review.html#footnotes",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "There’s a bit of a puzzle here—a gap in methods—and therefore an opportunity. The classical linear models fit by lm() extend naturally to non-gaussian data via glm() which provides for other families (binary: Bernoulli, count data: Poisson). The standard lm() extends quite naturally to a multivariate responses. Yet the combination of these ideas— non-gaussian multivariate models— remains elusive. The VGAM package (Yee (2015), Yee (2025)) handles the bivariate cases of logistic (and probit) regression, but not much more. See Friendly & Meyer (2016), Sec. 10.4 for an example and graphs of odds ratios in these models. There is a lot more to do on this topic.↩︎\nA slight hiccup in notation is that the uppercase for the Greek Beta (\\(\\boldsymbol{\\beta}\\)) is the same as the uppercase Roman \\(\\mathbf{B}\\), so I use \\(\\mathbf{b}_1 , \\mathbf{b}_2 , \\dots\\) below to refer to its’ columns.↩︎\nAn class of violation of exogeneity omitted variable bias, which occurs When a relevant variable that influences both the predictor and the outcome is left out of the model. Its effect gets absorbed into the error term, creating a correlation with the predictor. For example, if we model wages as a function of education, but omit the effect of ability, the predictor “education” becomes correlated with the error term because unmodelled ability goes into the error term, and is therefore also correlated with education.↩︎\nAmong the extensions of the classical OLS framework, errors in variables models attempt to correct for the downward bias of measurement error by adjusting for the known or estimated error variances in the independent variables, providing more accurate and consistent estimations.↩︎\nLine plots like this are nearly always more understandable with labels directly on the lines, rather than in a legend. Wrappers for ggplot2 simplify some things but can’t easily accommodate this kind of customization.↩︎\nPlotting the means plus and minus one standard error of the mean is often the simplest approach to visualizing the uncertainty associated with each estimate. But this practice can create problems of interpretation, such as concluding that means differ if their intervals do not overlap. Some solutions are to encode \\(p\\)-values for comparisons in a plot, for example using ggpubr::geom_pwc() for pairwise comparisons, or to use error bars with several widths representing difference confidence levels, as provided by the ggdist package (Kay, 2025). ↩︎\n“Automatic” model selection procedures like stepwise regression, while seemingly attractive are dangerous in that can increase false positives or drop variables that should, on logical grounds, be included in the model. See Stepwise selection of variables in regression is Evil and Harrell (Harrell, 2015).↩︎\nOther choices are possible: we could instead try to model the behavioral variables, antisocial and hyperact first, and then determine if the parental variables add appreciably to this. Modeling choices aren’t arbitrary. They should reflect the aims of a study and the story you want to tell about the result.↩︎\nIn this, schools 1–49 were PFT sites and the remaining sites 50–70 were NFT. A separate dataset schoolsites provides other information on the schools, such as the general education style, region of the U.S., size of the city and that of the student population.↩︎\nGGally has a function ggduo() that does something similar, plotting each of one set of variables against another. Like ggpairs() it allows for generalizations of a scatterplot where combinations of discrete factors and continuous variables can be displayed with appropriate visualizations for each.↩︎\nPaired-associate learning are among the simplest tests of memory and learning. The subject is given a list of pairs of words or nonsense syllables, like “banana - house” or “YYZ - Toronto” to learn. On subsequent trials she is given the stimulus term of each pair (“banana”, “YYZ”) and asked to reply with the correct response (“house”, “Toronto”).↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html",
    "href": "12-mlm-viz.html",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "11.1 HE plot framework\nThe methods discussed in Chapter 10 provide the basis for a rather complete multivariate analysis of traditional univariate methods for the same designs. You can carry out multivariate multiple regression, MANOVA, or indeed, any classical linear model with the standard collection of analysis tools you use for a single outcome variable, but naturally extended in most cases to having several outcomes to analyse together. The key points are:\nAs nice as these mathematical and statistical ideas might be, the fact that the analysis is conducted for the response variables collectively, means that it may be harder to interpret and explain what this means about the separate responses. Here’s where multivariate model visualization comes to the rescue!\nHuang (2019), in a provocative article titled, “MANOVA: A Procedure Whose Time Has Passed?” criticized these methods as (a) difficult to understand because they are framed in terms of linear combinations of the the responses; (b) more complicated and limited in interpreting MANOVA effects and (c) unwieldy post hoc strategies often employed for interpretation.\nBut that’s just you should expect to happen if you rely on tables of coefficients or ANOVA summary tables, even with significance stars (* … ***) to help interpret what is important. (You should be looking at effect sizes and practical significance, right?)\nThese difficulties in understanding multivariate models can, I believe, be cured by accessible graphical methods for visualizing hypothesis tests and for visualizing what these linear combinations reflect in terms of the observed variables. The HE plot framework described below provides powerful graphic methods available in easily used software, the heplots and candisc packages.\nThis chapter describes this framework and illustrates some concrete examples, first for MANOVA designs which are conceptually and visually simpler, and then for MMRA designs with quantitative predictors and finally for MANCOVA models. Many more worked examples are available in vignettes for the heplots package.1\nPackages\nIn this chapter I use the following packages. Load them now.\nChapter 9 illustrated the basic ideas of the framework for visualizing multivariate linear models in the context of a simple two group design, using Hotelling’s \\(T^2\\). The main ideas were illustrated in Figure 9.9.\nHaving described the statistical ideas behind the MLM in Chapter 10, we can proceed to extend this framework to larger designs. Figure 11.1 illustrates these ideas using the simple one-way MANOVA design of the dogfood data from Section 10.2.1.\nFigure 11.1: Dogfood quartet: Illustration of the conceptual ideas of the HE plot framework for the dogfood data. (a) Scatterplot of the data; (b) Summary using data ellipses; (c) HE plot shows the variation in the means in relation to pooled within group variance; (d) Transformation from data space to canonical space\nFor more complex models such as MANOVA with multiple factors or multivariate multivariate regression with several predictors, there is one sum of squares and products matrix (SSP), and therefore one \\(\\mathbf{H}\\) ellipse for each term in the model. For example, in a two-way MANOVA design with the model formula (y1, y2) ~ A + B + A*B and equal sample sizes in the groups, the total sum of squares accounted for by the model is the sum of their separate effects,\n\\[\n\\begin{aligned}\n\\mathbf{SSP}_{\\text{Model}} & = \\mathbf{SSP}_{A} + \\mathbf{SSP}_{B} + \\mathbf{SSP}_{AB} \\\\\n                            & = \\mathbf{H}_A + \\mathbf{H}_B + \\mathbf{H}_{AB} \\:\\: .\n\\end{aligned}\n\\tag{11.1}\\]\nAll of these hypotheses can be overlaid in a single HE plot showing their effects together in a comprehensive view.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-he-framework",
    "href": "12-mlm-viz.html#sec-he-framework",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "In (a) data space, each group is summarized in (b) by its data ellipse, representing the means and covariances.\nVariation against the hypothesis of equal means can be seen by the \\(\\mathbf{H}\\) ellipse in the (c) HE plot, representing the data ellipse of the fitted values. Error variance is shown in the \\(\\mathbf{E}\\) ellipse, representing the pooled within-group covariance matrix, \\(\\mathbf{S}_p\\) and the data ellipse of the residuals from the model. For the dogfood data, the group means have a negative relation: longer time to start eating is associated with a smaller amount eaten.\nThe MANOVA (or Hotelling’s \\(T^2\\)) is formally equivalent to a discriminant analysis, predicting group membership from the response variables which can be seen in data space. (The main difference is emphasis and goals: MANOVA seeks to test differences among group means, while discriminant analysis aims at classification of the observations into groups.)\nThis effectively projects the \\(p\\)-dimensional space of the predictors into the smaller (d) canonical space that shows the greatest differences among the groups. As in a biplot, vectors show the relations of the response variables with the canonical dimensions.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-he-plot-construct",
    "href": "12-mlm-viz.html#sec-he-plot-construct",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.2 HE plot construction",
    "text": "11.2 HE plot construction\nThe HE plot is constructed to allow a direct visualization of the “size” of hypothesized terms in a multivariate linear model in relation to unexplained error variation. These can be displayed in 2D or 3D plots, so I use the term “ellipsoid” below to cover all cases.\nError variation is represented by a standard 68% data ellipsoid of the \\(\\mathbf{E}\\) matrix of the residuals in \\(\\boldsymbol{\\Large\\varepsilon}\\). This is divided by the residual degrees of freedom, so the size of \\(\\mathbf{E} / \\text{df}_e\\) is analogous to a mean square error in univariate tests. The choice of 68% coverage allows you to ``read’’ the residual standard deviation as the half-length of the shadow of the \\(\\mathbf{E}\\) ellipsoid on any axis (see Figure 3.11).\nThe \\(\\mathbf{E}\\) ellipsoid is then translated to the overall (grand) means \\(\\bar{\\mathbf{y}}\\) of the variables plotted, which allows us to show the means for factor levels on the same scale, facilitating interpretation. In the notation of Equation 3.2, the error ellipsoid \\(\\mathcal{E}_c\\) of size \\(c\\) is given by\n\\[\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{E}) = \\bar{\\mathbf{y}} \\; \\oplus \\; c\\,\\mathbf{E}^{1/2} \\:\\: ,\n\\tag{11.2}\\] where \\(c = \\chi^2_2 (0.68)\\) for 2D plots and \\(c = \\chi^2_3 (0.68)\\) for 3D plots of standard 68% coverage.2 Ellipses of various coverage were shown in Figure 3.9.\nAn ellipsoid representing variation in the means of a factor (or any other term reflected in a general linear hypothesis test, Equation 10.11) uses the corresponding \\(\\mathbf{H}\\) matrix is simply the data ellipse of the fitted values for that term. But there is a question of the relative scaling of the \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) ellipsoids for interpretation.\nDividing the hypothesis matrix by the error degrees of freedom, giving \\(\\mathbf{H} / \\text{df}_e\\), puts this on the same scale as the \\(\\mathbf{E}\\) ellipse.  I refer to this as effect size scaling, because it is similar to an effect size index used in univariate models, e.g., \\(ES = (\\bar{y}_1 - \\bar{y}_2) / s_e\\) in a two-group, univariate design. An alternative, significance scaling (Section 11.4) provides a visual test of significance of a model \\(\\mathbf{H}\\) term.\n\nTo illustrate this concretely, consider the HE plot for the dogfood shown in Figure 11.1 (c), reproduced here as Figure 11.2.\n\nShow the codedata(dogfood, package=\"heplots\")\ndogfood.mod &lt;- lm(cbind(start, amount) ~ formula, data=dogfood)\n\nheplot(dogfood.mod,  \n       fill = TRUE, fill.alpha = 0.1, \n       cex.lab = 1.5, cex = 1.5,\n       xlim = c(-1, 4.5),\n       ylim = c(70, 100))\n\n\n\n\n\n\nFigure 11.2: HE plot for the dogfood data, showing the means of the four groups, which generates the \\(\\mathbf{H}\\) ellipse for the effect of formula. The \\(\\mathbf{E}\\) ellipse labeled ‘Error’ shows the within-group variances and covariance.\n\n\n\n\nFrom the analysis in Section 10.2.2, we found the \\(\\mathbf{H}\\) matrix for the formula effect in the dogfood.mod model to be as shown below. The negative covariance, -70.94, reflects a correlation of -0.94 between the means of start time and amount eaten.\n\ndogfood.aov &lt;- Anova(dogfood.mod) \nSSP_H &lt;- dogfood.aov$SSP[[1]] |&gt; print()\n#         start amount\n# start    9.69  -70.9\n# amount -70.94  585.7\n\nSimilarly, the \\(\\mathbf{E}\\) matrix, shown below, reflects a slight positive correlation, 0.12, for dogs fed the same formula.\n\nSSP_E &lt;- dogfood.aov$SSPE |&gt; print()\n#        start amount\n# start   25.8   11.8\n# amount  11.8  390.3\n\n\n\nExample 11.1 Iris data\nPerhaps the most famous (or infamous) dataset in the history of multivariate data analysis is that of measurements on three species of Iris flowers collected by Edgar Anderson (1935) in the Gaspé Peninsula of Québec, Canada. Anderson wanted to quantify the outward appearance (“morphology”: shape, structure, color, pattern, size) of species as a method to study variation within and between such groups. Although Anderson published in the obscure Bulletin of the American Iris Society, R. A. Fisher (1936) saw this as a challenge and opportunity to introduce the method now called discriminant analysis—how to find a weighted composite of variables to best discriminate among existing groups.\n\n\n\n\n\n\n\nHistory corner: Is the iris data racist?\n\n\n\nI said “infamous” above because Fisher published in the Annals of Eugenics. He was an ardent eugenicist himself, and the work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. Through guilt by association, the Iris data, having mistakenly been called “Fisher’s Iris Data”, has become deprecated, even called “racist data”.3 The voices of the Setosa, Versicolor and Virginica of Gaspé protest: we don’t have a racist bone in our body and nor prejudice against any other species, to no avail.\nBodmer et al. (2021) present a careful account of Fisher’s views on eugenics within the context of his time and his contributions to modern statistical theory and practice. Fisher’s views on race were largely formed by Darwin and Galton, but “nearly all of Fisher’s statements were about populations, groups of populations, or the human species as a whole”. Regardless, the iris data were Anderson’s and should not be blamed. After all, if Anderson had gave his car to Fisher, would the car be tainted by Fisher’s eugenicist leanings?\n\n\n\n\n\n\n\n\n\n\nFigure 11.3: Diagram of an iris flower showing the measurements of petal and sepal size. Each flower has three sepals and three alternating petals. The sepals have brightly colored central sections. Source: Gayan De Silva (2020).\n\n\n\n\nSo that we understand what the measurements represent, Figure 11.3 superposes labels on a typical iris flower, having three sepals which alternate with three petals. Sepals are like ostentatious petals, with attractive decorations in the central section. Length is the distance from the center to the tip and width is the transverse dimension.\nAs always, it is useful to start with overview displays to see the data. A scatterplot matrix (Figure 11.4) shows that versicolor and virginica are more similar to each other than either is to setosa, both in their pairwise means (setosa are smaller) and in the slopes of regression lines. Further, the ellipses suggest that the assumption of constant within-group covariance matrices is problematic: While the shapes and sizes of the concentration ellipses for versicolor and virginica are reasonably similar, the shapes and sizes of the ellipses for setosa are different from the other two.\n\niris_colors &lt;-c(\"blue\", \"darkgreen\", \"brown4\")\nscatterplotMatrix(~ Sepal.Length + Sepal.Width + \n                    Petal.Length + Petal.Width | Species,\n  data = iris,\n  col = iris_colors,\n  pch = 15:17,\n  smooth=FALSE,\n  regLine = TRUE,\n  ellipse=list(levels=0.68, fill.alpha=0.1),\n  diagonal = FALSE,\n  legend = list(coords = \"bottomleft\", \n                cex = 1.3, pt.cex = 1.2))\n\n\n\n\n\n\nFigure 11.4: Scatterplot matrix of the iris dataset. The species are summarized by 68% data ellipses and linear regression lines in each pairwise plot.\n\n\n\n\n\n\n11.2.1 MANOVA model\nWe proceed nevertheless to fit a multivariate one-way ANOVA model to the iris data. The MANOVA model for these data addresses the question: “Do the means of the Species differ significantly for the sepal and petal variables taken together?” \\[\n\\mathcal{H}_0 : \\boldsymbol{\\mu}_\\textrm{setosa} = \\boldsymbol{\\mu}_\\textrm{versicolor} = \\boldsymbol{\\mu}_\\textrm{virginica}\n\\]\nBecause there are three species, the test involves \\(s = \\min(p, g-1) =2\\) degrees of freedom, and we are entitled to represent this by two 1-df contrasts, or sub-questions. From the separation among the groups shown in Figure 11.4 (or more botanical knowledge), it makes sense to compare:\n\nSetosa vs. others: \\(\\mathbf{c}_1 = (1,\\: -\\frac12, \\: -\\frac12)\\)\n\nVersicolor vs. Virginica: : \\(\\mathbf{c}_1 = (0,\\: 1, \\: -1)\\)\n\n\nYou can do this by putting these vectors as columns in a matrix and assigning this to the contrasts() of Species. It is important to do this before fitting with lm(), because the contrasts in effect determine how the \\(\\mathbf{X}\\) matrix is setup, and hence the names of the coefficients representing Species.\n\nC &lt;- matrix(c(1,-1/2,-1/2,  \n              0,   1,  -1), nrow=3, ncol=2)\ncontrasts(iris$Species) &lt;- C\ncontrasts(iris$Species)\n#            [,1] [,2]\n# setosa      1.0    0\n# versicolor -0.5    1\n# virginica  -0.5   -1\n\nNow let’s fit the model. As you would expect from Figure 11.4, the differences among groups are highly significant.\n\niris.mod &lt;- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~\n                 Species, data=iris)\nAnova(iris.mod)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#         Df test stat approx F num Df den Df Pr(&gt;F)    \n# Species  2      1.19     53.5      8    290 &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAs a quick follow-up, it is useful to examine the univariate tests for each of the iris variables, using heplots::glance() or heplots::uniStats(). It is of interest that the univariate \\(R^2\\) values are much larger for the petal variables than the sepal length and width.4. For comparison, heplots::etasq() gives the overall \\(\\eta^2\\) proportion of variance accounted for in all responses.\n\nglance(iris.mod)\n# # A tibble: 4 × 8\n#   response     r.squared sigma fstatistic numdf dendf  p.value  nobs\n#   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n# 1 Sepal.Length     0.619 0.515      119.      2   147 1.67e-31   150\n# 2 Sepal.Width      0.401 0.340       49.2     2   147 4.49e-17   150\n# 3 Petal.Length     0.941 0.430     1180.      2   147 2.86e-91   150\n# 4 Petal.Width      0.929 0.205      960.      2   147 4.17e-85   150\n\netasq(iris.mod)\n#         eta^2\n# Species 0.596\n\nBut these statistics don’t help to understand how the species differ. For this, we turn to HE plots.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-he-plots",
    "href": "12-mlm-viz.html#sec-he-plots",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.3 HE plots",
    "text": "11.3 HE plots\nThe heplot() function takes a \"mlm\" object and produces an HE plot for one pair of variables specified by the variables argument. By default, it plots the first two. Figure 11.5 shows the HE plots for the two sepal and the two petal variables.\n\n\nheplot(iris.mod, size = \"effect\",\n       cex = 1.5, cex.lab = 1.5,\n       fill = TRUE, fill.alpha = c(0.3, 0.1))\nheplot(iris.mod, size = \"effect\", variables = 3:4,\n       cex = 1.5, cex.lab = 1.5,\n       fill = TRUE, fill.alpha = c(0.3, 0.1))\n\n\n\n\n\n\n\n\nFigure 11.5: HE plots for the multivariate model iris.mod. The left panel shows the plot for the Sepal variables; the right panel plots the Petal variables.\n\n\n\n\nThe interpretation of the plots in Figure 11.5 is as follows:\n\nFor the Sepal variables, length and width are positively correlated within species (the \\(\\mathbf{E}\\) = “Error” ellipsoid). The means of the groups (the \\(\\mathbf{H}\\) = “Species” ellipsoid), however, are negatively correlated. This plot is the HE plot representation of the data shown in row 2, column 1 of Figure 11.4. It reflects the relative shape of the iris sepals: shorter and wider for setosa than the other two species.\nFor the Petal variables length and width are again positively correlated within species, but now the means of the groups are positively correlated: longer petals go with wider ones across species. This reflects the relative size of the iris petals. The analogous data plot appears in row 4, column 3 of Figure 11.4.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-signif-scaling",
    "href": "12-mlm-viz.html#sec-signif-scaling",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.4 Significance scaling",
    "text": "11.4 Significance scaling\nThe geometry of ellipsoids and multivariate tests allow us to go further with another re-scaling of the \\(\\mathbf{H}\\) ellipsoid that gives a visual test of significance for any term in a MLM. This is done simply by dividing \\(\\mathbf{H} / df_e\\) further by the \\(\\alpha\\)-critical value of the corresponding test statistic to show the strength of evidence against the null hypothesis.\nAmong the various multivariate test statistics, Roy’s maximum root test, based on the largest eigenvalue \\(\\lambda_1\\) of \\(\\mathbf{H} \\mathbf{E}^{-1}\\), gives \\(\\mathbf{H} / (\\lambda_\\alpha df_e)\\) which has the visual property that the scaled \\(\\mathbf{H}\\) ellipsoid will protrude somewhere outside the standard \\(\\mathbf{E}\\) ellipsoid if and only if Roy’s test is significant at significance level \\(\\alpha\\). The critical value \\(\\lambda_\\alpha\\) for Roy’s test is \\[\n\\lambda_\\alpha = \\left(\\frac{\\text{df}_1}{\\text{df}_2}\\right) \\; F_{\\text{df}_1, \\text{df}_2}^{1-\\alpha} \\:\\: ,\n\\] where \\(\\text{df}_1 = \\max(p, \\text{df}_h)\\) and \\(\\text{df}_2 = \\text{df}_h + \\text{df}_e - \\text{df}_1\\).\nFor these data, the HE plot using significance scaling is shown in the right panel of Figure 11.6. The left panel is the same as that shown for sepal width and length in Figure 11.5, but with axis limits to make the two plots directly comparable.\n\n\n\n\n\n\n\nFigure 11.6: HE plots for sepal width and sepal length in the iris dataset. Left: effect scaling of the \\(\\mathbf{H}\\) matrix; right: significance scaling, where protrusion of \\(\\mathbf{H}\\) outside \\(\\mathbf{E}\\) indicates a significant effect by Roy’s test.\n\n\n\n\nYou can interpret the plot using effect scaling to indicate that the overall “size” of variation of the group means is roughly the same as that of within-group variation for the two sepal variables. Significance scaling weights the evidence against the null hypothesis that a given effect is zero. Clearly, the species vary significantly on the sepal variables, and the direction of the \\(\\mathbf{H}\\) ellipse suggests that those whose sepals are longer are also less wide.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-he-vis-contrasts",
    "href": "12-mlm-viz.html#sec-he-vis-contrasts",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.5 Visualizing contrasts and linear hypotheses",
    "text": "11.5 Visualizing contrasts and linear hypotheses\nAs described in Section 5.1.3, tests of linear hypotheses and contrasts represented by the general linear test \\(\\mathcal{H}_0: \\mathbf{C} \\;\\mathbf{B} = \\mathbf{0}\\) provide a powerful way to probe the specific effects represented within the global null hypothesis, \\(\\mathcal{H}_0: \\mathbf{B} = \\mathbf{0}\\), that all effects are zero.\nIn this example the contrasts \\(\\mathbf{c}_1\\) (Species1) and \\(\\mathbf{c}_2\\) (Species2) among the iris species are orthogonal, i.e., \\(\\mathbf{c}_1^\\top \\mathbf{c}_2 = 0\\). Therefore, their tests are statistically independent, and their \\(\\mathbf{H}\\) matrices are additive. They fully decompose the general question of differences among the groups into two independent questions regarding the contrasts.\n\\[\n\\mathbf{H}_\\text{Species} = \\mathbf{H}_\\text{Species1} + \\mathbf{H}_\\text{Species2}\n\\tag{11.3}\\]\ncar::linearHypothesis() is the means for testing these statistically, and heplot() provides the way to show these tests visually. Using the contrasts set up in Section 11.2.1, \\(\\mathbf{c}_1\\), representing the difference between setosa and the other species is labeled Species1 and the comparison of versicolor with virginica is Species2. The coefficients for these in \\(\\mathbf{B}\\) give the differences in the means. The line for (Intercept) gives grand means of the variables.\n\ncoef(iris.mod)\n#             Sepal.Length Sepal.Width Petal.Length Petal.Width\n# (Intercept)        5.843       3.057        3.758       1.199\n# Species1          -0.837       0.371       -2.296      -0.953\n# Species2          -0.326      -0.102       -0.646      -0.350\n\nNumerical tests of hypotheses using linearHypothesis() can be specified in a very general way: A matrix (or vector) \\(\\mathbf{C}\\) giving linear combinations of coefficients by rows, or a character vector giving the hypothesis in symbolic form. A character variable or vector tests whether the named coefficients are different from zero for all responses.\n\nlinearHypothesis(iris.mod, \"Species1\") |&gt; print(SSP=FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df Pr(&gt;F)    \n# Pillai            1      0.97     1064      4    144 &lt;2e-16 ***\n# Wilks             1      0.03     1064      4    144 &lt;2e-16 ***\n# Hotelling-Lawley  1     29.55     1064      4    144 &lt;2e-16 ***\n# Roy               1     29.55     1064      4    144 &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlinearHypothesis(iris.mod, \"Species2\") |&gt; print(SSP=FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df Pr(&gt;F)    \n# Pillai            1     0.745      105      4    144 &lt;2e-16 ***\n# Wilks             1     0.255      105      4    144 &lt;2e-16 ***\n# Hotelling-Lawley  1     2.925      105      4    144 &lt;2e-16 ***\n# Roy               1     2.925      105      4    144 &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe various test statistics are all equivalent here—they give the same \\(F\\) statistics— because they they have 1 degree of freedom.\nIn passing, from Equation 11.3, note that the joint test of these contrasts is exactly equivalent to the overall test of Species (results not shown).\n\nlinearHypothesis(iris.mod, c(\"Species1\", \"Species2\"))\n\nWe can show these contrasts in an HE plot by supplying a named list for the hypotheses argument. The names are used as labels in the plot. In the case of a 1-df multivariate test, the \\(\\mathbf{H}\\) ellipses plot as a degenerate line.\n\nhyp &lt;- list(\"S:Vv\" = \"Species1\", \"V:v\" = \"Species2\")\nheplot(iris.mod, hypotheses=hyp,\n       cex = 1.5, cex.lab = 1.5,\n       fill = TRUE, fill.alpha = c(0.3, 0.1),\n       col = c(\"red\", \"blue\", \"darkgreen\", \"darkgreen\"),\n       lty = c(0,0,1,1), label.pos = c(3, 1, 2, 1),\n       xlim = c(2, 10), ylim = c(1.4, 4.6))\n\n\n\n\n\n\nFigure 11.7: HE plot for sepal length and width in the iris data showing the tests of the two contrasts, using significance scaling.\n\n\n\n\nThis HE plot shows that, for the two sepal variables, the greatest between-species variation is accounted for by the contrast (S:Vv) between setosa and the others, for which the effect is very large in relation to error variation. The second contrast (V:v), between the versicolor and virginica species is relatively smaller, but still explains significant variation of the sepal variables among the species.\nThe directions of these hypotheses in a given plot show how the group means differ in terms of a given contrast.5 For example, the contrast S:Vv is the line that separates setosa from the others and indicates that setosa flowers have shorter but wider sepals.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-HEplot-matrices",
    "href": "12-mlm-viz.html#sec-HEplot-matrices",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.6 HE plot matrices",
    "text": "11.6 HE plot matrices\nIn base R graphics, 2D scatterplots are extended to all pairwise views of multivariate data with a pairs() method. For multivariate linear models, the heplots package defines a pairs.mlm() method to display HE plots for all pairs of the response variables.\n\npairs(iris.mod,\n      fill=TRUE, fill.alpha=c(0.3, 0.1))\n\n\n\n\n\n\nFigure 11.8: All pairwise HE plots for the iris data.\n\n\n\n\nFigure 11.8 provides a fairly complete visualization of the results of the multivariate tests and answers the question: how do the species differ? Sepal length and the two petal variables have the group means nearly perfectly correlated, in the order setosa &lt; versicolor &lt; virginica. For Sepal width, however, setosa has the largest mean, and so the \\(\\mathbf{H}\\) ellipses show a negative correlation in the second row and column.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-candisc",
    "href": "12-mlm-viz.html#sec-candisc",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.7 Low-D views: Canonical analysis",
    "text": "11.7 Low-D views: Canonical analysis\nThe HE plot framework so far provides views of all the effects in a MLM in variable space. We can view this in 2D for selected pairs of response variables, or for all pairwise views in scatterplot matrix format, as in Figure 11.8.\nThere is also an heplot3d() function giving plots for three response variables together. The 3D plots are interactive, in that they can be rotated and zoomed by mouse control, and dynamic, in that they can be made to spin and saved as movies. To save space, these plots are not shown here.\nHowever in a one-way MANOVA design with more than three response variables, it is difficult to visualize how the groups vary on all responses together, and how the different variables contribute to discrimination among groups. In this situation, canonical discriminant analysis (CDA) is often used, to provide a low-D visualization of between-group variation.\nWhen the predictors are also continuous, the analogous term is canonical correlation analysis (CCA), described in Section 11.10. The advantage in both cases is that we can also show the relations of the response variables to these dimensions, similar to a biplot (Section 4.3) for a PCA of purely quantitative variables.\nBut, to be clear: dimensions and variance accounted for in canonical space describe the relationships between response variables and factors or quantitative predictors, whereas PCA is only striving to account for the total variation in the space of all variables.\nThe key to CDA is the eigenvalue decomposition of the \\(\\mathbf{H}\\) relative to \\(\\mathbf{E}\\) \\(\\mathbf{H}\\mathbf{E}^{-1} \\lambda_i = \\lambda_i \\mathbf{v}_i\\) (Equation 10.9). The eigenvalues, \\(\\lambda_i\\), give the “size” of each \\(s\\) orthogonal dimensions on which the multivariate tests are based (Section 10.2.3). But the corresponding eigenvectors, \\(\\mathbf{v}_i\\), give the weights for the response variables in \\(s\\) linear combinations that maximally discriminate among the groups or equivalently maximize the (canonical) \\(R^2\\) of a linear combination of the predictor \\(\\mathbf{X}\\)s with a linear combination of the response \\(\\mathbf{Y}\\)s.\nThus, CDA amounts to a transformation of the \\(p\\) responses, \\(\\mathbf{Y}_{n \\times p}\\) into scores \\(\\mathbf{Z}\\) in the canonical space,\n\\[\n\\mathbf{Z}_{n \\times s} = \\mathbf{Y} \\; \\mathbf{E}^{-1/2} \\; \\mathbf{V} \\;,\n\\tag{11.4}\\]\nwhere \\(\\mathbf{V}\\) contains the eigenvectors of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) and \\(s=\\min ( p, \\textbf{df}_h )\\) dimensions, the degrees of freedom for the hypothesis. It is well-known (e.g., Gittins (1985)) that canonical discriminant plots of the first two (or three, in 3D) columns of \\(\\mathbf{Z}\\) corresponding to the largest canonical correlations provide an optimal low-D display of the variation between groups relative to variation within groups.\nCanonical discriminant analysis is typically carried out in conjunction with a one-way MANOVA design. The candisc package (Friendly & Fox, 2025) generalizes this to multi-factor designs in the candisc() function. For any given term in a \"mlm\", the generalized canonical discriminant analysis amounts to a standard discriminant analysis based on the \\(\\mathbf{H}\\) matrix for that term in relation to the full-model \\(\\mathbf{E}\\) matrix.6\nTests based on the eigenvalues \\(\\lambda_i\\), initially stated by Bartlett (1938), use Wilks’ \\(\\Lambda\\) likelihood ratio tests of these. This allow you to determine the number of significant canonical dimensions, or the number of different aspects to consider for the relations between the responses and predictors. This is a big win over univariate analyses for each dependent variable separately as follow-ups for a significant MANOVA result.\nThese take the form of sequential global tests of the hypothesis that the canonical correlation in the current row and all that follow are zero. Thus, if you find that the second dimension is insignificant, there is no need to look at any further down the list. The canonical \\(R^2\\), CanRsq, gives the R-squared value of fitting the \\(i\\)th response canonical variate to the corresponding \\(i\\)th canonical variate for the predictors.\nFor the iris data, we get the following printed summary:\n\niris.can &lt;- candisc(iris.mod) |&gt; \n  print()\n# \n# Canonical Discriminant Analysis for Species:\n# \n#   CanRsq Eigenvalue Difference Percent Cumulative\n# 1  0.970     32.192       31.9  99.121       99.1\n# 2  0.222      0.285       31.9   0.879      100.0\n# \n# Test of H0: The canonical correlations in the \n# current row and all that follow are zero\n# \n#   LR test stat approx F numDF denDF Pr(&gt; F)    \n# 1        0.023    199.1     8   288 &lt; 2e-16 ***\n# 2        0.778     13.8     3   145 5.8e-08 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis analysis shows a very simple result: The differences among the iris species can be nearly entirely accounted for by the first canonical dimension (99.1%). Interestingly, the second dimension is also highly significant, even though it accounts for only 0.88%.\n\n11.7.1 Coeficients\nThe coef() method for “candisc” objects returns a matrix of weights for the response variables in the canonical dimensions. By default, these are given for the response variables standardized to \\(\\bar{y}=0\\) and \\(s^2_y = 1\\).\nThe type argument also allows for raw score weights (type = \"raw\") used to compute scores for the observations on the canonical variables Can1, Can2, … . Using type = \"structure\" gives the canonical structure coefficients, which are the correlations between each response and the canonical scores.\n\ncoef(iris.can, type = \"std\")\n#                Can1    Can2\n# Sepal.Length  0.427  0.0124\n# Sepal.Width   0.521  0.7353\n# Petal.Length -0.947 -0.4010\n# Petal.Width  -0.575  0.5810\ncoef(iris.can, type = \"structure\")\n#                Can1  Can2\n# Sepal.Length -0.792 0.218\n# Sepal.Width   0.531 0.758\n# Petal.Length -0.985 0.046\n# Petal.Width  -0.973 0.223\n\nThe standardized (or raw score) weights are interpreted in terms of their signs and magnitudes, just as in coefficient weights in a multiple regression. From the numbers, Can1 seems to be a contrast between the sepal and petal variables. For Can2, sepal length doesn’t matter and the result contrasts the two width variables against petal length.\nI find it easier to interpret the correlations between the observed and canonical variables, given as the canonical structure coefficients. These are easily visualized as vectors in canonical space (similar to biplots for a PCA), as shown below (Figure 11.9).\n\n\n11.7.2 Canonical scores plot\nThe plot() method for \"candisc\" objects gives a plot of these observation scores in any two dimensions. The argument ellipse=TRUE overlays this with their standard data ellipses for each species, as shown in Figure 11.9.\nAnalogous to the biplot (Section 4.3), the response variables are shown as vectors, using the structure coefficients. Thus, the relative size of the projection of these vectors on the canonical axes reflects the correlation of the observed response on the canonical dimension. For ease of interpretation I flipped the sign of the first canonical dimension (rev.axes), so that the positive Can1 direction corresponds to larger flowers.\n\n\nvars &lt;- names(iris)[1:4] |&gt; \n  stringr::str_replace(\"\\\\.\", \"\\n\")\nplot(iris.can,\n     var.labels = vars,\n     var.col = \"black\",\n     var.lwd = 2,\n     ellipse=TRUE,\n     scale = 9,\n     col = iris_colors,\n     pch = 15:17,\n     cex = 0.7, var.cex = 1.25,\n     rev.axes = c(TRUE, FALSE),\n     xlim = c(-10, 10),\n     cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.9: Plot of canonical scores for the iris data. Ellipses give 68% coverage data ellipses for the canonical scores. Variable vectors make angles with the Can1 and Can2 axes indicating their correlations.\n\n\n\n\nThe interpretation of this plot is simple: in canonical space, variation of the means for the iris species is essentially one-dimensional (99.1% of the effect of Species), and this dimension corresponds to overall size of the iris flowers: The Setosas have smaller flowers.\nAll variables except for Sepal.Width are positively aligned with this axis, but the two petal variables show the greatest discrimination. The negative direction for Sepal.Width reflects the pattern seen in Figure 11.8, where setosa have wider sepals.\nFor the second dimension, look at the projections of the variable vectors on the Can2 axis. All are positive, but this is dominated by Sepal.Width. We could call this a flower shape dimension.\n\n11.7.3 Canonical HE plot\nFor a one-way design, the canonical HE plot is simply the HE plot of the canonical scores in the analogous MLM model that substitutes \\(\\mathbf{Z}\\) for \\(\\mathbf{Y}\\). In effect, it is a more compact visual summary of the plot shown of canonical scores in Figure 11.9.\nThis is shown in Figure 11.10 for the iris data. In canonical space, the residuals are always uncorrelated, so the \\(\\mathbf{E}\\) ellipse plots as a circle. The \\(\\mathbf{H}\\) ellipse for Species here reflects a data ellipse for the fitted values— group means— shown as labeled points in the plot. The differences among Species are so large that this plot uses size = \"effect\" scaling, making the axes comparable to those in Figure 11.9.7\nThe vectors for each predictor are the same structure coefficients as in the ordinary canonical plot. They can again be reflected for easier interpretation and scaled in length to fill the plot window.\n\n\nheplot(iris.can,\n       size = \"effect\",\n       scale = 8,\n       var.labels = vars, var.col = \"black\",\n       var.lwd = 2, var.cex = 1.25,\n       fill = TRUE, fill.alpha = 0.2,\n       rev.axes = c(TRUE, FALSE),\n       xlim = c(-10, 10),\n       cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.10: Canonical HE plot for the iris data. Compared with Figure 11.9, it substutes canonical \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) ellipses for the canonical scores shown there.\n\n\n\n\nThe collection of plots shown for the iris data here can be seen as progressive visual summaries of the data and increased visual understanding of the morphology of Anderson’s Iris flowers:\n\nThe scatterplot matrix in Figure 11.4 shows the iris flowers in the data space of the sepal and petal variables.\nCanonical analysis substitutes for these the two linear combinations reflected in Can1 and Can2. The plot in Figure 11.9 portrays exactly the same relations among the species, but in the reduced canonical space of only two dimensions.\nThe HE plot version, shown in Figure 11.10 summarizes the separate data ellipses for the species with pooled, within-group variance of the \\(\\mathbf{E}\\) matrix for the canonical variables, which are always uncorrelated. The variation among the group means is reflected in the size and shape of the ellipse for the \\(\\mathbf{E}\\) matrix.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-HE-factorial",
    "href": "12-mlm-viz.html#sec-HE-factorial",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.8 Factorial MANOVA",
    "text": "11.8 Factorial MANOVA\nWhen there are two or more factors, the overall model is comprised of main effects and possible interactions as shown in Equation 11.1. A significant advantage of HE plots is that they show how the response variables are related in their effects. Moreover, the main effects and interactions can be overlaid in the same plot showing how each term contributes to assessment of differences among the groups.\n\nExample 11.2 Plastic film data\nI illustrate these points below for the Plastic film data analyzed earlier in Example 10.1. The model contemplated there examined how the three response variables, resistance to tear, film gloss and the opacity of the film varied with the two experimental factors, rate of extrusion and amount of some additive, both at two levels, labeled High and Low. Both main effects and the interaction of rate and additive were fit in the model plastic.mod:\n\nplastic.mod &lt;- lm(cbind(tear, gloss, opacity) ~ rate * additive, \n                  data=Plastic)\n\nWe can visualize all these effects for pairs of variables in an HE plot, showing the “size” and orientation of hypothesis variation (\\(\\mathbf{H}\\)) for each model term, in relation to error variation (\\(\\mathbf{E}\\)), as ellipsoids. When, as here, the model terms have 1 degree of freedom, the \\(\\mathbf{H}\\) ellipsoids for rate, additive and rate:additive each degenerate to a line.\nFigure 11.11 shows the HE plot for the responses tear and gloss, the strongest by univariate tests. This plot takes advantage of another feature of heplot(): You can overlay plots using add = TRUE, as is done here to show both significance and effect size scaling in a single plot.\n\ncolors = c(\"red\", \"darkblue\", \"darkgreen\", \"brown4\")\nheplot(plastic.mod, size=\"significance\", \n       col=colors, cex=1.5,  cex.lab = 1.5,\n       fill=TRUE, fill.alpha=0.1)\nheplot(plastic.mod, size=\"effect\", \n       col=colors, lwd=6,\n       add=TRUE, term.labels=FALSE)\n\n\n\n\n\n\nFigure 11.11: HE plot for effects on tear and gloss according to the factors rate, additive and their interaction, rate:additive. The thicker lines show effect size scaling; thinner lines show significance scaling.\n\n\n\n\nIn this view, the main effect of extrusion rate is highly significant, with the high level giving larger tear resistance and lower gloss on average. High level of additive produces greater tear resistance and higher gloss. The interaction effect, rate:additive, while non-significant, points nearly entirely in the direction of gloss. You can see this more directly in Figure 10.9, where the lines for gloss diverge.\nBut what if you also wanted to show the means for the combinations of rate and additive in an HE plot? By design, means for the levels of interaction terms are not shown in the HE plot, because doing so in general can lead to messy displays.\nWe can add them here for the term rate:additive as shown in Figure 11.12. This uses heplots::termMeans() to find the cell means for the combinations of the two factors and then lines() to connect the pairs of points for the low and high levels of additive.\n\npar(mar = c(4,4,1,1)+.1)\nheplot(plastic.mod, size=\"evidence\", \n       col=colors, cex=1.5, cex.lab = 1.5, \n       lwd = c(1, 5),\n       fill=TRUE, fill.alpha=0.05)\n\n# add interaction means\nintMeans &lt;- termMeans(plastic.mod, 'rate:additive', \n                      abbrev.levels=3)\npoints(intMeans[,1], intMeans[,2], pch=18, cex=1.2, col=\"brown4\")\ntext(intMeans[,1], intMeans[,2], rownames(intMeans), \n     adj=c(0.5, 1), col=\"brown4\")\nlines(intMeans[c(1,3),1], intMeans[c(1,3),2], \n      col=\"brown4\", lwd = 3)\nlines(intMeans[c(2,4),1], intMeans[c(2,4),2], \n      col=\"brown4\", lwd = 3)\n\n\n\n\n\n\nFigure 11.12: HE plot for effects on tear and gloss using significance scaling. To this is added points showing the means for the combinations of rate and additive.\n\n\n\n\nFigure 11.12 is somewhat more complicated to interpret than the simple line plots in Figure 10.9, but has the advantage that it shows effects on the two response variables jointly.\n\n\nExample 11.3 MockJury: Manipulation check\nIn Example 10.2, I examined the effects of the attractiveness of photos of hypothetical women defendants and the nature of a crime on the judgments made by members of a mock jury, on the rated seriousness of the crime and the length of a prison sentence participants would give to a guilty defendant. This analysis used the following model, fitting Serious and Years of sentence to the combinations of Attr and Crime:\n\ndata(MockJury, package = \"heplots\")\njury.mod &lt;- lm(cbind(Serious, Years) ~ Attr * Crime, \n                data=MockJury)\n\nThe photos were classified as “beautiful”, of “average” beauty or “unattractive”, and as a validity check on this experimental manipulation, 1–9 ratings on twelve attributes were also collected. Of these, the direct rating of physical attractiveness, physattr, was most important, but it was also of interest to see how other ratings differentiated the photos. The rating scales are the following:\n\nnames(MockJury)[-(1:4)] \n#  [1] \"exciting\"      \"calm\"          \"independent\"   \"sincere\"      \n#  [5] \"warm\"          \"phyattr\"       \"sociable\"      \"kind\"         \n#  [9] \"intelligent\"   \"strong\"        \"sophisticated\" \"happy\"        \n# [13] \"ownPA\"\n\nTo keep the graphs below simple, I consider only a subset of the ratings here, and fit the full \\(3 \\times 2\\) MANOVA model for Attr and Crime and their interaction. The main interest is in the attractiveness of the photo, but the other terms are included in the model to control for them.8\n\njury.mod1 &lt;- lm(cbind(phyattr, exciting, sociable, happy, independent) ~ Attr * Crime, \n                  data=MockJury)\nuniStats(jury.mod1)\n# Univariate tests for responses in the multivariate linear model jury.mod1 \n# \n#               R^2     F df1 df2 Pr(&gt;F)    \n# phyattr     0.639 38.24   5 108 &lt;2e-16 ***\n# exciting    0.139  3.49   5 108 0.0058 ** \n# sociable    0.134  3.33   5 108 0.0078 ** \n# happy       0.112  2.73   5 108 0.0233 *  \n# independent 0.120  2.94   5 108 0.0159 *  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe pairs.mlm() plot for this model would be a \\(5 \\times 5\\) display showing all pairwise HE plots for this model. Figure 11.13 selects two of these showing phyattr against exciting and independent. Although the model includes the full factorial of Attr and Crime, I only want to show the effect of Attr here, so I do this using the terms and factor.means arguments. Because the ratings are on the same 1–9 scale, I also use asp = 1 to make response variables visually comparable.\n\nheplot(jury.mod1, \n       terms = \"Attr\", factor.means = \"Attr\",\n       fill = TRUE, fill.alpha = 0.2,\n       cex = 1.2, cex.lab = 1.6, asp = 1)\n\nheplot(jury.mod1,\n       variables = c(1,5),\n       terms = \"Attr\", factor.means = \"Attr\",\n       fill = TRUE, fill.alpha = 0.2,\n       cex = 1.2, cex.lab = 1.6, asp = 1)\n\n\n\n\n\n\nFigure 11.13: Two pairwise HE plots showing the effect of the classified attractivenes of the photo and ratings of those photos. Left: exciting vs. phyattr ratings; right: independent vs. phyattr ratings.\n\n\n\n\nThese plots show that the means of the ratings of phyattr of the photos are in the expected order (Beautiful &gt; Average &gt; Unattractive), though the largest difference is between Beautiful and the others in both panels.9\nIn the left panel of Figure 11.13, the means for exciting are nearly perfectly correlated with those for phyattr, and there is little difference between the Beautiful photos and the others. The means for independent are slightly positively correlated with those for phyattr, but there is a wider separation between Average and Unattractive photos. The right panel shows the same order of the means for phyattr, but the photos of Average attractiveness are rated as highest on independence.\nCanonical analysis\nAs before, a canonical analysis squeezes the juice in a collection of responses into fewer dimensions, allowing you to see relationships not not apparent in all pairwise views. With 2 degrees of freedom for Attr, the space for all the ratings is 2D. candisc() for this term10 shows that 91% of the variation between photo groups is accounted for in one dimension, but there is still some significant variation associated with the 2\\(^\\text{nd}\\) dimension.\n\njury.can1 &lt;- candisc(jury.mod1, term = \"Attr\") |&gt;\n  print()\n# \n# Canonical Discriminant Analysis for Attr:\n# \n#   CanRsq Eigenvalue Difference Percent Cumulative\n# 1  0.642       1.79       1.61   90.87       90.9\n# 2  0.153       0.18       1.61    9.13      100.0\n# \n# Test of H0: The canonical correlations in the \n# current row and all that follow are zero\n# \n#   LR test stat approx F numDF denDF Pr(&gt; F)    \n# 1        0.303    17.46    10   214  &lt;2e-16 ***\n# 2        0.847     4.87     4   108  0.0012 ** \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe 2D plot of canonical scores in Figure 11.14 gives a very simple description for this model. Dimension Can1 widely separates the photo groups, and this dimension is nearly perfectly aligned with the phyattr ratings. Ratings for exciting are also strongly associated with this. This finding is sufficient to claim the validity of the classification used in the experiment.\n\ncol &lt;- c(\"blue\", \"darkgreen\", \"red\")\nplot(jury.can1, rev.axes = c(TRUE, TRUE),\n     col = col,\n     ellipse = TRUE, ellipse.prob = 0.5,\n     lwd = 3,\n     var.lwd = 2,\n     var.cex = 1.4,\n     var.col = \"black\",\n     pch = 15:17,\n     cex = 1.4,\n     cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.14: Canonical discriminant plot for the factor Attr in the model jury.mod1 for the ratings of the photos classified as Beautiful, Average or Unattractive. Ellipses have 50% coverage for the canonical scores. Variable vectors reflect the correlations of the rating scales with the canonical dimensions.\n\n\n\n\nThe second canonical dimension, Can2 is also of some interest. The photo means differ here mainly between those considered of Average beauty and those classified as Unattractive. The ratings for independent and happy are most strongly associated with this dimension. Ratings on sociable are related to both dimensions, but more so with Can2.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-he-mmra",
    "href": "12-mlm-viz.html#sec-he-mmra",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.9 Quantitative predictors: MMRA",
    "text": "11.9 Quantitative predictors: MMRA\nThe ideas behind HE plots extend naturally to multivariate multiple regression (MMRA).  A purely visual feature of HE plots in these cases is that the \\(\\mathbf{H}\\) ellipse for a quantitative predictor with 1 df appears as a degenerate line. But consequently, the angles between these for different predictors has a simple interpretation as as the correlation between their predicted effects. Moreover, it is easy to show visual overall tests of joint linear hypotheses for two or more predictors together.\nTODO: Use for an exercise heplots::Hernior: Recovery from Elective Herniorrhaphy -&gt; HE_mmra vignette\n\n\nExample 11.4 NLSY data\nHere I’ll continue the analysis of the NLSY data from Section 10.6.1. In the model NLSY.mod1, I used only father’s income and education to predict scores in reading and math, and both of these demographic variables were highly significant. Figure 11.15 shows what this looks like in an HE plot.\n\ndata(NLSY, package = \"heplots\")\nNLSY.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY)\n\nheplot(NLSY.mod1, \n  fill=TRUE, fill.alpha = 0.2, \n  cex = 1.5, cex.lab = 1.5,\n  lwd=c(2, 3, 3),\n  label.pos = c(\"bottom\", \"top\", \"top\")\n  )\n\n\n\n\n\n\nFigure 11.15: HE plot for the simple model for the NLSY data fitting reading and math scores from income and education.\n\n\n\n\nFathers income and education are positively correlated in their effects on the outcome scores. From the angles in the plot, income is most related to the math score, while education is related to both, but slightly more to the reading score.\nThe overall joint test for both predictors can then be visualized as the test of the linear hypothesis \\(\\mathcal{H}_0 : \\mathbf{B} = [\\boldsymbol{\\beta}_\\text{income}, \\boldsymbol{\\beta}_\\text{educ}] = \\mathbf{0}\\). For heplot(), we specify the names of the coefficients to be tested with the hypotheses argument.\n\ncoefs &lt;- rownames(coef(NLSY.mod1))[-1] |&gt; print()\n# [1] \"income\" \"educ\"\n\nheplot(NLSY.mod1, \n       hypotheses = list(\"Overall\" = coefs),\n       fill=TRUE, fill.alpha = 0.2, \n       cex = 1.5, cex.lab = 1.5,\n       lwd=c(2, 3, 3, 2),\n       label.pos = c(\"bottom\", \"top\"))\n\n\n\n\n\n\nFigure 11.16: HE plot adding the \\(\\mathbf{H}\\) ellipse for the overall test that both predictors have no effect on the outcome scores.\n\n\n\n\nThe geometric relations of the \\(\\mathbf{H}\\) ellipses for the overall test and the individual predictors in Figure 11.16 are worth noting here. Those for the separate coefficients always lie within the overall ellipse. The contribution for income makes the overall ellipse larger in the direction of the math score, while the contribution of education makes it larger in both directions.\n\n\n\nExample 11.5 School data: HE plots\nThe schooldata dataset analyzed in Section 10.6.2 can also be illuminated by the methods of this chapter. There I fit the multivariate regression model predicting students scores on reading, mathematics and a measure of self-esteem using as predictors measures of parents’ education, occupation, school visits, counseling help with school assignments and number of teachers per school.\nBut I also found two highly influential observations (cases 44, 59; see Figure 10.19) whose effect on the coefficients is rather large; so, I remove them from the analysis here.11\n\ndata(schooldata, package = \"heplots\")\n\nbad &lt;- c(44, 59)\nOK &lt;- (1:nrow(schooldata)) |&gt; setdiff(bad)\nschool.mod2 &lt;- lm(cbind(reading, mathematics, selfesteem) ~ ., \n                  data=schooldata[OK, ])\n\nIn this model, parent’s education and occupation and their visits to the schools were highly predictive of student’s outcomes but their counseling efforts and the number of teachers in the schools did not contribute much. However, the nature of these relationships was largely uninterpreted in that analysis.\nHere is where HE plots can help. You can think of this as a way to visualize what is entailed in the coefficients for this model by showing the magnitude of the predictor effects by their size and their relations to the outcome variable by their direction. The table of raw score coefficients isn’t very helpful in this regard.\n\ncoef(school.mod2)\n#             reading mathematics selfesteem\n# (Intercept)  2.7096       3.561    0.39751\n# education    0.2233       0.132   -0.01088\n# occupation   3.3336       4.284    1.79574\n# visit        0.0101      -0.123    0.20005\n# counseling  -0.3953      -0.293    0.00868\n# teacher     -0.1945      -0.360    0.01129\n\nFigure 11.17 shows the HE plot for reading and mathematics scores in this model, using the default significance scaling.\n\nheplot(school.mod2, \n       fill=TRUE, fill.alpha=0.1,\n       cex = 1.5,\n       cex.lab = 1.5,\n       label.pos = c(rep(\"top\", 4), \"bottom\", \"bottom\"))\n\n\n\n\n\n\nFigure 11.17: HE plot for reading and mathematics scores in the multivariate regression model for the school dataset. Predictor effects appear as lines whose lenght indicates the magnitude of the relationship and whose orientation reflects their correlations with the outcome variables shown in the plot.\n\n\n\n\nIn Figure 11.17, you can readily see:\n\nParent’s occupation and education are both significant in this view, but what is more important is their orientation. Both are positively associated with reading and math scores, but education is somewhat more related to reading than to mathematics.\nNumber of teachers and degree of parental counseling have a similar orientation, with teachers having a greater relation to mathematics scores.\nVisits to school and number of teachers are not significant in this plot, but both are positively correlated with reading and math and are coincident in the plot.\nThe parent time counseling measure, while also insignificant, tilts in the opposite direction, having different signs for reading and math.\n\nIn the pairs() plot for all three responses (Figure 11.18), we see something different in the relations for self-esteem. While occupation has a large positive relation in all the plots in the third row and column, education, counseling and teachers have negative relations in these plots, particularly with mathematics scores.\n\npairs(school.mod2, \n      fill=TRUE, fill.alpha=0.1,\n      var.cex = 2.5,\n      cex = 1.3)\n\n\n\n\n\n\nFigure 11.18: Pairwise HE plots for the three outcome variables in the multivariate regression model for the school dataset.\n\n\n\n\nThe analysis of this data is continued below in Example 11.6.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-cancor",
    "href": "12-mlm-viz.html#sec-cancor",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.10 Canonical correlation analysis",
    "text": "11.10 Canonical correlation analysis\nJust as we saw for MANOVA designs, a canonical analysis for multivariate regression involves finding a low-D view of the relations between predictors and outcomes that maximally explains their relations in terms of linear combinations of each. That is, the goal is to find weights for one set of variables, say \\(\\mathbf{X}\\) not to predict each of the other set \\(\\mathbf{Y} =[\\mathbf{y}_1, \\mathbf{y}_2, \\dots]\\) individually, but rather to also find weights for the \\(\\mathbf{y}\\)s which is most highly correlated with the linear combination of the \\(\\mathbf{x}\\)s.\nIn this sense, canonical correlation analysis (CCA) is symmetric in the \\(x\\) and \\(y\\) variables: the \\(y\\) set is not considered responses. Rather the goal is simply to explain the correlations between the two sets. For a thorough treatment of this topic, see Gittins (1985).\nGeometrically, these linear combinations are vectors representing projections in the observation space of the \\(x\\) and \\(y\\) variables, and CCA can also be thought of as minimizing the angle between these vectors or maximizing the cosine of this angle. This is illustrated in Figure 11.19.\n\n\n\n\n\n\n\nFigure 11.19: Diagram illustrating canonical correlation. For two \\(y\\) variables, all linear combinations are vectors in their plane, and similarly for the \\(x\\) variables. Maximizing the correlation between linear combinations of each is equivalent to making the angle \\(\\phi\\) between them as small as possible, or maximizing \\(\\cos({\\theta})\\), shown in the diagram at the right. The thick grey arrow indignates that the two planes should be overlaid at a common origin. Source: Re-drawn by Udi Alter following a Cross-Validated discussion by user ‘ttnphns’, https://bit.ly/4dgq2cp\n\n\n\n\nSpecifically, we want to find one set of weights \\(\\mathbf{a}_1\\) for the \\(x\\) variables and another for the \\(y\\) variables to give the linear combinations \\(\\mathbf{u}_1\\) and \\(\\mathbf{v}_1\\),\n\\[\\begin{aligned}\n\\mathbf{u}_1 & = \\mathbf{X} \\ \\mathbf{a}_1 = a_{11} \\mathbf{x}_1 + a_{12} \\mathbf{x}_2 + \\cdots + a_{11} \\mathbf{x}_q \\\\\n\\mathbf{v}_1 & = \\mathbf{Y} \\ \\mathbf{b}_1 = b_{11} \\mathbf{y}_1 + b_{12} \\mathbf{y}_1 + \\cdots + b_{11} \\mathbf{y}_p \\; ,\n\\end{aligned}\\]\nsuch that the correlation \\(\\rho_1 = \\textrm{corr}(\\mathbf{u}_1, \\mathbf{v}_1)\\) is maximized, or equivalently, minimizing the angle between them.\nUsing \\(\\mathbf{S}_{xx}\\), \\(\\mathbf{S}_{yy}\\) to represent the covariance matrices of the \\(x\\) and \\(y\\) variables, and \\(\\mathbf{S}_{xy}\\) for the cross-covariances between the two sets, the correlation between the linear combinations of each can be expressed as\n\\[\\begin{aligned}\n\\rho_1 & = \\textrm{corr}(\\mathbf{u}_1, \\mathbf{v}_1)\n         = \\textrm{corr}(\\mathbf{X} \\ \\mathbf{a}_1, \\mathbf{Y} \\ \\mathbf{b}_1) \\\\\n       & = \\frac{\\mathbf{a}_1^\\top \\ \\mathbf{S}_{xy} \\ \\mathbf{b}_1 }{\\sqrt{\\mathbf{a}_1^\\top \\ \\mathbf{S}_{xx} \\  \\mathbf{a}_1 } \\sqrt{\\mathbf{b}_1^\\top \\ \\mathbf{S}_{yy} \\ \\mathbf{b}_1 }}\n\\end{aligned}\\]\nBut, the \\(y\\) variables lie in a \\(p\\)-dimensional (observation) space, and the \\(x\\) in \\(q\\) dimensions, so what they have common is a space of \\(s = \\min(p, q)\\) dimensions. Therefore, we can find additional pairs of canonical variables,\n\\[\\begin{aligned}\n\\mathbf{u}_2 = \\mathbf{X} \\ \\mathbf{a}_2 & \\quad\\quad \\mathbf{v}_2 = \\mathbf{Y} \\ \\mathbf{b}_2 \\\\\n                                         & \\vdots \\\\\n\\mathbf{u}_s = \\mathbf{X} \\ \\mathbf{a}_s & \\quad\\quad \\mathbf{v}_s = \\mathbf{Y} \\ \\mathbf{b}_s \\\\\n\\end{aligned}\\]\nsuch that each pair \\((\\mathbf{u}_i, \\mathbf{v}_i)\\) has the maximum possible correlation and all distinct pairs are uncorrelated:\n\\[\\begin{aligned}\n\\rho_i & =\\max _{\\mathbf{a}_i, \\mathbf{b}_i}\\left\\{\\mathbf{u}_i^{\\top} \\mathbf{v}_i\\right\\} = \\\\\n\\left\\|\\mathbf{u}_i\\right\\| & =1, \\quad\\left\\|\\mathbf{v}_i\\right\\|=1, \\\\\n\\mathbf{u}_i^{{\\top}} \\mathbf{u}_j & =0, \\quad \\mathbf{v}_i^{\\top} \\mathbf{v}_j=0 \\quad\\quad \\forall j \\neq i: i, j \\in\\{1,2, \\ldots, s\\} \\ .\n\\end{aligned}\\]\nIn words, the correlations among canonical variables are zero except when when they are associated with the same canonical correlation or the weights \\((\\mathbf{a}_i, \\mathbf{b}_i)\\) for the same pair. Alternatively, all \\(p \\times q\\) correlations the variables in \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\) are fully summarized in the \\(s = \\min(p, q)\\) canonical correlations \\(\\rho_i\\) for \\(i = 1, 2, \\dots, s\\).\nThe solution, developed by Hotelling (1936), is a form of a generalized eigenvalue problem, that can be stated in two equivalent ways,\n\\[\n\\begin{aligned}\n& \\left(\\mathbf{S}_{y x} \\ \\mathbf{S}_{x x}^{-1} \\ \\mathbf{S}_{x y} - \\rho^2 \\ \\mathbf{S}_{y y}\\right) \\mathbf{b} = \\mathbf{0} \\\\\n& \\left(\\mathbf{S}_{x y} \\ \\mathbf{S}_{y y}^{-1} \\ \\mathbf{S}_{y x} - \\rho^2 \\ \\mathbf{S}_{x x}\\right) \\mathbf{a} = \\mathbf{0} \\ .\n\\end{aligned}\n\\] Both equations have the same form and have the same eigenvalues. And, given the eigenvectors for one of these equations, we can find the eigenvectors for the other.\n\n\n\nExample 11.6 School data: Canonical analysis\nFor the school data, with \\(p = 3\\) responses and \\(q = 5\\) predictors there are three possible sets of canonical variables. Together these account for 100% of the total linear relations between them. heplots::cancor() gives the percentage associated with each of the eigenvalues and the canonical correlations.\nFor this dataset, the first canonical variates, with Can \\(R = 0.995\\), accounts for 98.6%, so you might think that that is sufficient. Yet the likelihood ratio tests show that the second set, with Can \\(R = 0.774\\), is also significant, even though it only accounts for 1.3%.\n\nschool.can2 &lt;- cancor(\n  cbind(reading, mathematics, selfesteem) ~\n        education + occupation + visit + counseling + teacher,\n  data=schooldata[OK, ])\nschool.can2\n# \n# Canonical correlation analysis of:\n#    5   X  variables:  education, occupation, visit, counseling, teacher \n#   with   3   Y  variables:  reading, mathematics, selfesteem \n# \n#     CanR CanRSQ    Eigen  percent    cum\n# 1 0.9946 0.9892 91.41999 98.57540  98.58\n# 2 0.7444 0.5541  1.24267  1.33994  99.92\n# 3 0.2698 0.0728  0.07852  0.08466 100.00\n#                          scree\n# 1 ****************************\n# 2                             \n# 3                             \n# \n# Test of H0: The canonical correlations in the \n# current row and all that follow are zero\n# \n#    CanR LR test stat approx F numDF denDF Pr(&gt; F)    \n# 1 0.995        0.004     67.5    15   166 &lt; 2e-16 ***\n# 2 0.744        0.413      8.5     8   122 4.1e-09 ***\n# 3 0.270        0.927      1.6     3    62    0.19    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe virtue of CCA is that all correlations between the X and Y variables are completely captured in the correlations between the pairs of canonical scores: The \\(p \\times q\\) correlations between the sets are entirely represented by the \\(s = \\min(p, q)\\) canonical ones. Whether the second dimension is useful here depends on whether it adds some interpretable increment to what is going on in these relations. One could be justifiably happy with an explanation based on the first dimension that accounts for nearly all the total association between the sets.\nThe class \"cancor\" object returned by cancor() contains the canonical coefficients, for which there is a coef() method as in candisc(), and also a scores() method to return the scores on the canonical variables, called Xcan1, Xcan2, … and Ycan1, Ycan2.\n\nnames(school.can2)\n#  [1] \"cancor\"    \"names\"     \"ndim\"      \"dim\"       \"coef\"     \n#  [6] \"scores\"    \"X\"         \"Y\"         \"weights\"   \"structure\"\n# [11] \"call\"      \"terms\"\n\nYou can use the plot() method or heplot() method to visualize and help interpret the results. The plot() method plots the canonical scores$X against the scores$Y for a given dimension (selected by the which argument). The id.n argument gives a way to flag noteworthy observations.\n\nplot(school.can2, \n     pch=16, id.n = 3,\n     cex.lab = 1.5, id.cex = 1.5,\n     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))\ntext(-2, 1.5, paste(\"Can R =\", round(school.can2$cancor[1], 3)), \n     cex = 1.4, pos = 4)\n\nplot(school.can2, which = 2, \n     pch=16, id.n = 3,\n     cex.lab = 1.5, id.cex = 1.5,\n     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))\ntext(-3, 3, paste(\"Can R =\", round(school.can2$cancor[2], 3)), \n     cex = 1.4, pos = 4)\npar(op)\n\n\n\n\n\n\nFigure 11.20: Plots of canonical scores for the first two canonical dimensions of the schooldata dataset, omitting the two highly influential cases.\n\n\n\n\nIt is worthwhile to look at an analogous plot of canonical scores for the original dataset including the two highly influential cases. As you can see in Figure 11.21, cases 44 and 59 are way outside the range of the rest of the data. Their influence increases the canonical correlation to a near perfect \\(\\rho = 0.997\\).\n\nschool.can &lt;- cancor(cbind(reading, mathematics, selfesteem) ~\n                       education + occupation + visit + counseling + teacher,\n                     data=schooldata)\nplot(school.can, \n     pch=16, id.n = 3,\n     cex.lab = 1.5, id.cex = 1.5,\n     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))\ntext(-5, 1, paste(\"Can R =\", round(school.can$cancor[1], 3)), \n     cex = 1.4, pos = 4)\n\n\n\n\n\n\nFigure 11.21: Plots of canonical scores on the first canonical dimension for the schooldata, including the influential cases, which stand out as so far frome the rest of the observations.\n\n\n\n\nPlots of canonical scores tell us of the strength of the canonical dimensions, but do not help interpreting the analysis in relation to the original variables. The HE plot version for canonical correlation analysis re-fits a multivariate regression model for the Y variables against the Xs, but substitutes the canonical scores for each, essentially projecting the data into canonical space.\nTODO: Check out signs of structure coefs from cancor(). Would be better to reflect the vectors for Ycan1.\n\nheplot(school.can2,\n       fill = TRUE, fill.alpha = 0.2,\n       var.col = \"red\", \n       asp = NA, scale = 0.25,\n       cex.lab = 1.5, cex = 1.25,\n       prefix=\"Y canonical dimension \")\n\n\n\n\n\n\nFigure 11.22: HE plot for the canonical correlation analysis of the schooldata. Vectors for the variables indicate their correlations with the canonical dimensions.\n\n\n\n\nThe red variable vectors shown in these plots are intended only to show the correlations of Y variables with the canonical dimensions. The fact that they are so closely aligned reflects the fact that the first dimension accounts for nearly all of their associations with the predictors. The orientation of the \\(\\mathbf{H}\\) ellipses/lines reflects the projection of those from Figure 11.18 into canonical space\nOnly their relative lengths and angles with respect to the Y canonical dimensions have meaning in these plots. Relative lengths correspond to proportions of variance accounted for in the Y canonical dimensions plotted; angles between the variable vectors and the canonical axes correspond to the structure correlations. The absolute lengths of these vectors are arbitrary and are typically manipulated by the scale argument to provide better visual resolution and labeling for the variables.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#sec-he-mancova",
    "href": "12-mlm-viz.html#sec-he-mancova",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.11 MANCOVA models",
    "text": "11.11 MANCOVA models\nHE plots for designs containing a collection of quantitative predictors and one or more factors are quite simple in MANCOVA models where the effects are additive, i.e., don’t involve interactions. They are a bit more challenging when you allow separate slopes for groups on all quantitative variables, because there get to be too many terms to usefully display. But these models are more complicated!\nIf the evidence for heterogeneity of regressions is not very strong, it is still useful to fit the MANCOVA model and display it in an HE plot.\nAn alternative is to fit separate models for the groups and display these as HE plots. As noted earlier (Section 10.8.1), this is not ideal for testing hypotheses, but provides a useful and informative display of the relations between the predictors and responses and the groups effect. I illustrate these approaches for the Rohwer data, encountered in Section 10.8.1, below.\n\n\nExample 11.7 Rowher data\nIn Section 10.8.1 I fit several models for Rohwer’s data on the relations between paired-associate tasks and scholastic performance. The first model was the MANCOVA model testing the difference between the high and low SES groups, controlling for, or taking into account differences on the paired-associate task.\n\nRohwer.mod1 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES + n + s + ns + na + ss, \n                 data=Rohwer)\n\nHE plots for this model for the pairs (SAT, PPVT) and (SAT, Raven) is shown in Figure 11.23. The result of an overall test for all predictors, \\(\\mathcal{H}_0 : \\mathbf{B} = \\mathbf{0}\\), is added to the basic plot using the hypotheses argument.\n\ncolors &lt;- c(\"red\", \"blue\", rep(\"black\",5), \"#969696\")\ncovariates &lt;- rownames(coef(Rohwer.mod1))[-(1:2)]\npairs(Rohwer.mod1, \n       col=colors,\n       hypotheses=list(\"Regr\" = covariates),\n       fill = TRUE, fill.alpha = 0.1,\n       cex=1.5, cex.lab = 1.5, var.cex = 3,\n       lwd=c(2, rep(3,5), 4))\n\n\n\n\n\n\nFigure 11.23: All-pairs HE plot for SAT, PPVT and Raven using the MANCOVA model. The ellipses labeled ‘Regr’ show the test of the overall effect of the quantitative predictors.\n\n\n\n\nThe positive effect of SES on the outcome measures is seen in all pairwise plots: the high SES group is better on all responses. The positive orientation of the Regr ellipses for the covariates shows that the predicted values for all three responses are positively correlated (more so for SAT and PPVT): higher performance on the paired associate tasks, in general, is associated with higher academic performance. The two significant predictors, na and ns are the only ones that extend outside the error ellipses, but their orientations differ.\nHomogeneity of regression\nA second model tested the assumption of homogeneity of regression by adding interactions of SES with the PA tasks, allowing separate slopes for the two groups on each of the other predictors.\n\nRohwer.mod2 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES * (n + s + ns + na + ss),\n                  data=Rohwer)\n\nThis model has 11 terms, excluding the intercept: SES, plus 5 main effects (\\(x\\)s) for the predictors and 5 interactions (slope differences), too many for an understandable display. To visualize this in an HE plot (Figure 11.24), I simplify, by showing the interaction terms collectively by a single ellipse, representing their joint effect, and specified as a linear hypothesis called slopes that picks out the interaction effects.\nThe argument terms limits the \\(\\mathbf{H}\\) ellipses for the right-hand-side of the model which are shown to just those terms specified. The combined effect of the interaction terms is specified as an hypothesis (slopes) testing the interaction terms (which have a “:” in their name). Because SES is “treatment-coded” in this model, the interaction terms reflect the difference in slopes for the high SES group compared to the low.\n\n(coefs &lt;- rownames(coef(Rohwer.mod2)))\n#  [1] \"(Intercept)\" \"SESLo\"       \"n\"           \"s\"          \n#  [5] \"ns\"          \"na\"          \"ss\"          \"SESLo:n\"    \n#  [9] \"SESLo:s\"     \"SESLo:ns\"    \"SESLo:na\"    \"SESLo:ss\"\n\ncolors &lt;- c(\"red\", \"blue\", rep(\"black\",5), \"#969696\")\nheplot(Rohwer.mod2, col=c(colors, \"darkgreen\"), \n       terms=c(\"SES\", \"n\", \"s\", \"ns\", \"na\", \"ss\"), \n       hypotheses=list(\"Regr\" = c(\"n\", \"s\", \"ns\", \"na\", \"ss\"),\n                       \"Slopes\" = coefs[grep(\":\", coefs)]),\n       fill = TRUE, fill.alpha = 0.2, cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.24: HE plot for SAT and PPVT using the heterogeneous regression model. The ellipse labeled ‘Regr’ shows the test of the covariates combined, and the ellipse labeled ‘slopes’ shows the combined difference in slopes between the two groups.\n\n\n\n\nSeparate models\nWhen there is heterogeneity of regressions, using submodels for each of the groups has the advantage that you can easily visualize the slopes for the predictors in each of the groups, particularly if you overlay the individual HE plots. In this example, I’m using the models Rohwer.sesLo and Rohwer.sesLo fit to each of the groups.\n\nRohwer.sesLo &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Lo\")\nRohwer.sesHi &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Hi\")\n\nHere I make use of the fact that several HE plots can be overlaid using the option add=TRUE as shown in Figure 11.25. The axis limits may need adjustment in the first plot so that the second one will fit.\n\nheplot(Rohwer.sesLo, \n       xlim = c(0,100),               # adjust axis limits\n       ylim = c(40,110), \n       col=c(\"red\", \"black\"), \n       fill = TRUE, fill.alpha = 0.1,\n       lwd=2, cex=1.2, cex.lab = 1.5)\nheplot(Rohwer.sesHi, \n       add=TRUE, \n       col=c(\"brown\", \"black\"), \n       grand.mean=TRUE, \n       error.ellipse=TRUE,            # not shown by default when add=TRUE\n       fill = TRUE, fill.alpha = 0.1,\n       lwd=2, cex=1.2)\n\n\n\n\n\n\nFigure 11.25: Overlaid HE plots for SAT and PPVT, for the low and high SES groups, when each group is fit separately.\n\n\n\n\nWe can readily see the difference in means for the two SES groups (Hi has greater scores on both variables) and it also appears that the slopes of the s and n predictor ellipses are shallower for the High than the Low group, indicating greater relation with the SAT score. As well, the error ellipses show that on these measures, error variation is somewhat smaller in the low SES group.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#what-have-we-learned",
    "href": "12-mlm-viz.html#what-have-we-learned",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.12 What have we learned?",
    "text": "11.12 What have we learned?\n\nHE plots clarify complex multivariate models through direct visualization - The HE plot framework solves the interpretability problem of multivariate models by visualizing hypothesis (H) ellipses against error (E) ellipses. Rather than navigate a confusing maze of tables of coefficients and test statistics, with HE plots you can see which effects matter, how they relate to each other, and whether they’re statistically significant. All of these benefits are given in a single, intuitive plot that reveals the geometric structure underlying your multivariate analysis.\nVisual hypothesis testing beats \\(p\\)-value hunting - HE plots make hypothesis testing immediate and intuitive: if a hypothesis ellipse extends outside the error ellipse (under significance scaling), the effect is significant. No more scanning tables of p-values or wrestling with multiple comparisons—the geometry tells the story directly. You can even decompose overall effects into meaningful contrasts and see their individual contributions as separate ellipses.\nEllipse orientations reveal the hidden correlational structure of your effects - The angles between hypothesis ellipses in HE plots directly show how different predictors relate to your response variables and to each other. When effect ellipses point in similar directions, those predictors have similar multivariate signatures; when they’re orthogonal, they capture independent aspects of variation. This geometric insight goes far beyond what correlation matrices can reveal.\nCanonical space is your secret weapon for high-dimensional visualization - When you have many response variables, canonical discriminant analysis and canonical correlation analysis project the complex multivariate relationships into a lower-dimensional space that captures the essential patterns. This isn’t just dimension reduction—it’s insight amplification, revealing the fundamental directions of variation that matter most while showing how your original variables contribute to these meaningful dimensions.\nMultivariate models become tractable through progressive visual summaries - The chapter demonstrates a powerful visualization strategy: start with scatterplot matrices to see the raw data structure, move to HE plots to understand model effects, then project to canonical space for the clearest possible view of multivariate relationships. Each step preserves the essential information while making it more interpretable, turning the complexity of multivariate analysis into a comprehensible visual narrative.\n\n\n\n\n\nAnderson, E. (1935). The irises of the Gaspé peninsula. Bulletin of the American Iris Society, 35, 2–5.\n\n\nBartlett, M. S. (1938). Further aspects of the theory of multiple regression. Mathematical Proceedings of the Cambridge Philosophical Society, 34(1), 33–40. https://doi.org/10.1017/s0305004100019897\n\n\nBodmer, W., Bailey, R. A., Charlesworth, B., Eyre-Walker, A., Farewell, V., Mead, A., & Senn, S. (2021). The outstanding scientist, r.a. Fisher: His views on eugenics and race. Heredity, 126(4), 565–576. https://doi.org/10.1038/s41437-020-00394-6\n\n\nFisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x\n\n\nFriendly, M., & Fox, J. (2025). Candisc: Visualizing generalized canonical discriminant and canonical correlation analysis. https://github.com/friendly/candisc/\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGayan De Silva. (2020). Exploring the world of artificial neural networks -a beginner’s overview. https://doi.org/10.13140/RG.2.2.14790.14406\n\n\nGittins, R. (1985). Canonical analysis: A review with applications in ecology. Springer-Verlag.\n\n\nHotelling, H. (1936). Relations between two sets of variates. Biometrika, 28(3/4), 321. https://doi.org/10.2307/2333955\n\n\nHuang, F. L. (2019). MANOVA: A procedure whose time has passed? Gifted Child Quarterly, 64(1), 56–60. https://doi.org/10.1177/0016986219887200",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-mlm-viz.html#footnotes",
    "href": "12-mlm-viz.html#footnotes",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "See the heplots vignettes vignette(\"HE_manova\", package=\"heplots\") and vignette(\"HE_mmra\", package=\"heplots\"). ↩︎\nIn smallish samples (\\(n &lt; 30\\)) we use the better approximations, \\(c = \\sqrt{2 F_{2, n-2}^{0.68}}\\) for 2D plots and \\(c = \\sqrt{3 F_{3, n-3}^{0.68}}\\) for 3D. The difference between these is usually invisible in plots.↩︎\nFor example, Megan Stodel in a blog post Stop using iris says, “It is clear to me that knowingly using work that was itself used in pursuit of racist ideals is totally unacceptable.” A Reddit discussion on this topic, Is it socially acceptable to use the Iris dataset? has some interesting replies.↩︎\nRecall that \\(R^2\\) for a linear model is the the proportion of variation in the response that is explained by the model, calculated as \\(R^2 = \\text{SS}_H / \\text{SS}_T = \\text{SS}_H / (\\text{SS}_H + \\text{SS}_E )\\). For a multivariate model, these are obtained from the diagonal elements of \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\).↩︎\nThat the \\(\\mathbf{H}\\) ellipses for the contrasts subtend that for the overall test of Species is no accident. In fact, this is true in \\(p\\)-dimensional space for any linear hypothesis, and orthogonal contrasts have the additional geometric property that they form conjugate axes for the overall \\(\\mathbf{H}\\) ellipsoid relative to the \\(\\mathbf{E}\\) ellipsoid (Friendly et al., 2013).↩︎\nThe related candiscList() function performs a generalized canonical discriminant analysis for all terms in a multivariate linear model, returning a list of the results for each factor.↩︎\nIf significance scaling was used the interpretation of the canonical HE plot plot would the same as before: if the hypothesis ellipse extends beyond the error ellipse, then that dimension is significant.↩︎\nAs in univariate linear models, any factors or covariates ignored in the model formula have their effects pooled with the error term, reducing the sensitivity of their tests.↩︎\nYou could test this comparison with a more focused 1 df linear hypothesis using the contrast \\((1, -\\frac12, -\\frac12)\\) for the levels of Attr.↩︎\nFor MANOVA designs there is a separate analysis for each term in the model because there are different weights for the response variables.↩︎\nAn alternative to fitting the model removing specific cases deemed troublesome is to use a robust method, such as heplots::roblm(). This uses re-weighted least squares to down-weight observations with large residuals or other problems. These methods are illustrated in Section 13.5.↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html",
    "href": "13-eqcov.html",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "",
    "text": "12.1 Homogeneity of Variance in Univariate ANOVA\nThis chapter concerns the extension of tests of homogeneity of variance from the classical univariate ANOVA setting to the analogous multivariate (MANOVA) setting. Such tests are a routine but important aspect of data analysis, as particular violations can drastically impact model estimates and appropriate conclusions that can be drawn (Lix & Keselman, 1996).\nBeyond issues of model assumptions, the question of equality of covariance matrices is often of general interest itself. For instance, variability is often an important issue in studies of strict equivalence in laboratories comparing results across multiple patient measurements and in other applied contexts (see Gastwirth et al., 2009 for other exemplars).\nMoreover the outcome of such tests often have important consequences for the details of a main method of analysis. Just as the Welch \\(t\\)-test (Welch, 1947) is now commonly used and reported for a two-group test of differences in means under unequal variances, a preliminary test of equality of covariance matrices is often used in discriminant analysis to decide whether linear (LDA) or quadratic discriminant analysis (QDA) should be applied in a given problem. In such cases, the data at hand should inform the choice of statistical analysis to utilize.\nI provide some answers to the following questions:\nThe following sections provide a capsule summary of the issues in this topic following Friendly & Sigal (2018). Most of the discussion is couched in terms of a one-way design for simplicity, but the same ideas can apply to two-way (and higher) designs, where a “group” factor is defined as the product combination (interaction) of two or more factor variables.\nWhen there are also numeric covariates, this topic can also be extended to the multivariate analysis of covariance (MANCOVA) setting. This is accomplished simply by applying these techniques to the residuals from predictions by the covariates alone.\nPackages\nIn this chapter I use the following packages. Load them now.\nThe main methods described here are implemented in the heplots package.\nIn classical (Gaussian) univariate ANOVA models, the main interest is typically on tests of mean differences in a response \\(y\\) according to one or more factors. The validity of the typical \\(F\\) test, however, relies on the assumption of homogeneity of variance: all groups have the same (or similar) variance,\n\\[\n\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_g^2 \\; .\n\\]\nIt turns out that the \\(F\\) test for differences in means is relatively robust to violation of this assumption (Harwell et al., 1992), as long as the group sample sizes are roughly equal.1 This applies to Type I error \\(\\alpha\\) rates, which are not much affected. However, unequal variance makes the ANOVA tests less efficient: you lose power to detect significant differences.\nA variety of classical test statistics for homogeneity of variance are available, including Hartley’s \\(F_{max}\\) (Hartley, 1950), Cochran’s C (Cochran, 1941),and Bartlett’s test (Bartlett, 1937), but these have been found to have terrible statistical properties (Rogan & Keselman, 1977), which prompted Box’s famous quote.\nLevene (1960) introduced a different form of test, based on the simple idea that when variances are equal across groups, the average absolute values of differences between the observations and group means will also be equal, i.e., substituting an \\(L_1\\) norm for the \\(L_2\\) norm of variance. In a one-way design, this is equivalent to a test of group differences in the means of the auxilliary variable \\(z_{ij} = | y_{ij} - \\bar{y}_i |\\).\nMore robust versions of this test were proposed by Brown & Forsythe (1974). These tests substitute the group mean by either the group median or a trimmed mean in the ANOVA of the absolute deviations. Some suggest these should be almost always preferred to Levene’s version using the mean deviation. See Conover et al. (1981) for an early review and Gastwirth et al. (2009) for a general discussion of these tests. In what follows, we refer to this class of tests as “Levene-type” tests and suggest a multivariate extension described below (Section 12.2).\nThese deviations from a group central can be calculated using heplots::colDevs() and the central value can be a function, like mean, median or an anonymous one like function(x) mean(x, trim = 0.1)) that trims 10% off each side of the distribution. With a response Y Levene-type tests then be performed “by hand” as follows:\n# Levene\nZ.mean &lt;- abs( colDevs(Y, group) )\nlm(Z.mean ~ group)\n\n# Brown-Forsythe\nZ.med &lt;- abs( colDevs(Y, group, median) )\nlm(Z.med ~ group)\nThe function car::leveneTest() does this, so we could examine whether the variances are equal in the Penguin variables, one at a time, like so:\ndata(peng, package = \"heplots\")\nleveneTest(bill_length ~ species, data=peng)\n# Levene's Test for Homogeneity of Variance (center = median)\n#        Df F value Pr(&gt;F)\n# group   2    2.29    0.1\n#       330\n\nleveneTest(bill_depth ~ species, data=peng)\n# Levene's Test for Homogeneity of Variance (center = median)\n#        Df F value Pr(&gt;F)\n# group   2    1.91   0.15\n#       330\n\n# ...\n\nleveneTest(body_mass ~ species, data=peng)\n# Levene's Test for Homogeneity of Variance (center = median)\n#        Df F value Pr(&gt;F)   \n# group   2    5.13 0.0064 **\n#       330                  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nMore conveniently, heplots:leveneTests() with an “s”, does this for each of a set of response variables, specified in a data frame, a model formula or a \"mlm\" object. It also formats the results in a more pleasing way:\npeng.mod &lt;- lm(cbind(bill_length, bill_depth, flipper_length, body_mass) ~ species, \n               data = peng)\nleveneTests(peng.mod)\n# Levene's Tests for Homogeneity of Variance (center = median)\n# \n#                df1 df2 F value Pr(&gt;F)   \n# bill_length      2 330    2.29 0.1033   \n# bill_depth       2 330    1.91 0.1494   \n# flipper_length   2 330    0.44 0.6426   \n# body_mass        2 330    5.13 0.0064 **\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nSo, this tells us that the groups do not differ in variances on first three variables, but they do for body_mass.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#sec-mlevene",
    "href": "13-eqcov.html#sec-mlevene",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.2 Visualizing Levene’s test",
    "text": "12.2 Visualizing Levene’s test\nTo gain some insight into the problem of homogeneity of variance it is helpful so see how the situation looks in terms of data. For the Penguin data, it might be simplest just just to look at boxplots of the variables and try to see whether the widths of the central 50% boxes seem to be the same, as in Figure 12.1. However, it is perceptually difficult to focus on differences with widths of the boxes within each panel when their centers also differ from group to group.\n\nSee the codesource(\"R/penguin/penguin-colors.R\")\ncol &lt;- peng.colors(\"dark\")\nclr &lt;- c(col, gray(.20))\npeng_long &lt;- peng |&gt; \n  pivot_longer(bill_length:body_mass, \n               names_to = \"variable\", \n               values_to = \"value\") \n\npeng_long |&gt;\n  group_by(species) |&gt; \n  ggplot(aes(value, species, fill = species)) +\n  geom_boxplot() +\n  facet_wrap(~ variable, scales = 'free_x') +\n  theme_penguins() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'none') \n\n\n\n\n\n\nFigure 12.1: Boxplots for the Penguin variables. For assessing homogeneity of variance, we should be looking for differences in width of the central 50% boxes in each panel, rather than difference in central tendency.\n\n\n\n\nInstead, you can see more directly what is tested by the Levene test by graphing the absolute deviations from the group means or medians. This is another example of the graphic idea that you can make visual comparisons easier by plotting quantities of direct interest. You can calculate the median deviation values as follows:\n\nvars &lt;- c(\"bill_length\", \"bill_depth\", \"flipper_length\", \"body_mass\")\npengDevs &lt;- colDevs(peng[, vars], peng$species, median) |&gt;\n  abs()\n\nFrom a boxplot of the absolute deviations in Figure 12.2 your eye can now focus on the central value, shown by the median ‘|’ line, because Levene’s method is testing whether these differ across groups.\n\nSee the code# calculate absolute differences from median\ndev_long &lt;- data.frame(species = peng$species, pengDevs) |&gt; \n  pivot_longer(bill_length:body_mass, \n               names_to = \"variable\", \n               values_to = \"value\") \n\ndev_long |&gt;\n  group_by(species) |&gt; \n  ggplot(aes(value, species, fill = species)) +\n  geom_boxplot() +\n  facet_wrap(~ variable, scales = 'free_x') +\n  xlab(\"absolute median deviation\") +\n  theme_penguins() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'none') \n\n\n\n\n\n\nFigure 12.2: Boxplots for absolute differences from group medians for the Penguin data. The visual test of equality of variance is whether the median lines in the boxplots align.\n\n\n\n\nIt is now easy to see that the medians largely align for all the variables except for body_mass.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#sec-homogeneity-MANOVA",
    "href": "13-eqcov.html#sec-homogeneity-MANOVA",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.3 Homogeneity of variance in MANOVA",
    "text": "12.3 Homogeneity of variance in MANOVA\nIn the MANOVA context, the main emphasis, of course, is on differences among mean vectors, testing\n\\[\n\\mathcal{H}_0 : \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 = \\cdots = \\boldsymbol{\\mu}_g \\; .\n\\] However, the standard test statistics (Wilks’ Lambda, Hotelling-Lawley trace, Pillai-Bartlett trace, Roy’s maximum root) rely upon the analogous assumption that the within-group covariance matrices \\(\\boldsymbol{\\Sigma}_i\\) are equal for all groups,\n\\[\n\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\cdots = \\boldsymbol{\\Sigma}_g \\; .\n\\] This is much stronger than in the univariate case, because it also requires that all the correlations between pairs of variables are the same for all groups. For example, for two responses, there are three parameters (\\(\\rho, \\sigma_1^2, \\sigma_2^2\\)) assumed equal across all groups; for \\(p\\) responses, there are \\(p (p+1) / 2\\) assumed equal. The variances relate to size differences among data ellipses while the differences in the covariances appear as differences in shape.\n\n\nExample 12.1 Penguin data: Covariance ellipses\nTo preview a main example, Figure 12.3 shows data ellipses for the main size variables in the Penguins data (peng).   heplots::covEllipses() is specialized for viewing the relations among the data ellipsoids representing the sample covariance matrices, \\(\\mathbf{S}_1 = \\mathbf{S}_2 = \\cdots = \\mathbf{S}_g\\). It draws the data ellipse for each group, and also for the pooled within-group \\(\\mathbf{S}_p\\), as shown in Figure 12.3 for bill length and bill depth.\nYou can see that the sizes and shapes of the data ellipses are sort of similar in the left panel. The visual comparison becomes more precise when the data ellipses are all shifted to a common origin at the grand means (using center = TRUE). From this you can see that the Adélie group differs most from the others.\n\nSee the codeop &lt;- par(mar = c(4, 4, 1, 1) + .5,\n          mfrow = c(c(1,2)))\ncovEllipses(cbind(bill_length, bill_depth) ~ species, data=peng,\n  fill = TRUE,\n  fill.alpha = 0.1,\n  lwd = 3,\n  col = clr)\n\ncovEllipses(cbind(bill_length, bill_depth) ~ species, data=peng,\n  center = TRUE, \n  fill = c(rep(FALSE,3), TRUE), \n  fill.alpha = .1, \n  lwd = 3,\n  col = clr,\n  label.pos = c(1:3,0))\npar(op)\n\n\n\n\n\n\nFigure 12.3: Data ellipses for bill length and bill depth in the penguins data, also showing the pooled covariance. Left: As is; right: these are centered at the grand means for easier comparison.\n\n\n\n\nAll such pairwise plots in scatterplot matrix format are produced using the variables argument to covEllipses(), giving Figure 12.4.\n\nSee the codeclr &lt;- c(peng.colors(), \"black\")\ncovEllipses(peng[,3:6], peng$species, \n  variables=1:4,\n  col = clr,\n  fill=TRUE, \n  fill.alpha=.1)\n\n\n\n\n\n\nFigure 12.4: All pairwise covariance ellipses for the penguins data. The covariance matrices are homogeneous when the ellipses for the groups all have the same size and shape as that for the mean-centered pooled data (shown in black).\n\n\n\n\nThe covariance ellipses in Figure 12.4 look pretty similar in size, shape and orientation. But what does Box’s \\(\\mathcal{M}\\) test (described below) say?\n\npeng.boxm &lt;- boxM(cbind(bill_length, bill_depth, flipper_length, body_mass) \n                  ~ species,  data=peng) |&gt;\n  print()\n# \n#   Box's M-test for Homogeneity of Covariance Matrices\n# \n# data:  peng\n# Chi-Sq (approx.) = 75, df = 20, p-value = 3e-08\n\nAs you can see, it concludes strongly against the null hypothesis, because the test is highly sensitive to small differences among the covariance matrices.\n\n\nExample 12.2 Iris data: Covariance ellipses\nIt will be useful to have another example as we proceed, so Figure 12.5 shows an analogous plot for the iris data we examined in Section 11.6.\nEven when these are shown uncentered, the differences in size, shape and orientation are much more apparent. Iris setosa stands out as having smaller variance on some of the variables, while the ellipses for virginica tend to be larger. Their orientation (slopes) also differ quite a bit.\n\nCodeiris.colors &lt;- c(\"red\", \"darkgreen\", \"blue\")\ncovEllipses(iris[,1:4], iris$Species, \n  variables=1:4, \n  fill = TRUE,\n  fill.alpha=.1,\n  col = c(iris.colors, \"black\"),\n  label.pos=c(1:3,0))\n\n\n\n\n\n\nFigure 12.5: All pairwise covariance ellipses for the iris data. The large differences in size and shape indicate substantial heterogeneity in variances and covariances.\n\n\n\n\nboxM() gives the following results:\n\niris.boxm &lt;- boxM(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) \n                  ~ Species, \n                  data=iris) |&gt;\n  print()\n# \n#   Box's M-test for Homogeneity of Covariance Matrices\n# \n# data:  iris\n# Chi-Sq (approx.) = 141, df = 20, p-value &lt;2e-16\n\nUnsurprisingly, Box’s \\(\\mathcal{M}\\) tests gives \\(\\chi^2_{20} = 140.94\\) with a \\(p\\)-value &lt; 2.2e-16, putting numbers to the visual conclusion from Figure 12.5.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#sec-boxM",
    "href": "13-eqcov.html#sec-boxM",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.4 Box’s \\(\\mathcal{M}\\) test",
    "text": "12.4 Box’s \\(\\mathcal{M}\\) test\nTake a moment and think, “How could we generalize a test of equality of variances, \\(s_1^2 = s_2^2 = \\cdots = s_g^2\\), to the multivariate case, where we have \\((p \\times p)\\) matrices, \\(\\mathbf{S}_1 = \\mathbf{S}_2 = \\cdots = \\mathbf{S}_g\\) for each of \\(g\\) groups?”.\nMultivariate thinking suggests that that we calculate some measure of “size” of each \\(\\mathbf{S}_i\\), in a similar way to what is done in multivariate tests comparing \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) matrices.\nBox (1949) proposed the following likelihood-ratio test (LRT) statistic \\(\\mathcal{M}\\) for testing the hypothesis of equal covariance matrices. It uses the log of the determinant \\(\\vert \\mathbf{S}_i \\vert\\) as the measure of size.\n\\[\n\\mathcal{M} = (N -g) \\; \\ln \\vert \\mathbf{S}_p \\vert - \\sum_{i=1}^g (n_i -1) \\; \\ln \\vert \\mathbf{S}_i \\vert \\; ,\n\\tag{12.1}\\]\nwhere \\(N = \\sum n_i\\) is the total sample size and\n\\[\n\\mathbf{S}_p = (N-g)^{-1} \\sum_{i=1}^g (n_i - 1) \\mathbf{S}_i\n\\]\nis the pooled covariance matrix.\n\\(\\mathcal{M}\\) can thus be thought of as a ratio of the determinant of the pooled \\(\\mathbf{S}_p\\) to the geometric mean of the determinants of the separate \\(\\mathbf{S}_i\\).\nIn practice, there are various transformations of the value of \\(M\\) to yield a test statistic with an approximately known distribution (Timm, 1975). Roughly speaking, when each \\(n_i &gt; 20\\), a \\(\\chi^2\\) approximation is often used; otherwise an \\(F\\) approximation is known to be more accurate.\nAsymptotically, \\(-2 \\ln (\\mathcal{M})\\) has a \\(\\chi^2\\) distribution. The \\(\\chi^2\\) approximation due to Box (1949, 1950) is that\n\\[\nX^2 = -2 (1-c_1) \\; \\ln (\\mathcal{M}) \\quad \\sim \\quad \\chi^2_\\text{df}\n\\] with \\(\\text{df} = (g-1) p (p+1)/2\\) degrees of freedom. A bias correction constant \\(c_1\\) improves accuracy of the approximation:\n\\[\nc_1 = \\left(\n\\sum_i \\frac{1}{n_i -1}\n- \\frac{1}{N-g}\n\\right)\n\\frac{2p^2 +3p -1}{6 (p+1)(g-1)} \\; .\n\\]\nIn this form, Bartlett’s test for equality of variances in the univariate case is the special case of Box’s \\(\\mathcal{M}\\) when there is only one response variable, so Bartlett’s test is sometimes used as univariate follow-up to determine which response variables show heterogeneity of variance.\nYet, like its univariate counterparts, Box’s test is well-known to be highly sensitive to violation of (multivariate) normality and the presence of outliers2, as Box (1953) suggested in the opening chapter quote. Yet, it provides a nice framework for thinking about this problem more generally and gives rise to interesting graphic methods.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#visualizing-heterogeneity",
    "href": "13-eqcov.html#visualizing-heterogeneity",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.5 Visualizing heterogeneity",
    "text": "12.5 Visualizing heterogeneity\nA larger goal of this chapter is to use this background as another illustration of multivariate thinking, here, for visualizing and testing the heterogeneity of covariance matrices in multivariate designs.\nWhile researchers often rely on a single number to determine if their data have met a particular threshold, such compression will often obscure interesting information, particularly when a test concludes that differences exist, and one is left to wonder ``why?’’. It is within this context where, again, visualizations often reign supreme.\nWe have already seen one useful method in Section 12.3, which uses direct visualization of the information in the \\(\\mathbf{S}_i\\) and \\(\\mathbf{S}_p\\) using data ellipsoids to show size and shape as minimal schematic summaries; In what follows, I propose three additional visualization-based approaches to questions of heterogeneity of covariance in MANOVA designs:\n\na simple dotplot (Section 12.6) of the components of Box’s \\(\\mathcal{M}\\) test: the log determinants of the \\(\\mathbf{S}_i\\) together with that of the pooled \\(\\mathbf{S}_p\\). Extensions of these simple plots raise the question of whether measures of heterogeneity other than that captured in Box’s test might also be useful; and,\nPCA low-rank views (Section 12.7) to highlight features more easily seen there than in the full data space.\nThe analogy with the four types of multivariate test statistics (Section 10.3) suggests other measures that could be used to compare the \\(\\mathbf{S}_i\\) with \\(\\mathbf{S}_p\\) (Section 12.8).\nthe connection between Levene-type tests and an ANOVA (of centered absolute differences) suggests a parallel with a multivariate extension of Levene-type tests and a MANOVA (Section 12.9). We explore this with a version of Hypothesis-Error (HE) plots that we found useful for visualizing mean differences in MANOVA designs.\n\nThese methods are described and illustrated in Friendly & Sigal (2018).",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#sec-viz-boxM",
    "href": "13-eqcov.html#sec-viz-boxM",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.6 Visualizing Box’s \\(\\mathcal{M}\\)\n",
    "text": "12.6 Visualizing Box’s \\(\\mathcal{M}\\)\n\nBox’s test is based on a comparison of the log determinants of the \\(\\mathbf{S}_i\\) relative to that of the pooled \\(\\mathbf{S}_p\\), so the simplest thing to do is just plot them!\nboxM() produces a \"boxm\" object, for which there are summary() (details) and plot() methods. The plot() method gives a dot plot of the log determinants \\(\\ln \\vert \\mathbf{S}_i \\vert\\) together with that for the pooled covariance \\(\\ln \\vert \\mathbf{S}_p \\vert\\). Cai et al. (2015) provide the theory for the (asymptotic) confidence intervals shown.\n\nplot(peng.boxm, gplabel=\"species\", cex.lab = 1.5)\n\nplot(iris.boxm, gplabel=\"Species\", cex.lab = 1.5)\n\n\n\n\n\n\nFigure 12.6: Plots of the contributions to Box’s \\(\\mathcal{M}\\) statistic for the Penguin and iris data.\n\n\n\n\nIn these plots (Figure 12.6), the value for the pooled covariance appears within the range of the groups, because it is a weighted average. If you take a moment to look back at Figure 12.4, you’ll see that the data ellipses for Gentoo are slightly smaller in most pairwise views, but it is much easier to see this in a plot that summarizes this, like Figure 12.6.\nFrom Figure 12.5, it is clear that setosa shows the smallest within-group variability. The numeric scale values on the horizontal axis give a sense that the range across groups is considerably greater for the iris data than for the Penguins. Some generalizations of Box’s test using other measures are illustrated in Section 12.8.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#sec-eqcov-low-rank-views",
    "href": "13-eqcov.html#sec-eqcov-low-rank-views",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.7 Low-rank views",
    "text": "12.7 Low-rank views\nWith \\(p&gt;3\\) response variables, a simple alternative to the pairwise 2D plots in data space (shown in Figure 12.4 and Figure 12.5) is the projection into the principal component space accounting for the greatest amounts of total variance in the data (Friendly & Sigal (2018)). For the Iris data, a simple PCA of the covariance matrix shows that nearly 96% of total variance in the data is accounted for in the first two dimensions.\n\niris.pca &lt;- prcomp(iris[,1:4], scale. = TRUE)\nsummary(iris.pca)\n# Importance of components:\n#                         PC1   PC2    PC3     PC4\n# Standard deviation     1.71 0.956 0.3831 0.14393\n# Proportion of Variance 0.73 0.229 0.0367 0.00518\n# Cumulative Proportion  0.73 0.958 0.9948 1.00000\n\nFigure 12.7 shows the plots of the covariance ellipsoids for the first two principal component scores, uncentered (left panel) and centered (right panel). The dominant PC1 (73% of total variance) essentially orders the species by a measure of overall size of their sepals and petals. In the centered view at the right, it can again be seen how Setosa differs in covariance from the other two species, and that while Virginca and Versicolor both have similar shapes to the pooled covariance matrix, Versicolor has somewhat greater variance on PC1.\n\nsource(here::here(\"R/util/text.usr.R\"))    # text in user coords\n\ncovEllipses(iris.pca$x, iris$Species, \n  col = c(iris.colors, \"black\"),                # uncentered\n  fill=TRUE, fill.alpha=.1,\n  cex.lab = 1.5,\n  label.pos = c(1, 3, 3, 0), asp=1)\ntext.usr(0.05, 0.95, \"(a) Uncentered\", pos = 4, cex = 1.5)\n\ncovEllipses(iris.pca$x, iris$Species,\n  center=TRUE,                                  # centered\n  col = c(iris.colors, \"black\"),\n  fill=TRUE, fill.alpha=.1,\n  cex.lab = 1.5,\n  label.pos = c(1, 3, 3, 0), asp=1)\ntext.usr(0.95, 0.95, \"(b) Centered\", pos = 2, cex = 1.5)\n\n\n\n\n\n\nFigure 12.7: Covariance ellipsoids for the first two principal components of the iris data. Left (a): uncentered, showing group means on the principal components; right (b): centered at the origin, making size and shape comparison easier.\n\n\n\n\n\n12.7.1 Small dimensions can matter\nFor the Iris data, the first two principal components account for 96% of total variance, so we might think we are done here. Yet, as we’ve seen in other problems (outliers, collinearity), important information also exists in the space of the smallest principal component dimensions.\nThis is also true, as we will see for Box’s \\(\\mathcal{M}\\) test, because it is a (linear) function of all the eigenvalues of the between and within group covariance matrices, is therefore also subject to the influence of the smaller dimensions, where differences among \\(\\mathbf{S}_i\\) and of \\(\\mathbf{S}_p\\) can lurk.\n\ncovEllipses(iris.pca$x, iris$Species,\n  variables = 3:4,\n  center=TRUE,        \n  col = c(iris.colors, \"black\"),\n  fill=TRUE, fill.alpha=.1,\n  cex.lab = 1.5,\n  label.pos = c(1, 3, 3, 4), asp=1)\n\n\n\n\n\n\nFigure 12.8: Covariance ellipses for the smallest principal components of the iris data.\n\n\n\n\nFigure 12.8 shows the covariance ellipsoids in (PC3, PC4) space. Even though these dimensions contribute little to total variance, there are more pronounced differences in the within-group shapes (correlations) relative to the pooled covariance, and these contribute to a rejection of homogeneity by Box’s \\(\\mathcal{M}\\) test. Here we see that the correlation for Virginca is of opposite sign from the other two groups.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#sec-other-measures",
    "href": "13-eqcov.html#sec-other-measures",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.8 Other measures of heterogeneity",
    "text": "12.8 Other measures of heterogeneity\nAs we saw above (Section 12.3), the question of equality of covariance matrices can be expressed in terms of the similarity in size and shape of the data ellipses for the individual group \\(\\mathbf{S}_i\\) relative to that of \\(\\mathbf{S}_p\\). Box’s \\(\\mathcal{M}\\) test uses just one possible function to describe this size: the logs of their determinants.\nWhen \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix of a multivariate vector \\(\\mathbf{y}\\) with eigenvalues \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\lambda_p\\), the properties shown in Table 12.1 represent methods of describing the size and shape of the ellipsoid in \\(\\mathbb{R}^{p}\\).3 Just as is the case for tests of the MLM itself where different functions of these give test statistics (Wilks’ \\(\\Lambda\\), Pillai trace, etc.), one could construct other test statistics for homogeneity of covariance matrices as shown in Table 12.1.\n\n\nTable 12.1: Statistical and geometrical properties of “size” of an ellipsoid\n\n\n\nSize\nConceptual formula\nGeometry\nFunction\n\n\n\n(a) Generalized variance:\n\\(\\det{\\boldsymbol{\\Sigma}} = \\prod_i \\lambda_i\\)\narea, (hyper)volume\ngeometric mean\n\n\n(b) Average variance:\n\\(\\text{tr}({\\boldsymbol{\\Sigma}}) = \\sum_i \\lambda_i\\)\nlinear sum\narithmetic mean\n\n\n(c) Average precision:\n\\(1/ \\text{tr}({\\boldsymbol{\\Sigma}^{-1}}) = 1/\\sum_i (1/\\lambda_i)\\)\n\nharmonic mean\n\n\n(d) Maximal variance:\n\\(\\lambda_1\\)\nmaximum dimension\nsupremum\n\n\n\n\n\n\nHence, for a sample covariance matrix \\(\\mathbf{S}\\), \\(\\vert \\mathbf{S} \\vert\\) is a measure of generalized variance and \\(\\ln \\vert \\mathbf{S} \\vert\\) is a measure of average variance across the \\(p\\) dimensions.\nThe plot methods in for \"boxM\" objects in heplots can compute and plot all of the functions of the eigenvalues in Table 12.1. The results for the Penguin data are shown in Figure 12.9.\n\nplot(peng.boxm, which=\"product\",   gplabel=\"species\")\nplot(peng.boxm, which=\"sum\",       gplabel=\"species\")\nplot(peng.boxm, which=\"precision\", gplabel=\"species\")\nplot(peng.boxm, which=\"max\",       gplabel=\"species\")\n\n\n\n\n\n\nFigure 12.9: Plot of eigenvalue statistics of the covariance matrices for the Penguin data.\n\n\n\n\nExcept for the absence of error bars, the plot for log product in the upper left panel of Figure 12.9 is the same as that in Figure 12.6, but with the sign reversed. In principle, it is possible to add such confidence intervals for all these measures through the use of bootstrapping, but this has not yet been implemented in the heplots package.\nFor this data set, the pattern of points in the plot for Box’s \\(\\mathcal{M}\\) is also more or less the same as that for the precision measure. The plots for the sum of and maximum eigenvalue are also similar to each other, but differ from those of the two measures in the left column of Figure 12.9.\nThe main point is that these are not all the same, so different functions reflect different patterns of the eigenvalues, and could be used to define new statistical tests, perhaps with greater power or sensitivity to outliers.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#sec-multivar-levene",
    "href": "13-eqcov.html#sec-multivar-levene",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.9 Multivariate analog of Levene’s test",
    "text": "12.9 Multivariate analog of Levene’s test\nThe fact that Levene’s test is just a simple ANOVA of the absolute deviations from the group means or medians suggests an easy multivariate generalization (which has not been well-studied) —Simply do a MANOVA of the absolute differences, \\(\\vert \\mathbf{Y} - \\overline{\\mathbf{Y}}_j \\vert\\), centering on the means or medians.4 For the iris data, this gives:\n\nirisdev &lt;- colDevs(iris[,1:4], iris$Species, median, \n                   group.var = \"Species\") |&gt;\n  mutate(across(where(is.numeric), abs))\n\nirisdev.mod &lt;- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~ \n                    Species, data=irisdev)\nAnova(irisdev.mod)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#         Df test stat approx F num Df den Df  Pr(&gt;F)    \n# Species  2     0.394     8.89      8    290 6.7e-11 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe irisdev.mod model is just another MANOVA, so we can visualize it in an HE plot (Figure 12.10). But remember that this is showing differences among groups in multivariate dispersion rather than in means.\n\npairs(irisdev.mod, variables=1:4, fill=TRUE, fill.alpha=.1)\n\n\n\n\n\n\nFigure 12.10: HE plots for the multivariate generalization of Levene’s test based on MANOVA of absolute deviations from group medians.\n\n\n\n\nIn this plot, the \\(\\mathbf{H}\\) ellipses for the groups show the variation in the overall dispersion—the size of absolute differences from the group medians. These are very highly correlated across groups in most of the subplots, particularly so for those involving sepal width.\n\n12.9.1 Canonical discriminant analysis\nBut we can go further. Just as canonical discriminant analysis provides a low-dimensional view of differences in means, the same method can be used to visualize heterogeneity of dispersion expressed by the the Levene-type MANOVA in 1D or 2D.\nFor the model irisdev.mod, this shows that 98% of groups differences shown in the HE plots (Figure 12.10) can be found in a single canonical dimension. The 4D information can be reduced to 1D.\n\nlibrary(candisc)\nirisdev.can &lt;- candisc(irisdev.mod) |&gt;\n  print()\n# \n# Canonical Discriminant Analysis for Species:\n# \n#   CanRsq Eigenvalue Difference Percent Cumulative\n# 1  0.381     0.6154      0.602    97.9       97.9\n# 2  0.013     0.0132      0.602     2.1      100.0\n# \n# Test of H0: The canonical correlations in the \n# current row and all that follow are zero\n# \n#   LR test stat approx F numDF denDF Pr(&gt; F)    \n# 1        0.611    10.06     8   288 2.3e-12 ***\n# 2        0.987     0.64     3   145    0.59    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPlotting the first canonical dimension in Figure 12.11 gives a 1D summary, using boxplots for the scores and vectors for the weights of the iris variables.\n\nplot(irisdev.can, which=1,\n     fill.alpha = .3,\n     col = iris.colors, lwd = 2,\n     var.cex = 1.2,\n     cex.lab = 1.5, cex.axis = 1.1)\n\n\n\n\n\n\nFigure 12.11: Canonical discriminant plot for the multivariate generalization of Levene’s test based on MANOVA of absolute deviations from group medians.\n\n\n\n\nOverall, the covariance matrices are smallest for setosa and largest for virginica. The structure coefficents for the variables reflect the pattern shown in the HE plots (Figure 12.10), where the direction of the \\(\\mathbf{H}\\) ellipses is negative for sepal width but positive for the others.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#what-have-we-learned",
    "href": "13-eqcov.html#what-have-we-learned",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.10 What Have We Learned?",
    "text": "12.10 What Have We Learned?\nThe quest to understand equality of covariance matrices in multivariate models has taken us on a journey from Box’s famous row boat metaphor to some novel visualization techniques. Here are the essential insights that will transform how you think about and explore heterogeneity in your multivariate data:\n\nVisualization beats test statistics alone - While Box’s \\(\\mathcal{M}\\)-test gives you a single \\(p\\)-value, covariance ellipses reveal the why behind heterogeneity. You can literally see differences in size (scatter) and shape (orientation) of group covariances, making the abstract concrete and interpretable.\nCentering makes differences more apparent - Shifting all group ellipses to a common center (the grand mean) is like removing visual noise to see the signal. This simple transformation makes differences in covariance structure apparent, turning subtle variations into obvious visual patterns.\nSmall dimensions often hold big secrets - Don’t ignore those “unimportant” principal components! The smallest PC dimensions frequently contain the most discriminating information about covariance differences. It’s often where the action is hiding, away from the obvious patterns in major dimensions.\nMultiple measures tell richer stories - Box’s \\(\\mathcal{M}\\)-test uses log determinants, but why stop there? Exploring different eigenvalue functions (sum, precision, maximum) can reveal distinct patterns of heterogeneity, like having multiple lenses to examine the same phenomenon.\nLevene meets MANOVA in beautiful harmony - The multivariate extension of Levene’s test (MANOVA on absolute deviations) creates a natural bridge between univariate and multivariate thinking, complete with interpretable HE plots that make variance differences as visual as mean differences.\n\n\n\nGraphics simplify complex concepts - These visualization tools transform an intimidating mathematical concept (equality of \\(p \\times p\\) matrices) into something any researcher can explore, understand, and communicate to others.\n\nThe chapter’s strength lies in showing that covariance matrices aren’t just mathematical abstractions—they’re visual stories waiting to be told, patterns waiting to be discovered, and insights illustrating the power of multivariate thinking.\n\n\n\n\n\n\nAnderson, M. J. (2006). Distance-based tests for homogeneity of multivariate dispersions. Biometrics, 62(1), 245–253. https://doi.org/10.1111/j.1541-0420.2005.00440.x\n\n\nBartlett, M. S. (1937). Properties of sufficiency and statistical tests. Proceedings of the Royal Society of London. Series A, 160(901), 268–282. https://doi.org/10.2307/96803\n\n\nBox, G. E. P. (1949). A general distribution theory for a class of likelihood criteria. Biometrika, 36(3-4), 317–346. https://doi.org/10.1093/biomet/36.3-4.317\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and wear curves. Biometrics, 6(4), 362–389.\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances. Biometrika, 40(3/4), 318–335. https://doi.org/10.2307/2333350\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the American Statistical Association, 69(346), 364–367. https://doi.org/10.1080/01621459.1974.10482955\n\n\nCai, T. T., Liang, T., & Zhou, H. H. (2015). Law of log determinant of sample covariance matrix and optimal estimation of differential entropy for high-dimensional gaussian distributions. Journal of Multivariate Analysis, 137, 161–172. https://doi.org/https://doi.org/10.1016/j.jmva.2015.02.003\n\n\nCochran, W. G. (1941). The distribution of the largest of a set of estimated variances as a fraction of their total. Annals of Eugenics, 11(1), 47–52. https://doi.org/10.1111/j.1469-1809.1941.tb02271.x\n\n\nConover, W. J., Johnson, M. E., & Johnson, M. M. (1981). A comparative study of tests for homogeneity of variances, with applications to the outer continental shelf bidding data. Technometrics, 23(4), 351–361. https://doi.org/10.1080/00401706.1981.10487680\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFriendly, M., & Sigal, M. (2018). Visualizing tests for equality of covariance matrices. The American Statistician, 72(4), 144–155. https://doi.org/10.1080/00031305.2018.1497537\n\n\nGastwirth, J. L., Gel, Y. R., & Miao, W. (2009). The impact of Levene’s test of equality of variances on statistical theory and practice. Statistical Science, 24(3), 343–360. https://doi.org/10.1214/09-STS301\n\n\nHartley, H. O. (1950). The use of range in analysis of variance. Biometrika, 37(3–4), 271–280. https://doi.org/10.1093/biomet/37.3-4.271\n\n\nHarwell, M. R., Rubinstein, E. N., Hayes, W. S., & Olds, C. C. (1992). Summarizing monte carlo results in methodological research: The one- and two-factor fixed effects ANOVA cases. Journal of Educational and Behavioral Statistics, 17(4), 315–339. https://doi.org/10.3102/10769986017004315\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. Olkin, S. G. Ghurye, W. Hoeffding, W. G. Madow, & H. B. Mann (Eds.), Contributions to probability and statistics: Essays in honor of Harold Hotelling (pp. 278–292). Stanford University Press.\n\n\nLix, J. M., L. M. Keselman, & Keselman, H. J. (1996). Consequences of assumption violations revisited: A quantitative review of alternatives to the one-way analysis of variance F test. Review of Educational Research, 66(4), 579–619. https://doi.org/10.3102/00346543066004579\n\n\nO’Brien, P. C. (1992). Robust procedures for testing equality of covariance matrices. Biometrics, 48(3), 819–827. http://www.jstor.org/stable/2532347\n\n\nRogan, J. C., & Keselman, H. J. (1977). Is the ANOVA f-test robust to variance heterogeneity when sample sizes are equal?: An investigation via a coefficient of variation. American Educational Research Journal, 14(4), 493–498. https://doi.org/10.3102/00028312014004493\n\n\nTiku, M. L., & Balakrishnan, N. (1984). Testing equality of population variances the robust way. Communications in Statistics - Theory and Methods, 13(17), 2143–2159. https://doi.org/10.1080/03610928408828818\n\n\nTimm, N. H. (1975). Multivariate analysis with applications in education and psychology. Wadsworth (Brooks/Cole).\n\n\nWelch, B. L. (1947). The generalization of \"student’s\" problem when several different population varlances are involved. Biometrika, 34(1–2), 28–35. https://doi.org/10.1093/biomet/34.1-2.28\n\n\nZhang, J., & Boos, D. D. (1992). Bootstrap critical values for testing homogeneity of covariance matrices. Journal of the American Statistical Association, 87(418), 425–429. http://www.jstor.org/stable/2290273",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-eqcov.html#footnotes",
    "href": "13-eqcov.html#footnotes",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "",
    "text": "If group sizes are greatly unequal and homogeneity of variance is violated, then the \\(F\\) statistic is too liberal (actual rejection rate greater than the nominal \\(\\alpha\\)) when large sample variances are associated with small group sizes. Conversely, the \\(F\\) statistic is too conservative (actual \\(&lt; \\alpha\\)) if large variances are associated with large group sizes.↩︎\nFor example, Tiku & Balakrishnan (1984) concluded from simulation studies that the normal-theory LRT provides poor control of Type I error under even modest departures from normality. O’Brien (1992) proposed some robust alternatives, and showed that Box’s normal theory approximation suffered both in controlling the null size of the test and in power. Zhang & Boos (1992) also carried out simulation studies with similar conclusions and used bootstrap methods to obtain corrected critical values.↩︎\nMore general theory and statistical applications of the geometry of ellispoids is given by Friendly et al. (2013).↩︎\nAnderson (2006) describes other procedures based on Euclidean distances \\(D_{ij} = [\\Sigma (y_{ij} - \\bar{y}_j)^2]^{1/2}\\) of the observations from their group centroids (means or spatial medians). Rather than fit a parametric MANOVA model, he suggests a permutation test method (PERMDIST) which randomly permutes the group assignments and yields non-parametric tests.↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "14-infl-robust.html",
    "href": "14-infl-robust.html",
    "title": "13  Multiviate Influence and Robust Estimation",
    "section": "",
    "text": "13.1 Multivariate influence\nIn the analysis of linear models, the identification and treatment of outliers and influential observations represents one of the most critical, yet challenging, aspects of statistical modeling. As you saw earlier (Section 6.5), even a single “bad” observation can completely alter the results of a linear model fit by ordinary least squares. In the analysis of the dataset heplots::schooldata (Section 10.7.3), we found a collection of cases that exerted undue influence on the fitted model and its’ interpretation. In this chapter, I extend the earlier discussion of multivariate influence and introduce some methods of robust estimation to deal with unruly observations.\nUnivariate influence diagnostics have been well-established since the pioneering work of Cook (1977) and others (Belsley et al. (1980); Cook & Weisberg (1982)). Their wide implementation in R packages such as stats and carmakes these readily accessible in statistical practice.1\nThus, if you seek statistical advice regarding a perplexing model, one that defies interpretation or has inconsistencies, your consultant may very well ask you:\nHowever, the extension to multivariate response models introduces additional complexity that goes far beyond simply applying univariate methods to each response variable separately. The multivariate case requires consideration of the joint influence structure across all responses simultaneously, accounting for the correlation patterns among dependent variables and the potential for observations to be influential in some linear combinations of responses while appearing benign when examined multivariate.\nThis multivariate perspective can reveal influence patterns that would otherwise remain hidden, as an observation might exert substantial leverage on the overall model fit through subtle but systematic effects across multiple responses. Detecting outliers and influential observations has now progressed to the point where the methods described below (Section 13.1) can usefully be applied to multivariate linear models.\nHaving found some troublesome cases, the question arises: what should you do about them? We are generally reluctant to simply ignore them, unless there is evidence of a gross data error (as in the Davis data, Section 2.1.2). Instead, a large class of robust methods, which reduce the impact of outliers on the analysis, have been developed. These are described for multivariate models in Section 13.5 below. The CRAN Task View on Robust Estimation provides a comprehensive overview of R packages for robust estimation.\nPackages\nIn this chapter I use the following packages. Load them now.\nAn elegant extension of the ideas behind leverage, studentized residuals and measures of influence to the case of multivariate response data is due to Barrett & Ling (1992), and illustrated in Barrett (2003). These methods have been implemented in the mvinfluence package (Friendly, 2025) which makes available several forms of influence plots to visualize the results.\nAs in the univariate case, the measures of multivariate influence stem from the case-deletion idea: comparing some statistic calculated from the full sample to that statistic calculated when case \\(i\\) is deleted. The Barrett-Ling approach generalized this to the case of deleting a set \\(I\\) of \\(m \\ge 1\\) cases. This can be useful because some cases can “mask” the influence of others in the sense that when one is deleted, others become much more influential. However, in most cases the default of deleting individual observations (\\(m=1\\)) is sufficient.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "14-infl-robust.html#sec-multivariate-influence",
    "href": "14-infl-robust.html#sec-multivariate-influence",
    "title": "13  Multiviate Influence and Robust Estimation",
    "section": "",
    "text": "13.1.1 Notation\nTo explain the extension of influence diagnostics to the multivariate case, it is useful to define some notation used to designate terms in the model calculated from the complete dataset versus those calculated with one or more observations excluded. As before, let \\(\\mathbf{X}\\) be the model matrix in the multivariate linear model, \\(\\mathbf{Y}_{n \\times p} = \\mathbf{X}_{n \\times q} \\; \\mathbf{B}_{q \\times p} + \\mathbf{E}_{n \\times p}\\). As we know, the usual least squares estimate of \\(\\mathbf{B}\\) is given by \\(\\mathbf{B} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}  \\mathbf{X}^\\top \\mathbf{Y}\\).\nThen let:\n\n\n\\(\\mathbf{X}_I\\) be the submatrix of \\(\\mathbf{X}\\) whose \\(m\\) rows are indexed by \\(I\\),\n\n\\(\\mathbf{X}_{(-I)}\\) is the complement, the submatrix of \\(\\mathbf{X}\\) with the \\(m\\) rows in \\(I\\) deleted,\n\nMatrices \\(\\mathbf{Y}_I\\), \\(\\mathbf{Y}_{(-I)}\\) are defined similarly, denoting the submatrix of \\(m\\) rows of \\(\\mathbf{Y}\\) and the submatrix with those rows deleted, respectively.\nThe calculation of regression coefficients when the cases indexed by \\(I\\) have been removed has the form \\(\\mathbf{B}_{(-I)} = (\\mathbf{X}_{(-I)}^\\top \\mathbf{X}_{(-I)})^{-1} \\mathbf{X}_{(-I)}^\\top \\mathbf{Y}_{I}\\). The corresponding residuals are expressed as \\(\\mathbf{E}_{(-I)} = \\mathbf{Y}_{(-I)} - \\mathbf{X}_{(-I)} \\mathbf{B}_{(-I)}\\).\n\n13.1.2 Hat values and residuals\nThe influence measures defined by Barrett & Ling (1992) are functions of two matrices \\(\\mathbf{H}_I\\) and \\(\\mathbf{Q}_I\\) corresponding to hat values and residuals, defined as follows:\n\nFor the full data set, the “hat matrix”, \\(\\mathbf{H}\\), is given by \\(\\mathbf{H} = \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top\\),\n\n\\(\\mathbf{H}_I\\) is the \\(m \\times m\\) the submatrix of \\(\\mathbf{H}\\) corresponding to the index set \\(I\\), \\(\\mathbf{H}_I = \\mathbf{X} (\\mathbf{X}_I^\\top \\mathbf{X}_I)^{-1} \\mathbf{X}^\\top\\),\n\n\\(\\mathbf{Q}\\) is the analog of \\(\\mathbf{H}\\) defined for the residual matrix \\(\\mathbf{E}\\), that is, \\(\\mathbf{Q} = \\mathbf{E} (\\mathbf{E}^\\top \\mathbf{E})^{-1} \\mathbf{E}^\\top\\), with corresponding submatrix \\(\\mathbf{Q}_I = \\mathbf{E} \\, (\\mathbf{E}_I^\\top \\mathbf{E}_I)^{-1} \\, \\mathbf{E}^\\top\\),\n\nBased on these quantitites, Barrett and Ling (1992) defined multivariate analogs of nearly all univariate measures suggested in the literature. I only consider their Cook’s D measure here.\n\n13.1.3 Cook’s distance\nMultivariate analogs of all the usual influence diagnostics (Cook’s D, CovRatio, …) can be defined in terms of \\(\\mathbf{H}\\) and \\(\\mathbf{Q}\\). For instance, Cook’s distance is defined for a univariate response by\n\\[\nD_I = (\\mathbf{b} - \\mathbf{b}_{(-I)})^T (\\mathbf{X}^T \\mathbf{X}) (\\mathbf{b} - \\mathbf{b}_{(-I)}) / p s^2 \\; ,\n\\]\na measure of the squared distance between the coefficients \\(\\mathbf{b}\\) for the full data set and those \\(\\mathbf{b}_{(-I)}\\) obtained when the cases in \\(I\\) are deleted.\nIn the multivariate case, Cook’s distance is obtained by replacing the vector of coefficients \\(\\mathbf{b}\\) by \\(\\mathrm{vec} (\\mathbf{B})\\), which is the result of stringing out the coefficients for all responses in a single \\((n \\times p)\\)-length vector.\n\\[\nD_I = \\frac{1}{p} \\; [\\mathrm{vec} (\\mathbf{B} - \\mathbf{B}_{(-I)})]^T \\; (S^{-1} \\otimes \\mathbf{X}^T \\mathbf{X}) \\; \\mathrm{vec} (\\mathbf{B} - \\mathbf{B}_{(-I)})  \\; ,\n\\] where \\(\\otimes\\) is the Kronecker (direct) product and \\(\\mathbf{S} = \\mathbf{E}^T \\mathbf{E} / (n-p)\\) is the covariance matrix of the residuals.\n\n13.1.4 Leverage and residual components\nWe gain further insight by considering how far we can generalize from the case for a univariate response. For single-case deletions (\\(m = 1\\)), Cook’s distance can be re-written as a product of leverage and residual components as:\n\\[\nD_i = \\left(\\frac{n-p}{p} \\right) \\frac{h_{ii} q_{ii}}{(1 - h_{ii})^2  } \\;.\n\\]\nThis suggests that we can factor that to define a leverage component \\(L_i\\) and residual component \\(R_i\\) as the product of two terms,\n\\[\nL_i = \\frac{h_{ii}}{1 - h_{ii}} \\quad\\quad R_i = \\frac{q_{ii}}{1 - h_{ii}} \\;.\n\\]\n\\(R_i\\) is the studentized residual here. Thus \\(D_i \\propto L_i \\times R_i\\). This idea is used to define a new version of influence plot, as illustrated in the following (Section 13.2.1).\nIn the general, multivariate case there are analogous matrix expressions for \\(\\mathbf{L}\\) and \\(\\mathbf{R}\\). When \\(m &gt; 1\\), the quantities \\(\\mathbf{H}_I\\), \\(\\mathbf{Q}_I\\), \\(\\mathbf{L}_I\\), and \\(\\mathbf{R}_I\\) are \\(m \\times m\\) matrices. Where scalar quantities are needed, the mvinfluence functions apply a function, FUN, either det() or tr() to calculate a measure of “size”. This is done as:\nH &lt;- sapply(x$H, FUN)\nQ &lt;- sapply(x$Q, FUN)\nL &lt;- sapply(x$L, FUN)\nR &lt;- sapply(x$R, FUN)\nThis is the same trick used in the calculation of the various multivariate test statistics like Wilks’ Lambda and Pillai’s trace. In this way, the full range of multivariate influence measures discussed by Barrett (2003) can be calculated.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "14-infl-robust.html#the-mysterious-case-9",
    "href": "14-infl-robust.html#the-mysterious-case-9",
    "title": "13  Multiviate Influence and Robust Estimation",
    "section": "\n13.2 The Mysterious Case 9",
    "text": "13.2 The Mysterious Case 9\nTo illustrate these ideas this toy example, from Barrett (2003), considers the simplest case, of one predictor (x) and two response variables, y1 and y2.\n\nToy &lt;- tibble(\n   case = 1:9,\n   x =  c(1,    1,    2,    2,    3,    3,    4,    4,    10),\n   y1 = c(0.10, 1.90, 1.00, 2.95, 2.10, 4.00, 2.95, 4.95, 10.00),\n   y2 = c(0.10, 1.80, 1.00, 2.93, 2.00, 4.10, 3.05, 4.93, 10.00)\n)\n\nA quick peek (Figure 13.1) at the data indicates that y1 and y2 are nearly perfectly correlated with each other. Both of these are also strongly linear with x and there is one extreme point (case 9). The data is pecuiliar, but looking at these pairwise plots doesn’t suggest that anything is terribly wrong. In the plots of y1 and y2 against x, case 9 simply looks like a good leverage point (Section 6.5).\n\n\nscatterplotMatrix(~ y1 + y2 + x, data=Toy, \n  cex=2,\n  col = \"blue\", pch = 16,\n  id = list(n=1, cex=2), \n  regLine = list(lwd = 2, col=\"red\"),\n  smooth = FALSE)\n\n\n\n\n\n\nFigure 13.1: Scatterplot matrix for the toy example.\n\n\n\n\nFor this example, we fit the univariate models with y1 and y2 separately and then the multivariate model.\n\nToy.lm1 &lt;- lm(y1 ~ x, data=Toy)\nToy.lm2 &lt;- lm(y2 ~ x, data=Toy)\nToy.mlm &lt;- lm(cbind(y1, y2) ~ x, data=Toy)\n\n\n13.2.1 Cook’s D\nFirst, let’s examine the Cook’s D statistics for the models. Note that the function cooks.distance() invokes stats::cooks.distance.lm() for the univariate response models, but invokes mvinfluence::cooks.distance.mlm() for the multivariate model.\n\ndf &lt;- Toy\ndf$D1  &lt;- cooks.distance(Toy.lm1)\ndf$D2  &lt;- cooks.distance(Toy.lm2)\ndf$D12 &lt;- cooks.distance(Toy.mlm)\n\ndf\n# # A tibble: 9 × 7\n#    case     x    y1    y2      D1      D2    D12\n#   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n# 1     1     1  0.1   0.1  0.121   0.118   0.125 \n# 2     2     1  1.9   1.8  0.124   0.103   0.298 \n# 3     3     2  1     1    0.0901  0.0886  0.0906\n# 4     4     2  2.95  2.93 0.0829  0.0819  0.0831\n# 5     5     3  2.1   2    0.0548  0.0673  0.182 \n# 6     6     3  4     4.1  0.0692  0.0852  0.232 \n# 7     7     4  2.95  3.05 0.0793  0.0651  0.203 \n# 8     8     4  4.95  4.93 0.0665  0.0643  0.0690\n# 9     9    10 10    10    0.00159 0.00830 3.22\n\nThe only thing remarkable here is for case 9: The univariate Cook’s \\(D\\)s, D1 and D2, are very small, yet the multivariate statistic, D12=3.22 is over 10 times the next largest value.\nLet’s see how case 9 stands out in the influence plots (Figure 13.2). It has an extreme hat value. But, because it’s residual is very small, it does not have much influence on the fitted models for either y1 or y2. Neither of these plots suggest that anything is terribly wrong with the univariate models—none of the points are in the “danger” zones of the upper- and lower-right corners.\n\nip1 &lt;- car::influencePlot(Toy.lm1, \n                          id = list(cex=1.5), cex.lab = 1.5)\nip2 &lt;- car::influencePlot(Toy.lm2, \n                          id = list(cex=1.5), cex.lab = 1.5)\n\n\n\n\n\n\nFigure 13.2: Influence plots for the univariate models for y1 and y2\n\n\n\n\nTODO: Check how these are defined in mvinfluence\nContrast these results with what we get for the model for y1 and y2 jointly (Figure 13.3) In the multivariate version, mvinfluence::influencePlot.mlm() plots the squared studentized residual (denoted R in the output) against the hat value; this is referred to as a `type = “stres” plot.2 Case 9 stands in Figure 13.3 out as wildly influential on the joint regression model. But there’s more: The cases in Figure 13.2 with large Cook’s D (bubble size) have only tiny influence in the multivariate model.\n\ninfluencePlot(Toy.mlm, type = \"stres\",\n              id.n=2, id.cex = 1.3,\n              cex.lab = 1.5)\n#       H     Q CookD     L     R\n# 1 0.202 0.177 0.125 0.253 0.222\n# 2 0.202 0.422 0.298 0.253 0.529\n# 6 0.113 0.587 0.232 0.127 0.662\n# 9 0.852 1.082 3.225 5.750 7.301\n\n\n\n\n\n\nFigure 13.3: Studentized residual influence plot for the multivariate model (y1, y2) ~ x. Dotted vertical lines mark large hat values, \\(H &gt; {2, 3} p/n\\). The dotted horizontal line marks large values of the squared studentized residual.\n\n\n\n\n\n\n\n\n\n\nTheory into Practice\n\n\n\nChairman Mao said, “Theory into practice”, but Tukey (1959) said that, “The practical power of a statistical test is the product of its’ statistical power and the probability of use”. The story for multivariate influence here illustrates a nice feature of the connections between statistical theory, graphic development and implemented in software here.\nA statistical development proposes a new way of thinking about a problem. Someone with a graphical bent looks at this and think, “How can I visualize this?”. A software developer then solves the remaining problem of how to incorporate that into easy-to-use functions or applications. If only this was easy. But sometimes, all three roles appear within a given person as was the case here.\n\n\nThe general formulation of Barrett (2003) suggests an alternative form of the multivariate influence plot (Figure 13.4) that uses the leverage (L) and residual (R) components directly, specified as type = \"LR\".\nBecause influence is the product of leverage and residual, a plot of \\(\\log(L)\\) versus \\(\\log(R)\\) has the attractive property that contours of constant Cook’s distance fall on diagonal lines with slope = -1. Adjacent reference lines represent constant multiples of influence. Further from the origin implies greater influence, as opposed to other forms, where conventional cutoffs are used to show regions of high influence.\n\ninfluencePlot(Toy.mlm, type=\"LR\",\n              id.n=2, id.cex = 1.3,\n              cex.lab = 1.5) -&gt; infl\n\n\n\n\n\n\nFigure 13.4: LR plot of \\(\\log(L)\\) versus \\(\\log(R)\\) for the multivariate model (y1, y2) ~ x. Dotted lines show contours of constant Cook’s distance.\n\n\n\n\n\n13.2.2 DFBETAS\nThe DFBETAS statistics give the estimated change in the regression coefficients when each case is deleted in turn. We can gain some insight as to why case 9 is unremarkable in the univariate regressions by plotting these, shown in Figure 13.5. The values come from stats::dfbetas() and return the standardized values.\n\nCodedb1 &lt;- as.data.frame(dfbetas(Toy.lm1))\ngg1 &lt;- ggplot(data = db1, aes(x=`(Intercept)`, y=x, label=rownames(db1))) +\n  geom_point(size=1.5) +\n  geom_label(size=6, fill=\"pink\") +\n  xlab(expression(paste(\"Deletion Intercept  \", b[0]))) +\n  ylab(expression(paste(\"Deletion Slope  \", b[1]))) +\n  ggtitle(\"dfbetas for y1\") +\n  theme_bw(base_size = 16)\n\ndb2 &lt;- as.data.frame(dfbetas(Toy.lm2))\ngg2 &lt;- ggplot(data = db2, aes(x=`(Intercept)`, y=x, label=rownames(db2))) +\n  geom_point(size=1.5) +\n  geom_label(size=6, fill=\"pink\") +\n  xlab(expression(paste(\"Deletion Intercept  \", b[0]))) +\n  ylab(expression(paste(\"Deletion Slope  \", b[1]))) +\n  ggtitle(\"dfbetas for y2\") +\n  theme_bw(base_size = 16)\n\ngg1 + gg2\n\n\n\n\n\n\nFigure 13.5: Plots of changes in the coefficients (DFBETAS) for \\(y_1\\) and \\(y_2\\) as each observation is deleted for the toy example.\n\n\n\n\nThe values for case 9 are nearly (0, 0) in both plots, indicating that deleting this case has negligible effect in both univariate regressions. Yet, case 9 appeared very influential in the multivariate model. Why did this happen?\nIn this contrived example, the problem arose from the very high correlation between y1 and y2, \\(r = 0.9997\\) as can be seen in the (y1, y2) panel in Figure 13.1. Although each of the y1 and y2 values for the high-leverage cases are in-line with the univariate regressions (and thus have small univariate Cook’s Ds), the ill-conditioning magnifies small discrepancies in their positions, making the multivariate Cook’s D larger. And that’s the solution to the Case of the Mysterious Case 9.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "14-infl-robust.html#example-nlsy-data",
    "href": "14-infl-robust.html#example-nlsy-data",
    "title": "13  Multiviate Influence and Robust Estimation",
    "section": "\n13.3 Example: NLSY data",
    "text": "13.3 Example: NLSY data\nThe heplots::NLSY data introduced in Section 10.6.1 provides a more realistic example of how the multivariate influence measures and their plots contribute to understanding multivariate data. Some plots were suggested there (Figure 10.12) to identify “noteworthy” points. The model NLSY.mod1 fits the child’s reading and math scores using parents’ education:\n\ndata(NLSY, package = \"heplots\")\nNLSY.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY)\n\nFor influence plots, the id.method = \"noteworthy\" method for point labeling selects observations with large values for any of the standardized residual, hat value or Cook’s D, so 6 points are labeled in Figure 13.6.\n\ninfluencePlot(NLSY.mod1,\n              id.method = \"noteworthy\",\n              id.n = 3, id.cex = 1.25,\n              cex.lab = 1.5)\n#           H        Q   CookD       L        R\n# 12  0.01547 0.090101 0.11149 0.01571 0.091516\n# 19  0.14656 0.004801 0.05629 0.17173 0.005625\n# 54  0.16602 0.016554 0.21986 0.19907 0.019849\n# 142 0.00509 0.208258 0.08478 0.00511 0.209323\n# 152 0.00422 0.049821 0.01682 0.00424 0.050032\n# 221 0.05707 0.000873 0.00399 0.06053 0.000926\n\n\n\n\n\n\nFigure 13.6: Influence plot for the NLSY data. Points with large standardized residual, or hat value or Cook’s D are labeled.\n\n\n\n\nThere is an interesting configuration of the points in Figure 13.6. One group of points (152, 12, 142) is apparent at the left side. These have relatively low leverage (hat value), and increasingly large residuals. Another group of points (221, 19, 54) have small residuals but increasingly large hat values. You should take a moment and compare these points with their positions in Figure 10.12. However, with id.n = 2 only two points are flagged: 54 and 142.\nThe LR plot is also instructive here.\n\ninfluencePlot(NLSY.mod1, type = \"LR\",\n              id.method = \"noteworthy\",\n              id.n = 3, id.cex = 1.25,\n              cex.lab = 1.5)\n#           H        Q   CookD       L        R\n# 12  0.01547 0.090101 0.11149 0.01571 0.091516\n# 19  0.14656 0.004801 0.05629 0.17173 0.005625\n# 54  0.16602 0.016554 0.21986 0.19907 0.019849\n# 142 0.00509 0.208258 0.08478 0.00511 0.209323\n# 152 0.00422 0.049821 0.01682 0.00424 0.050032\n# 221 0.05707 0.000873 0.00399 0.06053 0.000926\n\n\n\n\n\n\nFigure 13.7: LR Iinfluence plot for the NLSY data. The dashed lines are contours of increasing Cook’s D toward the upper right corner of the plot.\n\n\n\n\nIn Figure 13.7 the noteworthy points are arrayed within a diagonal band corresponding to contours of equal Cook’s D, and this statistic increases multiplicatively with each step away from the origin. Cases 54 and 142 stand out somewhat here.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "14-infl-robust.html#sec-peng-mvinfluence",
    "href": "14-infl-robust.html#sec-peng-mvinfluence",
    "title": "13  Multiviate Influence and Robust Estimation",
    "section": "\n13.4 Example: Penguin data",
    "text": "13.4 Example: Penguin data\nAnother example illustrates the appearance of influence plots with factors as predictors. Earlier chapters (Section 3.6.2, Section 4.7.1) presented a variety of plots (Figure 3.26, Figure 3.27, Figure 4.36) for the Penguin data in which a few cases were identified as unusual in exploratory analysis. But, are any of them influential in the context of a multivariate model?\nLet’s consider the simple model predicting species from the numeric variables\n\ndata(peng, package=\"heplots\")\npeng.mlm &lt;- lm(cbind(bill_length, bill_depth, flipper_length, body_mass) ~\n                 species, data=peng)\n\nIn the influence plot (Figure 13.8) the predictor species if of course discrete, so there are only three distinct hat-values and the values for each species appear as columns of points in this plot. For ease of interpretation, I use a little dplyr magic to label the species and their sample sizes.\n\n\nres &lt;- influencePlot(peng.mlm, id.n=3, type=\"stres\")\nres |&gt; arrange(desc(CookD)) |&gt; print(digits = 2)\n#          H      Q  CookD      L      R\n# 283 0.0147 0.0919 0.1486 0.0149 0.0932\n# 286 0.0147 0.0322 0.0520 0.0149 0.0327\n# 179 0.0084 0.0512 0.0474 0.0085 0.0517\n# 10  0.0068 0.0562 0.0424 0.0069 0.0566\n# 268 0.0147 0.0079 0.0129 0.0149 0.0081\n# 267 0.0147 0.0037 0.0061 0.0149 0.0038\n# 266 0.0147 0.0021 0.0034 0.0149 0.0021\n\n# labels for species, adding sample n\nloc &lt;- merge(peng |&gt; add_count(species), \n             res, \n             by = \"row.names\") |&gt;\n  group_by(species) |&gt;\n  slice(1) |&gt;\n  ungroup() |&gt;\n  select(species, H, n) |&gt;\n  mutate(label = glue::glue(\"{species}\\n(n={n})\"))\ntext(loc$H, 0.10, loc$label, xpd=TRUE)\n\n\n\n\n\n\nFigure 13.8: Influence plot for the Penguin data, showing the squared studentized residual vs. hat value. Unusual points on either variable or on the Cook’s D statistic are identified with their case number.\n\n\n\n\nWe know that hat-values are proportional to how unusual the observations are from the means of the predictors, but what is this for a factor, like species? The answer is that \\(H_j \\propto 1 / n_j\\), so Chinstrap with the smallest sample size (\\(n=68\\)) is the most unusual.\nThe most influential case (283) here is our Chinstrap friend “Cyrano” (see Figure 3.27). Among the others, case 10 is the Adélie bird we labeled “Hook Nose”. To better understand why these cases are influential, we can make plots of the data in data space or in PCA space as we did earlier.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "14-infl-robust.html#sec-robust-estimation",
    "href": "14-infl-robust.html#sec-robust-estimation",
    "title": "13  Multiviate Influence and Robust Estimation",
    "section": "\n13.5 Robust Estimation",
    "text": "13.5 Robust Estimation\nRobust methods for multivariate linear models aim to provide reliable parameter estimates and inference procedures that remain stable in the presence of outlying observations. As noted by Rousseeuw et al. (2004), “The main advantage of robust regression is that it provides reliable results even when some of the assumptions of classical regression are violated.” But this advantage comes at the cost of increased computational complexity.3\nRobust regression is a compromise between excluding “bad” data points entirely from analysis vs. including all the data and treating them equally in classical OLS regression. The essential idea of robust regression is to weight the observations differently based on how well behaved these observations are. Roughly speaking, it is a form of weighted least squares regression. But because the weights are derived from the data, this must be done iteratively, requiring more computation.\nSeveral general approaches have been developed for robust multivariate regression. These include M-estimators, S-estimators (Rousseeuw & Yohai, 1984), and MM-estimators (Yohai, 1987). Each approach offers different trade-offs between robustness properties, computational efficiency, and statistical efficiency under ideal conditions. See the CRAN Task View: Robust Statistical Methods for an extensive list of robust methods in R and the vignette for the rrcov package for a general overview of multivariate robust methods. \nThe method implemented in the heplots::robmlm() function belongs to the class of M-estimators. This generalizes OLS estimation by replacing the least squares criterion with a more robust version. The key idea is to relax the least squares criterion of minimizing \\(Q(\\mathbf{e}) = \\Sigma e_i^2 = \\Sigma (y_i - \\hat{y}_i)^2\\) by considering more general functions \\(Q(\\mathbf{e}, \\rho)\\), where the function \\(\\rho (e_i)\\) can be chosen to reduce the impact of large outliers. In these terms,\n\nOLS uses \\(\\rho(e_i) = e_i^2\\)\n\n\n\\(L_1\\) estimation uses \\(\\rho(e_i) = \\vert e_i \\vert\\), the least absolute values\nA bit more complicated, the biweight function uses a squared measure of error up to some value \\(c\\) and then levels off thereafter,\n\n\\[\n\\rho(e_i) =\n\\begin{cases}\n\\left[ 1 - \\left( \\frac{e_i}{c} \\right)^2 \\right]^2 & |e_i| \\leq c, \\\\\n1 & |e_i| &gt; c.\n\\end{cases}\n\\]\nThese functions are more easily understood in a graph (Figure 13.9). The biweight function (ref?) has a property like Windsorizing— the squared error remains constant for residuals \\(e_i &gt; c\\), with the tuning constant \\(c = 4.685\\) for MASS::psi.bisquare().\n\n\n\n\n\n\n\nFigure 13.9: Diagram plotting the function \\(\\rho(e_i)\\) of the contributions of the residuals \\(e_i\\) to what is minimized in various fitting methods.\n\n\n\n\nBut, there is a problem here, in that weighted least squares is designed for situations where the observation weights are known in advance, for instance, when data points have unequal variances or when some observations are more reliable than others.\nThe solution, called iteratively reweighted least squares (IRLS), is to substitute computation for theory, and iterate between estimating the coefficients (with given weights) and determining weights for the observations in the next iteration. This is implemented in heplots::robmlm(), which generalizes the methods used in MASS::rlm(). …",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "14-infl-robust.html#sec-peng-robust",
    "href": "14-infl-robust.html#sec-peng-robust",
    "title": "13  Multiviate Influence and Robust Estimation",
    "section": "\n13.6 Example: Penguin data",
    "text": "13.6 Example: Penguin data\n\nIn earlier analyses of the Penguin data, a few points appeared unusual for their species in various plots (Figure 3.21, Figure 3.27) and a few were deemed overly influential in Section 13.4 for the simple model peng.mlm predicting species. What effect does fitting a robust MANOVA have on the results? We can just substitute robmlm() for lm(). to fit the robust model.\n\npeng.rlm &lt;- robmlm(cbind(bill_length, bill_depth, flipper_length, body_mass) ~\n                    species, data=peng)\n\nThe plot() method for a \"robmlm\" object give an index plot of the observation weights in the final iteration. I take some care here to color the points by species, label the points with the lowest weights and add my names for a couple of birds.\n\n\nsource(here::here(\"R\", \"penguin\", \"penguin-colors.R\"))\ncol = peng.colors(\"dark\")[peng$species]\nplot(peng.rlm, \n     segments = TRUE,\n     id.weight = 0.6,\n     col = col,\n     cex.lab = 1.3)\n  # label old friends\nnotables &lt;- tibble(\n  id = c(10, 283),\n  name = c(\"HookNose\", \"Cyrano\"),\n  wts = peng.rlm$weights[id]\n)\ntext(notables$id, notables$wts, \n     label = notables$name, pos = 3,\n     xpd = TRUE)\n  # label species in axis at the top\nctr &lt;- split(seq(nrow(peng)), peng$species) |&gt; lapply(mean)\naxis(side = 3, at=ctr, labels = names(ctr), cex.axis=1.2)\n\n\n\n\n\n\nFigure 13.10: Index plot of the observation weights for the robust model, peng.rlm. Observations with weights &lt; 0.5 are labeled with their case number.\n\n\n\n\nThe observations that are labeled here are among those which stood out as notable in other plots. In Figure 13.10, the lowest four points are down-weighted considerably. Our friend “Cyrano” (283), the greatest outlier, gets a weight of exactly zero, so is effectively excluded from the model; “Hook Nose” (10) gets a weight of 0.13, and so he can’t do much damage.\n\ncases &lt;- c(10, 124, 179, 283)\nlowwt &lt;- peng.rlm$weights[cases]\nnames(lowwt) &lt;- cases\nlowwt\n#    10   124   179   283 \n# 0.137 0.367 0.287 0.000\n\nAnother way to assess the effect of using a robust model is to examine its’ effects on the model coefficients. heplots::rel_diff() calculates the relative difference, \\(\\vert x - y \\vert / x\\), expressed here as percent of change. As you can see below, the changes in most of the coefficients is rather small.\n\nrel_diff(coef(peng.mlm), coef(peng.rlm)) |&gt; \n  print(digits=2)\n#                  bill_length bill_depth flipper_length body_mass\n# (Intercept)            0.083       0.26          0.092      0.43\n# speciesChinstrap       0.572      78.10          8.053     76.20\n# speciesGentoo          0.857      -0.51          0.089      0.12\n\nThe largest differences are for the coefficients for Chinstrap, particularly for bill_depth and body_mass, which reflect the dramatic effect of effectively removing “Cyrano” from the analysis.\nIf you accept the premise of down-weighting cases with large residuals in robust methods, robmlm() provides a principled way to handle them. Bye-bye Cyrano—it’s been fun—but we can get along nicely without you.\nTODO: Add chapter “What have we learned?”\n\n\n\n\nBarrett, B. E. (2003). Understanding influence in multivariate regression. Communications in Statistics - Theory and Methods, 32(3), 667–680. https://doi.org/10.1081/STA-120018557\n\n\nBarrett, B. E., & Ling, R. F. (1992). General classes of influence measures for multivariate regression. Journal of the American Statistical Association, 87(417), 184–191. https://www.jstor.org/stable/i314301\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley; Sons.\n\n\nCook, R. D. (1977). Detection of influential observation in linear regression. Technometrics, 19(1), 15–18. https://doi.org/10.1080/00401706.1977.10489493\n\n\nCook, R. D., & Weisberg, S. (1982). Residuals and influence in regression. Chapman; Hall.\n\n\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: Algorithms, evidence, and data science. Cambridge University Press. https://doi.org/10.1017/9781108914062\n\n\nFriendly, M. (2025). Mvinfluence: Influence measures and diagnostic plots for multivariate linear models. https://github.com/friendly/mvinfluence\n\n\nHebbali, A. (2024). Olsrr: Tools for building OLS regression models. https://doi.org/10.32614/CRAN.package.olsrr\n\n\nLoy, A., & Hofmann, H. (2014). HLMdiag: A suite of diagnostics for hierarchical linear models in r. Journal of Statistical Software, 56(5), 1–28. https://doi.org/10.18637/jss.v056.i05\n\n\nRousseeuw, P. J., Van Aelst, S., Van Driessen, K., & Gulló, J. A. (2004). Robust multivariate regression. Technometrics, 46(3), 293–305. https://doi.org/10.1198/004017004000000329\n\n\nRousseeuw, P. J., & Yohai, V. J. (1984). Robust regression by means of S-estimators. In J. Franke, W. Härdle, & D. Martin (Eds.), Robust and nonlinear time series analysis (Vol. 26, pp. 256–272). Springer.\n\n\nTukey, J. W. (1959). A quick, compact, two sample test to Duckworth’s specifications. Technometrics, 1, 31–48. https://doi.org/10.2307/1266308\n\n\nYohai, V. J. (1987). High breakdown-point and high efficiency robust estimates for regression. The Annals of Statistics, 15(2), 642–656.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "14-infl-robust.html#footnotes",
    "href": "14-infl-robust.html#footnotes",
    "title": "13  Multiviate Influence and Robust Estimation",
    "section": "",
    "text": "Among other packages providing influence diagnostic plots for univariate response models, olsrr (Hebbali, 2024) provides a wide variety of measures. However, the graphic methods for these are mostly limited to simple index plots of the measure against case number. For hierarchical linear models, where observations are not independent, but are clustered within larger groups, the HLMdiag (Loy & Hofmann, 2014) provides functions like hlm_influence() and hlm_augment() to generate influence diagnostics and combine them with residuals.↩︎\nSimilar to the univariate version, hat values greater than 2 or 3 times their average, \\(\\bar{h} = p/n\\) here, are considered large in the multivariate case. Values of the squared studentized residual \\(R_i\\) are calibrated by the Beta distribution, \\(\\text{Beta}(\\alpha=0.95, q/2, (n-p-q)/2)\\).↩︎\nThe tradeoff between simplifying assumptions, which makes computations easier and less restrictive models, achieved by increased computation lies at the heart of modern statistical methods. In Computer Age Statistical Inference, Efron & Hastie (2021) trace this thread in a wide variety ranging from classics in frequentist and Bayesian, through early computer-age topics to the most recent twenty-first century methods.↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "15-case-studies.html",
    "href": "15-case-studies.html",
    "title": "\n14  Case Studies\n",
    "section": "",
    "text": "14.1 Neuro- and Social-cognitive measures in psychiatric groups\nFor ease of exposition, the chapters in Part IV of this book used small examples to illustrate the statistical and visualization aspects of the techniques as they were described. Some of the main examples were spread over Chapters 10–13 to illustrate the specific statistical and graphical methods of those chapters.\nHere I present more complete analyses of a few datasets to place the methods of this book in the wider research context. I illustrate a sequence of steps, starting with research questions involved and proceeding to the graphical, computing and statistical methods of this book required to understand and explain these data.1\nPackages\nIn this chapter I use the following packages. Load them now.\nA Ph.D. dissertation by Laura Hartman (2016) at York University was designed as a test case of ideas about whether cognitive aspects of clinical patients could be useful in understanding schizophrenia (Heinrichs et al., 2015). Her studies attempted to evaluate whether and how clinical patients diagnosed (on the DSM-IV) as schizophrenic or with a schizoaffective disorder could be distinguished from each other and from a normal, control sample on collections of standardized tests in the following domains:\nThe study is an important contribution to clinical research because the two diagnostic categories are subtly different and their manifest symptoms often overlap. Yet clinically, they’re very different and often require different treatments. A key difference between schizoaffective disorder and schizophrenia is the prominence of mood disorder involving bipolar, manic and depressive moods. With schizoaffective disorder, mood disorders are front and center. With schizophrenia, that is not a dominant part of the disorder, but psychotic ideation (hearing voices, seeing imaginary people) is.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "15-case-studies.html#neuro--and-social-cognitive-measures-in-psychiatric-groups",
    "href": "15-case-studies.html#neuro--and-social-cognitive-measures-in-psychiatric-groups",
    "title": "\n14  Case Studies\n",
    "section": "",
    "text": "Neuro-cognitive: processing speed, attention, verbal learning, visual learning and problem solving;\nSocial-cognitive: managing emotions, theory of mind, externalizing, personalizing bias.\n\n\n\n14.1.1 Research questions\nThis example is concerned with the following substantitive questions:\n\nTo what extent can patients diagnosed as schizophrenic or with schizoaffective disorder be distinguished from each other and from a normal control sample using a well-validated, comprehensive neurocognitive battery specifically designed for individuals with psychosis (Heinrichs et al., 2015)?\nIf the groups differ, do any of the cognitive domains show particularly larger or smaller differences among these groups?\nDo the neurocognitive measures discriminate among the in the same or different ways? If different, how many separate aspects or dimensions are distinguished?\n\nApart from the research interest, it could aid diagnosis and treatment if these similar mental disorders could be distinguished by tests in the cognitive domain.\n\n14.1.2 Data\nThe clinical sample comprised 116 male and female patients who met the following criteria: 1) a diagnosis of schizophrenia (\\(n\\) = 70) or schizoaffective disorder (\\(n\\) = 46) confirmed by the Structured Clinical Interview for DSM-IV-TR Axis I Disorders; 2) were outpatients; 3) a history free of developmental or learning disability; 4) age 18-65; 5) a history free of neurological or endocrine disorder; and 6) no concurrent diagnosis of substance use disorder. Non-psychiatric control participants (\\(n\\) = 146) were screened for medical and psychiatric illness and history of substance abuse and were recruited from three outpatient clinics. The dataset is heplots::NeuroCog.\n\ndata(NeuroCog, package=\"heplots\")\nstr(NeuroCog)\n# 'data.frame': 242 obs. of  10 variables:\n#  $ Dx       : Factor w/ 3 levels \"Schizophrenia\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#   ..- attr(*, \"contrasts\")= num [1:3, 1:2] -0.5 -0.5 1 1 -1 0\n#   .. ..- attr(*, \"dimnames\")=List of 2\n#   .. .. ..$ : chr [1:3] \"Schizophrenia\" \"Schizoaffective\" \"Control\"\n#   .. .. ..$ : NULL\n#  $ Speed    : int  19 8 14 7 21 31 -1 17 7 37 ...\n#  $ Attention: int  9 25 23 18 9 10 8 20 30 15 ...\n#  $ Memory   : int  19 15 15 14 35 26 3 27 26 17 ...\n#  $ Verbal   : int  33 28 20 34 28 29 20 30 26 33 ...\n#  $ Visual   : int  24 24 13 16 29 21 12 32 27 21 ...\n#  $ ProbSolv : int  39 40 32 31 45 33 29 29 30 33 ...\n#  $ SocialCog: int  28 37 24 36 28 28 28 44 39 24 ...\n#  $ Age      : int  44 26 55 53 51 21 53 56 48 46 ...\n#  $ Sex      : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 2 2 1 2 1 ...\n#  - attr(*, \"na.action\")= 'omit' Named int [1:21] 1 2 3 4 5 6 7 8 9 10 ...\n#   ..- attr(*, \"names\")= chr [1:21] \"1\" \"2\" \"3\" \"4\" ...\n\nThe diagnostic classification variable is called Dx in the dataset. To facilitate answering questions regarding group differences, the following contrasts were applied: the first column compares the control group to the average of the diagnosed groups, the second compares the schizophrenia group against the schizoaffective group.\n\ncontrasts(NeuroCog$Dx)\n#                 [,1] [,2]\n# Schizophrenia   -0.5    1\n# Schizoaffective -0.5   -1\n# Control          1.0    0\n\nIn this analysis, I ignore the SocialCog variable because aspects of social cognition were measured more extensively in a separate dataset discussed below (Section 14.4). The primary focus here is on the the cognitive and perceptual variables Attention : ProbSolv.\n\n14.1.3 A first look\nAs always, plot the data first! We want an overview of the distributions of the variables to see the centers, spread, shape and possible outliers for each group on each variable.\nThe plot below combines the use of boxplots and violin plots to give an informative display. As we saw earlier (e.g., Section 10.6.1), doing this with ggplot2 requires reshaping the data to long format.\n\n# Reshape from wide to long\nNC_long &lt;- NeuroCog |&gt;\n  dplyr::select(-SocialCog, -Age, -Sex) |&gt;\n  tidyr::gather(key = response, value = \"value\", Speed:ProbSolv)\n# view 3 observations per group and measure\nNC_long |&gt;\n  group_by(Dx) |&gt;\n  sample_n(3) |&gt; ungroup()\n# # A tibble: 9 × 3\n#   Dx              response  value\n#   &lt;fct&gt;           &lt;chr&gt;     &lt;int&gt;\n# 1 Schizophrenia   Speed        39\n# 2 Schizophrenia   Visual       21\n# 3 Schizophrenia   Memory       40\n# 4 Schizoaffective ProbSolv     40\n# 5 Schizoaffective Speed        25\n# 6 Schizoaffective Verbal       48\n# 7 Control         Speed        33\n# 8 Control         ProbSolv     43\n# 9 Control         Attention    37\n\nIn the plot, we take care to adjust the transparency (alpha) values for the points, violin plots and boxplots so that all can be seen. Options for geom_boxplot() are used to give these greater visual prominence.\n\nCodeggplot(NC_long, aes(x=Dx, y=value, fill=Dx)) +\n  geom_jitter(shape=16, alpha=0.8, size=1, width=0.2) +\n  geom_violin(alpha = 0.1) +\n  geom_boxplot(width=0.5, alpha=0.4, \n               outlier.alpha=1, \n               outlier.size = 3, outlier.color = \"red\") +\n  scale_x_discrete(labels = c(\"Schizo\", \"SchizAff\", \"Control\")) +\n  facet_wrap(~response, scales = \"free_y\", as.table = FALSE) +\n  theme_bw() +\n  theme(legend.position=\"bottom\",\n        axis.title = element_text(size = rel(1.2)),\n        axis.text  = element_text(face = \"bold\"),\n        strip.text = element_text(size = rel(1.2)))\n\n\n\n\n\n\nFigure 14.1: Boxplots combined with violin plots of the NeuroCog data.\n\n\n\n\nWe can see that the control participants score higher on all measures, but there is no consistent pattern of medians for the two patient groups. But these univariate summaries do not inform about the relations among variables.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "15-case-studies.html#bivariate-views",
    "href": "15-case-studies.html#bivariate-views",
    "title": "\n14  Case Studies\n",
    "section": "\n14.2 Bivariate views",
    "text": "14.2 Bivariate views\nCorrgram\n\nWith this many variables, it is useful to view the correlations among them in a corrgram (Section 3.8), giving a high-level reconnaissance. As noted earlier, the corrgram::corrgram() function can enhance perception by permuting the variables in the order of their variable vectors in a biplot, so more highly correlated variables are adjacent in the plot, an example of effect ordering for data displays (Friendly & Kwan, 2003).\n\nNeuroCog |&gt;\n  select(-Dx, -SocialCog) |&gt;\n  corrgram(order = TRUE,\n           diag.panel = panel.density,\n           upper.panel = panel.pie)\n\n\n\n\n\n\nFigure 14.2: Corrgram of the NeuroCog data. The upper and lower triangles use two different ways of encoding the value of the correlation for each pair of variables.\n\n\n\n\nIn this plot you can see that adjacent variables are more highly correlated than those more widely separated. The diagonal panels show that most variables are reasonably symmetric in their distributions. Age, not included in this analysis is negatively correlated with the others: older participants tend to do less well on these tests.\nScatterplot matrix\nA scatterplot matrix gives a more detailed overview of all pairwise relations. The plot below suppresses the data points and summarizes the relation using data ellipses and regression lines. The model syntax, ~ Speed + ... |Dx, treats Dx as a conditioning variable (similar to the use of the color aestheic in ggplot2) giving a separate data ellipse and regression line for each group. (The legend is suppressed here. The groups are: Schizophrenic, SchizoAffective, Normal.)\n\n\ncol &lt;- c(\"red\", \"darkgreen\", \"blue\")\nscatterplotMatrix(~ Speed + Attention + Memory + Verbal + Visual + ProbSolv | Dx,\n                  data=NeuroCog,\n                  plot.points = FALSE,\n                  smooth = FALSE,\n                  legend = FALSE,\n                  col = col,\n                  ellipse=list(levels=0.68))\n\n\n\n\n\n\nFigure 14.3: Scatterplot matrix of the NeuroCog data. Points are suppressed here, focusing on the data ellipses and regression lines. Colors for the groups: Schizophrenic SchizoAffective Control\n\n\n\n\nIn Figure 14.3, you can see that the regression lines have similar slopes and similar data ellipses for the groups, though with a few exceptions. You can also see that the Normal group has higher means on all the variables and that the Schizophrenic and SchizoAffective don’t seem to differ very much.\nBiplot\nWhile still in exploratory mode, we can gain greater insight with our trusty multivariate juicer, the biplot (Section 4.3). A PCA of the cognitive variables shows that nearly 80% of the total variance is accounted for by two dimensions, of which the first dimension accounts for 69%.\n\nneuro.pca &lt;- NeuroCog |&gt;\n  select(Speed:Visual) |&gt; \n  prcomp(scale. = TRUE)\nsummary(neuro.pca)\n# Importance of components:\n#                          PC1   PC2   PC3    PC4    PC5\n# Standard deviation     1.856 0.720 0.629 0.5761 0.5570\n# Proportion of Variance 0.689 0.104 0.079 0.0664 0.0621\n# Cumulative Proportion  0.689 0.793 0.872 0.9379 1.0000\n\nThe biplot for this analysis (Figure 14.4) provides a nice summary of what was seen in the scatterplot matrix (Figure 14.3): A large difference between the Control group and the others, which don’t differ much from each other.\n\nggbiplot(neuro.pca, \n         obs.scale = 1, var.scale = 1,\n         groups = NeuroCog$Dx, \n         varname.size = 5,\n         var.factor = 1.5,\n         ellipse = TRUE) +\n  scale_color_discrete(name = 'groups') +\n  theme_minimal() +\n  theme(legend.direction = 'horizontal', \n        legend.position = 'top')\n\n\n\n\n\n\nFigure 14.4: Biplot of the NeuroCog data. The three groups differ are ordered along the first PCA dimension. Variable vectors show the correlations of the response variables with the two components.\n\n\n\n\nThe variable vectors in Figure 14.4 reflect the correlations of the response variables with each other and with the principal components, PC1 and PC2. All of the variables are positively correlated in this 2D view, Memory and Speed most highly so.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "15-case-studies.html#fitting-the-mlm",
    "href": "15-case-studies.html#fitting-the-mlm",
    "title": "\n14  Case Studies\n",
    "section": "\n14.3 Fitting the MLM",
    "text": "14.3 Fitting the MLM\nWe proceed to fit the one-way MANOVA model to answer the main research question of how well these variables distinguish among the groups.\n\nNC.mlm &lt;- lm(cbind(Speed, Attention, Memory, Verbal, Visual, ProbSolv) ~ Dx,\n             data=NeuroCog)\nAnova(NC.mlm)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#    Df test stat approx F num Df den Df  Pr(&gt;F)    \n# Dx  2     0.299     6.89     12    470 1.6e-11 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe first research question is captured by the contrasts for the Dx factor shown above. We can test these with car::linearHypothesis(). The contrast Dx1 for control vs. the diagnosed groups is highly significant,\n\n# control vs. patients\nprint(linearHypothesis(NC.mlm, \"Dx1\"), SSP=FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n# Pillai            1     0.289     15.9      6    234 2.8e-15 ***\n# Wilks             1     0.711     15.9      6    234 2.8e-15 ***\n# Hotelling-Lawley  1     0.407     15.9      6    234 2.8e-15 ***\n# Roy               1     0.407     15.9      6    234 2.8e-15 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHowever, the second contrast, Dx2, comparing the schizophrenic and schizoaffective group, shows no difference in the collection of response means.\n\n# Schizo vs SchizAff\nprint(linearHypothesis(NC.mlm, \"Dx2\"), SSP=FALSE)\n# \n# Multivariate Tests: \n#                  Df test stat approx F num Df den Df Pr(&gt;F)\n# Pillai            1     0.006    0.249      6    234   0.96\n# Wilks             1     0.994    0.249      6    234   0.96\n# Hotelling-Lawley  1     0.006    0.249      6    234   0.96\n# Roy               1     0.006    0.249      6    234   0.96\n\nAs a quick check on the model, a \\(\\chi^2\\) QQ plot (Figure 14.5) reveals no problems with multivariate normality of residuals nor potentially harmful residuals.\n\ncqplot(NC.mlm, id.n = 3)\n\n\n\n\n\n\nFigure 14.5: Chisquare QQ plot of the MANOVA model\n\n\n\n\n\n14.3.1 HE plot\nSo the question becomes: how to understand these results.heplot() shows the visualization of the multivariate model in the space of two response variables (the first two by default). The result (Figure 14.6) tells a very simple story: The control group performs higher on higher measures than the diagnosed groups, which do not differ between themselves.\n(Because heplot() gets the levels of factors from the model object, in order to abbreviate the group labels in the plot, you need to update() the MLM model after the labels are reassigned.)\n\n# abbreviate levels for plots\nNeuroCog$Dx &lt;- factor(NeuroCog$Dx, \n                      labels = c(\"Schiz\", \"SchAff\", \"Contr\"))\nNC.mlm &lt;- update(NC.mlm)\n\nThen, feed the model to heplot() for a plot of the first two response variables, Speed and Attention.\n\nheplot(NC.mlm, \n       fill=TRUE, fill.alpha=0.1,\n       cex.lab=1.5, cex=1.35)\n\n\n\n\n\n\nFigure 14.6: HE plot of Speed and Attention in the MLM for the NeuroCog data. The labeled points show the means of the groups on the two variables. The blue \\(\\mathbf{H}\\) ellipse for groups indicates the strong positive correlation of the group means.\n\n\n\n\nThis pattern, of the control group higher than the others, is consistent across all of the response variables, as we see from a plot of pairs(NC.mlm) in Figure 14.7.\n\npairs(NC.mlm, \n      fill=TRUE, fill.alpha=0.1,\n      var.cex=2)\n\n\n\n\n\n\nFigure 14.7: HE plot matrix of the MLM for NeuroCog data. The visual evidence that these outcome measures are strongly related in distinguishing the groups is very clear.\n\n\n\n\nThe group means are nearly perfectly correlated in all panels. This signals that we are likely to see a very simple representation of the data in canonical space. Note that this differs from the biplot view (Figure 14.4) in that biplot ignores the Dx factor in the PCA, even though we represent it in the plot.\n\n14.3.2 Canonical space\nWe can gain further insight, and a simplified plot showing all the response variables by projecting the MANOVA into the canonical space, which is entirely 2-dimensional (because \\(\\text{df}_h=2\\)). However, the output from candisc() shows that 98.5% of the mean differences among groups can be accounted for in just one canonical dimension.\n\nNC.can &lt;- candisc(NC.mlm)\nNC.can\n# \n# Canonical Discriminant Analysis for Dx:\n# \n#    CanRsq Eigenvalue Difference Percent Cumulative\n# 1 0.29295    0.41433      0.408    98.5       98.5\n# 2 0.00625    0.00629      0.408     1.5      100.0\n# \n# Test of H0: The canonical correlations in the \n# current row and all that follow are zero\n# \n#   LR test stat approx F numDF denDF Pr(&gt; F)    \n# 1        0.703     7.53    12   468   9e-13 ***\n# 2        0.994     0.30     5   235    0.91    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFigure 14.8 is the result of the plot() method for class \"candisc\" objects, that is, the result of calling plot(NC.can, ...). It plots the two canonical scores, \\(\\mathbf{Z}_{n \\times 2}\\) for the subjects, together with data ellipses for each of the three groups.\n\npos &lt;- c(4, 1, 4, 4, 1, 3)\ncol &lt;- c(\"red\", \"darkgreen\", \"blue\")\nplot(NC.can, \n     ellipse=TRUE, \n     rev.axes=c(TRUE,FALSE), \n     pch=c(7,9,10),\n     var.cex=1.2, cex.lab=1.5, var.lwd=2,  scale=4.5, \n     col=col,\n     var.col=\"black\", var.pos=pos,\n     prefix=\"Canonical dimension \")\n\n\n\n\n\n\nFigure 14.8: Canonical discriminant plot for the NeuroCog data MANOVA. Scores on the two canonical dimensions are plotted, together with 68% data ellipses for each group.\n\n\n\n\nThe interpretation of Figure 14.8 is again fairly straightforward. As noted earlier (Section 11.7), the projections of the variable vectors in this plot on the coordinate axes are proportional to the correlations of the responses with the canonical scores. From this, we see that the normal group differs from the two patient groups, having higher scores on all the neurocognitive variables, most of which are highyl correlated. The problem solving measure is slightly different, and this, compared to the cluster of memory, verbal and attention, is what distinguishes the schizophrenic group from the schizoaffectives.\nThe separation of the groups is essentially one-dimensional, with the control group higher on all measures. Moreover, the variables processing speed and visual memory are the purest measures of this dimension, but all variables contribute positively. The second canonical dimension accounts for only 1.5% of group mean differences and is non-significant (by a likelihood ratio test). Yet, if we were to interpret it, we would note that the schizophrenia group is slightly higher on this dimension, scoring better in problem solving and slightly worse on working memory, attention, and verbal learning tasks.\nSummary\nThis analysis gives a very simple description of the data, in relation to the research questions posed earlier:\n\nOn the basis of these neurocognitive tests, the schizophrenic and schizoaffective groups do not differ significantly overall, but these groups differ greatly from the normal controls.\nAll cognitive domains distinguish the groups in the same direction, with the greatest differences shown for the variables most closely aligned with the horizontal axis in Figure 14.8. This means that these neuro-psychological measures can be considered to tap a single dimension of differences among the three diagnostic groups.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "15-case-studies.html#sec-social-cog",
    "href": "15-case-studies.html#sec-social-cog",
    "title": "\n14  Case Studies\n",
    "section": "\n14.4 Social cognitive measures",
    "text": "14.4 Social cognitive measures\nThe social cognitive measures in the Hartman (2016) study were designed to tap various aspects of the perception and cognitive processing of emotions of others. Emotion perception was assessed using a Managing Emotions score from the MCCB. A “theory of mind” (ToM) score assessed ability to read the emotions of others from photographs of the eye region of male and female faces. Two other measures, externalizing bias (ExtBias) and personalizing bias (PersBias) were calculated from a scale measuring the degree to which individuals attribute internal, personal or situational causal attributions to positive and negative social events.\nThe analysis of the SocialCog data proceeds in a similar way: first we fit the MANOVA model, then test the overall differences among groups using Anova(). We find that the overall multivariate test is again significant,\n\ndata(SocialCog, package=\"heplots\")\nSC.mlm &lt;-  lm(cbind(MgeEmotions,ToM, ExtBias, PersBias) ~ Dx,\n               data=SocialCog)\nAnova(SC.mlm)\n# \n# Type II MANOVA Tests: Pillai test statistic\n#    Df test stat approx F num Df den Df  Pr(&gt;F)    \n# Dx  2     0.212     3.97      8    268 0.00018 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTesting the same two contrasts using linearHypothesis() (results not shown), w e find that the overall multivariate test is again significant, but now both contrasts are significant (Dx1: \\(F(4, 133)=5.21, p &lt; 0.001\\); Dx2: \\(F(4, 133)=2.49, p = 0.0461\\)), the test for Dx2 just barely.\n\n# control vs. patients\nprint(linearHypothesis(SC.mlm, \"Dx1\"), SSP=FALSE)\n# Schizo vs. SchizAff\nprint(linearHypothesis(SC.mlm, \"Dx2\"), SSP=FALSE)\n\nThese results are important, because, if they are reliable and make sense substantively, they imply that patients with schizophrenia and schizoaffective diagnoses can be distinguished by their performance on tasks assessing social perception and cognition. This was potentially a new finding in the literature on schizophrenia.\nAs we did above, it is useful to visualize the nature of these differences among groups with HE plots for the SC.mlm model. Each contrast has a corresponding \\(\\mathbf{H}\\) ellipse, which we can show in the plot using the hypotheses argument. With a single degree of freedom, these degenerate ellipses plot as lines.\n\nheplot(SC.mlm, \n       hypotheses=list(\"Dx1\"=\"Dx1\", \"Dx2\"=\"Dx2\"),\n       fill=TRUE, fill.alpha=.1,\n       cex.lab=1.5, cex=1.2)\n\n\n\n\n\n\nFigure 14.9: HE plot of Speed and Attention in the MLM for the SocialCog data. The labeled points show the means of the groups on the two variables. The lines for Dx1 and Dx2 show the tests of the contrasts among groups.\n\n\n\n\nIt can be seen that the three group means are approximately equally spaced on the ToM measure, whereas for MgeEmotions, the control and schizoaffective groups are quite similar, and both are higher than the schizophrenic group. This ordering of the three groups was somewhat similar for the other responses, as we could see in a pairs(SC.mlm) plot.\n\n14.4.1 Model checking\nNormally, we would continue this analysis, and consider other HE and canonical discriminant plots to further interpret the results, in particular the relations of the cognitive measures to group differences, or perhaps an analysis of the relationships between the neuro- and social-cognitive measures. We don’t pursue this here for reasons of length, but this example actually has a more important lesson to demonstrate.\nBefore beginning the MANOVA analyses, extensive data screening was done by the client using SPSS, in which all the response and predictor variables were checked for univariate normality and multivariate normality (MVN) for both sets. This traditional approach yielded a huge amount of tabular output and no graphs, and did not indicate any major violation of assumptions.2\nA simple visual test of MVN and the possible presence of multivariate outliers is related to the theory of the data ellipse: Under MVN, the squared Mahalanobis distances \\(D^2_M (\\mathbf{y}) = (\\mathbf{y} - \\bar{\\mathbf{y}})' \\, \\mathbf{S}^{-1} \\, (\\mathbf{y} - \\bar{\\mathbf{y}})\\) should follow a \\(\\chi^2_p\\) distribution. Thus, a quantile-quantile plot of the ordered \\(D^2_M\\) values vs. corresponding quantiles of the \\(\\chi^2\\) distribution should approximate a straight line (Cox, 1968; Healy, 1968). Note that this should be applied to the residuals from the model – residuals(SC.mlm) – and not to the response variables directly.\nheplots::cqplot() implements this for \"mlm\" objects Calling this function for the model SC.mlm produces Figure 14.10. It is immediately apparent that there is one extreme multivariate outlier; three other points are identified, but the remaining observations are nearly within the 95% confidence envelope (using a robust MVE estimate of \\(\\mathbf{S}\\)).\n\ncqplot(SC.mlm, method=\"mve\", \n       id.n=4, \n       main=\"\", \n       cex.lab=1.25)\n\n\n\n\n\n\nFigure 14.10: Chi-square quantile-quantile plot for residuals from the model SC.mlm. The confidence band gives a point-wise 95% envelope, providing information about uncertainty. One extreme multivariate outlier is highlighted.\n\n\n\n\nFurther checking revealed that this was a data entry error where one case (15) in the schizophrenia group had a score of -33 recorded on the ExtBias measure, whose valid range was (-10, +10). In R, it is very easy to re-fit a model to a subset of observations (rather than modifying the dataset itself) using update(). The result of the overall Anova and the test of Dx1 were unchanged; however, the multivariate test for the most interesting contrast Dx2 comparing the schizophrenia and schizoaffective groups became non-significant at the \\(\\alpha=0.05\\) level (\\(F(4, 133)=2.18, p = 0.0742\\)).\n\nSC.mlm1 &lt;- update(SC.mlm, \n                  subset=rownames(SocialCog)!=\"15\")\n\nAnova(SC.mlm1)\nprint(linearHypothesis(SC.mlm1, \"Dx1\"), SSP=FALSE)\nprint(linearHypothesis(SC.mlm1, \"Dx2\"), SSP=FALSE)\n\n\n14.4.2 Canonical HE plot\nThis outcome creates a bit of a quandry for further analysis (do univariate follow-up tests? try a robust model?) and reporting (what to claim about the Dx2 contrast?) that we don’t explore here. Rather, we proceed to attempt to interpret the MLM with the aid of canonical analysis and a canonical HE plot. The canonical analysis of the model SC.mlm1 now shows that both canonical dimensions are significant, and account for 83.9% and 16.1% of between group mean differences respectively.\n\nSC.can1 &lt;- candisc(SC.mlm1)\nSC.can1\n# \n# Canonical Discriminant Analysis for Dx:\n# \n#   CanRsq Eigenvalue Difference Percent Cumulative\n# 1 0.1645     0.1969      0.159    83.9       83.9\n# 2 0.0364     0.0378      0.159    16.1      100.0\n# \n# Test of H0: The canonical correlations in the \n# current row and all that follow are zero\n# \n#   LR test stat approx F numDF denDF Pr(&gt; F)    \n# 1        0.805     3.78     8   264 0.00032 ***\n# 2        0.964     1.68     3   133 0.17537    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\nheplot(SC.can1, \n  fill=TRUE, fill.alpha=.1,\n  hypotheses=list(\"Dx1\"=\"Dx1\", \"Dx2\"=\"Dx2\"),\n  lwd = c(1, 2, 3, 3),\n  col=c(\"red\", \"blue\", \"darkgreen\", \"darkgreen\"),\n  var.lwd=2, \n  var.col=\"black\", \n  label.pos=c(3,1), \n  var.cex=1.2, \n  cex=1.25, cex.lab=1.2, \n  scale=2.8,\n  prefix=\"Canonical dimension \")\npar(op)\n\n\n\n\n\n\nFigure 14.11: Canonical HE plot for the corrected SocialCog MANOVA. The variable vectors show the correlations of the responses with the canonical variables. The embedded green lines show the projections of the H ellipses for the contrasts Dx1 and Dx2 in canonical space.\n\n\n\n\nThe HE plot version of this canonical plot is shown in Figure 14.11. Because the heplot() method for a \"candisc\" object refits the original model to the \\(\\mathbf{Z}\\) canonical scores, it is easy to also project other linear hypotheses into this space. Note that in this view, both the Dx1 and Dx2 contrasts project outside \\(\\mathbf{E}\\) ellipse.3.\nThis canonical HE plot has a very simple description:\n\nDimension 1 orders the groups from control to schizoaffective to schizophrenia, while dimension 2 separates the schizoaffective group from the others;\nExternalizing bias and theory of mind contributes most to the first dimension, while personal bias and managing emotions are more aligned with the second; and,\nThe relations of the two contrasts to group differences and to the response variables can be easily read from this plot.\n\n\n\n\n\nCox, D. R. (1968). Notes on some aspects of regression analysis. Journal of the Royal Statistical Society Series A, 131, 265–279.\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays. Computational Statistics and Data Analysis, 43(4), 509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nHartman, L. I. (2016). Schizophrenia and schizoaffective disorder: One condition or two? [PhD dissertation]. York University.\n\n\nHealy, M. J. R. (1968). Multivariate normal plotting. Journal of the Royal Statistical Society Series C, 17(2), 157–161.\n\n\nHeinrichs, R. W., Pinnock, F., Muharib, E., Hartman, L., Goldberg, J., & McDermid Vaz, S. (2015). Neurocognitive normality in schizophrenia revisited. Schizophrenia Research: Cognition, 2(4), 227–232. https://doi.org/10.1016/j.scog.2015.09.001\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis with applications. Biometrika, 57(3), 519–530. https://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMardia, K. V. (1974). Applications of some measures of multivariate skewness and kurtosis in testing normality and robustness studies. Sankhya: The Indian Journal of Statistics, Series B, 36(2), 115–128. http://www.jstor.org/stable/25051892",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "15-case-studies.html#footnotes",
    "href": "15-case-studies.html#footnotes",
    "title": "\n14  Case Studies\n",
    "section": "",
    "text": "More worked out examples are contained in the heplots vignettes vignette(\"HE_manova\", package=\"heplots\") and vignette(\"HE_mmra\", package=\"heplots\")↩︎\nActually, multivariate normality of the predictors in \\(\\mathbf{X}\\) is not required in the MLM. This assumption applies only to the conditional values \\(\\mathbf{Y} \\;|\\; \\mathbf{X}\\), i.e., that the errors \\(\\boldsymbol{\\epsilon}_{i}' \\sim \\mathcal{N}_{p}(\\mathbf{0},\\boldsymbol{\\Sigma})\\) with constant covariance matrix. Moreover, the widely used MVN test statistics, such as Mardia’s (1970) test based on multivariate skewness and kurtosis are known to be quite sensitive to mild departures in kurtosis (Mardia, 1974) which do not threaten the validity of the multivariate tests.↩︎\nThe direct application of significance tests to canonical scores probably requires some adjustment because these are computed to have the optimal between-group discrimination.↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "91-colophon.html",
    "href": "91-colophon.html",
    "title": "Colophon",
    "section": "",
    "text": "Package versions\nThis book was produced using R version 4.5.1 (2025-06-13 ucrt). Fundamental to this was the framework for reproducible documents provided by Yihui Xie’s knitr package.\nQuarto was used to compile and render the book in HTML and PDF formats. [** Don’t really need all this**]\nThe principal R package versions used in examples and illustrations are listed below. These were captured via sessioninfo:::package_info() from all library() commands in the text, and scripts which also updated the references to packages.\nAt the time of writing, most of these were current on CRAN repositories but some development versions are indicated as “local” in the source column.\npackage\nversion\ndate\nsource\n\n\n\nbayestestR\n0.17.0\n2025-08-29\nCRAN\n\n\nbroom\n1.0.10\n2025-09-13\nCRAN\n\n\ncandisc\n0.9.2\n2025-08-29\nlocal\n\n\ncar\n3.1-3\n2024-09-27\nCRAN\n\n\ncarData\n3.0-5\n2022-01-06\nCRAN\n\n\ncorpcor\n1.6.10\n2021-09-16\nCRAN\n\n\ncorrelation\n0.8.8\n2025-07-08\nCRAN\n\n\ncorrgram\n1.14\n2021-04-29\nCRAN\n\n\ncorrplot\n0.95\n2024-10-14\nCRAN\n\n\ndatawizard\n1.3.0\n2025-10-11\nCRAN\n\n\ndplyr\n1.1.4\n2023-11-17\nCRAN\n\n\neasystats\n0.7.5\n2025-07-11\nCRAN\n\n\neffects\n4.2-4\n2025-07-29\nCRAN\n\n\neffectsize\n1.0.1\n2025-05-27\nCRAN\n\n\nfactoextra\n1.0.7\n2020-04-01\nCRAN\n\n\nFactoMineR\n2.12\n2025-07-23\nCRAN\n\n\nforcats\n1.0.1\n2025-09-25\nCRAN\n\n\ngenridge\n0.8.0\n2024-12-02\nCRAN\n\n\nGGally\n2.4.0\n2025-08-23\nCRAN\n\n\ngganimate\n1.0.11\n2025-09-04\nCRAN\n\n\nggbiplot\n0.6.2\n2024-01-08\nCRAN\n\n\nggdensity\n1.0.0\n2023-02-09\nCRAN\n\n\nggeffects\n2.3.1\n2025-08-20\nCRAN\n\n\ngggda\n0.1.1\n2025-07-19\nCRAN\n\n\nggpcp\n0.2.0\n2022-11-28\nCRAN\n\n\nggplot2\n4.0.0\n2025-09-11\nCRAN\n\n\nggpubr\n0.6.2\n2025-10-17\nCRAN\n\n\nggrepel\n0.9.6\n2024-09-07\nCRAN\n\n\nggstats\n0.11.0\n2025-09-15\nCRAN\n\n\nheplots\n1.7.9\n2025-09-05\nlocal\n\n\nHotelling\n1.0-8\n2021-09-09\nCRAN\n\n\nimager\n1.0.5\n2025-08-02\nCRAN\n\n\ninsight\n1.4.2\n2025-09-02\nCRAN\n\n\nknitr\n1.50\n2025-03-16\nCRAN\n\n\nlubridate\n1.9.4\n2024-12-08\nCRAN\n\n\nmagrittr\n2.0.4\n2025-09-12\nCRAN\n\n\nmarginaleffects\n0.30.0\n2025-09-13\nCRAN\n\n\nMASS\n7.3-65\n2025-02-28\nCRAN\n\n\nmatlib\n1.0.1\n2025-10-09\nCRAN\n\n\nmodelbased\n0.13.0\n2025-08-30\nCRAN\n\n\nmodelsummary\n2.5.0\n2025-08-25\nCRAN\n\n\nmvinfluence\n0.9.3\n2025-08-08\nlocal\n\n\nMVN\n6.2\n2025-10-10\nCRAN\n\n\nnestedLogit\n0.3.2\n2023-06-22\nCRAN\n\n\nparameters\n0.28.2\n2025-09-10\nCRAN\n\n\npatchwork\n1.3.2\n2025-08-25\nCRAN\n\n\nperformance\n0.15.2\n2025-10-06\nCRAN\n\n\npurrr\n1.1.0\n2025-07-10\nCRAN\n\n\nqgraph\n1.9.8\n2023-11-03\nCRAN\n\n\nreadr\n2.1.5\n2024-01-10\nCRAN\n\n\nreport\n0.6.1\n2025-02-07\nCRAN\n\n\nRtsne\n0.17\n2023-12-07\nCRAN\n\n\nsee\n0.12.0\n2025-09-14\nCRAN\n\n\nstringr\n1.5.2\n2025-09-08\nCRAN\n\n\ntibble\n3.3.0\n2025-06-08\nCRAN\n\n\ntidyr\n1.3.1\n2024-01-24\nCRAN\n\n\ntidyverse\n2.0.0\n2023-02-22\nCRAN\n\n\ntourr\n1.2.6\n2025-07-13\nCRAN\n\n\nvcd\n1.4-13\n2024-09-16\nCRAN\n\n\nVisCollin\n0.1.2\n2023-09-05\nCRAN",
    "crumbs": [
      "End Matter",
      "Colophon"
    ]
  },
  {
    "objectID": "95-references.html",
    "href": "95-references.html",
    "title": "References",
    "section": "",
    "text": "Abbott, E. A. (1884). Flatland: A romance of many dimensions.\nBuccaneer Books.\n\n\nAluja, T., Morineau, A., & Sanchez, G. (2018). Principal\ncomponent analysis for data science. https://pca4ds.github.io/\n\n\nAnderson, E. (1935). The irises of the Gaspé peninsula.\nBulletin of the American Iris Society, 35, 2–5.\n\n\nAnderson, M. J. (2006). Distance-based tests for homogeneity of\nmultivariate dispersions. Biometrics, 62(1), 245–253.\nhttps://doi.org/10.1111/j.1541-0420.2005.00440.x\n\n\nAndrews, D. F. (1972). Plots of high dimensional data.\nBiometrics, 28, 123–136.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. The American\nStatistician, 27, 17–21.\n\n\nArel-Bundock, V. (2022). modelsummary: Data\nand model summaries in R. Journal of Statistical\nSoftware, 103(1), 1–23. https://doi.org/10.18637/jss.v103.i01\n\n\nArel-Bundock, V., Greifer, N., & Heiss, A. (2024). How to interpret\nstatistical models using marginaleffects for\nR and Python. Journal of Statistical\nSoftware, 111(9), 1–32. https://doi.org/10.18637/jss.v111.i09\n\n\nAsimov, D. (1985). Grand tour. SIAM Journal of Scientific and\nStatistical Computing, 6(1), 128–143.\n\n\nBarab’asi, A.-L. (2016). Network science. Cambridge University\nPress.\n\n\nBarrett, B. E. (2003). Understanding influence in multivariate\nregression. Communications in Statistics - Theory and Methods,\n32(3), 667–680. https://doi.org/10.1081/STA-120018557\n\n\nBarrett, B. E., & Ling, R. F. (1992). General classes of influence\nmeasures for multivariate regression. Journal of the American\nStatistical Association, 87(417), 184–191. https://www.jstor.org/stable/i314301\n\n\nBartlett, M. S. (1937). Properties of sufficiency and statistical tests.\nProceedings of the Royal Society of London. Series A,\n160(901), 268–282. https://doi.org/10.2307/96803\n\n\nBartlett, M. S. (1938). Further aspects of the theory of multiple\nregression. Mathematical Proceedings of the Cambridge Philosophical\nSociety, 34(1), 33–40. https://doi.org/10.1017/s0305004100019897\n\n\nBashaw, W. L., & Findley, W. G. (Eds.). (1967). Symposium on\ngeneral linear model approach to the analysis of experimental data in\neducational research, final report. https://files.eric.ed.gov/fulltext/ED026737.pdf\n\n\nBecker, R. A., Chambers, J. M., & Wilks, A. R. (1988). The new\nS language. Wadsworth & Brooks/Cole.\n\n\nBecker, R. A., Cleveland, W. S., & Shyu, M.-J. (1996). The visual\ndesign and control of trellis display. Journal of Computational and\nGraphical Statistics, 5(2), 123–155.\n\n\nBelsley, D. A. (1991). Conditioning diagnostics: Collinearity and\nweak data in regression. Wiley.\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression\ndiagnostics: Identifying influential data and sources of\ncollinearity. John Wiley; Sons.\n\n\nBiecek, P., Baniecki, H., Krzyzinski, M., & Cook, D. (2023).\nPerformance is not enough: A story of the rashomon’s quartet.\nhttps://arxiv.org/abs/2302.13356\n\n\nBlack, C., Southwell, C., Emmerson, L., Lunn, D., & Hart, T. (2018).\nTime-lapse imagery of Adélie penguins reveals differential\nwinter strategies and breeding site occupation. PLOS ONE,\n13(3), e0193532. https://doi.org/10.1371/journal.pone.0193532\n\n\nBlishen, B., Carroll, W., & Moore, C. (1987). The 1981 socioeconomic\nindex for occupations in canada. Canadian Review of Sociology/Revue\nCanadienne de Sociologie, 24(4), 465–488. https://doi.org/10.1111/j.1755-618x.1987.tb00639.x\n\n\nBock, R. D. (1963). Programming univariate and multivariate analysis of\nvariance. Technometrics, 5(1), 95–117. https://doi.org/10.1080/00401706.1963.10490061\n\n\nBock, R. D. (1964). A computer program forunivariate and multivariate\nanalysis of variance. Proceedings of Scientific Symposium on\nStatistics.\n\n\nBodmer, W., Bailey, R. A., Charlesworth, B., Eyre-Walker, A., Farewell,\nV., Mead, A., & Senn, S. (2021). The outstanding scientist, r.a.\nFisher: His views on eugenics and race. Heredity,\n126(4), 565–576. https://doi.org/10.1038/s41437-020-00394-6\n\n\nBondy, J. A., & Murty, U. S. R. (2008). Graph theory.\nSpringer.\n\n\nBorg, I., & Groenen, P. J. F. (2005). Modern Multidimensional Scaling: Theory and\nApplications. Springer.\n\n\nBorg, I., Groenen, P. J. F., & Mair, P. (2018). Applied\nmultidimensional scaling and unfolding. In SpringerBriefs in\nStatistics. Springer International Publishing. https://doi.org/10.1007/978-3-319-73471-2\n\n\nBox, G. E. P. (1949). A general distribution theory for a class of\nlikelihood criteria. Biometrika, 36(3-4), 317–346. https://doi.org/10.1093/biomet/36.3-4.317\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and wear\ncurves. Biometrics, 6(4), 362–389.\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances.\nBiometrika, 40(3/4), 318–335. https://doi.org/10.2307/2333350\n\n\nBrandmaier, A., Von Oertzen, T., Mcardle, J., & Lindenberger, U.\n(2014). Exploratory data mining with structural equation model trees. In\nJ. J. M. & G. Ritschard (Ed.), Contemporary issues in\nexploratory data mining in the behavioral sciences (pp. 96–127).\n\n\nBrinton, W. C. (1939). Graphic presentation. Brinton\nAssociates. https://archive.org/details/graphicpresentat00brinrich\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of\nvariances. Journal of the American Statistical Association,\n69(346), 364–367. https://doi.org/10.1080/01621459.1974.10482955\n\n\nBrown, P. J., & Zidek, J. V. (1980). Adaptive multivariate ridge\nregression. The Annals of Statistics, 8(1), 64–74. http://www.jstor.org/stable/2240743\n\n\nBrunson, J. C., & Gracey, J. (2025). Gggda: A ’ggplot2’\nextension for geometric data analysis. https://github.com/corybrunson/gggda\n\n\nBuja, A., Cook, D., Asimov, D., & Hurley, C. (2005). Computational\nmethods for high-dimensional rotations in data visualization. In J. S.\nCR Rao EJ Wegman (Ed.), Handbook of statistics (pp. 391–413).\nElsevier. https://doi.org/10.1016/s0169-7161(04)24014-7\n\n\ncagne, M. (1885). Coordonnées parallèles\net axiales: Méthode de transformation\ngéométrique et\nprocédé nouveau de calcul graphique\ndéduits de la considération des\ncoordonnées parallèlles.\nGauthier-Villars. http://historical.library.cornell.edu/cgi-bin/cul.math/docviewer?did=00620001&seq=3\n\n\nCai, T. T., Liang, T., & Zhou, H. H. (2015). Law of log determinant\nof sample covariance matrix and optimal estimation of differential\nentropy for high-dimensional gaussian distributions. Journal of\nMultivariate Analysis, 137, 161–172. https://doi.org/https://doi.org/10.1016/j.jmva.2015.02.003\n\n\nCajori, F. (1926). Origins of fourth dimension concepts. The\nAmerican Mathematical Monthly, 33(8), 397–406. https://doi.org/10.1080/00029890.1926.11986607\n\n\nCattell, R. B. (1966). The scree test for the number of factors.\nMultivariate Behavioral Research, 1(2), 245–276. https://doi.org/10.1207/s15327906mbr0102_10\n\n\nChambers, J. M., Cleveland, W. S., Kleiner, B., & Tukey, P. A.\n(1983). Graphical methods for data analysis. Wadsworth.\n\n\nChambers, J. M., & Hastie, T. J. (1991). Statistical models in\ns (p. 624). Chapman & Hall/CRC.\n\n\nCharnes, A., Cooper, W. W., & Rhodes, E. (1981). Evaluating program\nand managerial efficiency: An application of data envelopment analysis\nto program follow through. Management Science, 27(6),\n668–697. http://www.jstor.org/stable/2631155\n\n\nCleveland, W. S. (1979). Robust locally weighted regression and\nsmoothing scatterplots. Journal of the American Statistical\nAssociation, 74, 829–836.\n\n\nCleveland, W. S. (1985). The elements of graphing data.\nWadsworth Advanced Books.\n\n\nCleveland, W. S., & Devlin, S. J. (1988). Locally weighted\nregression: An approach to regression analysis by local fitting.\nJournal of the American Statistical Association, 83,\n596–610.\n\n\nCleveland, W. S., & McGill, R. (1984). Graphical perception: Theory,\nexperimentation and application to the development of graphical methods.\nJournal of the American Statistical Association, 79,\n531–554.\n\n\nCleveland, W. S., & McGill, R. (1985). Graphical perception and\ngraphical methods for analyzing scientific data. Science,\n229, 828–833.\n\n\nClyde, D. J., Cramer, E. M., & Sherin, R. J. (1966).\nMultivariate statistical programs. Biometric\nLaboratory,University of Miami.\n\n\nCochran, W. G. (1941). The distribution of the largest of a set of\nestimated variances as a fraction of their total. Annals of\nEugenics, 11(1), 47–52. https://doi.org/10.1111/j.1469-1809.1941.tb02271.x\n\n\nConover, W. J., Johnson, M. E., & Johnson, M. M. (1981). A\ncomparative study of tests for homogeneity of variances, with\napplications to the outer continental shelf bidding data.\nTechnometrics, 23(4), 351–361. https://doi.org/10.1080/00401706.1981.10487680\n\n\nCook, D., Buja, A., Cabrera, J., & Hurley, C. (1995). Grand tour and\nprojection pursuit. Journal of Computational and Graphical\nStatistics, 4(3), 155. https://doi.org/10.2307/1390844\n\n\nCook, D., Buja, A., Lee, E.-K., & Wickham, H. (2008). Grand tours,\nprojection pursuit guided tours, and manual controls. In Handbook of\ndata visualization (pp. 295–314). Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-33037-0_13\n\n\nCook, D., & Laa, U. (2024). Interactively exploring\nhigh-dimensional data and models in R. Online. https://dicook.github.io/mulgar_book/\n\n\nCook, D., & Swayne, D. F. (2007). Interactive and dynamic\ngraphics for data analysis : With R and\nGGobi. Springer. http://www.ggobi.org/book/\n\n\nCook, R. D. (1977). Detection of influential observation in linear\nregression. Technometrics, 19(1), 15–18. https://doi.org/10.1080/00401706.1977.10489493\n\n\nCook, R. D. (1993). Exploring partial residual plots.\nTechnometrics, 35(4), 351–362.\n\n\nCook, R. D. (1996). Added-variable plots and curvature in linear\nregression. Technometrics, 38(3), 275–278. https://doi.org/10.1080/00401706.1996.10484507\n\n\nCook, R. D., & Weisberg, S. (1982). Residuals and influence in\nregression. Chapman; Hall.\n\n\nCook, R. D., & Weisberg, S. (1994). ARES plots for generalized\nlinear models. Computational Statistics & Data Analysis,\n17(3), 303–315. https://doi.org/10.1016/0167-9473(92)00075-3\n\n\nCostantini, G., Epskamp, S., Borsboom, D., Perugini, M., Mõttus, R.,\nWaldorp, L. J., & Cramer, A. O. J. (2015). State of the aRt personality research: A tutorial on network\nanalysis of personality data in R. Journal of Research\nin Personality, 54, 13–29. https://doi.org/10.1016/j.jrp.2014.07.003\n\n\nCostelloe, M. F. P. (1915). Graphic methods and the presentation of this\nsubject to first year college students. Nebraska Blue Print.\n\n\nCotton, R. (2013). Learning R. O’Reilly Media.\n\n\nCox, D. R. (1968). Notes on some aspects of regression analysis.\nJournal of the Royal Statistical Society Series A,\n131, 265–279.\n\n\nCsárdi, G., Nepusz, T., Traag, V., Horvát, S., Zanini, F., Noom, D.,\n& Müller, K. (2024). igraph: Network\nanalysis and visualization in r. https://doi.org/10.5281/zenodo.7682609\n\n\nCurran, J., & Hersh, T. (2021). Hotelling: Hotelling’s t^2 test\nand variants. https://doi.org/10.32614/CRAN.package.Hotelling\n\n\nD’Agostino McGowan, L. (2023). Quartets: Datasets to help teach\nstatistics. https://r-causal.github.io/quartets/\n\n\nDavis, C. (1990). Body image and weight preoccupation: A comparison\nbetween exercising and non-exercising women. Appetite,\n16(1), 84. https://doi.org/10.1016/0195-6663(91)90115-9\n\n\nDempster, A. P. (1969). Elements of continuous multivariate\nanalysis. Addison-Wesley.\n\n\nDempster, A. P. (1972). Covariance selection. Biometrics,\n28(1), 157–175.\n\n\nDixon, W. J. (1965). BMD biomedical computer programs. Health\nSciences Computing Facility, School of Medicine, University of\nCalifornia; Health Sciences Computing Faculty.\n\n\nDray, S., & Siberchicot, A. (2025). Adegraphics: An S4\nlattice-based package for the representation of multivariate data.\nhttp://pbil.univ-lyon1.fr/ADE-4/\n\n\nDuncan, O. D. (1961). A socioeconomic index for all occupations. In Jr.\nA. J. Reiss, P. K. H. O. D. Duncan, & C. C. North (Eds.),\nOccupations and social status. The Free Press.\n\n\nEfron, B., & Hastie, T. (2021). Computer age statistical\ninference, student edition: Algorithms, evidence, and data science.\nCambridge University Press. https://doi.org/10.1017/9781108914062\n\n\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least\nangle regression. The Annals of Statistics, 32(2),\n407–499.\n\n\nEmerson, J. W., Green, W. A., Schloerke, B., Crowley, J., Cook, D.,\nHofmann, H., & Wickham, H. (2013). The generalized pairs plot.\nJournal of Computational and Graphical Statistics,\n22(1), 79–91. https://doi.org/10.1080/10618600.2012.694762\n\n\nEuler, L. (1758). Elementa doctrinae solidorum. Novi Commentarii\nAcademiae Scientiarum Petropolitanae, 4, 109–140. https://scholarlycommons.pacific.edu/euler-works/230/\n\n\nFarquhar, A. B., & Farquhar, H. (1891). Economic and industrial\ndelusions: A discourse of the case for protection. Putnam.\n\n\nFienberg, S. E. (1971). Randomization and social affairs: The 1970 draft\nlottery. Science, 171, 255–261.\n\n\nFinn, J. D. (1967). MULTIVARIANCE: Fortran program for\nunivariate and multivariate analysis of variance and covariance.\nSchool of Education, State University of New York at Buffalo.\n\n\nFisher, R. A. (1923). Studies in crop variation. II. The manurial\nresponse of different potato varieties. The Journal of Agricultural\nScience, 13(2), 311–320. https://hdl.handle.net/2440/15179\n\n\nFisher, R. A. (1925b). Statistical methods for research\nworkers. Oliver & Boyd.\n\n\nFisher, R. A. (1925a). Statistical methods for research workers\n(6th ed.). Oliver & Boyd.\n\n\nFisher, R. A. (1936). The use of multiple measurements in taxonomic\nproblems. Annals of Eugenics, 7(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x\n\n\nFishkeller, M. A., Friedman, J. H., & Tukey, J. W. (1974).\nPRIM-9, an interactive multidimensional data display and\nanalysis system. Proceedings of the Pacific ACM Regional\nConference.\n\n\nFlury, B., & Riedwyl, H. (1988). Multivariate statistics: A\npractical approach. Chapman & Hall.\n\n\nFox, J. (1987). Effect displays for generalized linear models. In C. C.\nClogg (Ed.), Sociological methodology, 1987 (pp. 347–361).\nJossey-Bass.\n\n\nFox, J. (2003). Effect displays in R for generalized linear\nmodels. Journal of Statistical Software, 8(15), 1–27.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear\nmodels (Third edition.). SAGE.\n\n\nFox, J. (2020). Regression diagnostics (2nd ed.).\nSAGE Publications, Inc. https://doi.org/10.4135/9781071878651\n\n\nFox, J. (2021). A mathematical primer for social statistics\n(2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878835\n\n\nFox, J., & Monette, G. (1992). Generalized collinearity diagnostics.\nJournal of the American Statistical Association,\n87(417), 178–183.\n\n\nFox, J., & Weisberg, S. (2018a). An R companion to\napplied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFox, J., & Weisberg, S. (2018b). Visualizing fit and lack of fit in\ncomplex regression models with predictor effect plots and partial\nresiduals. Journal of Statistical Software, 87(9). https://doi.org/10.18637/jss.v087.i09\n\n\nFox, J., & Weisberg, S. (2019). An R companion to\napplied regression (Third). Sage. https://www.john-fox.ca/Companion/\n\n\nFox, J., Weisberg, S., Price, B., Friendly, M., & Hong, J. (2025).\nEffects: Effect displays for linear, generalized linear, and other\nmodels. https://cran.r-project.org/package=effects\n\n\nFriedman, J. H. (1987). Exploratory projection pursuit. Journal of\nthe American Statistical Association, 82, 249–266.\n\n\nFriedman, J. H., & Tukey, J. W. (1974). A projection pursuit\nalgorithm for exploratory data analysis. IEEE Transactions on\nComputers, C-23(9), 881–890. https://doi.org/10.1109/T-C.1974.224051\n\n\nFriedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K.,\nSimon, N., & Yang, J. (2025). Glmnet: Lasso and elastic-net\nregularized generalized linear models. https://glmnet.stanford.edu\n\n\nFriendly, M. (1991). SAS System for statistical\ngraphics (1st ed.). SAS Institute. http://www.sas.\ncom/service/doc/pubcat/uspubcat/ind_files/56143.html\n\n\nFriendly, M. (1994). Mosaic displays for multi-way contingency tables.\nJournal of the American Statistical Association, 89,\n190–200. http://www.jstor.org/stable/2291215\n\n\nFriendly, M. (1999). Extending mosaic displays: Marginal, conditional,\nand partial views of categorical data. Journal of Computational and\nGraphical Statistics, 8(3), 373–395. http://datavis.ca/papers/drew/drew.pdf\n\n\nFriendly, M. (2000). Visualizing categorical data. SAS\nInstitute.\n\n\nFriendly, M. (2002). Corrgrams: Exploratory displays for correlation\nmatrices. The American Statistician, 56(4), 316–324.\nhttps://doi.org/10.1198/000313002533\n\n\nFriendly, M. (2007). HE plots for multivariate general\nlinear models. Journal of Computational and Graphical\nStatistics, 16(2), 421–444. https://doi.org/10.1198/106186007X208407\n\n\nFriendly, M. (2008). The Golden Age of statistical\ngraphics. Statistical Science, 23(4), 502–535. https://doi.org/10.1214/08-STS268\n\n\nFriendly, M. (2011). Generalized ridge trace plots: Visualizing bias\nand precision with the genridge R package. SCS\nSeminar.\n\n\nFriendly, M. (2013). The generalized ridge trace plot: Visualizing bias\nand precision. Journal of Computational and Graphical\nStatistics, 22(1), 50–68. https://doi.org/10.1080/10618600.2012.681237\n\n\nFriendly, M. (2022). The life and works of andré-michel\nguerry, revisited. Sociological Spectrum, 42(4-6),\n233–259. https://doi.org/10.1080/02732173.2022.2078450\n\n\nFriendly, M. (2024). Genridge: Generalized ridge trace plots for\nridge regression. https://doi.org/10.32614/CRAN.package.genridge\n\n\nFriendly, M. (2025a). Mvinfluence: Influence measures and diagnostic\nplots for multivariate linear models. https://github.com/friendly/mvinfluence\n\n\nFriendly, M. (2025b). vcdExtra: ’Vcd’ extensions and additions.\nhttps://doi.org/10.32614/CRAN.package.vcdExtra\n\n\nFriendly, M., & 6140 Members. (2012). Psychology 6140 research\napplications papers. online document. http://www.psych.yorku.ca/lab/psy6140/py614bib.pdf\n\n\nFriendly, M., & Denis, D. (2001). The roots and branches of\nstatistical graphics. Journal de La\nSociété Française de\nStatistique, 141(4), 51–60. http://www.numdam.org/item/JSFS_2000__141_4_51_0.pdf\n\n\nFriendly, M., & Fox, J. (2025). Candisc: Visualizing generalized\ncanonical discriminant and canonical correlation analysis. https://github.com/friendly/candisc/\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix\nfunctions for teaching and learning linear algebra and multivariate\nstatistics. https://github.com/friendly/matlib\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays.\nComputational Statistics and Data Analysis, 43(4),\n509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nFriendly, M., & Kwan, E. (2009). Where’s Waldo:\nVisualizing collinearity diagnostics. The American\nStatistician, 63(1), 56–65. https://doi.org/10.1198/tast.2009.0012\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with\nR: Visualization and modeling techniques for categorical\nand count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights:\nUnderstanding statistical methods through elliptical geometry.\nStatistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFriendly, M., & Sigal, M. (2018). Visualizing tests for equality of\ncovariance matrices. The American Statistician, 72(4),\n144–155. https://doi.org/10.1080/00031305.2018.1497537\n\n\nFriendly, M., Sigal, M., & Harnanansingh, D. (2015). The\nMilestones Project: A database for the history of data\nvisualization. In M. Kimball & C. Kostelnick (Eds.), Visible\nnumbers: The history of data visualization. Ashgate Press.\n\n\nFriendly, M., & Wainer, H. (2021). A history of data\nvisualization and graphic communication. Harvard University Press.\nhttps://doi.org/10.4159/9780674259034\n\n\nFuller, W. (2006). Measurement error models (2nd ed.). John\nWiley & Sons.\n\n\nFunkhouser, H. G. (1937). Historical development of the graphical\nrepresentation of statistical data. Osiris, 3(1),\n269–405. http://tinyurl.com/32ema9\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with\napplication to principal components analysis. Biometrics,\n58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGabriel, K. R. (1981). Biplot display of multivariate matrices for\ninspection of data and diagnosis. In V. Barnett (Ed.), Interpreting\nmultivariate data (pp. 147–173). John Wiley; Sons.\n\n\nGalton, F. (1863). Meteorographica, or methods of mapping the\nweather. Macmillan. http://www.mugu.com/galton/books/meteorographica/index.htm\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature.\nJournal of the Anthropological Institute, 15, 246–263.\nhttp://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGalton, F. (1889). Natural inheritance. Macmillan. http://galton.org/books/natural-inheritance/pdf/galton-nat-inh-1up-clean.pdf\n\n\nGannett, H. (1898). Statistical atlas of the united states, eleventh\n(1890) census. U.S. Government Printing Office.\n\n\nGastwirth, J. L., Gel, Y. R., & Miao, W. (2009). The impact of Levene’s test of equality of variances on\nstatistical theory and practice. Statistical Science,\n24(3), 343–360. https://doi.org/10.1214/09-STS301\n\n\nGayan De Silva. (2020). Exploring the world of artificial neural\nnetworks -a beginner’s overview. https://doi.org/10.13140/RG.2.2.14790.14406\n\n\nGelman, A., Hullman, J., & Kennedy, L. (2023). Causal quartets:\nDifferent ways to attain the same average treatment effect. http://www.stat.columbia.edu/~gelman/research/unpublished/causal_quartets.pdf\n\n\nGillespie, C., Locke, S., Davies, R., & D’Agostino McGowan, L.\n(2025). datasauRus: Datasets from the datasaurus dozen. https://doi.org/10.32614/CRAN.package.datasauRus\n\n\nGittins, R. (1985). Canonical analysis: A review with applications\nin ecology. Springer-Verlag.\n\n\nGoeman, J., Meijer, R., Chaturvedi, N., & Lueder, M. (2022).\nPenalized: L1 (lasso and fused lasso) and L2 (ridge) penalized\nestimation in GLMs and in the cox model. https://doi.org/10.32614/CRAN.package.penalized\n\n\nGorman, K. B., Williams, T. D., & Fraser, W. R. (2014). Ecological\nsexual dimorphism and environmental variability within a community of\nantarctic penguins (genus pygoscelis). PLoS\nONE, 9(3), e90081. https://doi.org/10.1371/journal.pone.0090081\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman &\nHall.\n\n\nGower, J. C., Lubbe, S. G., & Roux, N. J. L. (2011).\nUnderstanding biplots. Wiley. http://books.google.ca/books?id=66gQCi5JOKYC\n\n\nGrandjean, M. (2016). A social network analysis of Twitter:\nMapping the digital humanities community. Cogent Arts\n&Amp; Humanities, 3(1), 1171458. https://doi.org/10.1080/23311983.2016.1171458\n\n\nGraybill, F. A. (1961). An introduction to linear statistical\nmodels. McGraw-Hill.\n\n\nGreenacre, M. (1984). Theory and applications of correspondence\nanalysis. Academic Press.\n\n\nGreenacre, M. (2010). Biplots in practice.\nFundación BBVA. https://books.google.ca/books?id=dv4LrFP7U\\_EC\n\n\nGuerry, A.-M. (1833). Essai sur la statistique morale de la\nFrance. Crochard.\n\n\nHahsler, M., Buchta, C., & Hornik, K. (2024). Seriation:\nInfrastructure for ordering objects using seriation. https://github.com/mhahsler/seriation\n\n\nHaitovsky, Y. (1987). On multivariate ridge regression.\nBiometrika, 74(3), 563–570. https://doi.org/10.1093/biomet/74.3.563\n\n\nHarrell, F. E. (2015). Regression modeling strategies: With\napplications to linear models, logistic and ordinal regression, and\nsurvival analysis. Springer International Publishing. https://books.google.ca/books?id=sQ90rgEACAAJ\n\n\nHarrison, P. (2023a). Langevitour: Smooth interactive touring of high\ndimensions, demonstrated with scRNA-seq data. The R Journal,\n15(2), 206–219. https://doi.org/10.32614/RJ-2023-046\n\n\nHarrison, P. (2023b). Langevitour: Smooth interactive touring of high\ndimensions, demonstrated with scRNA-seq data. The R Journal,\n15(2), 206–219. https://doi.org/10.32614/RJ-2023-046\n\n\nHart, C., & Wang, E. (2022). Detourr: Portable and performant\ntour animations. https://CRAN.R-project.org/package=detourr\n\n\nHartigan, J. A. (1975a). Clustering algorithms. John Wiley;\nSons.\n\n\nHartigan, J. A. (1975b). Printer graphics for clustering. Journal of\nStatistical Computing and Simulation, 4, 187–213.\n\n\nHartley, H. O. (1950). The use of range in analysis of variance.\nBiometrika, 37(3–4), 271–280. https://doi.org/10.1093/biomet/37.3-4.271\n\n\nHartman, L. I. (2016). Schizophrenia and schizoaffective disorder:\nOne condition or two? [PhD dissertation]. York University.\n\n\nHarwell, M. R., Rubinstein, E. N., Hayes, W. S., & Olds, C. C.\n(1992). Summarizing monte carlo results in methodological research: The\none- and two-factor fixed effects ANOVA cases. Journal\nof Educational and Behavioral Statistics, 17(4), 315–339.\nhttps://doi.org/10.3102/10769986017004315\n\n\nHaskell, A. C. (1919). How to make and use graphic charts.\nCodex.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning: Data mining, inference and prediction (2nd\ned.). Springer. http://www-stat.stanford.edu/~tibs/ElemStatLearn/\n\n\nHealy, K. (2019). Data visualization: A practical introduction.\nPrinceton University Press. http://www.socviz.co\n\n\nHealy, M. J. R. (1968). Multivariate normal plotting. Journal of the\nRoyal Statistical Society Series C, 17(2), 157–161.\n\n\nHebbali, A. (2024). Olsrr: Tools for building OLS regression\nmodels. https://doi.org/10.32614/CRAN.package.olsrr\n\n\nHeinrichs, R. W., Pinnock, F., Muharib, E., Hartman, L., Goldberg, J.,\n& McDermid Vaz, S. (2015). Neurocognitive normality in schizophrenia\nrevisited. Schizophrenia Research: Cognition, 2(4),\n227–232. https://doi.org/10.1016/j.scog.2015.09.001\n\n\nHerschel, J. F. W. (1833). On the investigation of the orbits of\nrevolving double stars: Being a supplement to a paper entitled\n\"micrometrical measures of 364 double stars\". Memoirs of the Royal\nAstronomical Society, 5, 171–222.\n\n\nHertzsprung, E. (1911). Publikationen des astrophysikalischen\nobservatorium zu Potsdam.\n\n\nHoaglin, D. C., & Welsch, R. E. (1978). The hat matrix in regression\nand ANOVA. The American Statistician,\n32(1), 17–22. https://doi.org/10.1080/00031305.1978.10479237\n\n\nHocking, R. R. (2013). Methods and applications of linear models:\nRegression and the analysis of variance. Wiley. https://books.google.ca/books?id=iq2J-1iS6HcC\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression:\nBiased estimation for nonorthogonal problems.\nTechnometrics, 12, 55–67.\n\n\nHoerl, A. E., Kennard, R. W., & Baldwin, K. F. (1975). Ridge\nregression: Some simulations. Communications in Statistics,\n4(2), 105–123. https://doi.org/10.1080/03610927508827232\n\n\nHofmann, H., VanderPlas, S., & Ge, Y. (2022). Ggpcp: Parallel\ncoordinate plots in the ’ggplot2’ framework. https://doi.org/10.32614/CRAN.package.ggpcp\n\n\nHofstadter, D. R. (1979). Gödel, escher, bach: An eternal golden\nbraid. Basic Books.\n\n\nHøjsgaard, S., Edwards, D., & Lauritzen, S. (2012). Graphical\nmodels with R. Springer Science & Business Media.\n\n\nHorst, A. M., Hill, A. P., & Gorman, K. B. (2020).\nPalmerpenguins: Palmer archipelago (antarctica) penguin data.\nhttps://doi.org/10.5281/zenodo.3960218\n\n\nHotelling, H. (1931). The generalization of Student’s ratio. The Annals of Mathematical\nStatistics, 2(3), 360–378. https://doi.org/10.1214/aoms/1177732979\n\n\nHotelling, H. (1936). Relations between two sets of variates.\nBiometrika, 28(3/4), 321. https://doi.org/10.2307/2333955\n\n\nHuang, F. L. (2019). MANOVA: A procedure whose time has\npassed? Gifted Child Quarterly, 64(1), 56–60. https://doi.org/10.1177/0016986219887200\n\n\nHusson, F., Le, S., & Pagès, J. (2017). Exploratory multivariate\nanalysis by example using r. Chapman & Hall. https://doi.org/10.1201/b21874\n\n\nIBM. (1965). Proceedings of the IBM scientific computing symposium\non statistics: Oct 21-23, 1963 (L. Robinson, Ed.). IBM. https://www.amazon.com/Proceedings-Scientific-Computing-Symposium-Statistics/dp/B000GL5RLU\n\n\nInselberg, A. (1985). The plane with parallel coordinates. The\nVisual Computer, 1, 69–91.\n\n\nIsvoranu, A.-M., Epskamp, S., Waldorp, L. J., & Borsboom, D. (2022).\nNetwork psychometrics with r: A guide for behavioral and social\nscientists. Routledge. https://doi.org/10.4324/9781003111238\n\n\nJohnson, R., & Wichern, D. (1998). Applied multivariate\nstatistical analysis (4th ed.). Prentice Hall.\n\n\nKassambara, A., & Mundt, F. (2020). Factoextra: Extract and\nvisualize the results of multivariate data analyses. https://doi.org/10.32614/CRAN.package.factoextra\n\n\nKastellec, J. P., & Leoni, E. L. (2007). Using graphs instead of\ntables in political science. Perspectives on Politics,\n5(04), 755–771. https://doi.org/10.1017/S1537592707072209\n\n\nKay, M. (2025). ggdist: Visualizations\nof distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKorkmaz, S., Goksuluk, D., & Zararsiz, G. (2014). MVN: An r package\nfor assessing multivariate normality. The R Journal,\n6(2), 151–162. https://journal.r-project.org/archive/2014-2/korkmaz-goksuluk-zararsiz.pdf\n\n\nKrijthe, J. (2023). Rtsne: T-distributed stochastic neighbor\nembedding using a barnes-hut implementation. https://github.com/jkrijthe/Rtsne\n\n\nKruskal, J. B. (1964). Multidimensional scaling by optimizing goodness\nof fit to a nonmetric hypothesis. Psychometrika,\n29(1), 1–27. https://doi.org/10.1007/bf02289565\n\n\nKwan, E., Lu, I. R. R., & Friendly, M. (2009). Tableplot: A new tool\nfor assessing precise predictions. Zeitschrift für\nPsychologie / Journal of Psychology, 217(1), 38–48. https://doi.org/10.1027/0044-3409.217.1.38\n\n\nLarmarange, J. (2025). Ggstats: Extension to ’ggplot2’ for plotting\nstats. https://doi.org/10.32614/CRAN.package.ggstats\n\n\nLarsen, W. A., & McCleary, S. J. (1972). The use of partial residual\nplots in regression analysis. Technometrics, 14,\n781–790.\n\n\nLauritzen, S. L. (1996). Graphical models. Oxford University\nPress.\n\n\nLawless, J. F., & Wang, P. (1976). A simulation study of ridge and\nother regression estimators. Communications in Statistics,\n5, 307–323.\n\n\nLê, S., Josse, J., & Husson, F. (2008). FactoMineR: A\npackage for multivariate analysis. Journal of Statistical\nSoftware, 25(1), 1–18. https://doi.org/10.18637/jss.v025.i01\n\n\nLee, E.-K., & Cook, D. (2009). A projection pursuit index for large\np small n data. Statistics and Computing, 20(3),\n381–392. https://doi.org/10.1007/s11222-009-9131-1\n\n\nLee, S. (2025). Liminal: Multivariate data visualization with tours\nand embeddings. https://github.com/sa-lee/liminal\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. Olkin,\nS. G. Ghurye, W. Hoeffding, W. G. Madow, & H. B. Mann (Eds.),\nContributions to probability and statistics: Essays in honor of\nHarold Hotelling (pp. 278–292). Stanford University\nPress.\n\n\nLix, J. M., L. M. Keselman, & Keselman, H. J. (1996). Consequences\nof assumption violations revisited: A quantitative review of\nalternatives to the one-way analysis of variance F test.\nReview of Educational Research, 66(4), 579–619. https://doi.org/10.3102/00346543066004579\n\n\nLong, J. D., & Teetor, P. (2019). R cookbook: Proven recipes for\ndata analysis, statistics, and graphics. O’Reilly. https://rc2e.com/\n\n\nLongley, J. W. (1967). An appraisal of least squares programs for the\nelectronic computer from the point of view of the user. Journal of\nthe American Statistical Association, 62, 819–841.\nhttps://doi.org/https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896\n\n\nLoy, A., & Hofmann, H. (2014). HLMdiag: A suite of diagnostics for\nhierarchical linear models in r. Journal of Statistical\nSoftware, 56(5), 1–28. https://doi.org/10.18637/jss.v056.i05\n\n\nLüdecke, D. (2018). Ggeffects: Tidy data frames of marginal effects from\nregression models. Journal of Open Source Software,\n3(26), 772. https://doi.org/10.21105/joss.00772\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., &\nMakowski, D. (2021). performance: An\nR package for assessment, comparison and testing of\nstatistical models. Journal of Open Source Software,\n6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., Bacher, E.,\nThériault, R., & Makowski, D. (2022). Easystats: Framework for easy\nstatistical modeling, visualization, and reporting. CRAN. https://doi.org/10.32614/CRAN.package.easystats\n\n\nMaaten, L. van der, & Hinton, G. (2008). Visualizing data using\nt-SNE. Journal of Machine Learning\nResearch, 9, 2579–2605. http://www.jmlr.org/papers/v9/vandermaaten08a.html\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis\nwith applications. Biometrika, 57(3), 519–530.\nhttps://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMardia, K. V. (1974). Applications of some measures of multivariate\nskewness and kurtosis in testing normality and robustness studies.\nSankhya: The Indian Journal of Statistics, Series B,\n36(2), 115–128. http://www.jstor.org/stable/25051892\n\n\nMarquardt, D. W. (1970). Generalized inverses, ridge regression, biased\nlinear estimation, and nonlinear estimation. Technometrics,\n12, 591–612.\n\n\nMartí, R., & Laguna, M. (2003). Heuristics and meta-heuristics for\n2-layer straight line crossing minimization. Discrete Applied\nMathematics, 127(3), 665–678.\n\n\nMatejka, J., & Fitzmaurice, G. (2017, May). Same stats, different\ngraphs. Proceedings of the 2017 CHI Conference on Human\nFactors in Computing Systems. https://doi.org/10.1145/3025453.3025912\n\n\nMatloff, N. (2011). The art of R programming:\nA tour of statistical software design. No Starch\nPress.\n\n\nMaunder, E. W. (1904). Note on the distribution of sun-spots in\nheliographic latitude, 1874 to 1902. Royal Astronomical Society\nMonthly Notices, 64, 747–761.\n\n\nMcDermott, G., Arel-Bundock, V., & Zeileis, A. (2025). tinyplot: Lightweight extension of the base r\ngraphics system. https://doi.org/10.32614/CRAN.package.tinyplot\n\n\nMcDonald, G. C. (2009). Ridge regression. Wiley Interdisciplinary\nReviews: Computational Statistics, 1(1), 93–100. https://doi.org/10.1002/wics.14\n\n\nMcGowan, L. D., Gerke, T., & Barrett, M. (2023). Causal inference is\nnot just a statistics problem. Journal of Statistics and Data\nScience Education, 1–9. https://doi.org/10.1080/26939169.2023.2276446\n\n\nMeyer, D., Zeileis, A., Hornik, K., & Friendly, M. (2024). Vcd:\nVisualizing categorical data. https://doi.org/10.32614/CRAN.package.vcd\n\n\nMeyers, L. S., Gamst, G., & Guarino, A. J. (2006). Applied\nmultivariate research: Design and interpretation. SAGE\nPublications.\n\n\nMonette, G. (1990). Geometry of multiple regression and interactive\n3-D graphics. In J. Fox & S. Long (Eds.), Modern\nmethods of data analysis (pp. 209–256). SAGE Publications.\n\n\nMoseley, H. (1913). The high frequency spectra of the elements.\nPhilosophical Magazine, 26, 1024–1034.\n\n\nMosteller, F., & Tukey, J. W. (1977). Data analysis and\nregression: A second course in statistics. Addison-Wesley\nPublishing Company.\n\n\nMurdoch, D., & Adler, D. (2025). Rgl: 3D visualization using\nOpenGL. https://doi.org/10.32614/CRAN.package.rgl\n\n\nO’Brien, P. C. (1992). Robust procedures for testing equality of\ncovariance matrices. Biometrics, 48(3), 819–827. http://www.jstor.org/stable/2532347\n\n\nOksanen, J., Simpson, G. L., Blanchet, F. G., Kindt, R., Legendre, P.,\nMinchin, P. R., O’Hara, R. B., Solymos, P., Stevens, M. H. H., Szoecs,\nE., Wagner, H., Barbour, M., Bedward, M., Bolker, B., Borcard, D.,\nBorman, T., Carvalho, G., Chirico, M., De Caceres, M., … Weedon, J.\n(2025). Vegan: Community ecology package. https://doi.org/10.32614/CRAN.package.vegan\n\n\nOlson, C. L. (1974). Comparative robustness of six tests in multivariate\nanalysis of variance. Journal of the American Statistical\nAssociation, 69(348), 894–908. https://doi.org/10.1080/01621459.1974.10480224\n\n\nOtto, J., & Kahle, D. (2023). Ggdensity: Interpretable bivariate\ndensity visualization with ’ggplot2’. https://doi.org/10.32614/CRAN.package.ggdensity\n\n\nPearson, K. (1896). Contributions to the mathematical theory of\nevolution—III, regression, heredity and panmixia.\nPhilosophical Transactions of the Royal Society of London,\n187, 253–318.\n\n\nPearson, K. (1901). On lines and planes of closest fit to systems of\npoints in space. Philosophical Magazine, 6(2),\n559–572.\n\n\nPearson, K. (1903). I. Mathematical contributions to the theory of\nevolution. —XI. On the influence of natural selection on the variability\nand correlation of organs. Philosophical Transactions of the Royal\nSociety of London, 200(321–330), 1–66. https://doi.org/10.1098/rsta.1903.0001\n\n\nPeddle, J. B. (1910). The construction of graphical charts.\nMcGraw-Hill.\n\n\nPedersen, T. L., & Robinson, D. (2025). Gganimate: A grammar of\nanimated graphics. https://doi.org/10.32614/CRAN.package.gganimate\n\n\nPesaran, M. H., & Smith, R. P. (2019). A bayesian analysis of linear\nregression models with highly collinear regressors. Econometrics and\nStatistics, 11, 1–21. https://doi.org/10.1016/j.ecosta.2018.10.001\n\n\nPineo, P. O., & Porter, J. (1967). Occupational prestige in canada*.\nCanadian Review of Sociology, 4(1), 24–40.\nhttps://doi.org/https://doi.org/10.1111/j.1755-618X.1967.tb00472.x\n\n\nPineo, P. O., & Porter, J. (2008). Occupational prestige in canada.\nCanadian Review of Sociology, 4(1), 24–40. https://doi.org/10.1111/j.1755-618x.1967.tb00472.x\n\n\nPlaster, M. E. (1989). The effect of defendent physical\nattractiveness on juridic decisions using felon inmates as mock\njurors [Unpublished master's thesis]. East Carolina University.\n\n\nPlayfair, W. (1786). Commercial and political atlas: Representing,\nby copper-plate charts, the progress of the commerce, revenues,\nexpenditure, and debts of england, during the whole of the eighteenth\ncentury. Debrett; Robinson;; Sewell. http://ucpj.uchicago.edu/Isis/journal/demo/v000n000/000000/000000.fg4.html\n\n\nPlayfair, W. (1801). Statistical breviary; shewing, on a principle\nentirely new, the resources of every state and kingdom in\nEurope. Wallis.\n\n\nProkofieva, M., Zarate, D., Parker, A., Palikara, O., &\nStavropoulos, V. (2023). Exploratory structural equation modeling: A\nstreamlined step by step approach using the r project software. BMC\nPsychiatry, 23(1). https://doi.org/10.1186/s12888-023-05028-9\n\n\nReaven, G. M., & Miller, R. G. (1968). Study of the relationship\nbetween glucose and insulin responses to an oral glucose load in man.\nDiabetes, 17(9), 560–569. https://doi.org/10.2337/diab.17.9.560\n\n\nReaven, G. M., & Miller, R. G. (1979). An attempt to define the\nnature of chemical diabetes using a multidimensional analysis.\nDiabetologia, 16, 17–24.\n\n\nRennie, N. (2025). The art of data visualization with ggplot2.\nCRC Press. https://nrennie.rbind.io/art-of-viz/\n\n\nRobinaugh, D. J., Hoekstra, R. H. A., Toner, E. R., & Borsboom, D.\n(2019). The network approach to psychopathology: A review of the\nliterature 2008–2018 and an agenda for future research.\nPsychological Medicine, 50(3), 353–366. https://doi.org/10.1017/s0033291719003404\n\n\nRogan, J. C., & Keselman, H. J. (1977). Is the ANOVA\nf-test robust to variance heterogeneity when sample sizes are equal?: An\ninvestigation via a coefficient of variation. American Educational\nResearch Journal, 14(4), 493–498. https://doi.org/10.3102/00028312014004493\n\n\nRousseeuw, Peter J., Ruits, I., & Tukey, J. W. (1999). The bagplot:\nA bivariate boxplot. The American Statistician, 53(4),\n382–387.\n\n\nRousseeuw, Peter J., Van Aelst, S., Van Driessen, K., & Gulló, J. A.\n(2004). Robust multivariate regression. Technometrics,\n46(3), 293–305. https://doi.org/10.1198/004017004000000329\n\n\nRousseeuw, Peter J., & Yohai, V. J. (1984). Robust regression by\nmeans of S-estimators. In J. Franke, W. Härdle, & D.\nMartin (Eds.), Robust and nonlinear time series analysis (Vol.\n26, pp. 256–272). Springer.\n\n\nRussell, H. N. (1914). Relations between the spectra and other\ncharacteristics of the stars. Popular Astronomy, 22,\n275–294.\n\n\nSarkar, D. (2008). Lattice: Multivariate data visualization with\nr. Springer. http://lmdvr.r-forge.r-project.org\n\n\nSchatzoff, M. (1966). Sensitivity comparisons among tests of the general\nlinear hypothesis. Journal of the American Statistical\nAssociation, 61(314), 415–435. https://doi.org/10.1080/01621459.1966.10480878\n\n\nScheffé, H. A. (1960). The analysis of variance. Wiley.\n\n\nSchloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M.,\nThoen, E., Elberg, A., & Crowley, J. (2025). GGally: Extension\nto ’ggplot2’. https://doi.org/10.32614/CRAN.package.GGally\n\n\nScott, D. W. (1992). Multivariate density estimation: Theory,\npractice, and visualization. Wiley.\n\n\nSearle, S. R., Speed, F. M., & Milliken, G. A. (1980). Population\nmarginal means in the linear model: An alternative to least squares\nmeans. The American Statistician, 34(4), 216–221.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\nfor normality (complete samples). Biometrika, 52(3–4),\n591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nShepard, R. N. (1962a). The analysis of proximities: Multidimensional\nscaling with an unknown distance function. i. Psychometrika,\n27(2), 125–140. https://doi.org/10.1007/bf02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities: Multidimensional\nscaling with an unknown distance function. II. Psychometrika,\n27(3), 219–246. https://doi.org/10.1007/bf02289621\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S.\n(1972a). Multidimensional scaling; theory and applications in the\nbehavioral sciences: Vols. II. Applications. Seminar Press. https://books.google.ca/books?id=PpFAAQAAIAAJ\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S.\n(1972b). Multidimensional scaling: Theory and applications in the\nbehavioral sciences: Vols. I. Theory. Seminar Press. https://books.google.ca/books?id=pJRAAQAAIAAJ\n\n\nShoben, E. J. (1983). Applications of multidimensional scaling in\ncognitive psychology. Applied Psychological Measurement,\n7(4), 473–490. https://doi.org/10.1177/014662168300700406\n\n\nSilverman, B. W. (1986). Density estimation for statistics and data\nanalysis. Chapman & Hall.\n\n\nSimpson, E. H. (1951). The interpretation of interaction in contingency\ntables. Journal of the Royal Statistical Society, Series B,\n30, 238–241.\n\n\nSpence, I., & Garrison, R. F. (1993). A remarkable scatterplot.\nThe American Statistician, 47(1), 12–19.\n\n\nSwayne, D. F., Cook, D., & Buja, A. (1998). XGobi: Interactive\ndynamic data visualization in the x window system. Journal of\nComputational and Graphical Statistics, 7(1), 113–130. https://doi.org/10.1080/10618600.1998.10474764\n\n\nSwayne, D. F., Lang, D. T., Buja, A., & Cook, D. (2003).\nGGobi: Evolving from XGobi into an extensible\nframework for interactive data visualization. Computational\nStatistics &Amp; Data Analysis, 43(4), 423–444. https://doi.org/10.1016/s0167-9473(02)00286-4\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso.\nJournal of the Royal Statistical Society, Series B:\nMethodological, 58, 267–288.\n\n\nTiku, M. L., & Balakrishnan, N. (1984). Testing equality of\npopulation variances the robust way. Communications in Statistics -\nTheory and Methods, 13(17), 2143–2159. https://doi.org/10.1080/03610928408828818\n\n\nTimm, N. H. (1975). Multivariate analysis with applications in\neducation and psychology. Wadsworth (Brooks/Cole).\n\n\nTorgerson, W. S. (1952). Multidimensional scaling: I. Theory and method.\nPsychometrika, 17(4), 401–419. https://doi.org/10.1007/bf02288916\n\n\nTufte, E. R. (1983). The visual display of quantitative\ninformation. Graphics Press.\n\n\nTukey, J. W. (1959). A quick, compact, two sample test to Duckworth’s specifications.\nTechnometrics, 1, 31–48. https://doi.org/10.2307/1266308\n\n\nTukey, J. W. (1962). The future of data analysis. The Annals of\nMathematical Statistics, 33(1), 1–67. https://doi.org/10.1214/aoms/1177704711\n\n\nTukey, J. W. (1977). Exploratory data analysis. Addison Wesley.\n\n\nTukey, J. W., & Tukey, P. A. (1985). Computer graphics and\nexploratory data analysis: An introduction. Proceedings of the Sixth\nAnnual Conference and Exposition: Computer Graphics85,\nIII, 773–785.\n\n\nTurk, M., & Pentland, A. (1991). Eigenfaces for recognition.\nJournal of Cognitive Neuroscience, 3(1), 71–86. https://doi.org/10.1162/jocn.1991.3.1.71\n\n\nUnwin, A. (2024). Getting (more out of) graphics: Practice and\nprinciples of data visualisation. Chapman; Hall/CRC. https://doi.org/10.1201/9781003131212\n\n\nVanderPlas, S., Ge, Y., Unwin, A., & Hofmann, H. (2023). Penguins go\nparallel: A grammar of graphics framework for generalized parallel\ncoordinate plots. Journal of Computational and Graphical\nStatistics, 1–16. https://doi.org/10.1080/10618600.2023.2195462\n\n\nVelleman, P. F., & Welsh, R. E. (1981). Efficient computing of\nregression diagnostics. The American Statistician,\n35(4), 234–242.\n\n\nVinod, H. D. (1978). A survey of ridge regression and related techniques\nfor improvements over ordinary least squares. The Review of\nEconomics and Statistics, 60(1), 121–131. http://www.jstor.org/stable/1924340\n\n\nVu, V. Q., & Friendly, M. (2024). Ggbiplot: A grammar of\ngraphics implementation of biplots. https://doi.org/10.32614/CRAN.package.ggbiplot\n\n\nWaddell, A., & Oldford, R. W. (2025). Loon: Interactive\nstatistical data visualization. https://doi.org/10.32614/CRAN.package.loon\n\n\nWarne, F. T. (2014). A primer on multivariate analysis of\nvariance(MANOVA) for behavioral scientists. Practical Assessment,\nResearch & Evaluation, 19(1). https://scholarworks.umass.edu/pare/vol19/iss1/17/\n\n\nWegman, E. J. (1990). Hyperdimensional data analysis using parallel\ncoordinates. Journal of the American Statistical Association,\n85(411), 664–675.\n\n\nWei, T., & Simko, V. (2024). R package ’corrplot’: Visualization\nof a correlation matrix. https://github.com/taiyun/corrplot\n\n\nWelch, B. L. (1947). The generalization of \"student’s\" problem when\nseveral different population varlances are involved.\nBiometrika, 34(1–2), 28–35. https://doi.org/10.1093/biomet/34.1-2.28\n\n\nWest, D. B. (2001). Introduction to graph theory. Prentice\nhall.\n\n\nWhittaker, J. (1990). Graphical models in applied multivariate\nstatistics. John Wiley; Sons.\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data\nanalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWickham, H. (2019). Advanced r. Chapman; Hall/CRC. https://doi.org/10.1201/9781351201315\n\n\nWickham, H., & Cook, D. (2025). Tourr: Tour methods for\nmultivariate data visualisation. https://github.com/ggobi/tourr\n\n\nWickham, H., Cook, D., Hofmann, H., & Buja, A. (2011). Tourr: An\nR package for exploring multivariate data with projections.\nJournal of Statistical Software, 40(2). https://doi.org/10.18637/jss.v040.i02\n\n\nWilke, C. O. (2019). Fundamentals of Data\nVisualization: A Primer on\nMaking Informative and Compelling\nFigures. O’Reilly Media. clauswilke.com/dataviz\n\n\nWilkinson, G. N., & Rogers, C. E. (1973). Symbolic description of\nfactorial models for analysis of variance. Applied Statistics,\n22(3), 392. https://doi.org/10.2307/2346786\n\n\nWilkinson, L. (1999). The grammar of graphics. Springer.\n\n\nWilkinson, L., Anand, A., & Grossman, R. L. (2005). Graph-theoretic\nscagnostics. In J. T. Stasko & M. O. Ward (Eds.), Proceedings of\nthe IEEE information visualization 2005 (pp. 157–164). IEEE\nComputer Society. http://dblp.uni-trier.de/db/conf/infovis/infovis2005.html#WilkinsonAG05\n\n\nWiner, B. J. (1962). Statistical principles in experimental\ndesign. McGraw-Hill.\n\n\nWood, S. N. (2006). Generalized additive models: An introduction\nwith r. Chapman; Hall/CRC Press.\n\n\nWright, K. (2021). Corrgram: Plot a correlogram. https://doi.org/10.32614/CRAN.package.corrgram\n\n\nXie, Y. (2021). Animation: A gallery of animations in statistics and\nutilities to create animations. https://yihui.org/animation/\n\n\nXu, Z., & Oldford, R. W. (2021). Loon.tour: Tour in ’loon’.\nhttps://cran.r-project.org/package=loon.tourr\n\n\nYee, T. W. (2015). Vector generalized linear and additive models:\nWith an implementation in r. Springer.\n\n\nYee, T. W. (2025). VGAM: Vector generalized linear and\nadditive models. https://CRAN.R-project.org/package=VGAM\n\n\nYohai, V. J. (1987). High breakdown-point and high efficiency robust\nestimates for regression. The Annals of Statistics,\n15(2), 642–656.\n\n\nZhang, J., & Boos, D. D. (1992). Bootstrap critical values for\ntesting homogeneity of covariance matrices. Journal of the American\nStatistical Association, 87(418), 425–429. http://www.jstor.org/stable/2290273",
    "crumbs": [
      "End Matter",
      "References"
    ]
  }
]