[
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Why use a multivariate design\nA major goal of this book is to introduce you to multivariate thinking in research design and data analysis. This is accomplished with a collection of powerful graphical methods designed to make it easier to understand and communicate results when there are multiple response variables to be considered together. However, these methods are somewhat more complex than the standard univariate ones, so it is worth considering these questions: Why should you to go to this trouble? What’s in it for me?\nThe first important point is that reality in many research domains is inherently multivariate, particularly in the social and behavioral sciences. Theoretical constructs such as “depression”, “academic achievement”, “self-concept”, “happiness” or “perfectionism” can often be measured by different scales, or have been identified to have more than one aspect or context worthy of study.\nConceptual advantages\nAnalyzing these together from a multivariate approach can reveal relationships completely missed or ignored by simpler univariate analyses. Theory is strengthened and becomes more nuanced from a multivariate perspective.\nYou can understand each of those terms, but actually all of these constructs are inherently multidimensional. The idea of “self-concept”, for instance is comprised of all the beliefs that an individual has about him/herself. These include physical, social, intellectual, family, emotional, and professional/academic domains, which reflect how individuals perceive their bodies, their roles in society, their abilities, their emotional responses, their familial connections, and their competencies in work and school.\nEven something simpler, like academic achievements of adolescents, aged 10–16 is comprised of measures of their knowledge and performance in the domains of reading, mathematics, science, history, etc. Say we want to assess the influence of predictors such as parent encouragement, their socioeconomic status and school environmental variables on these outcomes. In a comprehensive study, the following questions are of interest:\nSimilarly, if psychiatric patients in various diagnostic categories are measured on a battery of tests related to social skills and cognitive functioning, we might want to know:\nSuch questions obviously concern more than just the separate univariate relations of each response to the predictors. Equally, perhaps more importantly, are questions of how the response variables are predicted jointly considering their correlations that a multivariate approach can reveal.\nStatistical advantages\nA second important point is that a multivariate modeling approach can offer statistical advantages:",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#why-use-a-multivariate-design",
    "href": "01-intro.html#why-use-a-multivariate-design",
    "title": "1  Introduction",
    "section": "",
    "text": "Do predictors (parental encouragement, …) affect all of these outcomes? Or do some of these have weak or null effects?\nDo they affect them in the same or different ways? That is, do their effects tend to go in the same directions?\nHow many different aspects of academic achievement can be distinguished in the predictors? Equivalently, is academic achievement unidimensional or multidimensional in relation to the predictors?\n\n\n\nWhich measures best discriminate among the diagnostic groups?\nWhich measures are most predictive of positive outcomes?\nFurther, how are the relationships between the outcomes affected by the predictors?\n\n\n\n\n\n\n\n\nWhat about SEM?\n\n\n\nStructural equation modeling (SEM) offers another route to explore and analyze the relationships among multiple predictors and multiple responses. They have the advantage of being able to test potentially complex systems of linear equations in very flexible ways. However, these methods are often far removed from data analysis per se because they typically start with data summaries such as means and covariance matrices. The raw data is nicely bundled up in these summaries, but consequently there is little room to learn from features such as non-linear relations, interesting clusters, or anomalous observations that might threaten the validity of a proposed model. But see Prokofieva et al. (2023) for an approach (ESEM) using R that combines exploratory analysis with standard SEM, or Brandmaier et al. (2014) for an approach combining SEM and decision trees to allow data-driven refinement of models.\nMoreover, except for path diagrams, they usually offer little in the way of visualization methods to aid in understanding and communicating the results. The graphical methods we describe here can also be useful in a SEM context. For example, multivariate multiple regression models (Section 10.6) and canonical correlation (Section 11.10) can be used to explore relationships among the observed variables used in a SEM, but in a data-centric way. \n\n\n\n\n\nThey can have increased statistical power to detect a significant effect, because multivariate tests pool the strength of effects across a collection of positively related response variables.\nThis easily avoids inflated Type I error rates (false positives) from multiple separate tests on each of the outcomes.\nWe gain deeper insights into how outcome variables vary together.\nWith more than a few outcome variables dimension reduction methods offer views of the data and model in 2D plots that capture their essence.\n\n\n1.1.1 Multivariate vs. multivariable methods\n\nmultivariate \\(\\ne\\) multivariable\n\n“Multi-” terminology is important. In this era of multivitamins, multitools, multifactor authentication and even the multiverse, it is well to understand the distinction between multivariate and multivariable methods as these terms are generally used and as I use them here in relation to statistical methods and data visualization. The distinction is simple:\n\nMulti_variable_ methods have a single dependent variable and more than one independent variables or covariates, such as in multiple regression. Multiple regression is the prime example.\nMulti_variate_ methods for linear models, such as multivariate multiple regression or multivariate analysis of variance, have more than one dependent, response or outcome variable. Canonical correlation analysis considers the relation between two sets of variables, but treats them symmetrically. Other multivariate methods such as principal components analysis or factor analysis don’t distinguish between independent and dependent, but treat all variables on an equal footing in a single set.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#linear-models-univariate-to-multivariate",
    "href": "01-intro.html#linear-models-univariate-to-multivariate",
    "title": "1  Introduction",
    "section": "\n1.2 Linear models: Univariate to multivariate",
    "text": "1.2 Linear models: Univariate to multivariate\nThe path to multivariate thinking in this book is eased by the simplicity of extending univariate models to multivariate ones in notation and computation. For classical linear models for ANOVA and regression, the step from a univariate model for a single response, \\(y\\), to a multivariate one for a collection of \\(p\\) responses, \\(\\mathbf{y}\\) is conceptually very easy. That’s because the univariate model with \\(q\\) predictors,\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_q x_q + \\epsilon_i \\;, \\]\nwhen cast in matrix terms,\n\\[\n\\mathbf{y} = \\mathbf{X} \\; \\mathbf{\\beta} + \\mathbf{\\epsilon}\\;, \\quad\\mbox{   with   }\\quad \\mathbf{\\epsilon} \\sim \\mathcal{N} (0, \\sigma^2 \\mathbf{I}) \\;,\n\\]\ngeneralizes directly to an analogous multivariate linear model (MLM),\n\\[\n\\mathbf{Y} = [\\mathbf{y_1}, \\mathbf{y_2}, \\dots, \\mathbf{y_p}] = \\mathbf{X} \\; \\mathbf{B} + \\boldsymbol{\\Large\\varepsilon}\\quad\\mbox{   with   }\\quad \\boldsymbol{\\Large\\varepsilon}\\sim \\mathcal{N} (\\mathbf{0}, \\mathbf{\\Sigma}) \\; ,\n\\]\nfor multiple responses (as will be discussed in detail in Chapter 10). The design matrix, \\(\\mathbf{X}\\) remains the same, and the vector \\(\\beta\\) of coefficients simply becomes a matrix \\(\\mathbf{B}\\), with one column for each of the \\(p\\) outcome variables.\nHappily as well, hypothesis tests for the MLM are also straight-forward generalizations of the familiar \\(F\\) and \\(t\\)-tests for univariate response models. Moreover, there is a rich geometry underlying these generalizations (Friendly et al., 2013) which we can exploit for understanding and visualization in Chapter 11.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#visualization-is-harder",
    "href": "01-intro.html#visualization-is-harder",
    "title": "1  Introduction",
    "section": "\n1.3 Visualization is harder",
    "text": "1.3 Visualization is harder\nHowever, with two or more response variables, visualizations for multivariate models are not as simple as they are for their univariate counterparts. These include plots for understanding the effects of predictors, model parameters, or model diagnostics. Consequently, the results of such studies are often explored and discussed solely in terms of coefficients and significance, and visualizations of the relationships are only provided for one response variable at a time, if at all. This tradition can mask important nuances, and lead researchers to draw erroneous conclusions.\nThe aim of this book is to describe and illustrate some central methods that we have developed over the last thirty years that aid in the understanding and communication of the results of multivariate models [Friendly:94a; Friendly:02:corrgram; Friendly (2007);  Friendly & Meyer (2016)]. These methods for quantitative data rely on data ellipsoids as simple, minimally sufficient visualizations of variance that can be shown in 2D and 3D plots. As will be demonstrated, the Hypothesis-Error (HE) plot framework (Section 11.1) applies this idea to the results of multivariate tests of linear hypotheses. \nFurther, in the case where there are more than just a few outcome variables, the important nectar of their relationships to predictors can often be distilled in a multivariate juicer— a projection of the multivariate relationships to the predictors in the low-D space that captures most of the flavor. This idea can be applied using canonical correlation plots and with canonical discriminant HE plots (Section 11.7). \nTODO: explain relevance of this fig.\n\n\nProjection: The cover image from Douglas Hofstadter’s Gödel, Bach and Escher (1979) illustrates projection of 3D solids onto each 2D plane. Each 2D view captures some salient aspect of the complete figure.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-problems",
    "href": "01-intro.html#sec-problems",
    "title": "1  Introduction",
    "section": "\n1.4 Problems in understanding and communicating MLM results",
    "text": "1.4 Problems in understanding and communicating MLM results\nIn my consulting practice within the Statistical Consulting Service at York University, I see hundreds of clients each year ranging from advanced undergraduate thesis students, to graduate students and faculty from a variety of fields. Initially, most clients used SAS or SPSS, but now most use R or Stata for their analyses. Over the last three decades, and across each of these groups, I have noticed an increasing desire to utilize multivariate methods. As researchers are exposed to the utility and power of multivariate tests, they see them as an appealing alternative to running many univariate ANOVAs or multiple regressions for each response variable separately.\nHowever, multivariate analyses are more complicated than such approaches, especially when it comes to understanding and communicating results. Output is typically voluminous, and researchers will often get lost in the numbers. While software in SAS, SPSS and R make tabular summary displays easy, these often obscure the findings that researchers are most interested in. The most common analytic oversights that I have observed are:\n\nAtomistic data screening: Researchers have mostly learned the assumptions (the Holy Trinity of normality, constant variance and independence) of univariate linear models, but then apply univariate tests (e.g., Shapiro-Wilk) and diagnostic plots (normal QQ plots) to every predictor and every response.\nBonferroni everywhere: Faced with the task of reporting the results for multiple response measures and a collection of predictors for each, a common tendency is to run (and sometimes report) each of the separate univariate response models and then apply a correction for multiple testing. Not only is this confusing and awkward to report, but it is largely unnecessary because the multivariate tests provide protection for multiple testing.\nReverting to univariate visualizations: To display results, SPSS and SAS make some visualization methods available through menu choices or syntax, but usually these are the wrong (or at least unhelpful) choices, in that they generate separate univariate graphs for the individual responses.\n\nThis book discusses a few essential procedures for multivariate linear models, how their interpretation can be aided through the use of well-crafted (though novel) visualizations, and provides replicable sample code in R to showcase their use in applied behavioral research. \n\n\n\n\n\n\nBrandmaier, A., Von Oertzen, T., Mcardle, J., & Lindenberger, U. (2014). Exploratory data mining with structural equation model trees. In J. J. M. & G. Ritschard (Ed.), Contemporary issues in exploratory data mining in the behavioral sciences (pp. 96–127).\n\n\nFriendly, M. (2007). HE plots for multivariate general linear models. Journal of Computational and Graphical Statistics, 16(2), 421–444. https://doi.org/10.1198/106186007X208407\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nHofstadter, D. R. (1979). Gödel, escher, bach: An eternal golden braid. Basic Books.\n\n\nProkofieva, M., Zarate, D., Parker, A., Palikara, O., & Stavropoulos, V. (2023). Exploratory structural equation modeling: A streamlined step by step approach using the r project software. BMC Psychiatry, 23(1). https://doi.org/10.1186/s12888-023-05028-9",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html",
    "href": "02-getting_started.html",
    "title": "2  Getting Started",
    "section": "",
    "text": "2.1 Why plot your data?\nAt the time the Farhquhar brothers wrote this pithy aphorism, graphical methods for understanding data had advanced considerably, but were not universally practiced, prompting their complaint. Most data tables we see are designed for looking something up like the number of measles cases in Ontario compared to other provinces. Tables are not usually designed to reveal patterns, trends or anomalies, although this can be accomplished with modern table generating software such as the tinytable or gt packages.1\nThe main graphic forms we use today—the pie chart, line graphs and bar—were invented by William Playfair around 1800 (Playfair, 1786, 1801). The scatterplot arrived shortly after (Herschel, 1833) and thematic maps showing the spatial distributions of social variables (crime, suicides, literacy) were used for the first time to reason about important societal questions (Guerry, 1833) such as “is increased education associated with lower rates of crime?”\nIn the last half of the 18th Century, the idea of correlation was developed (Galton, 1886; Pearson, 1896) and the period, roughly 1860–1890, dubbed the “Golden Age of Graphics (Funkhouser, 1937) became the richest period of innovation and beauty in the entire history of data visualization. During this time there was an incredible development of visual thinking, represented by the work of Charles Joseph Minard, advances in the role of visualization within scientific discovery, as illustrated through Francis Galton, and graphical excellence, embodied in state statistical atlases produced in France and elsewhere. See Friendly (2008); Friendly & Wainer (2021) for this history. .\nThis chapter introduces the importance of graphing data through three nearly classic stories with the following themes:",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#sec-why_plot",
    "href": "02-getting_started.html#sec-why_plot",
    "title": "2  Getting Started",
    "section": "",
    "text": "summary statistics are not enough: Anscombe’s Quartet demonstrates datasets that are indistinguishable by numerical summary statistics (mean, standard deviation, correlation), but whose relationships are vastly different.\none lousy point can ruin your day: A researcher is mystified by a difference between a correlation for men and women until she plots the data.\nfinding the signal in noise: The story of the US 1970 Draft Lottery shows how a weak, but reliable signal, reflecting bias in a process can be revealed by graphical enhancement and summarization.\n\n\n2.1.1 Anscombe’s Quartet\nIn 1973, Francis Anscombe (Anscombe, 1973) famously constructed a set of four datasets illustrate the importance of plotting the graphs before analyzing and model building, and the effect of unusual observations on fitted models. Now known as Anscombe’s Quartet, these datasets had identical statistical properties: the same means, standard deviations, correlations and regression lines. \nHis purpose was to debunk three notions that had been prevalent at the time:\n\nNumerical calculations are exact, but graphs are coarse and limited by perception and resolution;\nFor any particular kind of statistical data there is just one set of calculations constituting a correct statistical analysis;\nPerforming intricate calculations is virtuous, whereas actually looking at the data is cheating.\n\nThe dataset datasets::anscombe has 11 observations, recorded in wide format, with variables x1:x4 and y1:y4.\n\ndata(anscombe) \nhead(anscombe)\n#&gt;   x1 x2 x3 x4   y1   y2    y3   y4\n#&gt; 1 10 10 10  8 8.04 9.14  7.46 6.58\n#&gt; 2  8  8  8  8 6.95 8.14  6.77 5.76\n#&gt; 3 13 13 13  8 7.58 8.74 12.74 7.71\n#&gt; 4  9  9  9  8 8.81 8.77  7.11 8.84\n#&gt; 5 11 11 11  8 8.33 9.26  7.81 8.47\n#&gt; 6 14 14 14  8 9.96 8.10  8.84 7.04\n\nThe following code transforms this data to long format and calculates some summary statistics for each dataset.\n\nanscombe_long &lt;- anscombe |&gt; \n  pivot_longer(everything(), \n               names_to = c(\".value\", \"dataset\"), \n               names_pattern = \"(.)(.)\"\n  ) |&gt;\n  arrange(dataset)\n\nanscombe_long |&gt;\n  group_by(dataset) |&gt;\n  summarise(xbar      = mean(x),\n            ybar      = mean(y),\n            r         = cor(x, y),\n            intercept = coef(lm(y ~ x))[1],\n            slope     = coef(lm(y ~ x))[2]\n         )\n#&gt; # A tibble: 4 × 6\n#&gt;   dataset  xbar  ybar     r intercept slope\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 1           9  7.50 0.816      3.00 0.500\n#&gt; 2 2           9  7.50 0.816      3.00 0.5  \n#&gt; 3 3           9  7.5  0.816      3.00 0.500\n#&gt; 4 4           9  7.50 0.817      3.00 0.500\n\nAs we can see, all four datasets have nearly identical univariate and bivariate statistical measures. You can only see how they differ in graphs, which show their true natures to be vastly different.\nFigure 2.1 is an enhanced version of Anscombe’s plot of these data, adding helpful annotations to show visually the underlying statistical summaries.\n\n\n\n\n\n\n\nFigure 2.1: Scatterplots of Anscombe’s Quartet. Each plot shows the fitted regression line and a 68% data ellipse representing the correlation between \\(x\\) and \\(y\\).\n\n\n\n\nThis figure is produced as follows, using a single call to ggplot(), faceted by dataset. As we will see later (Section 3.2), the data ellipse (produced by stat_ellipse()) reflects the correlation between the variables. \n\ndesc &lt;- tibble(\n  dataset = 1:4,\n  label = c(\"Pure error\", \"Lack of fit\", \"Outlier\", \"Influence\")\n)\n\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 4) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"red\", linewidth = 1.5) +\n  scale_x_continuous(breaks = seq(0,20,2)) +\n  scale_y_continuous(breaks = seq(0,12,2)) +\n  stat_ellipse(level = 0.5, color=col, type=\"norm\") +\n  geom_label(data=desc, aes(label = label), x=6, y=12) +\n  facet_wrap(~dataset, labeller = label_both) \n\nThe subplots are labeled with the statistical idea they reflect:\n\ndataset 1: Pure error. This is the typical case with well-behaved data. Variation of the points around the line reflect only measurement error or unreliability in the response, \\(y\\). \ndataset 2: Lack of fit. The data is clearly curvilinear, and would be very well described by a quadratic, y ~ poly(x, 2). This violates the assumption of linear regression that the fitted model has the correct form. \ndataset 3: Outlier. One point, second from the right, has a very large residual. Because this point is near the extreme of \\(x\\), it pulls the regression line towards it, as you can see by imagining a line through the remaining points. \ndataset 4: Influence. All but one of the points have the same \\(x\\) value. The one unusual point has sufficient influence to force the regression line to fit it exactly. \n\nOne moral from this example:\n\nLinear regression only “sees” a line. It does its’ best when the data are really linear. Because the line is fit by least squares, it pulls the line toward discrepant points to minimize the sum of squared residuals.\n\n\n\n\n\n\n\nDatasaurus Dozen\n\n\n\nThe method Anscombe used to compose his quartet is unknown, but it turns out that that there is a method to construct a wider collection of datasets with identical statistical properties. After all, in a bivariate dataset with \\(n\\) observations, the correlation has \\((n-2)\\) degrees of freedom, so it is possible to choose \\(n-2\\) of the \\((x, y)\\) pairs to yield any given value. As it happens, it is also possible to create any number of datasets with the same means, standard deviations and correlations with nearly any shape you like — even a dinosaur! \nThe Datasaurus Dozen was first publicized by Alberto Cairo in a blog post and are available in the datasauRus package package (Gillespie et al., 2025). As shown in Figure 2.2, the sets include a star, cross, circle, bullseye, horizontal and vertical lines, and, of course the “dino”. The method (Matejka & Fitzmaurice, 2017) uses simulated annealing, an iterative process that perturbs the points in a scatterplot, moving them towards a given shape while keeping the statistical summaries close to the fixed target value.\nThe datasauRus package just contains the datasets, but a general method, called statistical metamers, for producing such datasets has been described by Elio Campitelli and implemented in the metamer package.\n\n\n\n\n\n\n\n\n\nFigure 2.2: Animation of the Dinosaur Dozen datasets. Source: https://youtu.be/It4UA75z_KQ\n\n\n\n\n\n\n\n\n\n\nQuartets\n\n\n\nThe essential idea of a statistical “quartet” is to illustrate four quite different datasets or circumstances that seem superficially the same, but yet are paradoxically very different when you look behind the scenes.\nFor example, in the context of causal analysis Gelman et al. (2023), illustrated sets of four graphs, within each of which all four represent the same average (latent) causal effect but with much different patterns of individual effects. McGowan et al. (2023) provide another illustration with four seemingly identical data sets each generated by a different causal mechanism. \nAs an example of machine learning models, Biecek et al. (2023), introduced the “Rashamon Quartet”, a synthetic dataset for which four models from different classes (linear model, regression tree, random forest, neural network) have practically identical predictive performance. In all cases, the paradox is solved when their visualization reveals the distinct ways of understanding structure in the data.  The quartets package package (R-quartets?) contains these and other variations on this theme. \n\n\n\n\n2.1.2 One lousy point can ruin your day\nIn the mid 1980s, a consulting client had a strange problem.2 She was conducting a study of the relation between body image and weight preoccupation in exercising and non-exercising people (Davis, 1990). As part of the design, the researcher wanted to know if self-reported weight could be taken as a reliable indicator of true weight measured on a scale. It was expected that the correlations between reported and measured weight should be close to 1.0, and the slope of the regression lines for men and women should also be close to 1.0. The dataset is carData::Davis.\nShe was therefore very surprised to see the following numerical results: For men, the correlation was nearly perfect, but not so for women. \n\ndata(Davis, package=\"carData\")\nDavis &lt;- Davis |&gt;\n  drop_na()          # drop missing cases\nDavis |&gt;\n  group_by(sex) |&gt;\n  select(sex, weight, repwt) |&gt;\n  summarise(r = cor(weight, repwt))\n#&gt; # A tibble: 2 × 2\n#&gt;   sex       r\n#&gt;   &lt;fct&gt; &lt;dbl&gt;\n#&gt; 1 F     0.501\n#&gt; 2 M     0.979\n\nSimilarly, the regression lines showed the expected slope for men, but that for women was only 0.26.\n\nDavis |&gt;\n  nest(data = -sex) |&gt;\n  mutate(model = map(data, ~ lm(repwt ~ weight, data = .)),\n         tidied = map(model, tidy)) |&gt;\n  unnest(tidied) |&gt;\n  filter(term == \"weight\") |&gt;\n  select(sex, term, estimate, std.error)\n#&gt; # A tibble: 2 × 4\n#&gt;   sex   term   estimate std.error\n#&gt;   &lt;fct&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 M     weight    0.990    0.0229\n#&gt; 2 F     weight    0.262    0.0459\n\n“What could be wrong here?”, the client asked. The consultant replied with the obvious question:\n\nDid you plot your data?\n\nThe answer turned out to be one discrepant point, a female (case 12), whose measured weight was 166 kg (366 lbs!). This single point exerted so much influence that it pulled the fitted regression line down to a slope of only 0.26. \n\n# shorthand to position legend inside the figure\nlegend_inside &lt;- function(position) {          # simplify legend placement\n  theme(legend.position = \"inside\",\n        legend.position.inside = position)\n}\n\nDavis |&gt;\n  ggplot(aes(x = weight, y = repwt, \n             color = sex, shape = sex, linetype = sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  labs(x = \"Measured weight (kg)\", \n       y = \"Reported weight (kg)\") +\n  scale_linetype_manual(values = c(F = \"longdash\", \n                                   M = \"solid\")) +\n  legend_inside(c(.8, .8))\n\n\n\n\n\n\nFigure 2.3: Regression for Davis’ data on reported weight and measures weight for men and women. Separate regression lines, predicting reported weight from measured weight are shown for males and females. One highly unusual point is highlighted.\n\n\n\n\nIn this example, it was arguable that \\(x\\) and \\(y\\) axes should be reversed, to determine how well measured weight can be predicted from reported weight. In ggplot this can easily be done by reversing the x and y aesthetics.\n\nDavis |&gt;\n  ggplot(aes(y = weight, x = repwt, color = sex, shape=sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  labs(y = \"Measured weight (kg)\", \n       x = \"Reported weight (kg)\") +\n    geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  legend_inside(c(.8, .8))\n\n\n\n\n\n\nFigure 2.4: Regression for Davis’ data on reported weight and measures weight for men and women. Separate regression lines, predicting measured weight from reported weight are shown for males and females. The highly unusual point no longer has an effect on the fitted lines.\n\n\n\n\nIn Figure 2.4, this discrepant observation again stands out like a sore thumb, but it makes very little difference in the fitted line for females. The reason is that this point is well within the range of the \\(x\\) variable (repwt). To impact the slope of the regression line, an observation must be unusual in both \\(x\\) and \\(y\\). We take up the topic of how to detect influential observations and what to do about them in Chapter 6.\nThe value of such plots is not only that they can reveal possible problems with an analysis, but also help identify their reasons and suggest corrective action. What went wrong here? Examination of the original data showed that this woman (case 12) mistakenly switched the values, recording her reported weight in the box for measured weight and vice versa.\n\n\n2.1.3 Shaken, not stirred: The 1970 Draft Lottery\n\nAlthough we often hear that data speak for themselves, their voices can be soft and sly.—Frederick Mosteller (1983), Beginning Statistics with Data Analysis, p. 234.\n\nThe power of graphics is particularly evident when data contains a weak signal embedded in a field of noise. To the casual glance, there may seem to be nothing going on, but the signal can be made apparent in an incisive graph.\nA dramatic example of this occurred in 1969 when the U.S. military conducted a lottery, the first since World War II, to determine which young men would be called up to serve in the Vietnam War for 1970. The U.S. Selective Service had devised a system to rank eligible men according to a random drawing of their birthdays. There were 366 blue plastic capsules containing birth dates placed in a transparent glass container and drawn by hand to assign ranked order-of-call numbers to all men within the 18-26 age range.\n\n\n\n\n\n\n\nFigure 2.5: Congressman Alexander Pirnie (R-NY) drawing the first capsule for the Selective Service draft, Dec 1, 1969. Source: https://bit.ly/45c23sB\n\n\n\n\n\nIn an attempt to make the selection process also transparent, the proceeding was covered on radio, TV and film and the dates posted in order on a large display board. The first capsule—drawn by Congressman Alexander Pirnie (R-NY) of the House Armed Services Committee—contained the date September 14, so all men born on September 14 in any year between 1944 and 1950 were assigned lottery number 1, and would be drafted first. April 24 was drawn next, then December 30, February 14, and so on until June 8, selected last. At the time of the drawing, US officials stated that those with birthdays drawn in the first third would almost certainly be drafted, while those in the last third would probably avoid the draft (Fienberg, 1971).\nI watched this unfold with considerable interest because I was eligible for the Draft that year. I was dismayed when my birthday, May 7, came up ranked 35. Ugh! Could some data analysis and graphics get me out of my funk?\nThe data, from the official Selective Service listing are contained in the dataset vcdExtra::Draft1970, ordered by Month and birthdate (Day), with Rank as the order in which the birthdates were drawn.\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\ndata(Draft1970, package = \"vcdExtra\")\ndplyr::glimpse(Draft1970)\n#&gt; Rows: 366\n#&gt; Columns: 3\n#&gt; $ Day   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n#&gt; $ Rank  &lt;int&gt; 305, 159, 251, 215, 101, 224, 306, 199, 194, 325, 32…\n#&gt; $ Month &lt;ord&gt; Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Ja…\n\nA basic scatterplot, slightly prettified, is shown in Figure 2.6. The points are colored by month, and month labels are shown at the bottom.\n\nShow the code# make markers for months at their mid points\nmonths &lt;- data.frame(\n  month =unique(Draft1970$Month),\n  mid = seq(15, 365-15, by = 30))\n\nggplot2:: theme_set(theme_bw(base_size = 16))\ngg &lt;- ggplot(Draft1970, aes(x = Day, y = Rank)) +\n  geom_point(size = 2.5, shape = 21, \n             alpha = 0.3, \n             color = \"black\", \n             aes(fill=Month)\n  ) +\n  scale_fill_manual(values = rainbow(12)) +\n  geom_text(data=months, aes(x=mid, y=0, label=month), nudge_x = 5) +\n  geom_smooth(method = \"lm\", formula = y ~ 1,\n              col = \"black\", fill=\"grey\", linetype = \"dashed\", alpha=0.6) +\n  labs(x = \"Day of the year\",\n       y = \"Lottery rank\") +\n  theme(legend.position = \"none\") \ngg\n\n\n\n\n\n\nFigure 2.6: Basic scatterplot of 1970 Draft Lottery data plotting rank order of selection against birthdates in the year. Points are colored by month. The horizontal line is at the average rank.\n\n\n\n\nThe ranks do seem to be essentially random. Is there any reason to suspect a flaw in the selection process, as I firmly hoped at the time?\nIf you stare at the graph in Figure 2.6 long enough, you just can make out a sparsity of points in the upper right corner and also in the lower left corner compared to the opposite corners. But probably not until I told you where to look.\nVisual smoothers\nFitting a linear regression line or a smoothed (loess) curve can bring out the signal lurking in the background of a field of nearly random points. Figure 2.7 shows a definite trend to lower ranks for birthdays toward the end of the year. Those born earlier in the year were more likely to be given lower ranks, calling them up sooner for the draft. \n\nShow the codeggplot(Draft1970, aes(x = Day, y = Rank)) +\n  geom_point(size = 2.5, shape = 21, \n             alpha = 0.3, \n             color = \"black\", \n             aes(fill=Month)) +\n  scale_fill_manual(values = rainbow(12)) +\n  geom_smooth(method = \"lm\", formula = y~1,\n              se = FALSE,\n              col = \"black\", fill=\"grey\", linetype = \"dashed\", alpha=0.6) +\n  geom_smooth(method = \"loess\", formula = y~x,\n              color = \"blue\", se = FALSE,\n              alpha=0.25) +\n  geom_smooth(method = \"lm\", formula = y~x,\n              color = \"darkgreen\",\n              fill = \"darkgreen\", \n              alpha=0.25) +\n  geom_text(data=months, aes(x=mid, y=0, label=month), nudge_x = 5) +\n  labs(x = \"Day of the year\",\n       y = \"Lottery rank\") +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\nFigure 2.7: Enhanced scatterplot of 1970 Draft Lottery data adding a linear regression line and loess smooth.\n\n\n\n\nIs this a real effect? Even though the points seem to be random over the year, linear regression of Rank on Day shows a highly significant negative effect even though the correlation3 is small (\\(r = -0.226\\)). The slope, -0.226, means that for each additional day in the year the lottery rank decreases about 1/4 toward the front of the draft line; that’s nearly 7 ranks per month.\n\ndraft.mod &lt;- lm(Rank ~ Day, data=Draft1970)\nwith(Draft1970, cor(Day, Rank))\n#&gt; [1] -0.226\ncoef(draft.mod)\n#&gt; (Intercept)         Day \n#&gt;     224.913      -0.226\n\nSo, smoothing the data, using either the linear regression line or a nonparametric smoother is one important technique for seeing a weak signal in a noisy background.\nVisual summaries\nAnother way to enhance the signal-to-noise ratio of a graph is to plot summaries of the messy data points. For example, you might make boxplots of the ranks by month, or calculate and plot the mean or median rank by month and plot those together with some indication of variability within month.\nFigure 2.8 plots the average Rank for each month with error bars showing the mean \\(\\pm 1\\) standard errors against the average Day. The message of rank decreasing nearly linearly with month is now more dramatic, partly because I decreased the range of the y-axis.4 The correlation between the means is \\(r = -0.867\\); the slope is -0.231, similar to what we found for the raw data.\n\nCodemeans &lt;- Draft1970 |&gt;\n  group_by(Month) |&gt;\n  summarize(Day = mean(Day),\n            se = sd(Rank/ sqrt(n())),\n            Rank = mean(Rank)) \n\nggplot(aes(x = Day, y = Rank), data=means) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", formula = y~x,\n              color = \"blue\", fill = \"blue\", alpha = 0.1) +\n  geom_errorbar(aes(ymin = Rank-se, ymax = Rank+se), \n                width = 8, linewidth = 1.3) +\n  geom_text(data=months, aes(x=mid, y=100, label=month), nudge_x = 5) +\n  ylim(100, 250) +\n  labs(x = \"Average day of the year\",\n       y = \"Average lottery rank\")\n\n\n\n\n\n\nFigure 2.8: Plot of the average rank per month with \\(\\pm 1\\) standard error bars. The line shows the least squares regression line, treating months as equally spaced. The vertical axis has been truncated to highlight the decrease in lottery rank over the year.\n\n\n\n\nThe visual impression of a linearly decreasing trend in lottery rank is much stronger in Figure 2.8 than in Figure 2.7 for two reasons:\n\nReplacing the data points with their means strengthens the signal in relation to noise, which is essentially eliminated by plotting means and error bars rather than the raw data. This is an example of visual thinning (Section 3.2.4), reducing visual complexity to highlight an overall pattern.\nThe narrower vertical range (100–250) in the plot of means makes the slope of the line appear steeper. (However, the slope of the means, \\(b = -0.231\\) is nearly the same as that for the data points.) The narrower range also makes deviations from the regression line more noticeable.\nWhat happened here?\nPrevious lotteries carried out by drawing capsules from a container had occasionally suffered the embarrassment that an empty capsule was selected because of vigorous mixing (Fienberg, 1971). So for the 1970 lottery, the birthdate capsules were put in cardboard boxes, one for each month and these were carefully emptied into the glass container in order of month: Jan., Feb., … Dec., gently shaken in atop the pile already there.\nAll might have been well had the persons drawing the capsules put their hand in truly randomly, but generally they picked from toward the top of the container. Consequently, those born later in the year had a greater chance of being picked earlier.\nThere was considerable criticism of this procedure once the flaw had been revealed by analyses such as described here. In the following year, the Selective Service called upon the National Bureau of Standards to devise a better procedure. In 1971 they used two drums, one with the dates of the year and another with the rank numbers 1-366. As a date capsule was drawn randomly from the first drum, another from the numbers drum was picked simultaneously, giving a doubly-randomized sequence.\nOf course, if they had R, the entire process could have been done using sample():\n\nset.seed(42)\ndate = seq(as.Date(\"1971-01-01\"), \n           as.Date(\"1971-12-31\"), by=\"+1 day\")\nrank = sample(seq_along(date))\ndraft1971 &lt;- data.frame(date, rank)\n\nhead(draft1971, 3)\n#&gt;         date rank\n#&gt; 1 1971-01-01   49\n#&gt; 2 1971-01-02  321\n#&gt; 3 1971-01-03  153\ntail(draft1971, 3)\n#&gt;           date rank\n#&gt; 363 1971-12-29    8\n#&gt; 364 1971-12-30  333\n#&gt; 365 1971-12-31  132\n\nAnd, what would have happened to me and all others born on a May 7th, if they did it this way? My lottery rank would have 274!5\n\nme &lt;- as.Date(\"1971-05-07\")\ndraft1971[draft1971$date == me,]\n#&gt;           date rank\n#&gt; 127 1971-05-07  274",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#plots-for-data-analysis-sec-plots-data-analysis",
    "href": "02-getting_started.html#plots-for-data-analysis-sec-plots-data-analysis",
    "title": "2  Getting Started",
    "section": "\n2.2 Plots for data analysis {sec-plots-data-analysis}",
    "text": "2.2 Plots for data analysis {sec-plots-data-analysis}\nVisualization methods take an enormous variety of forms, so it is useful to distinguish several broad categories according to their use in data analysis:\n\ndata plots: primarily plot the raw data, often with annotations to aid interpretation. 1D plots include boxplots, violin plots and dot plots, sometimes in combination; univariate distributions can also be portrayed in histograms and density estimates. 2D plots are most often scatterplots, favorably enhanced using regression lines and smooths, data ellipses, rug plots and marginal distributions. A survey of these methods is presented in Section 3.1. \nreconnaissance plots: with more than a few variables, reconnaissance plots provide a high-level, bird’s-eye overview of the data, allowing you to see patterns that might not be visible in a set of separate plots. Some examples are scatterplot matrices (Section 3.7) showing all bivariate plots of variables in a dataset; correlation diagrams (Section 3.8), using visual glyphs to represent the correlations between all pairs of variables and “trellis” or faceted plots that show how a focal relation of one or more variables differs across values of other variables. \nmodel plots: plot the results of a fitted model, such as a regression line or curve to show uncertainty, or a regression surface in 3D, or a plot of coefficients in model together with confidence intervals. Figure 2.8 is a simple example.\n\nOther model plots try to take into account that a fitted model may involve more variables than can be shown in a static 2D plot. Some examples of these are added variable plots (Section 6.4), and marginal effect plots (Section 6.5), both of which attempt to show the net relation of two focal variables, controlling or adjusting for other variables in a model. \n\ndiagnostic plots: indicating potential problems with the fitted model. For linear models, these include residual plots, influence plots, plots for testing homogeneity of variance and so forth, illustrated in Section 6.1.2. Plots for diagnosing problems with multivariate models are discussed in Section 10.7. \ndimension reduction plots : plot representations of the data in a space of fewer dimensions than the number of variables in the analysis. Simple examples include principal components analysis (PCA) and the related biplots, and multidimensional scaling (MDS) methods. This is the topic of Chapter 4, but this powerful idea runs through the rest of the book. I refer to such plots as multivariate juicers, because they can squeeze the essence of your data into low-D package, enhancing the flavor.\n\n\n2.2.1 Diagnostic plots\nHaving fit a model, your next step should usually be to try to criticize it by checking whether the assumptions of the model are met in your data.\nFor example, the plot of the Davis data in Figure 2.3 effectively fits a separate regression line for males and females, which is expressed by the model formula repwt ~ weight * sex. Having fit the model using lm(), the plot method for a \"lm\" object produces a set of four diagnostic plots (called the “regression quartet” Section 6.1) designed to highlight problems with the fitted model.\n\ndavis.mod &lt;- lm(repwt ~ weight * sex, data=Davis)  \nplot(davis.mod, \n     cex.lab = 1.2, cex = 1.1, \n     id.n = 2, cex.id = 1.2, lwd = 2)\n\n\n\n\n\n\nFigure 2.9: Regression quartet: Four diagnostic plots for the linear model fit to the Davis data.\n\n\n\n\nThe details of such plots are discussed and illustrated in Section 6.1, so I will just point out one useful feature here: that of point identification: the id.n argument controls the number of most-extreme points to be labeled in each panel. The point for the erroneous case 12 female who mis-recorded her height as her weight sticks out a mile in all four panels.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#principles-of-graphical-display",
    "href": "02-getting_started.html#principles-of-graphical-display",
    "title": "2  Getting Started",
    "section": "\n2.3 Principles of graphical display",
    "text": "2.3 Principles of graphical display\nTODO: This could be a separate chapter, supplementary materials or excluded here\n\nCriteria for assessing graphs: communication goals\nEffective data display:\n\nMake the data stand out\nMake graphical comparison easy\nEffect ordering: For variables and unordered factors, arrange them according to the effects to be seen\n\n\nVisual thinning: As the data becomes more complex, focus more on impactful summaries",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#what-have-we-learned",
    "href": "02-getting_started.html#what-have-we-learned",
    "title": "2  Getting Started",
    "section": "\n2.4 What have we learned?",
    "text": "2.4 What have we learned?\nThis chapter demonstrates why visualization isn’t just a “nice-to-have” feature in data analysis–—it’s absolutely essential. Through compelling historical examples and modern techniques, we’ve discovered some fundamental idea that every data analyst should embrace:\n\nSummary statistics can lie (beautifully): Anscombe’s Quartet reveals that datasets can have identical means, correlations, and regression coefficients yet tell completely different stories. The quartet’s four plots–—pure error, lack of fit, outliers, and influence—–show that numerical summaries without visualization can lead us wildly astray. Modern extensions like the Datasaurus Dozen prove this isn’t just a quirky historical example–—you can literally hide a dinosaur in your data while maintaining identical statistical properties! Talk about the dinosaur in the room!\nOne rogue data point can hijack your entire analysis: Plotting the raw data facilitates critical engagement with our statistical models. The Davis weight study demonstrates how a single influential observation (one participant who accidentally switched their reported and measured weights) can completely distort relationships between variables. What appeared to be a puzzling gender difference in the reliability of self-reported weight vanished once the data was plotted and the outlier revealed itself as a simple recording error.\nMeaning becomes more apparent through thoughtful visualizations of our well-considered models: Statistical modeling helps guide our attention in what might otherwise be a chaotic plot of raw data. The 1970 Draft Lottery story shows how graphics can reveal systematic bias hiding in apparently random data. While individual lottery numbers seemed random, smoothing techniques and summary plots exposed a clear pattern–—later birthdays were systematically favored due to poor mixing of the lottery capsules. Sometimes the most important patterns are the ones that whisper rather than shout.\nDifferent plot types serve different purposes: The chapter introduces a taxonomy of visualization goals that helps us choose the right tool for each analytical task. Data plots show raw observations, reconnaissance plots provide bird’s-eye overviews of complex datasets, model plots reveal fitted relationships, diagnostic plots expose model problems, and dimension reduction plots tame high-dimensional complexity. Each serves a distinct role in the analytical process.\nVisual enhancement amplifies signal over noise: Whether through smoothing lines, statistical summaries, or careful use of color and annotation, the chapter shows how thoughtful visual design can make weak patterns stand out dramatically. The Draft Lottery analysis becomes far more convincing when we plot monthly averages rather than individual data points, transforming a subtle correlation into an unmistakable trend.\n\nThe overarching message is clear: in an era where we can compute any statistic imaginable, the humble graph remains our most powerful tool for understanding what our data are really trying to tell us. As the Farquhar brothers noted over a century ago, getting information from tables alone is like extracting sunlight from a cucumber—possible in theory, but why make it so hard on yourself?\n\n\n\n\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. The American Statistician, 27, 17–21.\n\n\nBiecek, P., Baniecki, H., Krzyzinski, M., & Cook, D. (2023). Performance is not enough: A story of the rashomon’s quartet. https://arxiv.org/abs/2302.13356\n\n\nDavis, C. (1990). Body image and weight preoccupation: A comparison between exercising and non-exercising women. Appetite, 16(1), 84. https://doi.org/10.1016/0195-6663(91)90115-9\n\n\nFarquhar, A. B., & Farquhar, H. (1891). Economic and industrial delusions: A discourse of the case for protection. Putnam.\n\n\nFienberg, S. E. (1971). Randomization and social affairs: The 1970 draft lottery. Science, 171, 255–261.\n\n\nFriendly, M. (2008). The Golden Age of statistical graphics. Statistical Science, 23(4), 502–535. https://doi.org/10.1214/08-STS268\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034\n\n\nFunkhouser, H. G. (1937). Historical development of the graphical representation of statistical data. Osiris, 3(1), 269–405. http://tinyurl.com/32ema9\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGelman, A., Hullman, J., & Kennedy, L. (2023). Causal quartets: Different ways to attain the same average treatment effect. http://www.stat.columbia.edu/~gelman/research/unpublished/causal_quartets.pdf\n\n\nGillespie, C., Locke, S., Davies, R., & D’Agostino McGowan, L. (2025). datasauRus: Datasets from the datasaurus dozen. https://doi.org/10.32614/CRAN.package.datasauRus\n\n\nGuerry, A.-M. (1833). Essai sur la statistique morale de la France. Crochard.\n\n\nHerschel, J. F. W. (1833). On the investigation of the orbits of revolving double stars: Being a supplement to a paper entitled \"micrometrical measures of 364 double stars\". Memoirs of the Royal Astronomical Society, 5, 171–222.\n\n\nMatejka, J., & Fitzmaurice, G. (2017, May). Same stats, different graphs. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3025453.3025912\n\n\nMcGowan, L. D., Gerke, T., & Barrett, M. (2023). Causal inference is not just a statistics problem. Journal of Statistics and Data Science Education, 1–9. https://doi.org/10.1080/26939169.2023.2276446\n\n\nPearson, K. (1896). Contributions to the mathematical theory of evolution—III, regression, heredity and panmixia. Philosophical Transactions of the Royal Society of London, 187, 253–318.\n\n\nPlayfair, W. (1786). Commercial and political atlas: Representing, by copper-plate charts, the progress of the commerce, revenues, expenditure, and debts of england, during the whole of the eighteenth century. Debrett; Robinson;; Sewell. http://ucpj.uchicago.edu/Isis/journal/demo/v000n000/000000/000000.fg4.html\n\n\nPlayfair, W. (1801). Statistical breviary; shewing, on a principle entirely new, the resources of every state and kingdom in Europe. Wallis.\n\n\nTufte, E. R. (1983). The visual display of quantitative information. Graphics Press.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#footnotes",
    "href": "02-getting_started.html#footnotes",
    "title": "2  Getting Started",
    "section": "",
    "text": "For example, a cell in a table can be used to show a “sparklines” (Tufte (1983)), tiny versions of a line graph or bar chart. As well, table rows and/or columns can be sorted to show trends or background colors can be used to show unusual values.↩︎\nThis story is told apocryphally. The consulting client actually did plot the data, but needed help in understanding what went wrong in her analyses and in making better graphs.↩︎\nBecause both days of the year and rank in the lottery are the integers, 1 to 366, the Pearson correlation and Spearman rank order correlation are identical.↩︎\nRestricting the y-axis range in plots can sometimes be a graphical sin. It can distort the visual representation of the data, making differences appear larger than they actually are, and potentially misleading the viewer. But this is not a sin, when it serves a communication goal, which in Figure 2.8 is to focus attention on the relative changes in lottery rank over the year. Using a visual cue like a broken axis in the axis is one way to avoid misinterpretation. For ggplot graphs, the ggbreak package is useful for this purpose.↩︎\nA personal note: I escaped being drafted, but I moved to Canada in 1971. Looking back today, it’s one of the best decisions I ever made.↩︎",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html",
    "href": "07-lin-mod-topics.html",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "7.1 Ellipsoids in data space and \\(\\mathbf{\\beta}\\) space\nThe geometric and graphical approach of earlier chapters has already introduced some new ideas for thinking about multivariate data, models for explaining them, and graphical methods for understanding their results. These can be applied to better understand common problems that arise in data analysis.\nIn Section 7.1 I explore the geometric relationships between ellipses in data space and how these appear in the space of the estimated coefficients of linear models, called \\(\\beta\\) space. It turns out that points in one space correspond to lines in the other, a reflection that each one is in some sense the inverse and dual of the other. This geometry can also be used to understand the effect of measurement errors in the predictor variables, as illustrated in (meas-error?).\nPackages\nIn this chapter I use the following packages. Load them now:\nIt is most common to look at data and fitted models in “data space,” where axes correspond to variables, points represent observations, and fitted models are plotted as lines (or planes) in this space. As we’ve suggested, data ellipsoids provide informative summaries of relationships in data space. For linear models, particularly regression models with quantitative predictors, there is another space—“\\(\\mathbf{\\beta}\\) space”—that provides deeper views of models and the relationships among them. This discussion extends Friendly et al. (2013), Sec. 4.6.\nIn \\(\\mathbf{\\beta}\\) space, the axes pertain to coefficients, for example \\((\\beta_0, \\beta_1)\\) in a simple linear regression. Points in this space are models (true, hypothesized, fitted) whose coordinates represent values of these parameters. For example, one point \\(\\widehat{\\mathbf{\\beta}}_{\\text{OLS}} = (\\hat{\\beta}_0, \\hat{\\beta}_1)\\) represents the least squares estimate; other points, \\(\\widehat{\\mathbf{\\beta}}_{\\text{WLS}}\\) and \\(\\widehat{\\mathbf{\\beta}}_{\\text{ML}}\\) would give weighted least squares and maximum likelihood estimates, and the line \\(\\beta_1 = 0\\) represents the null hypothesis that the slope is zero.\nIn the sense described below, data space and \\(\\boldsymbol{\\beta}\\) space are each dual to the other. In simple linear regression, for example:\nFigure 7.1: Duality of \\((x, y)\\) lines in data space (left) and points in \\(\\beta\\)-space (right). Each line in data space corresponds to a point, whose intercept and slope are shown in \\(\\beta\\)-space.\nThis is illustrated in Figure 7.1. The left panel shows three lines in data space, which can be expressed as linear equations in \\(\\mathbf{z} = (x, y)\\) of the form \\(\\mathbf{A} \\mathbf{z} = \\mathbf{d}\\). matlib::showEqn(A, d) prints these as equations in \\(x\\) and \\(y\\).\nA &lt;- matrix(c( 1, 1, 0,\n              -1, 1, 1), 3, 2) \nd &lt;- c(2, 1/2, 1)\nshowEqn(A, d, vars = c(\"x\", \"y\"), simplify = TRUE)\n#&gt;   x - 1*y  =    2 \n#&gt;   x   + y  =  0.5 \n#&gt; 0*x   + y  =    1\nThe first equation, \\(x - y = 2\\) can be expressed as the line \\(y = x - 2\\) and corresponds to the point \\((\\beta_0, \\beta_1) = (-2, 1)\\) in \\(\\beta\\) space, and similarly for the other two equations. The second equation, \\(x + y = \\frac{1}{2}\\), or \\(y = 0.5 - x\\) intersects the first at the point \\((x, y) = (1.25, 0.75)\\); this corresponds to the line connecting \\((-2, 1)\\) and \\((0.5, -1)\\) in \\(\\beta\\) space.\nThis lovely duality is an example of an important principle of duality in modern mathematics which translates concepts and structures from one perspective to another and back again. We get two views of the same thing, whose dual nature provides greater insight.\nWe have seen (Section 3.2) how ellipsoids in data space summarize variance (lack of precision) and correlation of our data. For the purpose of understanding linear models, ellipsoids in \\(\\beta\\) space do the same thing for the estimates of parameters. These ellipsoids are dual and inversely related to each other, a point first made clear by Dempster (1969, Ch. 6):\nIt is useful to understand the underlying geometry here connecting the ellipses for a matrix and its inverse. This can be seen in Figure 7.2, which shows an ellipse for a covariance matrix \\(\\mathbf{S}\\), whose axes, as we saw in Chapter 4 are the eigenvectors \\(\\mathbf{v}_i\\) of \\(\\mathbf{S}\\) and whose radii are the square roots \\(\\sqrt{\\lambda_i}\\) of the corresponding eigenvalues. The comparable ellipse for \\(2 \\mathbf{S}\\) has radii multiplied by \\(\\sqrt{2}\\).\nFigure 7.2: Geometric properties of an ellipse \\(\\mathbf{S}\\) and its inverse, \\(\\mathbf{S}^{-1}\\). The principal axes (dotted lines) are given by the eigenvectors, which are the same for \\(\\mathbf{S}\\) and \\(\\mathbf{S}^{-1}\\). Multiplying \\(\\mathbf{S}\\) by 2 makes it’s ellipse larger by \\(\\sqrt{2}\\), while the same factor makes the ellipse for \\((2 \\mathbf{S})^{-1}\\) smaller by the same factor.\nAs long as \\(\\mathbf{S}\\) is of full rank, the eigenvectors of \\(\\mathbf{S}^{-1}\\) are identical, while the eigenvalues are \\(1 / \\lambda_i\\), so the radii are the reciprocals \\(1 / \\sqrt{\\lambda_i}\\). The analogous ellipse for \\((2 \\mathbf{S}^{-1})\\) is smaller by a factor of \\(\\sqrt{2}\\).\nThus, in two dimensions, the ellipse for \\(\\mathbf{S}^{-1}\\) is a \\(90^o\\) rotation of that for \\(\\mathbf{S}\\). It is small in directions where the ellipse for \\(\\mathbf{S}\\) is large, and vice-versa. In our statistical applications, this translates as: parameter estimates in \\(\\beta\\) space are more precise (have less variance) in the directions where the data are more widely dispersed, having more information about the relationship.\nWe illustrate these ideas in the example below.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#sec-betaspace",
    "href": "07-lin-mod-topics.html#sec-betaspace",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "each line, like \\(\\mathbf{y} = \\beta_0 + \\beta_1 \\mathbf{x}\\) with intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) in data space corresponds to a point \\((\\beta_0,\\beta_1)\\) in \\(\\mathbf{\\beta}\\) space, and conversely;\nthe set of points on any line \\(\\beta_1 = x + y \\beta_0\\) in \\(\\mathbf{\\beta}\\) space corresponds to a set of lines through a given point \\((x, y)\\) in data space, and conversely;\nthe geometric proposition that “every pair of points defines a line in one space” corresponds to the proposition that “every two lines intersect in a point in the other space”.\n\n\n\n\n\n\n\n\nIn data space, joint confidence intervals for the mean vector or joint prediction regions for the data are given by the ellipsoids \\((\\bar{x}_1, \\bar{x}_2)^\\mathsf{T} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}}\\), where the covariance matrix \\(\\mathbf{S}_{\\mathbf{X}}\\) depends on \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) (\\(\\oplus\\) here shifts the ellipsoid to one centered at \\((\\bar{x}_1, \\bar{x}_2)\\) here, as in Equation 3.2).\nIn the dual \\(\\mathbf{\\beta}\\) space, joint confidence regions for the coefficients of a response variable \\(y\\) on \\((x_1, x_2)\\) are given by ellipsoids of the form \\(\\widehat{\\mathbf{\\beta}} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}^{-1}}\\), and depend on \\(\\mathbf{(\\mathbf{X}^\\mathsf{T}\\mathbf{X})}^{-1}\\).\n\n\n\n\n\n\n\n7.1.1 Coffee, stress and heart disease\nConsider the dataset coffee, giving measures of Heart (\\(y\\)), an index of cardiac damage, Coffee (\\(x_1\\)), a measure of daily coffee consumption, and Stress (\\(x_2\\)), a measure of occupational stress, in a contrived sample of \\(n=20\\) university people.1 For the sake of the example we assume that the main goal is to determine whether or not coffee is good or bad for your heart, and stress represents one potential confounding variable among others (age, smoking, etc.) that might be useful to control statistically.\n\nset.seed(1234)\ndata(coffee, package=\"matlib\")\ncoffee |&gt; dplyr::sample_n(6)\n#&gt;          Group Coffee Stress Heart\n#&gt; 1 Grad_Student    104    117    92\n#&gt; 2      Student     52     86    63\n#&gt; 3 Grad_Student     76     92    58\n#&gt; 4      Student    100    123    92\n#&gt; 5      Student     64     74    63\n#&gt; 6    Professor    141    175   145\n\nFigure 7.3 shows the scatterplot matrix, giving the marginal relations between all pairs of variables. The marginal message seems to be that coffee is bad for your heart, stress is bad for your heart and coffee consumption is also related to occupational stress.\n\nShow the codescatterplotMatrix(~ Heart + Coffee + Stress, data=coffee,\n    smooth = FALSE,\n    pch = 16, col = \"brown\",\n    cex.axis = 1.3, cex.labels = 3,\n    ellipse = list(levels = 0.68, fill.alpha = 0.1))\n\n\n\n\n\n\nFigure 7.3: Scatterplot matrix for the coffee data showing the pairwise relationships among Heart damage (\\(y\\)), Coffee consumption (\\(x_1\\)), and Stress (\\(x_2\\)), with linear regression lines and 68% data ellipses.\n\n\n\n\nYet, when we fit both variables together, we obtain the following results, suggesting that coffee is good for you—the coefficient for coffee is now negative, though non-significant. How can this be?\n\ncoffee.mod &lt;- lm(Heart ~ Coffee + Stress, data=coffee)\nbroom::tidy(coffee.mod)\n#&gt; # A tibble: 3 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)   -7.79      5.79      -1.35 0.196    \n#&gt; 2 Coffee        -0.409     0.292     -1.40 0.179    \n#&gt; 3 Stress         1.20      0.224      5.34 0.0000536\n\nThe answer is that the marginal plots of Heart vs. Coffee and Stress in the first row of Figure 7.3 each ignore the the other predictor. In contrast, the coefficients for coffee and stress in the multiple regression model coffee.mod are partial coefficients, giving the estimated change in heart damage for a unit change in each predictor, but adjusting for (controlling for, or holding constant) the other predictor.\nWe can see these effects directly in added variable plots (Section 6.4), but here I consider the relationship of coffee and stress in data space and beta space and how their ellipses relate to each other and to hypothesis tests.\nThe left panel in Figure 7.4 is the same as that in the (3,2) cell of Figure 7.3 for the relation Stress ~ Coffee but with data ellipses of 40% and 60% coverage. The shadows of the 40% ellipse on any axis give univariate intervals of the mean \\(\\bar{x} \\pm 1 s_x\\) (standard deviation) shown by the thick red lines; the shadow of the 68% ellipse corresponds to an interval \\(\\bar{x} \\pm 1.5 s_x\\).\nThe right panel shows the joint 95% confidence region for the coefficients \\((\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}})\\) and individual confidence intervals in \\(\\mathbf{\\beta}\\) space. These are determined as\n\\[\n\\widehat{\\mathbf{\\beta}} \\oplus \\sqrt{d F^{.95}_{d, \\nu}} \\times s_e \\times \\mathbf{S}_X^{-1/2} \\:\\: .\n\\] where \\(d\\) is the number of dimensions for which we want coverage, \\(\\nu\\) is the residual degrees of freedom for \\(s_e\\), and \\(\\mathbf{S}_X\\) is the covariance matrix of the predictors.\n\n\n\n\n\n\n\nFigure 7.4: Data space and \\(\\mathbf{\\beta}\\) space representations of Coffee and Stress. Left: 40% and 68% data ellipses. Right: Joint 95% confidence ellipse (blue) for (\\(\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}}\\)), confidence interval generating ellipse (red) with 95% univariate shadows. \\(H_0\\) marks the joint hypothesis that both coefficients equal zero.\n\n\n\n\nThus, the blue ellipse in Figure 7.4 (right) is the ellipse of joint 95% coverage, using the factor \\(\\sqrt{2 F^{.95}_{2, \\nu}}\\), which covers the true values of (\\(\\beta_{\\mathrm{Stress}}, \\beta_{\\mathrm{Coffee}}\\)) in 95% of samples. Moreover:\n\nAny joint hypothesis (e.g., \\(\\mathcal{H}_0:\\beta_{\\mathrm{Stress}}=0, \\beta_{\\mathrm{Coffee}}=0\\)) can be tested visually, simply by observing whether the hypothesized point, \\((0, 0)\\) here, lies inside or outside the joint confidence ellipse. That hypothesis is rejected\nThe shadows of this ellipse on the horizontal and vertical axes give Scheff'e joint 95% confidence intervals for the parameters, with protection for simultaneous inference (“fishing”) in a 2-dimensional space.\nSimilarly, using the factor \\(\\sqrt{F^{1-\\alpha/d}_{1, \\nu}} = t^{1-\\alpha/2d}_\\nu\\) would give an ellipse whose 1D shadows are \\(1-\\alpha\\) Bonferroni confidence intervals for \\(d\\) posterior hypotheses.\n\nVisual hypothesis tests and \\(d=1\\) confidence intervals for the parameters separately are obtained from the red ellipse in Figure 7.4, which is scaled by \\(\\sqrt{F^{.95}_{1, \\nu}} = t^{.975}_\\nu\\). We call this the confidence-interval generating ellipse (or, more compactly, the “confidence-interval ellipse”). The shadows of the confidence-interval ellipse on the axes (thick red lines) give the corresponding individual 95% confidence intervals, which are equivalent to the (partial, Type III) \\(t\\)-tests for each coefficient given in the standard multiple regression output shown above.\nThus, controlling for Stress, the confidence interval for the slope for Coffee includes 0, so we cannot reject the hypothesis that \\(\\beta_{\\mathrm{Coffee}}=0\\) in the multiple regression model, as we saw above in the numerical output. On the other hand, the interval for the slope for Stress excludes the origin, so we reject the null hypothesis that \\(\\beta_{\\mathrm{Stress}}=0\\), controlling for Coffee consumption.\nFinally, consider the relationship between the data ellipse and the confidence ellipse. These have exactly the same shape, but (with equal coordinate scaling of the axes), the confidence ellipse is exactly a \\(90^o\\) rotation and rescaling of the data ellipse. In directions in data space where the slice of the data ellipse is wide—where we have more information about the relationship between Coffee and Stress—the projection of the confidence ellipse is narrow, reflecting greater precision of the estimates of coefficients. Conversely, where slice of the the data ellipse is narrow (less information), the projection of the confidence ellipse is wide (less precision).\nConfidence ellipses are drawn using car::confidenceEllipse(). Click the button to show the code.\n\nCode for confidence ellipsesconfidenceEllipse(coffee.mod, \n    grid = FALSE,\n    xlim = c(-2, 1), ylim = c(-0.5, 2.5),\n    xlab = expression(paste(\"Coffee coefficient,  \", beta[\"Coffee\"])),\n    ylab = expression(paste(\"Stress coefficient,  \", beta[\"Stress\"])),\n    cex.lab = 1.5)\nconfidenceEllipse(coffee.mod, add=TRUE, draw = TRUE,\n    col = \"red\", fill = TRUE, fill.alpha = 0.1,\n    dfn = 1)\nabline(h = 0, v = 0, lwd = 2)\n\n# confidence intervals\nbeta &lt;- coef( coffee.mod )[-1]\nCI &lt;- confint(coffee.mod)\nlines( y = c(0,0), x = CI[\"Coffee\",] , lwd = 6, col = 'red')\nlines( x = c(0,0), y = CI[\"Stress\",] , lwd = 6, col = 'red')\npoints( diag( beta ), col = 'black', pch = 16, cex=1.8)\n\nabline(v = CI[\"Coffee\",], col = \"red\", lty = 2)\nabline(h = CI[\"Stress\",], col = \"red\", lty = 2)\n\ntext(-2.1, 2.35, \"Beta space\", cex=2, pos = 4)\narrows(beta[1], beta[2], beta[1], 0, angle=8, len=0.2)\narrows(beta[1], beta[2], 0, beta[2], angle=8, len=0.2)\n\ntext( -1.5, 1.85, \"df = 2\", col = 'blue', adj = 0, cex=1.2)\ntext( 0.2, .85, \"df = 1\", col = 'red', adj = 0, cex=1.2)\n\nheplots::mark.H0(col = \"darkgreen\", pch = \"+\", lty = 0, pos = 4, cex = 3)",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#sec-meas-error",
    "href": "07-lin-mod-topics.html#sec-meas-error",
    "title": "\n7  Topics in Linear Models\n",
    "section": "\n7.2 Measurement error",
    "text": "7.2 Measurement error\n\n7.2.1 OLS is BLUE\nIn classical linear models, the predictors are often considered to be fixed variables, or, if random, to be measured without error and independent of the regression errors. Either condition, along with the assumption of linearity, guarantees that the standard OLS estimators are unbiased. That is, in a simple linear regression, \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), the estimated slope \\(\\hat{\\beta}_1\\) will have an average, expected value \\(\\mathcal{E} (\\hat{\\beta}_1)\\) equal to the true population value \\(\\beta_1\\) over repeated samples.\nNot only this, but the Gauss-Markov theorem guarantees that the OLS estimator is also the most efficient because it has the least variance among all linear and unbiased estimators. The classical OLS estimator is said to be BLUE: Best (lowest variance), Linear (among linear estimators), Unbiased, Estimator.\n\n7.2.2 Errors in predictors\nErrors in the response \\(y\\) are accounted for in the model and measured by the mean squared error, \\(\\text{MSE} = \\hat{\\sigma}_\\epsilon^2\\). But in practice, of course, predictor variables are often also observed indicators, subject to their own error. Indeed, in the behavioral sciences it is rare that predictors are perfectly reliable and measured exactly. This fact that is recognized in errors-in-variables regression models (Fuller, 2006) and in more general structural equation models, but often ignored otherwise. Ellipsoids in data space and \\(\\beta\\) space are well suited to showing the effect of measurement error in predictors on OLS estimates.\nThe statistical facts are well known, though perhaps counter-intuitive in certain details: measurement error in a predictor biases regression coefficients (towards 0), while error in the measurement in \\(y\\) increases the MSE and thus standard errors of the regression coefficients but does not introduce bias in the coefficients.\n\n7.2.2.1 Example\nAn illuminating example can be constructed by starting with the simple linear regression\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\; ,\n\\] where \\(x_i\\) is the true, fully reliable predictor and \\(y\\) is the response, with error variance \\(\\sigma_\\epsilon^2\\). Now consider that we don’t measure \\(x_i\\) exactly, but instead observe \\(x^\\star_i\\).\n\\[\nx^\\star_i = x_i + \\eta_i \\; ,\n\\] where the measurement error \\(\\eta_i\\) is independent of the true \\(x_i\\) with variance \\(\\sigma^2_\\eta\\). We can extend this example to also consider the effect of adding additional, independent error variance to \\(y\\), so that instead of \\(y_i\\) we observe\n\\[\ny^\\star_i = y_i + \\nu_i\n\\] with variance \\(\\sigma^2_\\nu\\).\nLet’s simulate an example where the true relation is \\(y = 0.2 + 0.3 x\\) with error standard deviation \\(\\sigma = 0.5\\). I’ll take \\(x\\) to be uniformly distributed in [0, 10] and calculate \\(y\\) as normally distributed around that linear relation.\n\n\nset.seed(123)\nn &lt;- 300\n\na &lt;- 0.2    # true intercept\nb &lt;- 0.3    # true slope\nsigma &lt;- 0.5 # baseline error standard deviation\n\nx &lt;- runif(n, 0, 10)\ny &lt;- rnorm(n, a + b*x, sigma)\ndemo &lt;- data.frame(x,y)\n\nThen, generate alternative values \\(x^\\star\\) and \\(y^\\star\\) with additional error standard deviations around \\(x\\) given by \\(\\sigma_\\eta = 4\\) and around \\(y\\) given by \\(\\sigma_\\nu = 1\\).\n\nerr_y &lt;- 1   # additional error stdev for y\nerr_x &lt;- 4   # additional error stdev for x\ndemo  &lt;- demo |&gt;\n  mutate(y_star = rnorm(n, y, err_y),\n         x_star = rnorm(n, x, err_x))\n\nThere are four possible models we could fit and compare, using the combinations of \\((x, x^\\star)\\) and \\((y, y^\\star)\\)\n\nfit_1 &lt;- lm(y ~ x,           data = demo)   # no additional error\nfit_2 &lt;- lm(y_star ~ x,      data = demo)   # error in y\nfit_3 &lt;- lm(y ~ x_star,      data = demo)   # error in x\nfit_4 &lt;- lm(y_star ~ x_star, data = demo)   # error in x and y\n\nHowever, to show the differences visually, we can simply plot the data for each pair and show the regression lines (with confidence bands) and the data ellipses. To do this efficiently with ggplot2, it is necessary to transform the demo data to long format with columns x and y, distinguished by name for the four combinations.\n\n# make the demo dataset long, with names for the four conditions\ndf &lt;- bind_rows(\n  data.frame(x=demo$x,      y=demo$y,      name=\"No measurement error\"),\n  data.frame(x=demo$x,      y=demo$y_star, name=\"Measurement error on y\"),\n  data.frame(x=demo$x_star, y=demo$y,      name=\"Measurement error on x\"),\n  data.frame(x=demo$x_star, y=demo$y_star, name=\"Measurement error on x and y\")) |&gt;\n  mutate(name = fct_inorder(name)) \n\nThen, we can plot the data in df with points, regression lines and a data ellipse, faceting by name to give the measurement error quartet. \n\n\nggplot(df, aes(x, y)) +\n  geom_point(alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", \n               color = \"blue\",fill= \"blue\", \n               alpha=0.05, linewidth = 1.1) +\n  geom_smooth(method=\"lm\", formula = y~x, fullrange=TRUE, level=0.995,\n              color = \"red\", fill = \"red\", alpha = 0.2) +\n  facet_wrap(~name) \n\n\n\n\n\n\nFigure 7.5: The measurement error quartet: Each plot shows the linear regression of y on x, but where additional error variance has been added to y or x or both. The widths of the confidence bands and the vertical extent of the data ellipses show the effect on precision.\n\n\n\n\nComparing the plots in the first row, you can see that when additional error is added to \\(y\\), the regression slope remains essentially unchanged, illustrating that the estimate is unbiased. However, the confidence bounds on the regression line become wider, and the data ellipse becomes fatter in the \\(y\\) direction, illustrating the loss of precision.\nThe effect of error in \\(x\\) is less kind. Comparing the first row of plots with the second row, you can see that the estimated slope decreases when errors are added to \\(x\\). This is called attenuation bias, and it can be shown that \\[\n\\widehat{\\beta}_{x^\\star} \\longrightarrow \\frac{\\beta}{1+\\sigma^2_\\eta /\\sigma^2_x} \\; ,\n\\] where \\(\\beta\\) here refers to the regression slope and \\(\\longrightarrow\\) means “converges to”, as the sample size gets large. Thus, as \\(\\sigma^2_\\eta\\) increases, \\(\\widehat{\\beta}_{x^\\star}\\) becomes less than \\(\\beta\\).\nBeyond plots like Figure 7.5, we can see the effects of error in \\(x\\) or \\(y\\) on the model summary statistics such as the correlation \\(r_{xy}\\) or MSE by extracting these from the fitted models. This is easily done using dplyr::nest_by(name) and fitting the regression model to each subset, from which we can obtain the model statistics using sigma(), coef() and so forth. A bit of dplyr::mutate() magic is used to construct indicators errX and errY giving whether or not error was added to \\(x\\) and/or \\(y\\).\n\nmodel_stats &lt;- df |&gt;\n  dplyr::nest_by(name) |&gt;\n  mutate(model = list(lm(y ~ x, data = data)),\n         sigma = sigma(model),\n         intercept = coef(model)[1],\n         slope = coef(model)[2],\n         r = sqrt(summary(model)$r.squared)) |&gt;\n  mutate(errX = stringr::str_detect(name, \" x\"),\n         errY = stringr::str_detect(name, \" y\")) |&gt;\n  mutate(errX = factor(errX, levels = c(\"TRUE\", \"FALSE\")),\n         errY = factor(errY, levels = c(\"TRUE\", \"FALSE\"))) |&gt;\n  relocate(errX, errY, r, .after = name) |&gt;\n  select(-data) |&gt;\n  print()\n#&gt; # A tibble: 4 × 8\n#&gt; # Rowwise:  name\n#&gt;   name                errX  errY      r model sigma intercept  slope\n#&gt;   &lt;fct&gt;               &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lis&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 No measurement err… FALSE FALSE 0.858 &lt;lm&gt;  0.495    0.244  0.294 \n#&gt; 2 Measurement error … FALSE TRUE  0.648 &lt;lm&gt;  1.09     0.0838 0.329 \n#&gt; 3 Measurement error … TRUE  FALSE 0.481 &lt;lm&gt;  0.844    1.22   0.0946\n#&gt; 4 Measurement error … TRUE  TRUE  0.401 &lt;lm&gt;  1.31     1.12   0.117\n\nWe plot the model \\(R = r_{xy}\\) and the estimated residual standard error in Figure 7.6 below. The lines connecting the points are approximately parallel, indicating that errors of measurement in \\(x\\) and \\(y\\) have nearly additive effects on model summaries.\n\n\np1 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = r, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model R \",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") +\n  legend_inside(c(0.27, 0.8))\n\np2 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = sigma, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model residual standard error\",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\nFigure 7.6: Model statistics for the combinations of additional error variance in x or y or both. Left: model R; right: Residual standard error.\n\n\n\n\n\n7.2.3 Coffee data: \\(\\beta\\) space\nIn multiple regression the effects of measurement error in a predictor become more complex, because error variance in one predictor, \\(x_1\\), say, can affect the coefficients of other terms in the model.\nConsider the marginal relation between Heart disease and Stress in the coffee data. Figure 7.7 shows this with data ellipses in data space and the corresponding confidence ellipses in \\(\\beta\\) space. Each panel starts with the observed data (the darkest ellipse, marked \\(0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0.75, 1.0, 1.5\\}\\), to the value of Stress, while keeping the mean of Stress the same. All of the data ellipses have the same vertical shadows (\\(\\text{SD}_{\\textrm{Heart}}\\)), while the horizontal shadows increase with \\(\\delta\\), driving the slope for Stress toward 0.\nIn \\(\\beta\\) space, it can be seen that the estimated coefficients, \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) vary along a line and approach \\(\\beta_{\\textrm{Stress}}=0\\) as \\(\\delta\\) gets sufficiently large. The shadows of ellipses for \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) along the \\(\\beta_{\\textrm{Stress}}\\) axis also demonstrate the effects of measurement error on the standard error of \\(\\beta_{\\textrm{Stress}}\\).\n\n\n\n\n\n\n\nFigure 7.7: Effects of measurement error in Stress on the marginal relationship between Heart disease and Stress. Each panel starts with the observed data (\\(\\delta = 0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\text{SD}_\\text{Stress})\\) with standard deviations multiplied by \\(\\delta\\) = 0.75, 1.0, 1.5, to the value of Stress. Increasing measurement error biases the slope for Stress toward 0. Left: 50% data ellipses; right: 50% confidence ellipses.\n\n\n\n\nPerhaps less well-known, but both more surprising and interesting, is the effect that measurement error in one variable, \\(x_1\\), has on the estimate of the coefficient for an other variable, \\(x_2\\), in a multiple regression model. Figure 7.8 shows the confidence ellipses for \\((\\beta_{\\textrm{Coffee}}, \\beta_{\\textrm{Stress}})\\) in the multiple regression predicting Heart disease, adding random normal error \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0, 0.2, 0.4, 0.8\\}\\), to the value of Stress alone.\nAs can be plainly seen, while this measurement error in Stress attenuates its coefficient, it also has the effect of biasing the coefficient for Coffee toward that in the marginal regression of Heart disease on Coffee alone.\n\n\n\n\n\n\n\nFigure 7.8: Biasing effect of measurement error in one variable (Stress) on on the coefficient of another variable (Coffee) in a multiple regression. The coefficient for Coffee is driven towards its value in the marginal model using Coffee alone, as measurement error in Stress makes it less informative in the joint model.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#what-have-we-learned",
    "href": "07-lin-mod-topics.html#what-have-we-learned",
    "title": "\n7  Topics in Linear Models\n",
    "section": "\n7.3 What have we learned?",
    "text": "7.3 What have we learned?\n\nData space and \\(\\beta\\) space are dualities of each other - While we typically visualize regression models in data space (where points are observations), there’s a parallel \\(\\beta\\) space where points represent models and their coefficients. These spaces mirror each other in elegant ways: lines in one space become points in the other, and confidence ellipses in \\(\\beta\\) space are 90\\(^\\circ\\) rotations of data ellipses. This duality reveals that we gain precision in estimating coefficients precisely where our data spread the most.\nConfidence ellipses make hypothesis testing visual and intuitive - Instead of squinting at p-values in regression output, we can literally see whether hypotheses are supported by checking whether null hypothesis points fall inside or outside confidence ellipses. The shadows of these ellipses automatically give us individual confidence intervals, while the full ellipse captures joint uncertainty about multiple coefficients.\nMeasurement error in predictors is far more dangerous than measurement error in responses - While errors in your response variable (y) simply inflate standard errors without biasing coefficients, errors in predictors create attenuation bias that systematically pulls slope estimates toward zero. This “errors-in-variables” problem means that unreliable measurements of your predictors can make real effects appear weaker than they actually are.\nIn multiple regression, measurement error in one predictor contaminates estimates of other predictors - Perhaps most surprisingly, when one predictor in your model suffers from measurement error, it doesn’t just bias its own coefficient—it also distorts the coefficients of other variables in unpredictable ways. This distortion will cascade, meaning that measurement quality affects your entire model, not just individual variables.\nEllipses reveal the hidden geometry behind familiar statistical concepts - Data ellipses, confidence ellipses, and their mathematical relationships provide a geometric foundation for understanding correlation, regression coefficients, confidence intervals, and hypothesis tests. This visual approach transforms abstract statistical concepts into concrete geometric relationships that you can literally see and manipulate.\n\n\n\n\n\nDempster, A. P. (1969). Elements of continuous multivariate analysis. Addison-Wesley.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFuller, W. (2006). Measurement error models (2nd ed.). John Wiley & Sons.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#footnotes",
    "href": "07-lin-mod-topics.html#footnotes",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "This example was developed by Georges Monette.↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html",
    "href": "10-mlm-review.html",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "10.1 Structure of the MLM {sec-mlm-structure}\nChapter 9 introduced the essential ideas of multivariate analysis in the context of a two-group design using Hotelling’s \\(T^2\\). Through the magical power of multivariate thinking, I extend this here to a “Holy of Holies”, inner sanctuary of the Tabernacle, the awed general Multivariate Linear Model (MLM).1\nThis can be understood as a simple extension of the univariate linear model, with the main difference being that there are multiple response variables considered together, instead of just one, analysed alone. (Or, from my perspective, the univariate version is a restricted form of the MLM.) These outcomes might reflect several different ways or scales for measuring an underlying theoretical construct, or they might represent different aspects of some phenomenon that we hope to better understand when they are studied jointly.\nFor example, in the case of different measures, there are numerous psychological scales used to assess depression or anxiety and it may be important to include more than one measure to ensure that the construct has been measured adequately. It would add considerably to our understanding to know if the different outcome measures all had essentially the same relations to the predictor variables, or if they differ across measures.\nIn the second case of various aspects, student “aptitude” or “achievement” reflects competency in different various subjects (reading, math, history, science, …) that are better studied together. We get a better understanding of the factors that influence each of aspects by testing them jointly.\nJust as in univariate analysis there are variously named techniques (ANOVA, regression) that can be applied to several outcomes, depending on the structure of the predictors at hand. For instance, with one or more continuous predictors and multiple response variables, you could use multivariate multiple regression (MMRA) to obtain estimates useful for prediction.\nInstead, if the predictors are categorical factors, multivariate analysis of variance (MANOVA) can be applied to test for differences between groups. Again, this is akin to ANOVA in the univariate context— the same underlying model is utilized, but the tests for terms in the model are multivariate ones for the collection of all response variables, rather than univariate ones for a single response.\nThe main goal of this chapter is to describe the details of the extension of univariate linear models to the case of multiple outcome measures. But the larger goal is to set the stage for the visualization methods using HE plots and low-D views discussed separately in Chapter 11. Some of the example datasets used here will re-appear there, and also in Chapter 12 which concerns some model-diagnostic graphical methods.\nHowever, before considering the details and examples that apply separately to MANOVA and MMRA, it is useful to consider the general features of the multivariate linear model of which these cases are examples.\nPackages\nIn this chapter I use the following packages. Load them now:\nWith \\(p\\) response variables, the multivariate linear model is most easily appreciated as the collection of \\(p\\) linear models, one for each response. We have \\(p\\) outcomes, so why not just consider a separate model for each?\n\\[\\begin{aligned}\n\\mathbf{y}_1 =& \\mathbf{X}\\boldsymbol{\\beta}_1 + \\boldsymbol{\\epsilon}_1 \\\\\n\\mathbf{y}_2 =& \\mathbf{X}\\boldsymbol{\\beta}_2 + \\boldsymbol{\\epsilon}_2 \\\\\n  \\vdots      &  \\\\\n\\mathbf{y}_p =& \\mathbf{X}\\boldsymbol{\\beta}_p + \\boldsymbol{\\epsilon}_p \\\\\n\\end{aligned} \\tag{10.1}\\]\nBut the problems with fitting separate univariate models are that:\nThe model matrix \\(\\mathbf{X}\\) in Equation 10.1 is the same for all responses, but each one gets its own vector \\(\\boldsymbol{\\beta}_j\\) of coefficients for how the predictors in \\(\\mathbf{X}\\) fit a given response \\(\\mathbf{y}_j\\).\nAmong the beauties of multivariate thinking is that we can put these separate equations together in single equation by joining the responses \\(\\mathbf{y}_j\\) as columns in a matrix \\(\\mathbf{Y}\\) and similarly arranging the vectors of coefficients \\(\\boldsymbol{\\beta}_j\\) as columns in a matrix \\(\\mathbf{B}\\).2\nTODO: Revise notation here, to be explicit and consistent about inclusion of \\(\\boldsymbol{\\beta}_0\\) and size of \\(\\mathbf{B}\\).\nThe MLM then becomes:\n\\[\n\\mathord{\\mathop{\\mathbf{Y}}\\limits_{n \\times p}} =\n\\mathord{\\mathop{\\mathbf{X}}\\limits_{n \\times (q+1)}} \\, \\mathord{\\mathop{\\mathbf{B}}\\limits_{(q+1) \\times p}} + \\mathord{\\mathop{\\mathbf{\\boldsymbol{\\Large\\varepsilon}}}\\limits_{n \\times p}} \\:\\: ,\n\\tag{10.2}\\]\nwhere:\nWriting Equation 10.2 in terms of its elements, we have\n\\[\\begin{aligned}\n\\overset{\\mathbf{Y}}\n  {\\begin{bmatrix}\n  y_{11} & y_{12} & \\cdots & y_{1p} \\\\\n  y_{21} & y_{22} & \\cdots & y_{2p} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  y_{n1} & y_{n2} & \\cdots & y_{np}\n  \\end{bmatrix}\n  }\n& =\n\\overset{\\mathbf{X}}\n  {\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1q} \\\\\n  1 & x_{21} & \\cdots & x_{2q} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  1 & x_{n1} & \\cdots & x_{nq}\n  \\end{bmatrix}\n  }\n\\overset{\\mathbf{B}}\n  {\\begin{bmatrix}\n  \\beta_{01} & \\beta_{02} & \\cdots & \\beta_{0p} \\\\\n  \\beta_{11} & \\beta_{12} & \\cdots & \\beta_{1p} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\beta_{q1} & \\beta_{q2} & \\cdots & \\beta_{qp}\n  \\end{bmatrix}\n  } \\\\\n& + \\quad\\quad\n\\overset{\\mathcal{\\boldsymbol{\\Large\\varepsilon}}}\n  {\\begin{bmatrix}\n  \\epsilon_{11} & \\epsilon_{12} & \\cdots & \\epsilon_{1p} \\\\\n  \\epsilon_{21} & \\epsilon_{22} & \\cdots & \\epsilon_{2p} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\epsilon_{n1} & \\epsilon_{n2} & \\cdots & \\epsilon_{np}\n  \\end{bmatrix}\n  }\n\\end{aligned}\\]\nThe structure of the model matrix \\(\\mathbf{X}\\) is exactly the same as the univariate linear model, and may therefore contain,",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#structure-of-the-mlm-sec-mlm-structure",
    "href": "10-mlm-review.html#structure-of-the-mlm-sec-mlm-structure",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "They don’t give simultaneous tests for all regressions. The situation is similar to that in a one-way ANOVA, where an overall test for group differences is usually applied before testing individual comparisons to avoid problems of multiple testing: \\(g\\) groups gives \\(g \\times (g-1)/2\\) pairwise tests.\n\nMore importantly, fitting separate univariate models does not take correlations among the \\(\\mathbf{y}\\)s into account.\n\nIt might be the case that the response variables are all essentially measuring the same thing, but perhaps weakly. If so, the multivariate approach can pool strength across the outcomes to detect their common relations to the predictors, giving greater power.\nOn the other hand, perhaps the responses are related in different ways to the predictors. A multivariate approach can help you understand how many different ways there are, and characterize each.\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{Y} = (\\mathbf{y}_1 , \\mathbf{y}_2, \\dots ,  \\mathbf{y}_p )\\) is the matrix of \\(n\\) observations on \\(p\\) responses, with typical column \\(\\mathbf{y}_j\\);\n\n\\(\\mathbf{X}\\) is the model matrix with columns \\(\\mathbf{x}_i\\) for \\(q\\) regressors, which typically includes an initial column \\(\\mathbf{x}_0\\) of 1s for the intercept;\n\n\\(\\mathbf{B} = ( \\mathbf{b}_1 , \\mathbf{b}_2 , \\dots,  \\mathbf{b}_p )\\) is a matrix of regression coefficients, one column \\(\\mathbf{b}_j\\) for each response variable;\n\n\\(\\boldsymbol{\\Large\\varepsilon}\\) is a matrix of errors in predicting \\(\\mathbf{Y}\\).\n\n\n\n\n\n\nquantitative predictors, such as age, income, years of education\n\n\ntransformed predictors like \\(\\sqrt{\\text{age}}\\) or \\(\\log{(\\text{income})}\\)\n\n\npolynomial terms: \\(\\text{age}^2\\), \\(\\text{age}^3, \\dots\\) (using poly(age, k) in R)\n\ncategorical predictors (“factors”), such as treatment (Control, Drug A, drug B), or sex; internally a factor with k levels is transformed to k-1 dummy (0, 1) variables, representing comparisons with a reference level, typically the first.\n\ninteraction terms, involving either quantitative or categorical predictors, e.g., age * sex, treatment * sex.\n\n\n10.1.1 Assumptions\nJust as in univariate models, the assumptions of the multivariate linear model almost entirely concern the behavior of the errors (residuals). Let \\(\\mathbf{\\epsilon}_{i}^{\\prime}\\) represent the \\(i\\)th row of \\(\\boldsymbol{\\Large\\varepsilon}\\). Then it is assumed that:\n\n\nNormality: The residuals, \\(\\mathbf{\\epsilon}_{i}^{\\prime}\\) are distributed as multivariate normal, \\(\\mathcal{N}_{p}(\\mathbf{0},\\boldsymbol{\\Sigma})\\), where \\(\\mathbf{\\Sigma}\\) is a non-singular error-covariance matrix.\n\nStatistical tests of multivariate normality of the residuals include the Shapiro-Wilk (Shapiro & Wilk, 1965) and Mardia (1970, 1974) tests (and others, in the MVN package package). \nAs in univariate models, the MLM is relatively robust against non-normality. However this is often better assessed visually using a \\(\\chi^2\\) QQ plot of Mahalanobis squared distance against their corresponding \\(\\chi^2_p\\) values for \\(p\\) degrees of freedom using heplots::cqplot().\n\n\nHomoscedasticity: The error-covariance matrix \\(\\mathbf{\\Sigma}\\) is constant across all observations and grouping factors. Graphical methods to show if this assumption is met are illustrated in Chapter 12.\nIndependence: \\(\\mathbf{\\epsilon}_{i}^{\\prime}\\) and \\(\\mathbf{\\epsilon}_{j}^{\\prime}\\) are independent for \\(i\\neq j\\), so knowing the data for case \\(i\\) gives no information about case \\(j\\). This assumption would be violated if the observations consisted of pairs of husbands and wives, or clusters of students from different schools and this grouping was ignored in the model.\nError-free \\(\\mathbf{X}\\): The predictors, \\(\\mathbf{X}\\), are fixed and measured without error. The discussion in Section 7.2 illustrates how measurement error biases (toward 0) the estimated effects of predictors. A weaker form, called exogeneity merely asserts that the predictors are independent of (uncorrelated with) the errors, \\(\\boldsymbol{\\Large\\varepsilon}\\). 3\n\nThese statements are simply the multivariate analogs of the assumptions of normality, constant variance and independence of the errors in univariate models. Note that it is unnecessary to assume that the predictors (regressors, columns of \\(\\mathbf{X}\\)) are normally distributed.\nImplicit in the above is perhaps the most important assumption—that the model has been correctly specified. This means:\n\nLinearity: The form of the relations between each \\(\\mathbf{y}\\) and the \\(\\mathbf{x}\\)s is correct. Typically this means that the relations are linear, but if not, we have specified a correct transformation of \\(\\mathbf{y}\\) and/or \\(\\mathbf{x}\\), such as modeling \\(\\log(\\mathbf{y})\\), or using polynomial terms in one or more of the \\(\\mathbf{x}\\)s.\nCompleteness: No relevant predictors have been omitted from the model. For example in the coffee, stress example (Section 7.1.1), omitting stress from the model biases the effect of coffee on heart disease.\nAdditive effects: The combined effect of different terms in the model is the sum of their individual effects, so the impact of one predictor on the outcome variable is the same regardless of the values of other predictors in the model. When this is not the case, you can add an interaction term like \\(x_1 \\times x_2\\) to account for this.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#sec-mlm-fitting",
    "href": "10-mlm-review.html#sec-mlm-fitting",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.2 Fitting the model",
    "text": "10.2 Fitting the model\nThe least squares (and also maximum likelihood) solution for the coefficients \\(\\mathbf{B}\\) is given by\n\n\n\n\\[\n\\widehat{\\mathbf{B}} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{Y} \\:\\: .\n\\]\nThe coefficients are precisely the same as fitting the separate responses \\(\\mathbf{y}_1 , \\mathbf{y}_2 , \\dots , \\mathbf{y}_p\\), and placing the estimated coefficients \\(\\widehat{\\mathbf{b}}_i\\) as columns in \\(\\widehat{\\mathbf{B}}\\)\n\\[\n\\widehat{\\mathbf{B}} = [ \\widehat{\\mathbf{b}}_1, \\widehat{\\mathbf{b}}_2, \\dots , \\widehat{\\mathbf{b}}_p] \\:\\: .\n\\] In R, we fit the multivariate linear model with lm() simply by giving a collection of response variables y1, y2, ... on the left-hand side of the model formula, wrapped in cbind() which combines them to form a matrix response.\n\nlm(cbind(y1, y2, y3) ~ x1 + x2 + ..., data=)\n\nIn the presence of possible outliers, robust methods are available for univariate linear models (e.g., MASS::rlm()). So too, heplots::robmlm() provides robust estimation in the multivariate case as illustrated in Section 13.5.\n\n10.2.1 Example: Dog food data\nAs a toy example to make these ideas concrete, consider the dataset dogfood. Here, a dogfood manufacturer wanted to study preference for different dogfood formulas, two of their own (“Old”, “New”) and two from other manufacturers (“Major”, “Alps”).\nIn a between-dog design, each of \\(n=4\\) dogs were presented with a bowl of one formula and the time to start eating and amount eaten were recorded. Greater preference would be seen in a shorter delay to start eating and a greater amount, so these responses are expected to be negatively correlated.\n\ndata(dogfood, package = \"heplots\")\nstr(dogfood)\n#&gt; 'data.frame':  16 obs. of  3 variables:\n#&gt;  $ formula: Factor w/ 4 levels \"Old\",\"New\",\"Major\",..: 1 1 1 1 2 2 2 2 3 3 ...\n#&gt;  $ start  : int  0 1 1 0 0 1 2 3 1 5 ...\n#&gt;  $ amount : int  100 97 88 92 95 85 82 89 77 84 ...\n\nFor this data, boxplots for the two responses provide an initial look, shown in Figure 10.1. Putting these side-by-side makes it easy to see the inverse relation between the medians on the two response variables.\n\nCodedog_long &lt;- dogfood |&gt;\n  pivot_longer(c(start, amount),\n               names_to = \"variable\")\nggplot(data = dog_long, \n       aes(x=formula, y = value, fill = formula)) +\n  geom_boxplot(alpha = 0.2) +\n  geom_point(size = 2.5) +\n  facet_wrap(~ variable, scales = \"free\") +\n  theme_bw(base_size = 14) + \n  theme(legend.position=\"none\")\n\n\n\n\n\n\nFigure 10.1: Boxplots for time to start eating and amount eaten by dogs given one of four dogfood formulas.\n\n\n\n\nAs suggested above, the multivariate model for testing mean differences due to the dogfood formula is fit using lm() on the matrix \\(\\mathbf{Y}\\) constructed with cbind(start, amount).\n\ndogfood.mod &lt;- lm(cbind(start, amount) ~ formula, \n                  data=dogfood) |&gt; \n  print()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cbind(start, amount) ~ formula, data = dogfood)\n#&gt; \n#&gt; Coefficients:\n#&gt;               start   amount\n#&gt; (Intercept)     0.50   94.25\n#&gt; formulaNew      1.00   -6.50\n#&gt; formulaMajor    2.00  -12.25\n#&gt; formulaAlps     1.75  -16.00\n\nBy default, the factor formula is represented by three columns in the \\(\\mathbf{X}\\) matrix that correspond to treatment contrasts, which are comparisons of the Old formula (a baseline level) with each of the others. The coefficients, for example formulaNEW, are the difference in means from those for Old.\nThen, the overall multivariate test that means on both variables do not differ is carried out using car::Anova().\n\ndogfood.aov &lt;- Anova(dogfood.mod) |&gt;\n  print()\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;         Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; formula  3     0.702     2.16      6     24  0.083 .\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe details of these analysis steps are explained below.\n\n10.2.2 Sums of squares\nIn univariate response models, statistical tests and model summaries (like \\(R^2\\)) are based on the familiar decomposition of the total sum of squares \\(SS_T\\) into regression or hypothesis (\\(SS_H\\)) and error (\\(SS_E\\)) sums of squares. In the multivariate linear model each of these becomes a \\(p \\times p\\) matrix \\(SSP\\) containing sums of squares for the \\(p\\) responses on the diagonal and sums of cross products in the off-diagonal. elements. For the MLM this is expressed as:\n\\[\\begin{aligned}\n\\underset{(p\\times p)}{\\mathbf{SSP}_{T}}\n& =  \\mathbf{Y}^{\\top} \\mathbf{Y} - n \\overline{\\mathbf{y}}\\,\\overline{\\mathbf{y}}^{\\top} \\\\\n& =  \\left(\\widehat {\\mathbf{Y}}^{\\top}\\widehat{\\mathbf{Y}} - n\\overline{\\mathbf{y}}\\,\\overline{\\mathbf{y}}^{\\top} \\right) + \\widehat{\\boldsymbol{\\Large\\varepsilon}}^{\\top}\\widehat{\\boldsymbol{\\Large\\varepsilon}} \\\\\n& =   \\mathbf{SSP}_{H} + \\mathbf{SSP}_{E} \\\\\n& \\equiv  \\mathbf{H} + \\mathbf{E} \\:\\: ,\n\\end{aligned} \\tag{10.3}\\]\nwhere,\n\n\n\\(\\overline{\\mathbf{y}}\\) is the \\((p\\times 1)\\) vector of means for the response variables;\n\n\\(\\widehat{\\mathbf{Y}} = \\mathbf{X}\\widehat{\\mathbf{B}}\\) is the matrix of fitted values; and\n\n\\(\\widehat{\\boldsymbol{\\Large\\varepsilon}} = \\mathbf{Y} -\\widehat{\\mathbf{Y}}\\) is the matrix of residuals.\n\nWe can visualize this decomposition in the simple case of a two-group design (for the mathscore data in Section 9.2) as shown in Figure 10.2. Let \\(\\mathbf{y}_{ij}\\) be the vector of \\(p\\) responses for subject \\(j\\) in group \\(i, i=1,\\dots g\\) for \\(j = 1, \\dots n_i\\). Then, using \\(.\\) to represent a subscript averaged over, Equation 10.3 comes from the identity\n\\[\n\\underbrace{(\\mathbf{y}_{ij} - \\mathbf{y}_{\\cdot \\cdot})}_T =\n\\underbrace{(\\overline{\\mathbf{y}}_{i \\cdot} - \\mathbf{y}_{\\cdot \\cdot})}_H +\n\\underbrace{(\\mathbf{y}_{ij} - \\overline{\\mathbf{y}}_{i \\cdot})}_E\n\\tag{10.4}\\]\nwhere each side of Equation 10.4 is squared and summed over observations to give Equation 10.3. In Figure 10.2,\n\nThe total variance \\(\\mathbf{SSP}_T\\) reflects the deviations of the observations \\(\\mathbf{y}_{ij}\\) from the grand mean \\(\\overline{\\mathbf{y}}_{. .}\\) and has the data ellipse shown in gray.\nIn the middle \\(\\mathbf{SSP}_H\\) panel, all the observations are represented at their group means, \\(\\overline{\\mathbf{y}}_{i .}\\), the fitted values. Their variance and covariance is then reflected by deviations of the group means (weighted for the number of observations per group) around the grand mean.\nThe right \\(\\mathbf{SSP}_E\\) panel then shows the residual variance, which is the variation of the observations \\(\\mathbf{y}_{ij}\\) around their group means, \\(\\overline{\\mathbf{y}}_{i .}\\). Centering the two data ellipses at the centroid \\(\\overline{\\mathbf{y}}_{. .}\\) then gives the ellipse for the \\(\\mathbf{SSP}_E\\), also called the pooled within-group covariance matrix.\n\n\n\n\n\n\n\n\nFigure 10.2: Breakdown of the total \\(\\mathbf{SSP}_{T}\\) into sums of squares and products for between-group hypothesis variance (\\(\\mathbf{SSP}_{H}\\)) and within-group, error variance (\\(\\mathbf{SSP}_{E}\\)).\n\n\n\n\nThe formulas for these sum of squares and products matrices can be shown explicitly as follows, where the notation \\(\\mathbf{z} \\mathbf{z}^\\top\\) generates the \\(p \\times p\\) outer product of a vector \\(\\mathbf{z}\\), giving \\(z_k \\times z_\\ell\\) for all pairs of elements. \\[\n\\mathbf{SSP}_T =\n\\sum_{i=1}^{g} \\sum_{j=1}^{n_{i}}\\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)\\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)^{\\top}\n\\tag{10.5}\\]\n\\[\n\\mathbf{SSP}_H =\n\\sum_{i=1}^{g} \\mathbf{n}_{i}\\left(\\overline{\\mathbf{y}}_{i \\cdot}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)\\left(\\overline{\\mathbf{y}}_{i \\cdot}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)^{\\top}\n\\tag{10.6}\\]\n\\[\n\\mathbf{SSP}_E =\n\\sum_{i=1}^{g} \\sum_{j=1}^{n_{i}} \\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{i \\cdot}\\right) \\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{i \\cdot}\\right)^{\\top}\n\\tag{10.7}\\]\nThis is the decomposition that we visualize in HE plots, where the size and direction of \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) can be represented as ellipsoids.\nBut first, let’s find these results for the example. The easy way is to get them from the result returned by car::Anova(), where the hypothesis \\(\\mathbf{SSP}_{H}\\) for each term in the model is returned as an element in a named list SSP and the error \\(\\mathbf{SSP}_{E}\\) is returned as the matrix SSPE.\n\nSSP_H &lt;- dogfood.aov$SSP |&gt; print()\n#&gt; $formula\n#&gt;         start amount\n#&gt; start    9.69  -70.9\n#&gt; amount -70.94  585.7\n\nSSP_E &lt;- dogfood.aov$SSPE |&gt; print()\n#&gt;        start amount\n#&gt; start   25.8   11.8\n#&gt; amount  11.8  390.3\n\nYou can calculate these directly as shown below. sweep() is used to subtract the colMeans() from \\(\\mathbf{Y}\\) and \\(\\widehat{\\mathbf{Y}}\\) and crossprod() premultiplies a matrix by its’ transpose.\n\nY &lt;- dogfood[, c(\"start\", \"amount\")]\nYdev &lt;- sweep(Y, 2, colMeans(Y)) |&gt; as.matrix()\nSSP_T &lt;- crossprod(as.matrix(Ydev)) |&gt; print()\n#&gt;        start amount\n#&gt; start   35.4  -59.2\n#&gt; amount -59.2  975.9\n\nfitted &lt;- fitted(dogfood.mod)\nYfit &lt;- sweep(fitted, 2, colMeans(fitted)) |&gt; as.matrix()\nSSP_H &lt;- crossprod(Yfit) |&gt; print()\n#&gt;         start amount\n#&gt; start    9.69  -70.9\n#&gt; amount -70.94  585.7\n\nresiduals &lt;- residuals(dogfood.mod)\nSSP_E &lt;- crossprod(residuals) |&gt; print()\n#&gt;        start amount\n#&gt; start   25.8   11.8\n#&gt; amount  11.8  390.3\n\nThe decomposition of the total sum of squares and products in Equation 10.3 can be shown as:\n\\[\n\\overset{\\mathbf{SSP}_T}\n  {\\begin{pmatrix}\n   35.4 & -59.2 \\\\\n  -59.2 & 975.9 \\\\\n  \\end{pmatrix}}\n=\n\\overset{\\mathbf{SSP}_H}\n  {\\begin{pmatrix}\n    9.69 & -70.94 \\\\\n  -70.94 & 585.69 \\\\\n  \\end{pmatrix}}\n+\n\\overset{\\mathbf{SSP}_E}\n  {\\begin{pmatrix}\n   25.8 &  11.8 \\\\\n   11.8 & 390.3 \\\\\n  \\end{pmatrix}}\n\\] These numbers are the variances in the diagonal and covariance between start and amount in the off-diagonal, where the sign is important: In \\(\\text{SSP}_H\\) the the negative covariance reflects the fact that for these brands larger time to start eating is negatively related to the amount eaten. In \\(\\text{SSP}_E\\) the the covariance is slightly positive. This might reflect a mild hunger factor: for a given brand, dogs who lunge for their bowls sooner also eat more. But, they’re all good boys, right?\n\n10.2.3 How big is \\(SS_H\\) compared to \\(SS_E\\)?\nIn a univariate response model, \\(SS_H\\) and \\(SS_E\\) are both scalar numbers and the univariate \\(F\\) test statistic,\n\\[F = \\frac{\\text{SS}_H/\\text{df}_h}{\\text{SS}_E/\\text{df}_e} = \\frac{\\mathsf{Var}(H)}{\\mathsf{Var}(E)} \\:\\: ,\n\\tag{10.8}\\]\nassesses “how big” \\(\\text{SS}_H\\) is, relative to \\(\\text{SS}_E\\), the variance accounted for by a hypothesized model or model terms relative to error variance. The measure \\(R^2 = SS_H / (SS_H + SS_E) = SS_H / SS_T\\) gives the proportion of total variance accounted for by the model terms.\nIn the multivariate analog \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) are both \\(p \\times p\\) matrices, and \\(\\mathbf{H}\\) “divided by” \\(\\mathbf{E}\\) becomes \\(\\mathbf{H}\\mathbf{E}^{-1}\\). The answer, “how big” \\(\\text{SS}_H\\) is compared to \\(\\text{SS}_E\\) is expressed in terms of the \\(p\\) eigenvalues \\(\\lambda_i, i = 1, 2, \\dots p\\) of \\(\\mathbf{H}\\mathbf{E}^{-1}\\). These are the \\(p\\) values \\(\\lambda\\) which solve the determinant equation\n\\[\n\\text{det}{(\\mathbf{H}\\mathbf{E}^{-1} - \\lambda \\mathbf{I}}) = 0 \\:\\: .\n\\]\nThe solution also gives the \\(\\lambda_i\\) as the eigenvalues, with vectors \\(\\mathbf{v}_i\\) as the corresponding eigenvectors,\n\\[\n\\mathbf{H}\\mathbf{E}^{-1} \\; \\lambda_i = \\lambda_i \\mathbf{v}_i \\:\\: .\n\\tag{10.9}\\]\nThis can also be expressed in terms of the size of \\(\\mathbf{H}\\) relative to total variation \\((\\mathbf{H}+ \\mathbf{E})\\) as\n\\[\n\\mathbf{H}(\\mathbf{H}+\\mathbf{E})^{-1} \\; \\rho_i = \\rho_i \\mathbf{v}_i \\:\\: ,\n\\tag{10.10}\\]\nwhich has the same eigenvectors as Equation 10.9 and the eigenvalues are \\(\\rho_i = \\lambda_i / (1 + \\lambda_i)\\).\nHowever, when the hypothesized model terms have \\(\\text{df}_h\\) degrees of freedom (columns of the \\(\\mathbf{X}\\) matrix for that term), \\(\\mathbf{H}\\) is of rank \\(\\text{df}_h\\), so only \\(s=\\min(p, \\text{df}_h)\\) eigenvalues can be non-zero. For example, a test for a hypothesis about a single quantitative predictor \\(\\mathbf{x}\\), has \\(\\text{df}_h = 1\\) degree of freedom and \\(\\mathrm{rank} (\\mathbf{H}) = 1\\); for a factor with \\(g\\) groups, \\(\\text{df}_h = \\mathrm{rank} (\\mathbf{H}) = g-1\\).\nFor the dogfood data, we get the following results:\n\nHEinv &lt;- SSP_H %*% solve(SSP_E) |&gt; print()\n#&gt;         start amount\n#&gt; start   0.466 -0.196\n#&gt; amount -3.488  1.606\neig &lt;- eigen(HEinv)\neig$values\n#&gt; [1] 2.0396 0.0317\n\n# as proportions\neig$values / sum(eig$values)\n#&gt; [1] 0.9847 0.0153\n\nThe factor formula has four levels and therefore \\(\\text{df}_h = 3\\) degrees of freedom. But there are only \\(p = 2\\) responses, so there are \\(s=\\min(p, \\text{df}_h) = 2\\) non-zero eigenvalues (and corresponding eigenvectors). The eigenvalues tell us that 98.5% of the hypothesis variance due to formula can be accounted for by a single dimension.\n\n\n\n\n\n\n\n\nThe overall multivariate test for the model in Equation 10.2 is essentially a test of the hypothesis \\(\\mathcal{H}_0: \\mathbf{B} = 0\\) (excluding the row for the intercept). Equivalently, this is a test based on the incremental \\(\\mathbf{SSP}_{H}\\) for the hypothesized terms in the model—that is, the difference between the \\(\\mathbf{SSP}_{H}\\) for the full model and the null, intercept-only model. The same idea can be applied to test the difference between any pair of nested models—the added contribution of terms in a larger model relative to a smaller model containing a subset of terms.\nThe eigenvectors \\(\\mathbf{v}_i\\) in Equation 10.9 are also important. These are the weights for the variables in a linear combination \\(v_{i1} \\mathbf{y}_1 + v_{i2} \\mathbf{y}_2 + \\cdots + v_{ip} \\mathbf{y}_p\\) which produces the largest univariate \\(F\\) statistic for the \\(i\\)-th dimension. We exploit this in canonical discriminant analysis and the corresponding canonical HE plots (Section 11.7).\nThe eigenvectors of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) for the dogfood model are shown below:\n\nrownames(eig$vectors) &lt;- rownames(HEinv)\ncolnames(eig$vectors) &lt;- paste(\"Dim\", 1:2)\neig$vectors\n#&gt;         Dim 1  Dim 2\n#&gt; start   0.123 -0.411\n#&gt; amount -0.992 -0.911\n\nThe first column corresponds to the weighted sum \\(0.12 \\times\\text{start} - 0.99 \\times \\text{amount}\\), which as we saw above accounts for 95.5% of the differences in the group means.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#sec-multivar-tests",
    "href": "10-mlm-review.html#sec-multivar-tests",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.3 Multivariate test statistics",
    "text": "10.3 Multivariate test statistics\nIn the univariate case, the overall \\(F\\)-test of \\(\\mathcal{H}_0: \\boldsymbol{\\beta} = \\mathbf{0}\\) is the uniformly most powerful invariant test when the assumptions are met. There is nothing better. This is not the case in the MLM.\nThe reason is that when there are \\(p &gt; 1\\) response variables, and we are testing a hypothesis comprising \\(\\text{df}_h &gt;1\\) coefficients or degrees of freedom, there are \\(s &gt; 1\\) possible dimensions in which \\(\\mathbf{H}\\) can be large relative to \\(\\mathbf{E}\\). The size of each dimension is measured by the its’ eigenvalue \\(\\lambda_i\\). There are several test statistics that combine these into a single measure, shown in Table 10.1.\n\n\n\n\n\n\n\n\n\n\nTable 10.1: How test statistics for multivariate tests combine the size of dimensions of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) into a single measure.\n\n\n\n\nCriterion\nFormula\nPartial \\(\\eta^2\\)\n\n\n\n\n\nWilks’s \\(\\Lambda\\)\n\n\\(\\Lambda = \\prod^s_i \\frac{1}{1+\\lambda_i}\\)\n\\(\\eta^2 = 1-\\Lambda^{1/s}\\)\n\n\n\nPillai trace\n\\(V = \\sum^s_i \\frac{\\lambda_i}{1+\\lambda_i}\\)\n\\(\\eta^2 = \\frac{V}{s}\\)\n\n\n\nHotelling-Lawley trace\n\\(H = \\sum^s_i \\lambda_i\\)\n\\(\\eta^2 = \\frac{H}{H+s}\\)\n\n\n\nRoy maximum root\n\\(R = \\lambda_1\\)\n\\(\\eta^2 = \\frac{\\lambda_1}{1+\\lambda_1}\\)\n\n\n\n\n\n\n\n\nThese correspond to different kinds of “means” of the \\(\\lambda_i\\): geometric (Wilks), arithmetic (Pillai), harmonic (Hotelling-Lawley) and supremum (Roy). See Friendly et al. (2013) for the geometry behind these measures.\nEach of these statistics have different sampling distributions under the null hypothesis. But conveniently they can all be converted to \\(F\\) statistics. These conversions are exact when the hypothesis has \\(s \\le 2\\) degrees of freedom, and approximations otherwise.\n\n\n\n\n\n\nWhich multivariate test statistic is “best”?\n\n\n\nThe answer is that it depends on the structure of the size of dimensions of an effect reflected in the eigenvalues \\(\\lambda_i\\) and what possible violation of assumptions you want to protect against.\n\nSchatzoff (1966) compared the power and sensitivity of the four multivariate statistics and showed that Roy’s largest-latent root statistic was most sensitive when population centroids in MANOVA differed in a single dimension, but less sensitive when the group differences were more diffuse.\nIn addition to power to detect differences, Olson (1974) also examined the robustness of these tests against nonnormality and heterogeneity of covariance matrices. He found that Pillai’s trace was most robust. Under diffuse differences, Pillai’s trace had the greatest power, while Roy’s test was worse. Not surprisingly, when differences were largely one-dimensional, this ordering was reversed.\n\nThese differences among the test statistics disappear as your sample size grows large, because these tests become asymptotically equivalent. The default choice in car::Anova() of test.statistic = \"Pillai\" reflects the view that Pillai’s test is preferred over a wider range of circumstances. The summary() method for \"Anova.mlm\" objects permits the specification of more than one multivariate test statistic, and the default is to report all four.\n\n\nAs well, each test statistic has an analog of the \\(R^2\\)-like partial \\(\\eta^2\\) measure, giving the partial association accounted for by each term in the MLM. These reflect the proportion of total variation attributable to a given model term, partialling out (excluding) other factors from the total non-error variation. They can be used as a measure of effect size in a MLM and are calculated using heplots::etasq(). \n\n10.3.1 Testing contrasts and linear hypotheses\nEven more generally, these multivariate tests apply to every linear hypothesis concerning the coefficients in \\(\\mathbf{B}\\). Suppose we want to test the hypothesis that a subset of rows (predictors) and/or columns (responses) simultaneously have null effects. This can be expressed in the general linear test,\n\\[\n\\mathcal{H}_0 : \\mathbf{C}_{h \\times q} \\, \\mathbf{B}_{q \\times p} = \\mathbf{0}_{h \\times p} \\:\\: ,\n\\]\nwhere \\(\\mathbf{C}\\) is a full rank \\(h \\le q\\) hypothesis matrix of constants, that selects subsets or linear combinations (contrasts) of the coefficients in \\(\\mathbf{B}\\) to be tested in a \\(h\\) degree-of-freedom hypothesis.\nIn this case, the SSP matrix for the hypothesis has the form\n\\[\n\\mathbf{H}  =\n(\\mathbf{C} \\widehat{\\mathbf{B}})^\\mathsf{T}\\,\n[\\mathbf{C} (\\mathbf{X}^\\mathsf{T}\\mathbf{X} )^{-1} \\mathbf{C}^\\mathsf{T}]^{-1} \\,\n(\\mathbf{C} \\widehat{\\mathbf{B}}) \\:\\: ,\n\\tag{10.11}\\]\nwhere there are \\(s = \\min(h, p)\\) non-zero eigenvalues of \\(\\mathbf{H}\\mathbf{E}^{-1}\\). In Equation 10.11, \\(\\mathbf{H}\\) measures the (Mahalanobis) squared distances (and cross products) among the linear combinations \\(\\mathbf{C} \\widehat{\\mathbf{B}}\\) from the origin under the null hypothesis.  \nFor example, with three responses \\(y_1, y_2, y_3\\) and three predictors \\(x_1, x_2, x_3\\), we can test the hypothesis that neither \\(x_1\\) nor \\(x_2\\) contribute at all to predicting the \\(y\\)s in terms of the hypothesis that the coefficients for the corresponding rows of \\(\\mathbf{B}\\) are zero using a 1-row \\(\\mathbf{C}\\) matrix that simply selects those rows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\mathcal{H}_0 : \\mathbf{C} \\mathbf{B} & =\n\\begin{bmatrix}\n0 & 1 & 1 & 0\n\\end{bmatrix}\n\\begin{pmatrix}\n  \\beta_{0,y_1} & \\beta_{0,y_2} & \\beta_{0,y_3} \\\\\n  \\beta_{1,y_1} & \\beta_{1,y_2} & \\beta_{1,y_3} \\\\\n  \\beta_{2,y_1} & \\beta_{2,y_2} & \\beta_{2,y_3} \\\\\n  \\beta_{3,y_1} & \\beta_{3,y_2} & \\beta_{3,y_3}\n\\end{pmatrix} \\\\ \\\\\n& =\n\\begin{bmatrix}\n  \\beta_{1,y_1} & \\beta_{1,y_2} & \\beta_{1,y_3} \\\\\n  \\beta_{2,y_1} & \\beta_{2,y_2} & \\beta_{2,y_3} \\\\\n\\end{bmatrix}\n=\n\\mathbf{0}_{(2 \\times 3)}\n\\end{aligned}\\]\nIn MANOVA designs, it is often desirable to follow up a significant effect for a factor with subsequent tests to determine which groups differ. While you can simply test for all pairwise differences among groups (using Bonferonni or other corrections for multiplicity), a more substantively-driven approach uses planned comparisons or contrasts among the factor levels as described in Section 5.1.3.\n\nFor a factor with \\(g\\) groups, a contrast is simply a comparison of the mean of one subset of groups against the mean of another subset. This is specified as a weighted sum, \\(L\\) of the means with weights \\(\\mathbf{c}\\) that sum to zero,\n\\[\nL = \\mathbf{c}^\\mathsf{T} \\boldsymbol{\\mu} = \\sum_i c_i \\mu_i \\quad\\text{such that}\\quad \\Sigma c_i = 0\n\\] Two contrasts, \\(\\mathbf{c}_1\\) and \\(\\mathbf{c}_2\\) are orthogonal if the sum of products of their weights is zero, i.e., \\(\\mathbf{c}_1^\\mathsf{T} \\mathbf{c}_2 = \\Sigma c_{1i} \\times c_{2i} = 0\\). When contrasts are placed as columns of a matrix \\(\\mathbf{C}\\), they are all mutually orthogonal if each pair is orthogonal, which means \\(\\mathbf{C}^\\top \\mathbf{C}\\) is a diagonal matrix. Orthogonal contrasts correspond to statistically independent tests.\nThis is nice because orthogonal contrasts reflect separate, non-overlapping research questions. When these questions are posed apriori, in advance of analysis, there is no need to correct for multiple testing, on the grounds that you shouldn’t be penalized for having more ideas!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor example, with the \\(g=4\\) groups for the dogfood data, the company might want to test the following comparisons among the formulas Old, New, Major and Alps: (a) Ours vs. Theirs: The average of (Old, New) compared to (Major, Alps); (b) Old vs. New; (c) Major vs. Alps. The contrasts that do this are:\n\\[\\begin{aligned}\nL_1 & = \\textstyle{\\frac12} (\\mu_O + \\mu_N) -\n        \\textstyle{\\frac12} (\\mu_M + \\mu_A) & \\rightarrow\\: & \\mathbf{c}_1 =\n\\textstyle{\\frac12}\n     \\begin{pmatrix}\n      1 &  1 & -1 & -1\n     \\end{pmatrix} \\\\\nL_2 & = \\mu_O - \\mu_N                     & \\rightarrow\\: & \\mathbf{c}_2 =\n    \\begin{pmatrix}\n     1 &  -1 & 0 & 0\n    \\end{pmatrix} \\\\\nL_3 & = \\mu_M - \\mu_A                     & \\rightarrow\\: & \\mathbf{c}_3 =\n    \\begin{pmatrix}\n     0 &  0 & 1 & -1\n    \\end{pmatrix}\n\\end{aligned}\\]\nNote that these correspond to nested dichotomies among the four groups: first we compare groups (Old and New) against groups (Major and Alps), then subsequently within each of these sets. Nested dicontomy contrasts are always orthogonal, and therefore correspond to statistically independent tests. We are effectively taking a three degree-of-freedom question, “do the means differ?” and breaking it down into three separate 1 df tests that answer specific parts of that overall question. \nIn R, contrasts for a factor are specified as columns of matrix, each of which sums to zero. For this example, we can set this up by creating each as a vector and joining them as columns using cbind():\n\nc1 &lt;- c(1,  1, -1, -1)/2    # Old,New vs. Major,Alps\nc2 &lt;- c(1, -1,  0,  0)      # Old vs. New\nc3 &lt;- c(0,  0,  1, -1)      # Major vs. Alps\nC &lt;- cbind(c1,c2,c3) \nrownames(C) &lt;- levels(dogfood$formula)\n\nC\n#&gt;         c1 c2 c3\n#&gt; Old    0.5  1  0\n#&gt; New    0.5 -1  0\n#&gt; Major -0.5  0  1\n#&gt; Alps  -0.5  0 -1\n\n# show they are mutually orthogonal\nt(C) %*% C\n#&gt;    c1 c2 c3\n#&gt; c1  1  0  0\n#&gt; c2  0  2  0\n#&gt; c3  0  0  2\n\nFor the dogfood data, with formula as the group factor, you can set up the analyses to use these contrasts by assigning the matrix C to contrasts() for that factor in the dataset itself. The estimated coefficients then become the estimated mean differences for the contrasts. When the contrasts are changed, it is necessary to refit the model.\n\ncontrasts(dogfood$formula) &lt;- C\ndogfood.mod &lt;- lm(cbind(start, amount) ~ formula, \n                  data=dogfood)\ncoef(dogfood.mod)\n#&gt;              start amount\n#&gt; (Intercept)  1.688  85.56\n#&gt; formulac1   -1.375  10.88\n#&gt; formulac2   -0.500   3.25\n#&gt; formulac3    0.125   1.88\n\nFor example, the contrast “Ours vs. Theirs” estimated by formulac1 takes 1.375 less time to start eating and eats 10.88 more on average.\nFor multivariate tests, when all contrasts are pairwise orthogonal, the overall test of a factor with \\(\\text{df}_h = g-1\\) degrees of freedom can be broken down into \\(g-1\\) separate 1 df tests. This gives rise to a set of \\(\\text{df}_h\\) rank 1 \\(\\mathbf{H}\\) matrices that additively decompose the overall hypothesis SSCP matrix,\n\\[\n\\mathbf{H} = \\mathbf{H}_1 + \\mathbf{H}_2 + \\cdots + \\mathbf{H}_{\\text{df}_h} \\:\\: ,\n\\tag{10.12}\\]\nexactly as the univariate \\(\\text{SS}_H\\) can be decomposed using orthogonal contrasts in an ANOVA (Section 5.1.3)\nYou can test such contrasts or any other hypotheses involving linear combinations of the coefficients using car::linearHypothesis(). Here, \"formulac1\" refers to the contrast c1 for the difference between Ours and Theirs. Note that because this is a 1 df test, all four test statistics yield the same \\(F\\) values.\n\nhyp &lt;- rownames(coef(dogfood.mod))[-1] |&gt; \n  print()\n#&gt; [1] \"formulac1\" \"formulac2\" \"formulac3\"\nH1 &lt;- linearHypothesis(dogfood.mod, hyp[1], \n                       title=\"Ours vs. Theirs\") |&gt; \n  print()\n#&gt; \n#&gt; Sum of squares and products for the hypothesis:\n#&gt;         start amount\n#&gt; start    7.56  -59.8\n#&gt; amount -59.81  473.1\n#&gt; \n#&gt; Sum of squares and products for error:\n#&gt;        start amount\n#&gt; start   25.8   11.7\n#&gt; amount  11.7  390.3\n#&gt; \n#&gt; Multivariate Tests: Ours vs. Theirs\n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; Pillai            1     0.625     9.18      2     11 0.0045 **\n#&gt; Wilks             1     0.375     9.18      2     11 0.0045 **\n#&gt; Hotelling-Lawley  1     1.669     9.18      2     11 0.0045 **\n#&gt; Roy               1     1.669     9.18      2     11 0.0045 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSimilarly we can test the other two contrasts within these each of these two subsets, but I don’t print the results.\n\nH2 &lt;- linearHypothesis(dogfood.mod, hyp[2], \n                       title=\"Old vs. New\")\nH3 &lt;- linearHypothesis(dogfood.mod, hyp[3], \n                       title=\"Alps vs. Major\")\n\nThen, we can illustrate Equation 10.12 by extracting the 1 df \\(\\mathbf{H}\\) matrices (SSPH) from the results of linearHypothesis.\n\n\\[\n\\overset{\\mathbf{H}}\n{\\begin{pmatrix}\n  9.7 & -70.9 \\\\\n-70.9 & 585.7 \\\\\n\\end{pmatrix}}\n=\n\\overset{\\mathbf{H}_1}\n{\\begin{pmatrix}\n  7.6 & -59.8 \\\\\n-59.8 & 473.1 \\\\\n\\end{pmatrix}}\n+\n\\overset{\\mathbf{H}_2}\n{\\begin{pmatrix}\n0.13 &  1.88 \\\\\n1.88 & 28.12 \\\\\n\\end{pmatrix}}\n+\n\\overset{\\mathbf{H}_3}\n{\\begin{pmatrix}\n  2 & -13 \\\\\n-13 &  84 \\\\\n\\end{pmatrix}}\n\\]",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#anova-rightarrow-manova",
    "href": "10-mlm-review.html#anova-rightarrow-manova",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.4 ANOVA \\(\\rightarrow\\) MANOVA",
    "text": "10.4 ANOVA \\(\\rightarrow\\) MANOVA\nMultivariate analysis of variance (MANOVA) generalizes the familiar ANOVA model to situations where there are two or more response variables. Unlike ANOVA, which focuses on discerning statistical differences in one continuous dependent variable influenced by an independent variable (or grouping variable), MANOVA considers several dependent variables at once. It integrates these variables into a single, composite variable through a weighted linear combination, allowing for a comprehensive analysis of how these dependent variables collectively vary with respect to the levels of the independent variable. Essentially, MANOVA investigates whether the grouping variable explains significant variations in the combined dependent variables.\nThe situation is illustrated in Figure 10.3 where there are two response measures, \\(Y_1\\) and \\(Y_2\\) with data collected for three groups. For concreteness, \\(Y_1\\) might be a score on a math test and \\(Y_2\\) might be a reading score. Let’s also say that group 1 has been studying Shakespeare, while group 2 has concentrated on physics, but group 3 has done nothing beyond the normal curriculum.\n\n\n\n\n\n\n\nFigure 10.3: Diagram of data from simple MANOVA design involving three groups and two response measures, \\(Y_1\\) and \\(Y_2\\), summarized by their data ellipses.\n\n\n\n\nAs shown in the figure, the centroids, \\((\\mu_{1g}, \\mu_{2g})\\), clearly differ—the data ellipses barely overlap. A multivariate analysis would show a highly difference among groups. From a rough visual inspection, it seems that means differ on the math test \\(Y_1\\), with the physics group out-performing the other two. On the reading test \\(Y_2\\) however it might turn out that the three group means don’t differ significantly in an ANOVA, but the Shakespeare and physics groups appear to outperform the normal curriculum group. Doing separate ANOVAs on these variables would miss what is so obvious from Figure 10.3: there is wide separation among the groups in the two tests considered jointly.\nFigure 10.4 illustrates a second important advantage of performing a multivariate analysis over separate ANOVAS: that of determining the number of dimensions or aspects along which groups differ. In the panel on the left, the means of the three groups increase nearly linearly on the combination of \\(Y_1\\) and \\(Y_2\\), so their differences can be ascribed to a single dimension, which simplifies the interpretation: both memory and attention scores decrease together with increasing degree of schizophrenia.\nFor example, the groups here might be patients diagnosed as normal, mild schizophrenia and profound schizophrenia, and the measures could be tests of memory and attention. The obvious multivariate interpretation from the figure is that of increasing impairment of cognitive functioning across the groups, comprised by memory and attention. Note also the positive association within each group: those who perform better on the memory task also do better on attention.\n\n\n\n\n\n\n\nFigure 10.4: A simple MANOVA design involving three groups and two response measures, \\(Y_1\\) and \\(Y_2\\), but with different patterns of the differences among the group means. The red arrows suggest interpretations in terms of dimensions or aspects of the response variables.\n\n\n\n\nIn contrast, the right panel of Figure 10.4 shows a situation where the group means have a low correlation. Data like this might arise in a study of parental competency, where there are measures of both the degree of caring (\\(Y_1\\)) and time spent in play (\\(Y_2\\)) by fathers and groups consisting of fathers of children with no disability, or a physical disability or a mental ability.\nAs can be seen in Figure 10.4 fathers of the disabled children differ from those of the not disabled group in two different directions corresponding to being higher on either \\(Y_1\\) or \\(Y_2\\). The red arrows suggest that the differences among groups could be interpreted in terms of two uncorrelated dimensions, perhaps labeled overall competency and emphasis on physical activity. (The pattern in Figure 10.4 (right) is contrived for the sake of illustration; it does not reflect the data analyzed in the example below.)\n\n10.4.1 Example: Father parenting data\nI use a simple example of a three-group multivariate design to illustrate the basic ideas of fitting MLMs in R and testing hypotheses. Visualization methods using HE plots are discussed in Chapter 11.\nThe dataset Parenting come from an exercise (10B) in Meyers et al. (2006) and are probably contrived, but are modeled on a real study in which fathers were assessed on three subscales of a Perceived Parenting Competence Scale,\n\n\ncaring, caretaking responsibilities;\n\nemotion, emotional support provided to the child; and\n\nplay, recreational time spent with the child.\n\nThe dataset Parenting comprises 60 fathers selected from three groups of \\(n = 20\\) each: (a) fathers of a child with no disabilities (\"Normal\"); (b) fathers with a physically disabled child; (c) fathers with a mentally disabled child. The design is thus a three-group MANOVA, with three response variables.\n\nThe main questions concern whether the group means differ on these scales, and the nature of these differences. That is, do the means differ significantly on all three measures? Is there a consistent order of groups across these three aspects of parenting?\nMore specific questions are: (a) Do the fathers of typical children differ from the other two groups on average? (b) Do the physical and mental groups differ on these measures?\nThese questions can be tested using contrasts, and are specified by assigning a matrix to contrasts(Parenting$group); each column is a contrast whose values sum to zero. They are given labels \"group1\" (normal vs. other) and \"group2\" (physical vs. mental) in some output.\n\ndata(Parenting, package=\"heplots\")\nC &lt;- matrix(c(1, -.5, -.5,\n              0,  1,  -1), \n            nrow = 3, ncol = 2) |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,]  1.0    0\n#&gt; [2,] -0.5    1\n#&gt; [3,] -0.5   -1\ncontrasts(Parenting$group) &lt;- C\n\nExploratory plots\nBefore setting up a model and testing, it is well-advised to examine the data graphically. The simplest plots are side-by-side boxplots (or violin plots) for the three responses. With ggplot2, this is easily done by reshaping the data to long format and using faceting. In Figure 10.5, I’ve also plotted the group means with white dots.\n\nSee the ggplot codeparenting_long &lt;- Parenting |&gt;\n  tidyr::pivot_longer(cols=caring:play, \n                      names_to = \"variable\")\n\nggplot(parenting_long, \n       aes(x=group, y=value, fill=group)) +\n  geom_boxplot(outlier.size=2.5, \n               alpha=.5, \n               outlier.alpha = 0.9) + \n  stat_summary(fun=mean, \n               color=\"white\", \n               geom=\"point\", \n               size=2) +\n  scale_fill_hue(direction = -1) +     # reverse default colors\n  labs(y = \"Scale value\", x = \"Group\") +\n  facet_wrap(~ variable) +\n  theme_bw(base_size = 14) + \n  theme(legend.position=\"top\") +\n  theme(axis.text.x = element_text(angle = 15,\n                                   hjust = 1)) \n\n\n\n\n\n\nFigure 10.5: Faceted boxplots of scores on the three parenting scales, showing also the mean for each.\n\n\n\n\nIn this figure, differences among the groups on play are most apparent, with fathers of non-disabled children scoring highest. Differences among the groups on emotion are very small, but one high outlier for the fathers of mentally disabled children is apparent. On caring, fathers of children with a physical disability stand out as highest.\nFor exploratory purposes, you might also make a scatterplot matrix. Here, because the MLM assumes homogeneity of the variances and covariance matrices \\(\\mathbf{S}_i\\), I show only the data ellipses in scatterplot matrix format, using heplots:covEllipses() (with 50% coverage, for clarity):\n\ncolors &lt;- scales::hue_pal()(3) |&gt; rev()  # match color use in ggplot\ncovEllipses(cbind(caring, play, emotion) ~ group, \n  data=Parenting,\n  variables = 1:3,\n  fill = TRUE, fill.alpha = 0.2,\n  pooled = FALSE,\n  level = 0.50, \n  col = colors)\n\n\n\n\n\n\nFigure 10.6: Bivariate data ellipses for pairs of the three responses, showing the means, correlations and variances for the three groups.\n\n\n\n\nIf the covariance matrices were all the same, the data ellipses would have roughly the same size and orientation, but that is not the case in Figure 10.6. The normal group shows greater variability overall and the correlations among the measures differ somewhat from group to group. We’ll assess later whether this makes a difference in the conclusions that can be drawn (Chapter 12). The group centroids also differ, but the pattern is not particularly clear. We’ll see an easier to understand view in HE plots (Section 11.3) and their canonical discriminant cousins (Section 11.7).\nTesting the model\nLet’s proceed to fit the multivariate model predicting all three scales from the group factor. lm() for a multivariate response returns an object of class \"mlm\", for which there are many methods (use methods(class=\"mlm\") to find them).\n\nparenting.mlm &lt;- lm(cbind(caring, play, emotion) ~ group, \n                    data=Parenting) |&gt; print()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cbind(caring, play, emotion) ~ group, data = Parenting)\n#&gt; \n#&gt; Coefficients:\n#&gt;              caring   play     emotion\n#&gt; (Intercept)   5.8833   4.6333   5.9167\n#&gt; group1       -0.3833   2.4167  -0.0667\n#&gt; group2        1.7750  -0.2250  -0.6000\n\nThe coefficients in this model are the values of the contrasts set up above. group1 is the mean of the typical group minus the average of the other two, which is negative on caring and emotion but positive for play. group2 is the difference in means for the physical vs. mental groups.\nBefore doing multivariate tests, it is useful to see what would happen if we ran univariate ANOVAs on each of the responses. These can be extracted from an MLM using stats::summary.aov() and they give tests of the model terms for each response variable separately:\n\nsummary.aov(parenting.mlm)\n#&gt;  Response caring :\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; group        2    130    65.2    18.6  6e-07 ***\n#&gt; Residuals   57    200     3.5                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Response play :\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; group        2    177    88.6    27.6  4e-09 ***\n#&gt; Residuals   57    183     3.2                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Response emotion :\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; group        2     15    7.27    1.02   0.37\n#&gt; Residuals   57    408    7.16\n\nFor a more condensed summary, you can instead extract the univariate model fit statistics from the \"mlm\" object using the heplots::glance() method for a multivariate model object. The code below selects just the \\(R^2\\) and \\(F\\)-statistic for the overall model for each response, together with the associated \\(p\\)-value.\n\nglance(parenting.mlm) |&gt;\n  select(response, r.squared, fstatistic, p.value)\n#&gt; # A tibble: 3 × 4\n#&gt;   response r.squared fstatistic       p.value\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 caring      0.395       18.6  0.000000602  \n#&gt; 2 play        0.492       27.6  0.00000000405\n#&gt; 3 emotion     0.0344       1.02 0.369\n\nFrom this, one might conclude that there are differences only in caring and play and therefore ignore emotion, but this would be short-sighted. car::Anova(), shown below, gives the overall multivariate test \\(\\mathcal{H}_0: \\mathbf{B} = 0\\) of the group effect, that the groups don’t differ on any of the response variables. Note that this has a much smaller \\(p\\)-value than any of the univariate \\(F\\) tests.\n\nAnova(parenting.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;       Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; group  2     0.948     16.8      6    112  9e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova() returns an object of class \"Anova.mlm\" which has various methods. The summary() method for this gives more details, including all four test statistics. With \\(p=3\\) responses, these tests have \\(s = \\min(p, \\text{df}_h) = \\min(3,2) = 2\\) dimensions and the \\(F\\) approximations are not equivalent here. All four tests are highly significant, with Roy’s test giving the largest \\(F\\) statistic.\n\nparenting.summary &lt;- Anova(parenting.mlm) |&gt;  summary() \nprint(parenting.summary, SSP=FALSE)\n#&gt; \n#&gt; Type II MANOVA Tests:\n#&gt; \n#&gt; ------------------------------------------\n#&gt;  \n#&gt; Term: group \n#&gt; \n#&gt; Multivariate Tests: group\n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            2     0.948     16.8      6    112 9.0e-14 ***\n#&gt; Wilks             2     0.274     16.7      6    110 1.3e-13 ***\n#&gt; Hotelling-Lawley  2     1.840     16.6      6    108 1.8e-13 ***\n#&gt; Roy               2     1.108     20.7      3     56 3.8e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe summary() method by default prints the SSH = \\(\\mathbf{H}\\) and SSE = \\(\\mathbf{E}\\) matrices, but I suppressed them above. The data structure returned contains nested elements which can be extracted more easily from the object using purrr::pluck():\n\nH &lt;- parenting.summary |&gt; \n  purrr::pluck(\"multivariate.tests\", \"group\", \"SSPH\") |&gt; \n  print()\n#&gt;         caring    play emotion\n#&gt; caring   130.4 -43.767 -41.833\n#&gt; play     -43.8 177.233   0.567\n#&gt; emotion  -41.8   0.567  14.533\nE &lt;- parenting.summary |&gt; \n  purrr::pluck(\"multivariate.tests\", \"group\", \"SSPE\") |&gt; \n  print()\n#&gt;         caring  play emotion\n#&gt; caring   199.8 -45.8    35.2\n#&gt; play     -45.8 182.7    80.6\n#&gt; emotion   35.2  80.6   408.0\n\nLinear hypotheses & contrasts\nWith three or more groups or with a more complex MANOVA design, contrasts provide a way of testing questions of substantive interest regarding differences among group means.\nThe test of the contrast comparing the typical group to the average of the others is the test using the contrast \\(c_1 = (1, -\\frac12, -\\frac12)\\) which produces the coefficients labeled \"group1\". The function car::linearHypothesis() carries out the multivariate test that this difference is zero. This is a 1 df test, so all four test statistics produce the same \\(F\\) and \\(p\\)-values.\n\ncoef(parenting.mlm)[\"group1\",]\n#&gt;  caring    play emotion \n#&gt; -0.3833  2.4167 -0.0667\nlinearHypothesis(parenting.mlm, \"group1\") |&gt; \n  print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            1     0.521     19.9      3     55 7.1e-09 ***\n#&gt; Wilks             1     0.479     19.9      3     55 7.1e-09 ***\n#&gt; Hotelling-Lawley  1     1.088     19.9      3     55 7.1e-09 ***\n#&gt; Roy               1     1.088     19.9      3     55 7.1e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSimilarly, the difference between the physical and mental groups uses the contrast \\(c_2 = (0, 1, -1)\\) and the test that these means are equal is given by linearHypothesis() applied to group2.\n\ncoef(parenting.mlm)[\"group2\",]\n#&gt;  caring    play emotion \n#&gt;   1.775  -0.225  -0.600\nlinearHypothesis(parenting.mlm, \"group2\") |&gt; \n  print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1     0.429     13.8      3     55  8e-07 ***\n#&gt; Wilks             1     0.571     13.8      3     55  8e-07 ***\n#&gt; Hotelling-Lawley  1     0.752     13.8      3     55  8e-07 ***\n#&gt; Roy               1     0.752     13.8      3     55  8e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlinearHypothesis() is very general. The second argument (hypothesis.matrix) corresponds to \\(\\mathbf{C}\\), and can be specified as numeric matrix giving the linear combinations of coefficients by rows to be tested, or a character vector giving the hypothesis in symbolic form; \"group1\" is equivalent to \"group1 = 0\".\nBecause the two contrasts used here are orthogonal, they add together to give the overall test of \\(\\mathbf{B} = \\mathbf{0}\\), which implies that the means of the three groups are all equal. The test below gives the same results as Anova(parenting.mlm).\n\nlinearHypothesis(parenting.mlm, c(\"group1\", \"group2\")) |&gt; \n  print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            2     0.948     16.8      6    112 9.0e-14 ***\n#&gt; Wilks             2     0.274     16.7      6    110 1.3e-13 ***\n#&gt; Hotelling-Lawley  2     1.840     16.6      6    108 1.8e-13 ***\n#&gt; Roy               2     1.108     20.7      3     56 3.8e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n10.4.2 Ordered factors\nWhen groups are defined by an ordered factor, such as level of physical fitness (rated 1–5) or grade in school, it is tempting to treat that as a numeric variable and use a multivariate regression model. This would assume that the effect of that factor is linear and if not, we might consider adding polynomial terms.\nA different strategy, often preferable, is to make the group variable an ordered factor, for which R assigns polynomial contrasts. This gives separate tests of the linear, quadratic, cubic, … trends of the response, without the need to specify them separately in the model.\n\n10.4.3 Example: Adolescent mental health\nThe dataset AddHealth contains a large cross-sectional sample of participants from grades 7–12 from the National Longitudinal Study of Adolescent Health, described by Warne (2014). It contains responses to two Likert-scale (1–5) items, anxiety and depression. grade is an ordered factor, which means that the default contrasts are taken as orthogonal polynomials with linear (grade.L), quadratic (grade.Q), up to 5th degree (grade^5) trends, which decompose the total effect of grade.\n\ndata(AddHealth, package=\"heplots\")\nstr(AddHealth)\n#&gt; 'data.frame':  4344 obs. of  3 variables:\n#&gt;  $ grade     : Ord.factor w/ 6 levels \"7\"&lt;\"8\"&lt;\"9\"&lt;\"10\"&lt;..: 5 4 6 1 2 2 2 3 3 3 ...\n#&gt;  $ depression: int  0 0 0 0 0 0 0 0 1 2 ...\n#&gt;  $ anxiety   : int  0 0 0 1 1 0 0 1 1 0 ...\n\nThe research questions are:\n\nHow do the means for anxiety and depression vary separately with grade? Is there evidence for linear and nonlinear trends?\nHow do anxiety and depression vary jointly with grade?\nHow does the association (correlation, $R^2) of anxiety and depression vary with age?\n\nThe first question can be answered by fitting separate linear models for each response (e.g., lm(anxiety ~ grade))). However the second question is more interesting because it considers the two responses together and takes their correlation into account. This would be fit as the MLM:\n\\[\n\\mathbf{y} = \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1 x + \\boldsymbol{\\beta}_2 x^2 + \\cdots \\boldsymbol{\\beta}_5 x^5\n\\tag{10.13}\\]\nor, expressed in terms of the variables,\n\\[\\begin{aligned}\n\\begin{bmatrix} y_{\\text{anx}} \\\\y_{\\text{dep}} \\end{bmatrix} & =\n\\begin{bmatrix} \\beta_{0,\\text{anx}} \\\\ \\beta_{0,\\text{dep}} \\end{bmatrix} +\n\\begin{bmatrix} \\beta_{1,\\text{anx}} \\\\ \\beta_{1,\\text{dep}} \\end{bmatrix} \\text{grade} +\n\\begin{bmatrix} \\beta_{2,\\text{anx}} \\\\ \\beta_{2,\\text{dep}} \\end{bmatrix} \\text{grade}^2 \\\\\n& + \\cdots +\n\\begin{bmatrix} \\beta_{5,\\text{anx}} \\\\ \\beta_{5,\\text{dep}} \\end{bmatrix} \\text{grade}^5\n\\end{aligned} \\tag{10.14}\\]\nWithgrade represented as an ordered factor, the values of \\(x\\) in Equation 10.13 are those of the orthogonal polynomials given by poly(grade,5).\nExploratory plots\nSome exploratory analysis is useful before fitting and visualizing models. As a first step, we find the means, standard deviations, and standard errors of the means.\n\nmeans &lt;- AddHealth |&gt;\n  group_by(grade) |&gt;\n  summarise(\n    n = n(),\n    dep_sd = sd(depression, na.rm = TRUE),\n    anx_sd = sd(anxiety, na.rm = TRUE),\n    dep_se = dep_sd / sqrt(n),\n    anx_se = anx_sd / sqrt(n),\n    depression = mean(depression),\n    anxiety = mean(anxiety) ) |&gt; \n  relocate(depression, anxiety, .after = grade) |&gt;\n  print()\n#&gt; # A tibble: 6 × 8\n#&gt;   grade depression anxiety     n dep_sd anx_sd dep_se anx_se\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 7          0.881   0.751   622   1.11   1.05 0.0447 0.0420\n#&gt; 2 8          1.08    0.804   664   1.19   1.06 0.0461 0.0411\n#&gt; 3 9          1.17    0.934   778   1.19   1.08 0.0426 0.0387\n#&gt; 4 10         1.27    0.956   817   1.23   1.11 0.0431 0.0388\n#&gt; 5 11         1.37    1.12    790   1.20   1.16 0.0428 0.0411\n#&gt; 6 12         1.34    1.10    673   1.14   1.11 0.0439 0.0426\n\nNow, plot the means with \\(\\pm 1\\) error bars. It appears that average level of both depression and anxiety increase steadily with grade, except for grades 11 and 12 which don’t differ much. Alternatively, we could describe this as relationships that seem largely linear, with a hint of curvature at the upper end.\n\np1 &lt;-ggplot(data = means, aes(x = grade, y = anxiety)) +\n  geom_point(size = 4) +\n  geom_line(aes(group = 1), linewidth = 1.2) +\n  geom_errorbar(aes(ymin = anxiety - anx_se, \n                   ymax = anxiety + anx_se),\n                width = .2) \n\np2 &lt;-ggplot(data = means, aes(x = grade, y = depression)) +\n  geom_point(size = 4) +\n  geom_line(aes(group = 1), linewidth = 1.2) +\n  geom_errorbar(aes(ymin = depression - dep_se, \n                    ymax = depression + dep_se),\n                width = .2) \n\np1 + p2\n\n\n\n\n\n\nFigure 10.7: Means of anxiety and depression by grade, with \\(\\pm 1\\) standard error bars.\n\n\n\n\nIt is also useful to within-group correlations using covEllipses(), as shown in Figure 10.8. This also plots the bivariate means showing the form of the association , treating anxiety and depression as multivariate outcomes. (Because the variability of the scores within groups is so large compared to the range of the means, I show the data ellipses with coverage of only 10%.)\n\ncovEllipses(AddHealth[, 3:2], group = AddHealth$grade,\n            pooled = FALSE, level = 0.1,\n            center.cex = 2.5, cex = 1.5, cex.lab = 1.5,\n            fill = TRUE, fill.alpha = 0.05)\n\n\n\n\n\n\nFigure 10.8: Within-group covariance ellipses for the grade groups.\n\n\n\n\nFit the MLM\nNow, let’s fit the MLM for both responses jointly in relation to grade. The null hypothesis is that the means for anxiety and depression are the same at all six grades,\n\\[\n\\mathcal{H}_0: \\mathbf{\\mu}_7 = \\mathbf{\\mu}_8 = \\cdots = \\mathbf{\\mu}_{12} \\; ,\n\\] or equivalently, that all coefficients except the intercept in the model Equation 10.13 are zero,\n\\[\n\\mathcal{H}_0: \\boldsymbol{\\beta}_1 =  \\boldsymbol{\\beta}_2  = \\cdots =  \\boldsymbol{\\beta}_5 = \\boldsymbol{0} \\; .\n\\]\nWe fit the MANOVA model, and test the grade effect using car::Anova(). The effect of grade is highly significant, as we could tell from Figure 10.7.\n\nAH.mlm &lt;- lm(cbind(anxiety, depression) ~ grade, data = AddHealth)\n\n# overall test of `grade`\nAnova(AH.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;       Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; grade  5    0.0224     9.83     10   8676 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHowever, the overall test, with 5 degrees of freedom is diffuse, in that it can be rejected if any pair of means differ. Given that grade is an ordered factor, it makes sense to examine narrower hypotheses of linear and nonlinear trends, car::linearHypothesis() on the coefficients of model AH.mlm.\n\ncoef(AH.mlm) |&gt; rownames()\n#&gt; [1] \"(Intercept)\" \"grade.L\"     \"grade.Q\"     \"grade.C\"    \n#&gt; [5] \"grade^4\"     \"grade^5\"\n\nThe joint test of the linear coefficients \\(\\boldsymbol{\\beta}_1 = (\\beta_{1,\\text{anx}},  \\beta_{1,\\text{dep}})^\\mathsf{T}\\) for anxiety and depression, \\(\\mathcal{H}_0 : \\boldsymbol{\\beta}_1 = \\boldsymbol{0}\\) is highly significant,\n\n## linear effect\nlinearHypothesis(AH.mlm, \"grade.L\") |&gt; print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1     0.019     42.5      2   4337 &lt;2e-16 ***\n#&gt; Wilks             1     0.981     42.5      2   4337 &lt;2e-16 ***\n#&gt; Hotelling-Lawley  1     0.020     42.5      2   4337 &lt;2e-16 ***\n#&gt; Roy               1     0.020     42.5      2   4337 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe test of the quadratic coefficients \\(\\mathcal{H}_0 : \\boldsymbol{\\beta}_2 = \\boldsymbol{0}\\) indicates significant curvature in trends across grade, as we saw in the plots of their means in Figure 10.7. One interpretation might be that depression and anxiety after increasing steadily up to grade eleven could level off thereafter.\n\n## quadratic effect\nlinearHypothesis(AH.mlm, \"grade.Q\") |&gt; print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; Pillai            1     0.002     4.24      2   4337  0.014 *\n#&gt; Wilks             1     0.998     4.24      2   4337  0.014 *\n#&gt; Hotelling-Lawley  1     0.002     4.24      2   4337  0.014 *\n#&gt; Roy               1     0.002     4.24      2   4337  0.014 *\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAn advantage of linear hypotheses is that we can test several terms jointly. Of interest here is the hypothesis that all higher order terms beyond the quadratic are zero, \\(\\mathcal{H}_0 : \\boldsymbol{\\beta}_3 =  \\boldsymbol{\\beta}_4 =  \\boldsymbol{\\beta}_5 = \\boldsymbol{0}\\). Using linearHypothesis you can supply a vector of coefficient names to be tested for their joint effect when dropped from the model.\n\ncoefs &lt;- rownames(coef(AH.mlm)) |&gt; print()\n#&gt; [1] \"(Intercept)\" \"grade.L\"     \"grade.Q\"     \"grade.C\"    \n#&gt; [5] \"grade^4\"     \"grade^5\"\n## joint test of all higher terms\nlinearHypothesis(AH.mlm, coefs[3:5],\n                 title = \"Higher-order terms\") |&gt; \n  print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: Higher-order terms\n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; Pillai            3     0.002     1.70      6   8676   0.12  \n#&gt; Wilks             3     0.998     1.70      6   8674   0.12  \n#&gt; Hotelling-Lawley  3     0.002     1.70      6   8672   0.12  \n#&gt; Roy               3     0.002     2.98      3   4338   0.03 *\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#sec-factorial-manova",
    "href": "10-mlm-review.html#sec-factorial-manova",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.5 Factorial MANOVA",
    "text": "10.5 Factorial MANOVA\nWhen there are two or more categorical factors, the general linear model provides a way to investigate the effects (differences in means) of each simultaneously. More importantly, this allows you to determine if factors interact, so the effect of one factor varies depending on the levels of another factor. For instance, the effect of a weight loss drug treatment may vary with the patient’s ethnicity or age group.\nIn such situations, when an interaction is significant it is unwise to interpret the overall main effects of treatment and the other factor. Instead, you would look at the “simple effects”– how the treatment works at specific levels of the other factor separately.\n\n\n\nExample 10.1 Plastic film data\nAn industrial experiment was conducted to determine the optimal conditions for extruding plastic film. Of interest were three responses: resistance to tear, film gloss and the opacity of the film. Two factors were manipulated, both at two levels, labeled High and Low: change in rate of extrusion (-10%, +10%) and amount of some additive (1%, 1.5%), with \\(n=5\\) runs at each combination of the factor levels. The dataset heplots::Plastic comes from Johnson & Wichern (1998), Example 6.12.\n\ndata(Plastic, package=\"heplots\")\nstr(Plastic)\n#&gt; 'data.frame':  20 obs. of  5 variables:\n#&gt;  $ tear    : num  6.5 6.2 5.8 6.5 6.5 6.9 7.2 6.9 6.1 6.3 ...\n#&gt;  $ gloss   : num  9.5 9.9 9.6 9.6 9.2 9.1 10 9.9 9.5 9.4 ...\n#&gt;  $ opacity : num  4.4 6.4 3 4.1 0.8 5.7 2 3.9 1.9 5.7 ...\n#&gt;  $ rate    : Factor w/ 2 levels \"Low\",\"High\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ additive: Factor w/ 2 levels \"Low\",\"High\": 1 1 1 1 1 2 2 2 2 2 ...\n\nMultivariate tests\nThe MANOVA model plastic.mod fits the main effects of rate and additive and the interaction rate:additive. The Anova() summary shows a strong effect for rate of extrusions for the three responses jointly, a lesser, but still significant effect of additive and a non-significant interaction.\n\nplastic.mod &lt;- lm(cbind(tear, gloss, opacity) ~ rate*additive, \n                  data=Plastic)\nAnova(plastic.mod)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;               Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; rate           1     0.618     7.55      3     14  0.003 **\n#&gt; additive       1     0.477     4.26      3     14  0.025 * \n#&gt; rate:additive  1     0.223     1.34      3     14  0.302   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAs a reminder, if you want to see the results of univariate tests for the responses separately, heplots::uniStats() (or heplots::glance.mlm()) gives some answers, including \\(R^2\\) and univariate \\(F\\) tests.\n\nuniStats(plastic.mod)\n#&gt; Univariate tests for responses in the multivariate linear model plastic.mod \n#&gt; \n#&gt;           R^2    F df1 df2 Pr(&gt;F)   \n#&gt; tear    0.586 7.56   3  16 0.0023 **\n#&gt; gloss   0.483 4.99   3  16 0.0125 * \n#&gt; opacity 0.125 0.76   3  16 0.5315   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNote that these two summaries are complementary. Anova() collapses over the responses to give overall multivariate tests for each model term. uniStats() shows only the overall statistics for each response variable, combining the effects of all terms, and testing against the null model that none of them contribute to that response.\nPlotting main effects and interactions\nTo understand main effects and interactions in a simple two-way MANOVA design, the simplest thing to do is to plot the cell means and error bars in a traditional line plot with one factor on the horizontal axis and the other as separate lines within the panel and with one panel for each response variable.\nThis is slightly awkward using ggplot2 directly: you have to plot the points and lines, but to plot error bars with geom_errorbar() you must calculate the standard errors and work out the upper and lower limits. In Figure 10.9 I use ggpubr::ggline(), which simplifies such plots, though at the expense of a bit of control for ggplot2.4 The argument add = c(\"mean_se\") draws error bars5 showing \\(\\pm 1\\) standard error around the mean.\n\nlegend_inside &lt;- function(position) {     # simplify legend placement\n  theme(legend.position = \"inside\",\n        legend.position.inside = position)\n}\n\np1 &lt;- ggline(Plastic, \n  x = \"rate\", y = \"tear\",\n  color = \"additive\", shape = \"additive\", linetype = \"additive\",\n  add = c(\"mean_se\"), position = position_dodge(width = .1),\n  point.size = 5, linewidth = 1.5,\n  palette = c(\"red\", \"blue\"),\n  ggtheme = theme_pubr(base_size = 16)\n  ) +\n  xlab(\"Rate of extrusion\") +\n  ylab(\"Tear resistance\") +\n  legend_inside(c(.25, .8)) \n\np2 &lt;- ggline(Plastic, \n  x = \"rate\", y = \"gloss\",\n  color = \"additive\", shape = \"additive\", linetype = \"additive\",\n  add = c(\"mean_se\"), position = position_dodge(width = .1),\n  palette = c(\"red\", \"blue\"),\n  point.size = 5, linewidth = 1.5,\n  ggtheme = theme_pubr(base_size = 16)\n  ) +\n  xlab(\"Rate of extrusion\") +\n  ylab(\"Film gloss\") +\n  theme(legend.position = \"none\")\n\np1 + p2 \n\n\n\n\n\n\nFigure 10.9: Line plots of means and their standard errors for the response tear (left) and gloss (right) in the plastic film data.\n\n\n\n\nResistance to tear is greater with the high rate of extrusion and high level of the additive. The lines are parallel, so there is no interaction. In the panel for gloss, the means for additive don’t differ at the high rate, but do so substantially at the low extrusion rate, giving rise to the interaction for this outcome.\nSuch univariate plots are certainly useful for this simple \\(2 \\times 2\\) design with two response variables, but you can imagine that they get more complicated, both to construct and to understand with larger designs and more response variables. As well, plotting the responses separately give no information on how the outcomes vary jointly. I return to this data in Example 11.2, where I show how HE plots can give greater insight in this situation.\n\n\nExample 10.2 Does defendant physical attractiveness affect jury decisions?\nIn a social psychology study of influences on jury decisions by Plaster (1989), male participants (prison inmates) were shown a picture of one of three young women. Pilot work had indicated that one woman’s photo was “beautiful”, another of “average” physical attractiveness, and the third “unattractive”. Participants rated the woman they saw on each of twelve attributes on scales of 1–9. These measures were used to check on the manipulation of “attractiveness” by the photo.\nIn the main part of the study, other participants were told that the person in the photo had committed a non-violent crime, either burglary or swindling. They were asked to rate the seriousness of the crime and recommend a prison sentence, in Years.\nThe data are contained in the data frame heplots::MockJury. Only the first four are analyzed here: Attr, Crime, Year and Serious. The remaining twelve, exciting … ownPA relate to validity tests of whether the attractiveness classification Attr of the photo captures the essence of the attribute ratings.\n\ndata(MockJury, package = \"heplots\")\nnames(MockJury)\n#&gt;  [1] \"Attr\"          \"Crime\"         \"Years\"         \"Serious\"      \n#&gt;  [5] \"exciting\"      \"calm\"          \"independent\"   \"sincere\"      \n#&gt;  [9] \"warm\"          \"phyattr\"       \"sociable\"      \"kind\"         \n#&gt; [13] \"intelligent\"   \"strong\"        \"sophisticated\" \"happy\"        \n#&gt; [17] \"ownPA\"\n\nSample sizes were roughly balanced for the independent variables in the three conditions of the attractiveness of the photo, and the two combinations of this with Crime, which was burglary or swindling, giving a \\(3 \\times 2\\) factorial design.\n\ntable(MockJury$Attr)\n#&gt; \n#&gt;    Beautiful      Average Unattractive \n#&gt;           39           38           37\ntable(MockJury$Attr, MockJury$Crime)\n#&gt;               \n#&gt;                Burglary Swindle\n#&gt;   Beautiful          21      18\n#&gt;   Average            18      20\n#&gt;   Unattractive       20      17\n\nThe main questions of interest were:\n\nDoes attractiveness of the “defendant” influence the sentence or perceived seriousness of the crime?\n\nDoes attractiveness interact with the nature of the crime? That is, does attractiveness have the same pattern of means for both crimes?\n\nSo, I carry out a two-way MANOVA of the responses Years and Serious in relation to the independent variables Attr and Crime.\n\n# influence of Attr of photo and nature of crime on Serious and Years\njury.mod &lt;- lm( cbind(Serious, Years) ~ Attr * Crime, data=MockJury)\nAnova(jury.mod, test = \"Roy\")\n#&gt; \n#&gt; Type II MANOVA Tests: Roy test statistic\n#&gt;            Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; Attr        2    0.0756     4.08      2    108  0.020 *\n#&gt; Crime       1    0.0047     0.25      2    107  0.778  \n#&gt; Attr:Crime  2    0.0501     2.71      2    108  0.071 .\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe see that there is a strong main effect of Attr, no overall effect of Crime, and a nearly significant interaction between Attr and Crime. To probe these multivariate tests, you can also examine the univariate results for each response.\n\nuniStats(jury.mod)\n#&gt; Univariate tests for responses in the multivariate linear model jury.mod \n#&gt; \n#&gt;            R^2    F df1 df2 Pr(&gt;F)  \n#&gt; Serious 0.0117 0.26   5 108  0.936  \n#&gt; Years   0.0843 1.99   5 108  0.086 .\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAgain, you can interpret these results more easily from line plots of the means and standard error bars. In Figure 10.10, I assign attractiveness to the horizontal axis and plot separate lines for the types of crime.\n\np1 &lt;- ggline(MockJury, \n  x = \"Attr\", y = \"Years\",\n  color = \"Crime\", shape = \"Crime\", linetype = \"Crime\",\n  add = c(\"mean_se\"), position = position_dodge(width = .1),\n  point.size = 5, linewidth = 1.5,\n  palette = c(\"blue\", \"darkorange2\"),\n  ggtheme = theme_pubr(base_size = 16)\n  ) +\n  xlab(\"Physical attractiveness of photo\") +\n  ylab(\"Recommended years of sentence\") +\n  legend_inside(c(.25, .9))\n\np2 &lt;- ggline(MockJury, \n  x = \"Attr\", y = \"Serious\",\n  color = \"Crime\", shape = \"Crime\", linetype = \"Crime\",\n  add = c(\"mean_se\"), position = position_dodge(width = .1),\n  point.size = 5,  linewidth = 1.5,\n  palette = c(\"blue\", \"darkorange2\"),\n  ggtheme = theme_pubr(base_size = 16) \n  ) +\n  xlab(\"Physical attractiveness of photo\") +\n  ylab(\"Seriousness of crime\") +\n  legend_inside(c(.75, .9))\n\np1 + p2\n\n\n\n\n\n\nFigure 10.10: Line plots of means and their standard errors for the response Years (left) and Serious (right) in the Mock Jury data.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#sec-MRA-to-MMRA",
    "href": "10-mlm-review.html#sec-MRA-to-MMRA",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.6 MRA \\(\\rightarrow\\) MMRA",
    "text": "10.6 MRA \\(\\rightarrow\\) MMRA\nWhen all predictor variables are quantitative, the MLM Equation 10.2 becomes the extension of univariate multiple regression analysis (MRA) to the situation where there are \\(p\\) response variables (RA). Just as in univariate models, we might want to test hypotheses about subsets of the predictors, for example when some predictors are meant as controls or things you might want to adjust for in assessing the effects of predictors of main interest.\nBut first, there are a couple of aspects of statistical practice that should be mentioned.\nModel selection is one topic where univariate and multivariate approaches differ. When there are more than a few predictors, approaches like hierarchical regression, LASSO (Tibshirani, 1996) and stepwise selection can be used to eliminate uninformative predictors for each response.6 But this gives a different models for each response, based on the predictors included, each with its own interpretation. In contrast, the multivariate approach considers the outcome variables collectively. You can eliminate predictors that are unimportant, but the mechanics are geared toward removing them from the models for all responses.\nOverall tests In a one-way ANOVA, to control for multiple testing, it is common practice to carry out an overall \\(F\\)-test to see if the group means differ collectively before testing comparison between specific groups. Similarly, in univariate multiple regression, researchers sometimes report an overall \\(F\\)-test or test of the \\(R^2\\) so they can reject the hypothesis that all predictors have no effect, before considering them individually.\nSimilarly, the the case of multivariate linear models, some consider it necessary to reject the multivariate null hypothesis for a predictor term before considering how it contributes to each of the response variables. Some further suggest that the individual univariate models be tested after an overall significant effect. I believe the first of these is wise, but the second might be too much to require when the general goal is to understand the data.\n\n10.6.1 Example: NLSY data\nThe dataset NLSY comes from a small part of the National Longitudinal Survey of Youth, a series of annual surveys conducted by the U.S. Department of Labor to examine the transition of young people into the labor force. This particular subset gives measures of 243 children on mathematics and reading achievement and also measures of behavioral problems (antisocial, hyperactivity). Also available are the yearly income and education of the child’s father.\nIn this analysis the math and read scores are taken at the outcome variables.7 Among the remaining predictors, income and educ might be considered as background variables necessary to control for. Interest might then be focused on whether the behavioral variables antisoc and hyperact contribute beyond that.\n\ndata(NLSY, package = \"heplots\")\nstr(NLSY)\n#&gt; 'data.frame':  243 obs. of  6 variables:\n#&gt;  $ math    : num  50 28.6 50 32.1 21.4 ...\n#&gt;  $ read    : num  45.2 28.6 53.6 34.5 22.6 ...\n#&gt;  $ antisoc : int  4 0 2 0 0 1 0 1 1 4 ...\n#&gt;  $ hyperact: int  3 0 2 2 2 0 1 4 3 5 ...\n#&gt;  $ income  : num  52.52 42.6 50 6.08 7.41 ...\n#&gt;  $ educ    : int  14 12 12 12 14 12 12 12 12 9 ...\n\nExploratory plots\nTo begin, I would examine some scatterplots and univariate displays. I’ll start with density plots for all the variables to see the shapes of their distributions, wikh rug plots at the bottom to show where the observations are located. From Figure 10.11 we see that math and reading scores are positively skewed, anti-social and hyperactivity have distributions highly concentrated in the lower scores. As we would suspect, father’s income is quite positively skewed. Father’s education is reasonably symmetric, but highly peaked at 12 years of schooling in this sample. The spikes reflect the fact that education is measured in discrete years.\n\nNLSY_long &lt;- NLSY |&gt; \n  tidyr::pivot_longer(math:educ, names_to = \"variable\") |&gt;\n  dplyr::mutate(variable = forcats::fct_inorder(variable))\n\nggplot(NLSY_long, aes(x=value, fill=variable)) +\n  geom_density(alpha = 0.5) +\n  geom_rug() +\n  facet_wrap(~variable, scales=\"free\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\nFigure 10.11: Density plots for the variables in the NLSY dataset.\n\n\n\n\nIn terms of an analysis focused on math and read as outcomes, a scatterplot of one against the other is useful, as is collection of scatterplots of each against the remaining variables. The second of these is left as an exercise to the reader.\n\nset.seed(47)\nggplot(NLSY, aes(x = read, y = math)) +\n  geom_jitter()+\n  geom_smooth(method = lm, formula = y~x, fill = \"blue\", alpha = 0.2) +\n  geom_smooth(method = loess, se = FALSE, color = \"red\", linewidth = 2)\n\n\n\n\n\n\nFigure 10.12: Scatterplot of mathematics score against reading score in the NLSY data\n\n\n\n\nThe non-linear trend in Figure 10.12 may be due to the sparsity of data in the upper range of reading, and there are also a few unusual points shown in this plot. The function heplots::noteworthy() provides a variety of methods to identify such noteworthy points in scatterplots. The default method uses Mahalanobis \\(D^2\\). The plot below labels the five largest observations.\n\nids &lt;- heplots::noteworthy(NLSY[, 1:2], method = \"mahal\", n=5)\nggplot(NLSY, aes(x = read, y = math)) +\n  geom_jitter()+\n  geom_smooth(method = lm, formula = y~x, fill = \"blue\", alpha = 0.2) +\n  geom_text(data = NLSY[ids, ], label = ids, size = 5, nudge_y = 2) \n\n\n\n\n\n\nFigure 10.13: Scatterplot of mathematics score against reading score in the NLSY data, highlighting noteworthy points\n\n\n\n\nFitting models\nWe could of course include all of the predictors in a single model, and perhaps be done with it. To develop some model-thinking, it is more useful to proceed in smaller steps to see what we can learn from each. If we view parents’ income and education as the most obvious predictors of reading and mathematics scores, those are the variables to fit first.\n\n\n\n\n\nNLSY.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY)\n\nAnova(NLSY.mod1)  # Type II, partial test\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;        Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; income  1    0.0345     4.27      2    239 0.0151 * \n#&gt; educ    1    0.0515     6.49      2    239 0.0018 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nOverall test\nThe Anova() results above give multivariate tests of the contributions of each predictor separately to explaining reading and math and how they vary together. To get an overall test of the global null hypothesis \\(\\mathcal{H}_0 : \\mathbf{B} =(\\boldsymbol{\\beta}_{\\text{inc}}, \\boldsymbol{\\beta}_{\\text{educ}}) =\\mathbf{0}\\) for all predictors together, you can use linearHypothesis():\n\ncoefs &lt;- rownames(coef(NLSY.mod1))[-1]\nlinearHypothesis(NLSY.mod1, coefs, title = \"income, educ = 0\") |&gt; \n  print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: income, educ = 0\n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            2     0.117     7.44      4    480 8.1e-06 ***\n#&gt; Wilks             2     0.884     7.59      4    478 6.2e-06 ***\n#&gt; Hotelling-Lawley  2     0.130     7.75      4    476 4.7e-06 ***\n#&gt; Roy               2     0.123    14.79      2    240 8.7e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis joint multivariate test is more highly significant than either of those for the separate effects of the predictors, again because it pools strength.\nCoefficient plots\nAs usual, you can display the coefficients using coef(). The tidy method for \"mlm\" objects defined in heplots shows these in a tidy format with \\(t\\)-tests for each coefficient., arranged by the response variable.\n\ncoef(NLSY.mod1)\n#&gt;                read   math\n#&gt; (Intercept) 15.8848 8.7829\n#&gt; income       0.0137 0.0893\n#&gt; educ         0.9495 1.2755\n\ntidy(NLSY.mod1)\n#&gt; # A tibble: 6 × 6\n#&gt;   response term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 read     (Intercept)  15.9       4.25       3.74  0.000233\n#&gt; 2 read     income        0.0137    0.0325     0.420 0.675   \n#&gt; 3 read     educ          0.949     0.360      2.64  0.00894 \n#&gt; 4 math     (Intercept)   8.78      4.33       2.03  0.0437  \n#&gt; 5 math     income        0.0893    0.0331     2.70  0.00749 \n#&gt; 6 math     educ          1.28      0.367      3.47  0.000607\n\nHowever, a bivariate plot of these coefficients is more useful, because it provides visual tests of multivariate hypotheses. heplots::coefplot.mlm() gives displays of the coefficients for a given pair of response variables. For interpretation, it adds the bivariate confidence ellipse for the coefficients, as well as univariate confidence intervals for each response. The univariate intervals are simply the horizontal and vertical shadows of the ellipses on the response variable axes.\nA wrinkle here, as in Section 6.3, is that the coefficients are measured in different units and so coefficient plots for different predictors are more easily compared for standardized variables. To do this, I first re-fit the model using scale(NLSY) …\n\nNLSY_std &lt;- scale(NLSY) |&gt;\n  as.data.frame()\n\nNLSY_std.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY_std)\n\n\ncoefplot(NLSY_std.mod1, fill = TRUE,\n   col = c(\"darkgreen\", \"brown\"),\n   lwd = 2,\n   cex.lab = 1.5,\n   ylim = c(-0.1, 0.5),\n   xlab = \"read coefficient (std)\",\n   ylab = \"math coefficient (std)\")\n\n\n\n\n\n\nFigure 10.14: Bivariate coefficient plot for reading and math with 95% confidence ellipses. The variables have been standardized to make their units comparable.\n\n\n\n\nIn Figure 10.14, the confidence ellipses for income and educ both exclude the origin, which represents the multivariate hypothesis \\(\\mathcal{H}_0 : ( \\beta_\\textrm{read}, \\beta_\\textrm{math} ) = (0, \\, 0)\\), so this hypothesis is rejected. Note that if we only examined the univariate tests for each of the four parameters, we would conclude that for reading, income is not a significant predictor. The orientation of the confidence ellipses indicates the positive correlation between reading and mathematics scores.\n\n10.6.1.1 Behavioral measures\nGiven that the parental background variables are highly predictive of student performance, we might want to know if the behavioral measures antisoc and hyperact add importantly to this. One way to do this is to add these predictors to the model and test for their additional contributions over and above the baseline model.\nYou can do this using update(). In the model formula, “.” on the left hand side corresponds to the previous \\(y\\) variables; on the right-hand side it refers to the \\(x\\)s in the previous model, so I just add the new predictors to that.\n\nNLSY.mod2 &lt;- update(NLSY.mod1, . ~ . + antisoc + hyperact)\nAnova(NLSY.mod2)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;          Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; income    1    0.0383     4.72      2    237 0.0098 **\n#&gt; educ      1    0.0532     6.65      2    237 0.0015 **\n#&gt; antisoc   1    0.0193     2.34      2    237 0.0988 . \n#&gt; hyperact  1    0.0144     1.74      2    237 0.1784   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nEach of these new predictors are individually non-significant according to the Type II tests. Using linearHypothesis() you can test them jointly:\n\ncoefs &lt;- rownames(coef(NLSY.mod2))[-1] |&gt; print()\n#&gt; [1] \"income\"   \"educ\"     \"antisoc\"  \"hyperact\"\n\nlinearHypothesis(NLSY.mod2, coefs[3:4], \n                 title = \"NLSY.mod2 | NLSY.mod1\") |&gt;\n  print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: NLSY.mod2 | NLSY.mod1\n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; Pillai            2     0.024     1.45      4    476  0.218  \n#&gt; Wilks             2     0.976     1.44      4    474  0.218  \n#&gt; Hotelling-Lawley  2     0.024     1.44      4    472  0.218  \n#&gt; Roy               2     0.022     2.64      2    238  0.073 .\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n10.6.2 Example: School data\nCharnes et al. (1981) describe a large scale “social experiment” in public school education. Seventy school sites across the U.S. participated and a number of variables related to attributes of parents and teachers were used to predict aspects of students’ success in academic indicators (reading, mathematics), but also in their self-esteem. It was conceived in the late 1960’s in relation to a federally sponsored program charged with providing remedial assistance to educationally disadvantaged early primary school students.\nThe study was focused on the management styles used to guide educational planning across schools. In particular, it was primarily designed to compare schools using Program Follow Through (PFT) management methods of taking actions to achieve goals with those of Non Follow Through (NFT).\nHere, I simply focus on the relations between outcome scores on tests of reading, mathematics and self-esteem in relation to five explanatory variables related to parents and teachers:\n\n\neducation level of mother as measured by the percentage of high school graduates among female parents.\n\noccupation, highest occupation of a family member on a rating scale.\n\nvisit, an index of the number of parental visits to the school site.\n\ncounseling, a measure calculated from data on time spent with child on school-related topics such as reading together, etc.\n\nteacher, number of teachers at the given site.\n\nThe dataset, given in schooldata contains observations for 70 schools.8\nExploratory plots\nThere are eight variables in this example, so a scatterplot matrix or even a corrgram might not be sufficiently revealing. As usual, I tried a number of different methods and found a couple that were interesting and useful.\nMultivariate normality is not required for all the variables in \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\)— it is only required for the residuals, \\(\\boldsymbol{\\Large\\varepsilon}= \\mathbf{Y} - \\widehat{\\mathbf{Y}}\\). Yet, for MMRA problems, sometimes an initial \\(\\chi^2\\) QQ plot provides a handy way to flag possibly unusual values to pay attention to as the analysis proceeds. In Figure 10.15 we see five cases outside the 95% confidence envelope.\n\ndata(schooldata, package = \"heplots\")\nres &lt;- cqplot(schooldata, id.n = 5) |&gt; print()\n#&gt;     DSQ quantile       p\n#&gt; 59 44.6     21.0 0.00714\n#&gt; 44 38.8     18.0 0.02143\n#&gt; 33 27.9     16.5 0.03571\n#&gt; 66 24.0     15.5 0.05000\n#&gt; 35 21.7     14.7 0.06429\n\n# save the case ID numbers\noutliers &lt;- rownames(res) |&gt; as.numeric() |&gt; print()\n#&gt; [1] 59 44 33 66 35\n\n\n\n\n\n\nFigure 10.15: \\(\\chi^2\\) QQ plot of the schooldata variables.\n\n\n\n\nRather than a complete \\(8 \\times 8\\) scatterplot matrix, it is useful here to examine the scatterplots only for each \\(y\\) variable against each of the predictors in \\(\\mathbf{X}\\).9 I’ll take steps to flag some of these possibly unusual cases to see where they appear in these pairwise relations.\nTo prepare for this with ggplot2, it is necessary to reshape the data to long format twice—once for the (\\(q=5\\)) \\(x\\) variables and again for the (\\(p=3\\)) \\(y\\) responses to get all of their \\(q \\times p\\) combinations. That way, we get a data set with variables x and y whose variable names are by xvar and yvar.\n\n# plot predictors vs each response\nxvars &lt;- names(schooldata)[1:5]\nyvars &lt;- names(schooldata)[6:8]\n\nschool_long &lt;- schooldata |&gt;\n  tibble::rownames_to_column(var = \"site\") |&gt;\n  pivot_longer(cols = all_of(xvars), \n               names_to = \"xvar\", values_to = \"x\") |&gt;\n  pivot_longer(cols = all_of(yvars), \n               names_to = \"yvar\", values_to = \"y\") |&gt;\n  mutate(xvar = factor(xvar, xvars), \n         yvar = factor(yvar, yvars))\n\ncar::some(school_long, n=8)\n#&gt; # A tibble: 8 × 5\n#&gt;   site  xvar           x yvar            y\n#&gt;   &lt;chr&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n#&gt; 1 1     teacher     9    reading     54.5 \n#&gt; 2 18    education  28    mathematics 38.2 \n#&gt; 3 26    teacher     7    selfesteem  31.2 \n#&gt; 4 49    occupation  5.29 reading     12.2 \n#&gt; 5 53    counseling 26.3  mathematics 22.0 \n#&gt; 6 61    occupation  2.59 mathematics  7.1 \n#&gt; 7 62    visit       9.89 reading      9.35\n#&gt; 8 64    occupation  8.91 mathematics 24.5\n\nWith this data structure, each scatterplot is a plot of a y against and x, and we can facet this using facet_grid(yvar ~ xvar), giving Figure 10.16.\n\np1 &lt;- ggplot(school_long, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x) +\n  stat_ellipse(geom = \"polygon\", \n               level = 0.95, fill = \"blue\", alpha = 0.2) +\n  facet_grid(yvar ~ xvar, scales = \"free\") +\n  labs(x = \"predictor\", y = \"response\") +\n  theme_bw(base_size = 16)\n\n# label the 3 most unusual points in each panel\np1 + geom_text_repel(data = school_long |&gt; \n                       filter(site %in% outliers[1:3]), \n                     aes(label = site))\n\n\n\n\n\n\nFigure 10.16: Scatterplots of each of the three response variables against each of the five predictors in the schooldata dataset. Three of the points identified as possible multivariate outliers are labeled.\n\n\n\n\nAll of the predictors except for number of teachers show very strong linear relations with the outcome scores. Among the identified points, cases 44 and 59 stand out in all the plots, case 59 being particularly high on all the measures. As well, there is a small cluster of unusual points in the plots for number of teachers.\nFiting models\nLet’s proceed to fit the multivariate regression model. Here “.” on the right-hand side of the model formula means all the other variables in the dataset.\n\nschool.mod &lt;- lm(cbind(reading, mathematics, selfesteem) ~ ., \n                 data=schooldata)\ncar::Anova(school.mod)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;            Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; education   1     0.376    12.43      3     62 1.8e-06 ***\n#&gt; occupation  1     0.567    27.02      3     62 2.7e-11 ***\n#&gt; visit       1     0.260     7.27      3     62 0.00029 ***\n#&gt; counseling  1     0.065     1.43      3     62 0.24297    \n#&gt; teacher     1     0.049     1.07      3     62 0.37003    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThese multivariate tests have a seemingly simple interpretation: parent’s education and occupation and their visits to the schools are highly predictive of student’s outcomes; their counseling efforts and the number of teachers in the schools, not so much.\nYou can get an assessment of the strength of multivariate association from the \\(R^2\\) for each of the responses using glance() for the MLM. All of these are very high.\n\nglance(school.mod)\n#&gt; # A tibble: 3 × 8\n#&gt;   response    r.squared sigma fstatistic numdf dendf  p.value  nobs\n#&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 reading         0.929  4.83       167.     5    64 2.34e-35    70\n#&gt; 2 mathematics     0.917  6.16       141.     5    64 3.37e-33    70\n#&gt; 3 selfesteem      0.993  1.17      1852.     5    64 8.47e-68    70\n\nSimilarly etasq() for an MLM gives an \\(R^2\\)-like measure called \\(\\eta^2\\) of the partial association accounted for each of the predictor terms in the model. These are analogous to the Type II tests from Anova(), which test the additional contribution of each term in the model beyond all the others.\nIn spite of the overwhelming significance of the first three predictors, their variance accounted for is more modest. It is highest for parent’s occupation, followed by education. Parent counseling and teachers contribute very little.\n\netasq(school.mod)\n#&gt;             eta^2\n#&gt; education  0.3756\n#&gt; occupation 0.5666\n#&gt; visit      0.2603\n#&gt; counseling 0.0647\n#&gt; teacher    0.0491",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#sec-model-disgnostics-MLM",
    "href": "10-mlm-review.html#sec-model-disgnostics-MLM",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.7 Model diagnostics for MLMs",
    "text": "10.7 Model diagnostics for MLMs\nModel building, visualization and interpretation is often an iterative process. You fit a model and calculate some goodness of fit measures (\\(R^2\\) for responses, \\(\\eta^2\\) for predictors). If these are reasonably strong, you feel happy and proceed to graphical displays to help you understand what you’ve found and explain it to others.\nBut wait: did you check the assumptions of the MLM? As in univariate models, diagnostic plots can help you spot problems in the data (unusual cases) or in the model (nonlinear relations, omitted predictors or interactions). You sometimes need to go circle back and fit a revised model, starting the process again.\n\n\nFor multivariate regression models, I consider the assumed multivariate normality of residuals and multivariate influence here. For MANOVA models, the question of homogeneity of covariance matrices is deferred until Chapter 12.\n\n10.7.1 Multivariate normality of residuals\nOne easy thing to do is to check for multivariate normality of the residuals. Given that we found a few noteworthy points in Figure 10.15, a \\(\\chi^2\\) QQ plot of the residuals in the model will tell us if any of these are really problematic. The pattern of points relative to the confidence band gives a rough indication of overall multivariate normality.\n\ncqplot(school.mod, id.n = 5)\n\n\n\n\n\n\nFigure 10.17: \\(\\chi^2\\) QQ plot of the residuals in the schooldat multivariate regression model.\n\n\n\n\nSo, you can see that among the cases that stood out in the cqplot() of the observed variables (Figure 10.15), only case 35 attracts attention here, and it is well within the confidence band. Case 59, which was the largest in all the pairwise scatterplots (Figure 10.16) seems not unusual in the fitted model. It is a high-leverage point, but appeared to be well-fitted in all the simple regressions, except in those for teacher.\nIt is useful to contrast this with what we get from formal tests that the residuals are strictly multivariate normal. The MVN package package (Korkmaz et al., 2014) provides mvn() for this, which performs a wide variety of normality tests. The most widely used of these is due to Mardia (1974), which gives multivariate tests of skewness (lack of symmetry) and kurtosis (length of the tails).\nApplying this to the residuals from the schools multivariate regression shows that multivariate normality is rejected here. Based on other evidence, this doesn’t seem particularly troubling.\n\nschool.mvn &lt;- mvn(residuals, mvn_test = \"mardia\")\n# print multivariate and univariate tests\nsummary(school.mvn, select = \"mvn\")\n#&gt;              Test Statistic p.value     Method      MVN\n#&gt; 1 Mardia Skewness      1.92   0.750 asymptotic ✓ Normal\n#&gt; 2 Mardia Kurtosis     -1.16   0.244 asymptotic ✓ Normal\nsummary(school.mvn, select = \"univariate\")\n#&gt;               Test Variable Statistic p.value Normality\n#&gt; 1 Anderson-Darling    start     0.307   0.524  ✓ Normal\n#&gt; 2 Anderson-Darling   amount     0.625   0.085  ✓ Normal\n\nmvn() also provides a variety of tests for univariate normality for each of the response variables. These are all OK.\n\n10.7.2 Distance plot\nAnother useful screening plot (suggested by Rousseeuw et al. (2004)) is a plot of Mahalanobis distances of the predictors against the Mahalanobis distances of the corresponding residuals for a fitted model. This diagnostic plot combines the information on leverage points and regression outliers in an interesting way.\nIt is much more useful than plotting them individually (against the theoretical values) as in a \\(\\chi^2\\) QQ plot. Moreover, plotting them against each other is visually informative, because it places the leverage points (unusual in \\(\\mathbf{X}\\)}) and the outliers (unusual in \\(\\mathbf{Y}\\)}) in different regions of the plot. To judge notably “large” values, they suggest using cutoffs of \\(\\sqrt{\\chi^2_{p}(0.975)}\\) for the predictor distances and \\(\\sqrt{\\chi^2_{q}(0.975)}\\) for the residuals.\nSuch plots are produced by heplots::distPlot(). Running this on the school.mod model gives Figure 10.18. Points beyond the horizontal and vertical cutoff values are labeled with their case numbers.\n\ndistancePlot(school.mod, cex = 1.5, cex.lab = 1.2)\n#&gt; 0.975 X, Y distance cutoffs: 3.58 3.06\n\n\n\n\n\n\nFigure 10.18: Plot of Mahalanobis distances of the least squares residuals vs. Mahalanobis distances of the predictors in the model\n\n\n\n\nMost of the points identified in the \\(\\chi^2\\) QQ plot Figure 10.15 are labeled here. Cases 44 and 59 are high leverage points with large \\(\\mathbf{X}\\) distances. Case 35 is the only one beyond the cutoff for residuals. Interestingly, case 66 appeared near 35 in Figure 10.15, but is unusual for a different reason as we can see in Figure 10.18.\nRousseeuw et al. (2004) also suggested methods of robust multivariate regression using robust estimates of location and scatter rather than the classical \\(\\overline{\\mathbf{y}}\\) and \\(\\mathbf{S}\\). The minimum covariance determinant (MCD) estimator is a robust estimator with high breakdown value and bounded influence. It looks for the subset of size \\(h\\), whose covariance matrix has the smallest determinant, where \\(h : \\lceil n/2 \\rceil &lt; h &lt; n\\) controls the robustness. The distancePlot() function implements this, as well as a minimum variance ellipse (MVE) method. Some robust methods are illustrated in Section 13.5.\n\n10.7.3 Multivariate influence\nTODO Sort out coverage here vs. Chapter 13\nAgain, what we see in simple scatterplots can be misleading because they ignore all the other variables in a model. But looking back after fitting a model and examining diagnostic plots can often be illuminating. Among the ideas we inherit from univariate models, the influence of particular observations on the results of analysis should be high on your list.\nThe multivariate extension of the diagnostic measures of leverage and influence, and influence plots (Section 6.6) is provided by the mvinfluence package package (Friendly, 2025). The theory behind this is due to Barrett & Ling (1992) and better illustrated in Barrett (2003). Mathematical details of this generalization are given in help(\"mvinfluence-package\").\nFigure 10.19 shows one form of an influence plot for the school.mod model. Because multiple response variables are involved, this plots a measure of the squared studentized residuals for each observation against a generalized version of hat values, so potentially “bad” observations appear in the upper left corner. The size of the bubble symbol is proportional to a generalization of Cook’s distance, the measure of influence based on the change in all coefficients if each case was deleted from the analysis.\n\ninfluencePlot(school.mod, id.n=4, \n              type=\"stres\",\n              cex.lab = 1.5)\n#&gt;        H      Q  CookD     L      R\n#&gt; 33 0.328 0.2017 0.7054 0.488 0.3001\n#&gt; 35 0.101 0.2825 0.3038 0.112 0.3142\n#&gt; 44 0.503 0.2984 1.6022 1.014 0.6009\n#&gt; 59 0.568 0.4937 2.9938 1.317 1.1441\n#&gt; 66 0.355 0.0174 0.0657 0.551 0.0269\n\n\n\n\n\n\nFigure 10.19: Influence plot for the schooldat multivariate regression model. Five cases are labeled as “noteworthy” on either axis.\n\n\n\n\nThat does not look good! You can see that cases 44 and 59 are actually quite troublesome here, but it turns out for different reasons. Take another look at Figure 10.16. You can see that case 59 is the most extreme on all the predictors, giving it very high leverage and therefore pulling the regression lines toward it in most of the plots, except those for number of teachers, where it also has large residuals. Case 44, on the other hand, stands out as high-leverage on only a few of the predictor–response combinations, but enough to give it a large multivariate hat value. It is also a point that is furthest from the regression lines.\nWhat should be done? An appropriate action would be to re-fit the model, reducing the impact of these cases, in what I call a sensitivity test:\n\nDo the main conclusions change with those cases removed? That is, do any model terms change in significance tests or do coefficients change sign?\nDo the relative sizes of effects for predictors change enough to affect interpretation? That is, how much do the coefficients change?\n\nThe easiest solution is to just omit these troubling cases. You can do this using update(), specifying the data argument to be the subset of rows without the bad boys.\n\nbad &lt;- c(44, 59)\nOK &lt;- (1:nrow(schooldata)) |&gt; setdiff(bad)\nschool.mod2 &lt;- update(school.mod, data = schooldata[OK,])\nAnova(school.mod2)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;            Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; education   1     0.211     5.35      3     60  0.0025 ** \n#&gt; occupation  1     0.422    14.62      3     60 2.9e-07 ***\n#&gt; visit       1     0.191     4.73      3     60  0.0050 ** \n#&gt; counseling  1     0.053     1.13      3     60  0.3459    \n#&gt; teacher     1     0.064     1.37      3     60  0.2618    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe results of Anova() on this model tell us that the three significant predictors— occupation, education and visit— are still so, but slightly less so for the last two of these. To compare the coefficients in the new model compared to the old you can calculate the relative difference, \\(| \\mathbf{B}_1 - \\mathbf{B}_2 | / \\mathbf{B}_1\\), which are\n\nreldiff &lt;- function(x, y, pct=TRUE) {\n  res &lt;- abs(x - y) / x\n  if (pct) res &lt;- 100 * res\n  res\n}\n\nreldiff(coef(school.mod)[-1,], coef(school.mod2)[-1,]) |&gt;\n  round(1)\n#&gt;            reading mathematics selfesteem\n#&gt; education     10.1        19.8      -75.7\n#&gt; occupation    11.0        10.2       19.4\n#&gt; visit       -103.6       -76.8        2.8\n#&gt; counseling  -332.2       305.3     -115.4\n#&gt; teacher       -7.5        -5.6        7.0\n\nAs you can see, the effects on the coefficients for visit and counseling` are dramatic.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#sec-ANCOVA-MANCOVA",
    "href": "10-mlm-review.html#sec-ANCOVA-MANCOVA",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.8 ANCOVA \\(\\rightarrow\\) MANCOVA",
    "text": "10.8 ANCOVA \\(\\rightarrow\\) MANCOVA\nTODO: Consider moving this to Chapter 11 and use much of the heplots MMRA vignette.\nIn univariate linear models, analysis of covariance (ANCOVA) is most often used in a situation where we want to compare the mean response, \\(\\bar{y}_j\\) for different groups (defined by one or more factors), but where there are one or more quantitative predictors \\(\\mathbf{x}_1, \\dots\\) that should be taken into account for our comparisons to make sense. The simplest case is when \\(\\mathbf{x}\\) is a pre-test score on the same measure, or when it is a background measure like age or level of education that we want to control for, to adjust for differences among the groups.\nMore generally, ANCOVA and its’ multivariate MANCOVA brother are used for situations where the model matrix \\(\\mathbf{X}\\) contains a mixture of factor variables and quantitative predictors, called “covariates”. In this wider context, there are two flavors of analysis with different emphasis on the factors or the covariates:\n\ntrue ANCOVA/MANOVA: Attention is centered on the differences between the group means, but controlling for any difference in the covariate(s). This requires assuming that the slopes for the groups are all the same.\nhomogeneity of regression: Here the focus is on the regression relations between the \\(\\mathbf{y}\\)s and the predictor \\(\\mathbf{x}\\)s, but we might also want to determine if the regression slopes are the same for all groups defined by the factors.\n\nIn the ANCOVA flavor, the model fits additive effects of the group factor(s) and the covariate(s), while the homogeneity of regression flavor adds interaction terms between groups and the \\(\\mathbf{x}\\)s. The test for homogeneity of regression is the added effect of the interaction terms:\nmod1 &lt;- lm(y ~ Group + x)            # ANCOVA model\nmod2 &lt;- lm(y ~ Group + x + Group:x)  # allow separate slopes\nmod2 &lt;- lm(y ~ Group * x)            # same as above\n\nanova(mod1, mod2)            # test homogeneity of regression\n\nFigure 10.20 illustrates these cases for a hypothetical two-group design studying the the effect of an exercise program treatment on weight, recorded pre- (\\(x\\)) and post- (\\(y\\)) compared to a control group given no treatment. In panel (a) the slopes for the two groups are approximately equal, so the effect of treatment can be estimated by the difference in the fitted values of \\(\\hat{y}_i\\) at the average value of \\(x\\). In panel (b), the slope for the treated group is considerably greater than that for the control group, so the difference between the groups varies with \\(x\\).\n\n\n\n\n\n\n\nFigure 10.20: Two possible outcome patterns for a two-group design assessing the effect of a treatment on weight, measured pre- and post-treatment. (a) Additive effects of Group and \\(x\\); (b) Different slopes for the two groups. Plus signs show the means \\((\\bar{x}_i, \\bar{y}_i)\\) for the two groups.\n\n\n\n\n\n\n10.8.1 Example: Paired-associate tasks and academic performance\nTo what extent can simple tests of paired-associate learning10 predict measures of aptitude and achievement in kindergarten children? This was the question behind an experiment by William Rohwer at the University of California, Berkeley.\nThere were three outcome measures, one verbal and two visually-based:\n\nA student achievement test (SAT),\nPeabody Picture Vocabulary test (PPVT),\nRaven Progressive Matrices test (Raven).\n\nFour paired-associate tasks were used, which differed in the syntactic and semantic relationship between the stimulus and response terms in each pair. These are called named (n), still (s), named still (ns), named action (na), and sentence still (ss).\nRohwer’s data, taken from Timm (1975), is given in Rohwer. But there’s a MANCOVA wrinkle: Performance on the academic tasks is well-known to vary with socioeconomic status of the parents or the school they attend. A simple design was to collect data from children in two schools, one in a low SES neighborhood (\\(n=37\\)) and the other an upper-class high SES one (\\(n=32\\)). The data look like this:\n\n\ndata(Rohwer, package = \"heplots\")\nset.seed(42)\nRohwer |&gt; dplyr::sample_n(6)\n#&gt;    group SES SAT PPVT Raven n  s ns na ss\n#&gt; 49     2  Hi  88  105    21 2 11 10 26 22\n#&gt; 65     2  Hi  50   96    13 5  8 20 28 26\n#&gt; 25     1  Lo   6   57    10 0  1 16 15 17\n#&gt; 18     1  Lo  45   54    10 0  6  6 14 16\n#&gt; 69     2  Hi  50   78    19 5 10 18 27 26\n#&gt; 64     2  Hi  24  102    16 4 17 21 27 31\n\nFollowing the scheme for reshaping the data used in Figure 10.16, a set of scatterplots of each predictor against each response will give a useful initial look at the data. There’s a lot to see here, so the plot in Figure 10.21 focuses attention on the regression lines for the two groups and their data ellipses.\n\nCodeyvars &lt;- c(\"SAT\", \"PPVT\", \"Raven\" )      # outcome variables\nxvars &lt;- c(\"n\", \"s\", \"ns\", \"na\", \"ss\")   # predictors\n\nRohwer_long &lt;- Rohwer %&gt;%\n  dplyr::select(-group) |&gt;\n  tidyr::pivot_longer(cols = all_of(xvars), \n                      names_to = \"xvar\", values_to = \"x\") |&gt;\n  tidyr::pivot_longer(cols = all_of(yvars), \n                      names_to = \"yvar\", values_to = \"y\") |&gt;\n  dplyr::mutate(xvar = factor(xvar, levels = xvars),\n                yvar = factor(yvar, levels = yvars))\n\nggplot(Rohwer_long, aes(x, y, color = SES, shape = SES, fill = SES)) +\n  geom_jitter(size=0.8) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              formula = y ~ x, \n              linewidth = 1.5) +\n  stat_ellipse(geom = \"polygon\", alpha = 0.1) +\n  labs(x = \"Predictor (PA task)\",\n       y = \"Response (Academic)\") +\n  facet_grid(yvar ~ xvar,            # plot matrix of Y by X\n             scales = \"free\") +\n  theme_bw(base_size = 16) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 10.21: Scatterplots of each of the three response variables against each of the five predictors in the Rohwer dataset.\n\n\n\n\nYou can see here that the high-SES group generally performs better than the low group. The regression lines have similar slopes in some of the panels, but not all. The low SES group also appears to have larger variance on most of the PA tasks.\nMANCOVA model\nNevertheless, I fit the MANCOVA model that allows a test of different means for the two SES groups on the responses, but constrains the slopes for the PA covariates to be equal. Only two of the PA tasks (na and ns) show individually significant effects in the multivariate tests.\n\n# Make SES == 'Lo' the reference category\nRohwer$SES &lt;- relevel(Rohwer$SES, ref = \"Lo\")\n\nRohwer.mod1 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES + n + s + ns + na + ss, \n                  data=Rohwer)\nAnova(Rohwer.mod1)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;     Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; SES  1     0.379    12.18      3     60 2.5e-06 ***\n#&gt; n    1     0.040     0.84      3     60  0.4773    \n#&gt; s    1     0.093     2.04      3     60  0.1173    \n#&gt; ns   1     0.193     4.78      3     60  0.0047 ** \n#&gt; na   1     0.231     6.02      3     60  0.0012 ** \n#&gt; ss   1     0.050     1.05      3     60  0.3770    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nYou can also examine the tests of the univariate ANCOVA models for each of the responses using glance() or heplots::uniStats(). All are significantly related, but the PPVT measure has the largest \\(R^2\\) by far.\n\nuniStats(Rohwer.mod1)\n#&gt; Univariate tests for responses in the multivariate linear model Rohwer.mod1 \n#&gt; \n#&gt;         R^2     F df1 df2 Pr(&gt;F)    \n#&gt; SAT   0.295  4.33   6  62  0.001 ** \n#&gt; PPVT  0.628 17.47   6  62  1e-11 ***\n#&gt; Raven 0.211  2.76   6  62  0.019 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTo help interpret these effects, bivariate coefficient plots for the paired associate tasks are shown in Figure 10.22. (The coefficients for the group variable SES are on a different scale and so are omitted here.) From this you can see that the named still and named action tasks have opposite signs: contrary to expectations, ns is negatively associated with the measures of aptitude and achievement (when the other predictors are adjusted for).\n\ncoefplot(Rohwer.mod1, parm = 2:6,\n         fill = TRUE,\n         level = 0.68,\n         cex.lab = 1.5)\ncoefplot(Rohwer.mod1, parm = 2:6, variables = c(1,3),\n         fill = TRUE,\n         level = 0.68,\n         cex.lab = 1.5)\n\n\n\n\n\n\nFigure 10.22: Bivariate coefficient plots for the MANCOVA model with confidence ellipses of 68% coverage.\n\n\n\n\nTODO: For interpretation, it would be nice to know how many items were used for each of the PA tasks. The range of na goes up to 35, but others are less.\nAdjusted means\nFrom the analysis of covariance perspective, interest is often centered on estimating the differences between the group means, but adjusting or controlling for differences on the covariates. From the table of means below, you can see that the high SES group performs better on all three response variables, but this group also has higher scores on the paired associate tasks.\n\nmeans &lt;- Rohwer |&gt;\n  group_by(SES) |&gt;\n  summarise_all(mean) |&gt;\n  print()\n#&gt; # A tibble: 2 × 10\n#&gt;   SES   group   SAT  PPVT Raven     n     s    ns    na    ss\n#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Lo        1  31.3  62.6  13.2  2.54  6.92  13.5  22.4  18.4\n#&gt; 2 Hi        2  47.7  83.1  15    4.59  7.25  14.5  24.3  21.5\n\nThe adjusted mean differences are simply the values estimated by the coefficients for SES in the model. These are smaller than the differences between the observed means.\n\nmeans[2, 3:5] - means[1, 3:5]  \n#&gt;    SAT PPVT Raven\n#&gt; 1 16.4 20.4  1.76\n\n# adjusted means\ncoef(Rohwer.mod1)[2,]\n#&gt;   SAT  PPVT Raven \n#&gt;  8.80 16.88  1.59\n\nTODO: do this with a CI for the effects\nHomogeneity of regression\nThe MANCOVA model, Rohwer.mod1, has relatively simple interpretations (a large effect of SES, with ns and na as the major predictors) but the test of the SES effect relies on the assumption of homogeneity of slopes for the predictors. We can test this assumption as follows, by adding interactions of SES with each of the covariates:\n\nRohwer.mod2 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES * (n + s + ns + na + ss),\n                  data=Rohwer)\nAnova(Rohwer.mod2)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;        Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; SES     1     0.391    11.78      3     55 4.5e-06 ***\n#&gt; n       1     0.079     1.57      3     55 0.20638    \n#&gt; s       1     0.125     2.62      3     55 0.05952 .  \n#&gt; ns      1     0.254     6.25      3     55 0.00100 ***\n#&gt; na      1     0.307     8.11      3     55 0.00015 ***\n#&gt; ss      1     0.060     1.17      3     55 0.32813    \n#&gt; SES:n   1     0.072     1.43      3     55 0.24417    \n#&gt; SES:s   1     0.099     2.02      3     55 0.12117    \n#&gt; SES:ns  1     0.118     2.44      3     55 0.07383 .  \n#&gt; SES:na  1     0.148     3.18      3     55 0.03081 *  \n#&gt; SES:ss  1     0.057     1.12      3     55 0.35094    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt appears from the above that there is only weak evidence of unequal slopes from the separate SES: terms; only that for SES:na is individually significant. The evidence for heterogeneity is stronger, however, when these terms are tested collectively using linearHypothesis(). I use a small grep() trick here to find the interaction terms, which have a “:” in their names.\n\n# test interaction terms jointly\ncoefs &lt;- rownames(coef(Rohwer.mod2)) \ninteractions &lt;- coefs[grep(\":\", coefs)]\n\nprint(linearHypothesis(Rohwer.mod2, interactions), SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; Pillai            5     0.418     1.85     15    171 0.0321 * \n#&gt; Wilks             5     0.624     1.89     15    152 0.0277 * \n#&gt; Hotelling-Lawley  5     0.539     1.93     15    161 0.0240 * \n#&gt; Roy               5     0.385     4.38      5     57 0.0019 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSeparate models\nModel Rohwer.mod2 with all interaction terms essentially fits a separate slope for each of the low and high SES groups for all responses with each of the predictor PA tasks. This is similar in spirit to what we would get if we fit a separate multivariate regression model for each of the groups, but parameterized differently: The heterogeneous regression model gives, for the interaction terms estimates of the difference in slopes between groups, while the separate-regressions approach gives separate slope estimates for each of the groups. These are equivalent, in the sense that the estimates for each approach can be derived from the other.\nThey are not equivalent in testing however, because the full model uses a combined pooled within-group error covariance, allows hypotheses about equality of slopes and intercepts to be tested directly and has greater power because it uses the total sample size. Here, I simply illustrate the mechanics of fitting separate models using the subset argument to lm().\n\nRohwer.sesHi &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Hi\")\nAnova(Rohwer.sesHi)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; n   1     0.202     2.02      3     24 0.1376   \n#&gt; s   1     0.310     3.59      3     24 0.0284 * \n#&gt; ns  1     0.358     4.46      3     24 0.0126 * \n#&gt; na  1     0.465     6.96      3     24 0.0016 **\n#&gt; ss  1     0.089     0.78      3     24 0.5173   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRohwer.sesLo &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Lo\")\nAnova(Rohwer.sesLo)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; n   1    0.0384     0.39      3     29  0.764  \n#&gt; s   1    0.1118     1.22      3     29  0.321  \n#&gt; ns  1    0.2252     2.81      3     29  0.057 .\n#&gt; na  1    0.2675     3.53      3     29  0.027 *\n#&gt; ss  1    0.1390     1.56      3     29  0.220  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe strength of evidence for the predictors na and ns is weaker here than when tested in the full heterogeneous model.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#what-have-we-learned",
    "href": "10-mlm-review.html#what-have-we-learned",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.9 What have we learned?",
    "text": "10.9 What have we learned?\n\nThe multivariate linear model is an extension of the (univariate) general linear model. Three major forms of the multivariate linear model are the MANOVA, MMRA and MANCOVA, all of which have their equivalents in ANOVA, MRA and ANCOVA.\nWhat the multivariate linear model offers over the univariate linear model is not just more powerful statistical tests, but also examinations of how the associations amongst the response variables varies with the explanatory variables.\nJust as with the general linear model, model diagnostics transfer to the multivariate linear model and are important for identifying potential issues in the validity of the estimated statistical model.\nGiven that the multivariate linear model can do everything that the general linear model does, there is no reason to not consider the multivariate linear model. Confining discussions of multivariate response models to structural equation modeling may pedagogically create a cognitive blind spot that the association amongst response variables even can vary with the explanatory variables. Rather than something separate, for the purposes of causal inference, it would be better to use structural equation modeling as a more confirmatory statistical model than the multivariate linear model, which is perfectly well suited for exploratory purposes.\nOverall, we also see a glimpse of the importance of data visualization in the analysis of data using the multivariate linear model. The following chapter will explore this issue in more depth.\n\n\nPackages used here:\n14 packages used here: broom, car, carData, dplyr, ggplot2, ggpubr, ggrepel, heplots, knitr, matlib, mvinfluence, MVN, patchwork, tidyr\n\n\n\n\n\nBarrett, B. E. (2003). Understanding influence in multivariate regression. Communications in Statistics - Theory and Methods, 32(3), 667–680. https://doi.org/10.1081/STA-120018557\n\n\nBarrett, B. E., & Ling, R. F. (1992). General classes of influence measures for multivariate regression. Journal of the American Statistical Association, 87(417), 184–191. https://www.jstor.org/stable/i314301\n\n\nCharnes, A., Cooper, W. W., & Rhodes, E. (1981). Evaluating program and managerial efficiency: An application of data envelopment analysis to program follow through. Management Science, 27(6), 668–697. http://www.jstor.org/stable/2631155\n\n\nFriendly, M. (2025). Mvinfluence: Influence measures and diagnostic plots for multivariate linear models. https://github.com/friendly/mvinfluence\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nHarrell, F. E. (2015). Regression modeling strategies: With applications to linear models, logistic and ordinal regression, and survival analysis. Springer International Publishing. https://books.google.ca/books?id=sQ90rgEACAAJ\n\n\nJohnson, R., & Wichern, D. (1998). Applied multivariate statistical analysis (4th ed.). Prentice Hall.\n\n\nKay, M. (2025). ggdist: Visualizations of distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKorkmaz, S., Goksuluk, D., & Zararsiz, G. (2014). MVN: An r package for assessing multivariate normality. The R Journal, 6(2), 151–162. https://journal.r-project.org/archive/2014-2/korkmaz-goksuluk-zararsiz.pdf\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis with applications. Biometrika, 57(3), 519–530. https://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMardia, K. V. (1974). Applications of some measures of multivariate skewness and kurtosis in testing normality and robustness studies. Sankhya: The Indian Journal of Statistics, Series B, 36(2), 115–128. http://www.jstor.org/stable/25051892\n\n\nMeyers, L. S., Gamst, G., & Guarino, A. J. (2006). Applied multivariate research: Design and interpretation. SAGE Publications.\n\n\nOlson, C. L. (1974). Comparative robustness of six tests in multivariate analysis of variance. Journal of the American Statistical Association, 69(348), 894–908. https://doi.org/10.1080/01621459.1974.10480224\n\n\nPlaster, M. E. (1989). The effect of defendent physical attractiveness on juridic decisions using felon inmates as mock jurors [Unpublished master's thesis]. East Carolina University.\n\n\nRousseeuw, P. J., Van Aelst, S., Van Driessen, K., & Gulló, J. A. (2004). Robust multivariate regression. Technometrics, 46(3), 293–305. https://doi.org/10.1198/004017004000000329\n\n\nSchatzoff, M. (1966). Sensitivity comparisons among tests of the general linear hypothesis. Journal of the American Statistical Association, 61(314), 415–435. https://doi.org/10.1080/01621459.1966.10480878\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52(3–4), 591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B: Methodological, 58, 267–288.\n\n\nTimm, N. H. (1975). Multivariate analysis with applications in education and psychology. Wadsworth (Brooks/Cole).\n\n\nWarne, F. T. (2014). A primer on multivariate analysis of variance(MANOVA) for behavioral scientists. Practical Assessment, Research & Evaluation, 19(1). https://scholarworks.umass.edu/pare/vol19/iss1/17/\n\n\nYee, T. W. (2015). Vector generalized linear and additive models: With an implementation in r. Springer.\n\n\nYee, T. W. (2025). VGAM: Vector generalized linear and additive models. https://CRAN.R-project.org/package=VGAM",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#footnotes",
    "href": "10-mlm-review.html#footnotes",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "There’s a bit of a puzzle here, and therefore a gap in methods and therefore an opportunity. The classical linear models fit by lm() extend naturally to non-gaussian data via glm() which provides for other families (binary: Bernoulli, count data: Poisson). The standard lm() extends quite naturally to a multivariate responses. Yet the combination of these ideas— non-gaussian multivariate models— remains elusive. The VGAM package (Yee (2015), Yee (2025)) handles the bivariate cases of logistic (and probit) regression, but not much more. See Friendly & Meyer (2016), Sec. 10.4 for an example and graphs of odds ratios in these models. There is a lot more to do on this topic.↩︎\nA slight hiccup in notation is that the uppercase for the Greek Beta (\\(\\boldsymbol{\\beta}\\)) is the same as the uppercase Roman \\(\\mathbf{B}\\), so I use \\(\\mathbf{b}_1 , \\mathbf{b}_2 , \\dots\\) below to refer to its’ columns.↩︎\nAmong the extensions of the classical OLS framework, errors in variables models attempt to correct for the downward bias of measurement error by adjusting for the known or estimated error variances in the independent variables, providing more accurate and consistent estimations.↩︎\nLine plots like this are nearly always more understandable with labels directly on the lines, rather than in a legend. Wrappers for ggplot2 simplify some things but can’t easily accommodate this kind of customization.↩︎\nPlotting the means plus and minus one standard error of the mean is often the simplest approach to visualizing the uncertainty associated with each estimate. But this practice can create problems of interpretation, such as concluding that means differ if their intervals do not overlap. Some solutions are to encode \\(p\\)-values for comparisons in a plot, for example using ggpubr::geom_pwc() for pairwise comparisons, or to use error bars with several widths representing difference confidence levels, as provided by the ggdist package package (Kay, 2025).↩︎\n“Automatic” model selection procedures like stepwise regression, while seemingly attractive are dangerous in that can increase false positives or drop variables that should, on logical grounds, be included in the model. See Stepwise selection of variables in regression is Evil and Harrell (Harrell, 2015).↩︎\nOther choices are possible: we could instead try to model the behavioral variables, antisocial and hyperact first, and then determine if the parental variables add appreciably to this. Modeling choices aren’t arbitrary. They should reflect the aims of a study and the story you want to tell about the result.↩︎\nIn this, schools 1–49 were PFT sites and the remaining sites 50–70 were NFT. A separate dataset schoolsites provides other information on the schools, such as the general education style, region of the U.S., size of the city and that of the student population.↩︎\nGGally has a function ggduo() that does something similar, plotting each of one set of variables against another. Like ggpairs() it allows for generalizations of a scatterplot where combinations of discrete factors and continuous variables can be displayed with appropriate visualizations for each.↩︎\nPaired-associate learning are among the simplest tests of memory and learning. The subject is given a list of pairs of words or nonsense syllables, like “banana - house” or “YYZ - Toronto” to learn. On subsequent trials she is given the stimulus term of each pair (“banana”, “YYZ”) and asked to reply with the correct response (“house”, “Toronto”).↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html",
    "href": "11-mlm-viz.html",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "11.1 HE plot framework\nThe methods discussed in Chapter 10 provide the basis for a rather complete multivariate analysis of traditional univariate methods for the same designs. You can carry out multiple regression, ANOVA, or indeed, any classical linear model with the standard collection of analysis tools you use for a single outcome variable, but naturally extended in most cases to having several outcomes to analyse together. The key points are:\nAs nice as these mathematical and statistical ideas might be, the fact that the analysis is conducted for the response variables collectively, means that it may be harder to interpret and explain what this means about the separate responses. Here’s where multivariate model visualization comes to the rescue!\nHuang (2019), in an article titled, “MANOVA: A Procedure Whose Time Has Passed?” (and others) criticized these methods as (a) difficult to understand because they are framed in terms of linear combinations of the the responses; (b) more complicated and limited in interpreting MANOVA effects and (c) unwieldy post hoc strategies often employed for interpretation.\nBut that’s just you should expect to happen if you rely on tables of coefficients or ANOVA summary tables, even with significance stars (* … ***) to help interpret what is important. (You should be looking at effect sizes and practical significance, right?)\nThese difficulties in understanding multivariate models can, I believe, be cured by accessible graphical methods for visualizing hypothesis tests and for visualizing what these linear combinations reflect in terms of the observed variables. The HE plot framework described below provides powerful graphic methods available in easily used software, the heplots and candisc packages.\nThis chapter describes this framework and illustrates some concrete examples, first for MANOVA designs which are conceptually and visually simpler, and then for MMRA designs with quantitative predictors and finally for MANCOVA models. Many more worked examples are available in vignettes for the heplots package.1\nPackages\nIn this chapter I use the following packages. Load them now.\nChapter 9 illustrated the basic ideas of the framework for visualizing multivariate linear models in the context of a simple two group design, using Hotelling’s \\(T^2\\). The main ideas were illustrated in Figure 9.9.\nHaving described the statistical ideas behind the MLM in Chapter 10, we can proceed to extend this framework to larger designs. Figure 11.1 illustrates these ideas using the simple one-way MANOVA design of the dogfood data from Section 10.2.1.\nFigure 11.1: Dogfood quartet: Illustration of the conceptual ideas of the HE plot framework for the dogfood data. (a) Scatterplot of the data; (b) Summary using data ellipses; (c) HE plot shows the variation in the means in relation to pooled within group variance; (d) Transformation from data space to canonical space\nFor more complex models such as MANOVA with multiple factors or multivariate multivariate regression with several predictors, there is one sum of squares and products matrix (SSP), and therefore one \\(\\mathbf{H}\\) ellipse for each term in the model. For example, in a two-way MANOVA design with the model formula (y1, y2) ~ A + B + A*B and equal sample sizes in the groups, the total sum of squares accounted for by the model is the sum of their separate effects,\n\\[\\begin{aligned}\n\\mathbf{SSP}_{\\text{Model}} & = \\mathbf{SSP}_{A} + \\mathbf{SSP}_{B} + \\mathbf{SSP}_{AB} \\\\\n                            & = \\mathbf{H}_A + \\mathbf{H}_B + \\mathbf{H}_{AB} \\:\\: .\n\\end{aligned} \\tag{11.1}\\]\nAll of these hypotheses can be overlaid in a single HE plot showing their effects together in a comprehensive view.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-he-framework",
    "href": "11-mlm-viz.html#sec-he-framework",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "In (a) data space, each group is summarized in (b) by its data ellipse, representing the means and covariances.\nVariation against the hypothesis of equal means can be seen by the \\(\\mathbf{H}\\) ellipse in the (c) HE plot, representing the data ellipse of the fitted values. Error variance is shown in the \\(\\mathbf{E}\\) ellipse, representing the pooled within-group covariance matrix, \\(\\mathbf{S}_p\\) and the data ellipse of the residuals from the model. For the dogfood data, the group means have a negative relation: longer time to start eating is associated with a smaller amount eaten.\nThe MANOVA (or Hotelling’s \\(T^2\\)) is formally equivalent to a discriminant analysis, predicting group membership from the response variables which can be seen in data space. (The main difference is emphasis and goals: MANOVA seeks to test differences among group means, while discriminant analysis aims at classification of the observations into groups.)\nThis effectively projects the \\(p\\)-dimensional space of the predictors into the smaller (d) canonical space that shows the greatest differences among the groups. As in a biplot, vectors show the relations of the response variables with the canonical dimensions.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-he-plot-construct",
    "href": "11-mlm-viz.html#sec-he-plot-construct",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.2 HE plot construction",
    "text": "11.2 HE plot construction\nThe HE plot is constructed to allow a direct visualization of the “size” of hypothesized terms in a multivariate linear model in relation to unexplained error variation. These can be displayed in 2D or 3D plots, so I use the term “ellipsoid” below to cover all cases.\nError variation is represented by a standard 68% data ellipsoid of the \\(\\mathbf{E}\\) matrix of the residuals in \\(\\boldsymbol{\\Large\\varepsilon}\\). This is divided by the residual degrees of freedom, so the size of \\(\\mathbf{E} / \\text{df}_e\\) is analogous to a mean square error in univariate tests. The choice of 68% coverage allows you to ``read’’ the residual standard deviation as the half-length of the shadow of the \\(\\mathbf{E}\\) ellipsoid on any axis (see Figure 3.11).\nThe \\(\\mathbf{E}\\) ellipsoid is then translated to the overall (grand) means \\(\\bar{\\mathbf{y}}\\) of the variables plotted, which allows us to show the means for factor levels on the same scale, facilitating interpretation. In the notation of Equation 3.2, the error ellipsoid \\(\\mathcal{E}_c\\) of size \\(c\\) is given by\n\\[\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{E}) = \\bar{\\mathbf{y}} \\; \\oplus \\; c\\,\\mathbf{E}^{1/2} \\:\\: ,\n\\tag{11.2}\\] where \\(c = \\chi^2_2 (0.68)\\) for 2D plots and \\(c = \\chi^2_3 (0.68)\\) for 3D plots of standard 68% coverage.2 Ellipses of various coverage were shown in Figure 3.9.\nAn ellipsoid representing variation in the means of a factor (or any other term reflected in a general linear hypothesis test, Equation 10.11) uses the corresponding \\(\\mathbf{H}\\) matrix is simply the data ellipse of the fitted values for that term. But there is a question of the relative scaling of the \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) ellipsoids for interpretation.\nDividing the hypothesis matrix by the error degrees of freedom, giving \\(\\mathbf{H} / \\text{df}_e\\), puts this on the same scale as the \\(\\mathbf{E}\\) ellipse.  I refer to this as effect size scaling, because it is similar to an effect size index used in univariate models, e.g., \\(ES = (\\bar{y}_1 - \\bar{y}_2) / s_e\\) in a two-group, univariate design. An alternative, significance scaling (Section 11.4) provides a visual test of significance of a model \\(\\mathbf{H}\\) term.\n\nTo illustrate this concretely, consider the HE plot for the dogfood shown in Figure 11.1 (c), reproduced here as Figure 11.2.\n\nShow the codedata(dogfood, package=\"heplots\")\ndogfood.mod &lt;- lm(cbind(start, amount) ~ formula, data=dogfood)\n\nheplot(dogfood.mod,  \n       fill = TRUE, fill.alpha = 0.1, \n       cex.lab = 1.5, cex = 1.5,\n       xlim = c(-1, 4.5),\n       ylim = c(70, 100))\n\n\n\n\n\n\nFigure 11.2: HE plot for the dogfood data, showing the means of the four groups, which generates the \\(\\mathbf{H}\\) ellipse for the effect of formula. The \\(\\mathbf{E}\\) ellipse labeled ‘Error’ shows the within-group variances and covariance.\n\n\n\n\nFrom the analysis in Section 10.2.2, we found the \\(\\mathbf{H}\\) matrix for the formula effect in the dogfood.mod model to be as shown below. The negative covariance, -70.94, reflects a correlation of -0.94 between the means of start time and amount eaten.\n\ndogfood.aov &lt;- Anova(dogfood.mod) \nSSP_H &lt;- dogfood.aov$SSP[[1]] |&gt; print()\n#&gt;         start amount\n#&gt; start    9.69  -70.9\n#&gt; amount -70.94  585.7\n\nSimilarly, the \\(\\mathbf{E}\\) matrix, shown below, reflects a slight positive correlation, 0.12, for dogs fed the same formula.\n\nSSP_E &lt;- dogfood.aov$SSPE |&gt; print()\n#&gt;        start amount\n#&gt; start   25.8   11.8\n#&gt; amount  11.8  390.3\n\n\n\nExample 11.1 Iris data\nPerhaps the most famous (or infamous) dataset in the history of multivariate data analysis is that of measurements on three species of Iris flowers collected by Edgar Anderson (1935) in the Gaspé Peninsula of Québec, Canada. Anderson wanted to quantify the outward appearance (“morphology”: shape, structure, color, pattern, size) of species as a method to study variation within and between such groups. Although Anderson published in the obscure Bulletin of the American Iris Society, R. A. Fisher (1936) saw this as a challenge and opportunity to introduce the method now called discriminant analysis—how to find a weighted composite of variables to best discriminate among existing groups.\n\n\n\n\n\n\n\nHistory corner\n\n\n\nI said “infamous” above because Fisher published in the Annals of Eugenics. He was an ardent eugenicist himself, and the work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. Through guilt by association, the Iris data, having mistakenly been called “Fisher’s Iris Data”, has become deprecated, even called “racist data”.3 The voices of the Setosa, Versicolor and Virginica of Gaspé protest: we don’t have a racist bone in our body and nor prejudice against any other species, to no avail.\nBodmer et al. (2021) present a careful account of Fisher’s views on eugenics within the context of his time and his contributions to modern statistical theory and practice. Fisher’s views on race were largely formed by Darwin and Galton, but “nearly all of Fisher’s statements were about populations, groups of populations, or the human species as a whole”. Regardless, the iris data were Anderson’s and should not be blamed. After all, if Anderson had gave his car to Fisher, would the car be tainted by Fisher’s eugenicist leanings?\n\n\n\n\n\n\n\n\n\n\nFigure 11.3: Diagram of an iris flower showing the measurements of petal and sepal size. Each flower has three sepals and three alternating petals. The sepals have brightly colored central sections. Source: Gayan De Silva (2020)\n\n\n\n\nSo that we understand what the measurements represent, Figure 11.3 superposes labels on a typical iris flower, having three sepals which alternate with three petals. Sepals are like ostentatious petals, with attractive decorations in the central section. Length is the distance from the center to the tip and width is the transverse dimension.\nAs always, it is useful to start with overview displays to see the data. A scatterplot matrix (Figure 11.4) shows that versicolor and virginica are more similar to each other than either is to setosa, both in their pairwise means (setosa are smaller) and in the slopes of regression lines. Further, the ellipses suggest that the assumption of constant within-group covariance matrices is problematic: While the shapes and sizes of the concentration ellipses for versicolor and virginica are reasonably similar, the shapes and sizes of the ellipses for setosa are different from the other two.\n\niris_colors &lt;-c(\"blue\", \"darkgreen\", \"brown4\")\nscatterplotMatrix(~ Sepal.Length + Sepal.Width + \n                    Petal.Length + Petal.Width | Species,\n  data = iris,\n  col = iris_colors,\n  pch = 15:17,\n  smooth=FALSE,\n  regLine = TRUE,\n  ellipse=list(levels=0.68, fill.alpha=0.1),\n  diagonal = FALSE,\n  legend = list(coords = \"bottomleft\", \n                cex = 1.3, pt.cex = 1.2))\n\n\n\n\n\n\nFigure 11.4: Scatterplot matrix of the iris dataset. The species are summarized by 68% data ellipses and linear regression lines in each pairwise plot.\n\n\n\n\n\n**TODO*: Should this be a numbered section?\n\n11.2.1 MANOVA model\nWe proceed nevertheless to fit a multivariate one-way ANOVA model to the iris data. The MANOVA model for these data addresses the question: “Do the means of the Species differ significantly for the sepal and petal variables taken together?” \\[\n\\mathcal{H}_0 : \\mathbf{\\mu}_\\textrm{setosa} = \\mathbf{\\mu}_\\textrm{versicolor} = \\mathbf{\\mu}_\\textrm{virginica}\n\\]\nBecause there are three species, the test involves \\(s = \\min(p, g-1) =2\\) degrees of freedom, and we are entitled to represent this by two 1-df contrasts, or sub-questions. From the separation among the groups shown in Figure 11.4 (or more botanical knowledge), it makes sense to compare:\n\nSetosa vs. others: \\(\\mathbf{c}_1 = (1,\\: -\\frac12, \\: -\\frac12)\\)\n\nVersicolor vs. Virginica: : \\(\\mathbf{c}_1 = (0,\\: 1, \\: -1)\\)\n\n\nYou can do this by putting these vectors as columns in a matrix and assigning this to the contrasts() of Species. It is important to do this before fitting with lm(), because the contrasts in effect determine how the \\(\\mathbf{X}\\) matrix is setup, and hence the names of the coefficients representing Species.\n\nC &lt;- matrix(c(1,-1/2,-1/2,  \n              0,   1,  -1), nrow=3, ncol=2)\ncontrasts(iris$Species) &lt;- C\ncontrasts(iris$Species)\n#&gt;            [,1] [,2]\n#&gt; setosa      1.0    0\n#&gt; versicolor -0.5    1\n#&gt; virginica  -0.5   -1\n\nNow let’s fit the model. As you would expect from Figure 11.4, the differences among groups are highly significant.\n\niris.mod &lt;- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~\n                 Species, data=iris)\nAnova(iris.mod)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;         Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Species  2      1.19     53.5      8    290 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAs a quick follow-up, it is useful to examine the univariate tests for each of the iris variables, using heplots::glance() or heplots::uniStats(). It is of interest that the univariate \\(R^2\\) values are much larger for the petal variables than the sepal length and width.4. For comparison, heplots::etasq() gives the overall \\(\\eta^2\\) proportion of variance accounted for in all responses.\n\nglance(iris.mod)\n#&gt; # A tibble: 4 × 8\n#&gt;   response     r.squared sigma fstatistic numdf dendf  p.value  nobs\n#&gt;   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Sepal.Length     0.619 0.515      119.      2   147 1.67e-31   150\n#&gt; 2 Sepal.Width      0.401 0.340       49.2     2   147 4.49e-17   150\n#&gt; 3 Petal.Length     0.941 0.430     1180.      2   147 2.86e-91   150\n#&gt; 4 Petal.Width      0.929 0.205      960.      2   147 4.17e-85   150\n\netasq(iris.mod)\n#&gt;         eta^2\n#&gt; Species 0.596\n\nBut these statistics don’t help to understand how the species differ. For this, we turn to HE plots.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-he-plots",
    "href": "11-mlm-viz.html#sec-he-plots",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.3 HE plots",
    "text": "11.3 HE plots\nThe heplot() function takes a \"mlm\" object and produces an HE plot for one pair of variables specified by the variables argument. By default, it plots the first two. Figure 11.5 shows the HE plots for the two sepal and the two petal variables.\n\n\nheplot(iris.mod, size = \"effect\",\n       cex = 1.5, cex.lab = 1.5,\n       fill = TRUE, fill.alpha = c(0.3, 0.1))\nheplot(iris.mod, size = \"effect\", variables = 3:4,\n       cex = 1.5, cex.lab = 1.5,\n       fill = TRUE, fill.alpha = c(0.3, 0.1))\n\n\n\n\n\n\n\n\nFigure 11.5: HE plots for the multivariate model iris.mod. The left panel shows the plot for the Sepal variables; the right panel plots the Petal variables.\n\n\n\n\nThe interpretation of the plots in Figure 11.5 is as follows:\n\nFor the Sepal variables, length and width are positively correlated within species (the \\(\\mathbf{E}\\) = “Error” ellipsoid). The means of the groups (the \\(\\mathbf{H}\\) = “Species” ellipsoid), however, are negatively correlated. This plot is the HE plot representation of the data shown in row 2, column 1 of Figure 11.4. It reflects the relative shape of the iris sepals: shorter and wider for setosa than the other two species.\nFor the Petal variables length and width are again positively correlated within species, but now the means of the groups are positively correlated: longer petals go with wider ones across species. This reflects the relative size of the iris petals. The analogous data plot appears in row 4, column 3 of Figure 11.4.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-signif-scaling",
    "href": "11-mlm-viz.html#sec-signif-scaling",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.4 Significance scaling",
    "text": "11.4 Significance scaling\nThe geometry of ellipsoids and multivariate tests allow us to go further with another re-scaling of the \\(\\mathbf{H}\\) ellipsoid that gives a visual test of significance for any term in a MLM. This is done simply by dividing \\(\\mathbf{H} / df_e\\) further by the \\(\\alpha\\)-critical value of the corresponding test statistic to show the strength of evidence against the null hypothesis.\nAmong the various multivariate test statistics, Roy’s maximum root test, based on the largest eigenvalue \\(\\lambda_1\\) of \\(\\mathbf{H} \\mathbf{E}^{-1}\\), gives \\(\\mathbf{H} / (\\lambda_\\alpha df_e)\\) which has the visual property that the scaled \\(\\mathbf{H}\\) ellipsoid will protrude somewhere outside the standard \\(\\mathbf{E}\\) ellipsoid if and only if Roy’s test is significant at significance level \\(\\alpha\\). The critical value \\(\\lambda_\\alpha\\) for Roy’s test is \\[\n\\lambda_\\alpha = \\left(\\frac{\\text{df}_1}{\\text{df}_2}\\right) \\; F_{\\text{df}_1, \\text{df}_2}^{1-\\alpha} \\:\\: ,\n\\] where \\(\\text{df}_1 = \\max(p, \\text{df}_h)\\) and \\(\\text{df}_2 = \\text{df}_h + \\text{df}_e - \\text{df}_1\\).\nFor these data, the HE plot using significance scaling is shown in the right panel of Figure 11.6. The left panel is the same as that shown for sepal width and length in Figure 11.5, but with axis limits to make the two plots directly comparable.\n\n\n\n\n\n\n\nFigure 11.6: HE plots for sepal width and sepal length in the iris dataset. Left: effect scaling of the \\(\\mathbf{H}\\) matrix; right: significance scaling, where protrusion of \\(\\mathbf{H}\\) outside \\(\\mathbf{E}\\) indicates a significant effect by Roy’s test.\n\n\n\n\nYou can interpret the plot using effect scaling to indicate that the overall “size” of variation of the group means is roughly the same as that of within-group variation for the two sepal variables. Significance scaling weights the evidence against the null hypothesis that a given effect is zero. Clearly, the species vary significantly on the sepal variables, and the direction of the \\(\\mathbf{H}\\) ellipse suggests that those whose sepals are longer are also less wide.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-he-vis-contrasts",
    "href": "11-mlm-viz.html#sec-he-vis-contrasts",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.5 Visualizing contrasts and linear hypotheses",
    "text": "11.5 Visualizing contrasts and linear hypotheses\nAs described in Section 5.1.3, tests of linear hypotheses and contrasts represented by the general linear test \\(\\mathcal{H}_0: \\mathbf{C} \\;\\mathbf{B} = \\mathbf{0}\\) provide a powerful way to probe the specific effects represented within the global null hypothesis, \\(\\mathcal{H}_0: \\mathbf{B} = \\mathbf{0}\\), that all effects are zero.\nIn this example the contrasts \\(\\mathbf{c}_1\\) (Species1) and \\(\\mathbf{c}_2\\) (Species2) among the iris species are orthogonal, i.e., \\(\\mathbf{c}_1^\\top \\mathbf{c}_2 = 0\\). Therefore, their tests are statistically independent, and their \\(\\mathbf{H}\\) matrices are additive. They fully decompose the general question of differences among the groups into two independent questions regarding the contrasts.\n\\[\n\\mathbf{H}_\\text{Species} = \\mathbf{H}_\\text{Species1} + \\mathbf{H}_\\text{Species2}\n\\tag{11.3}\\]\ncar::linearHypothesis() is the means for testing these statistically, and heplot() provides the way to show these tests visually. Using the contrasts set up in Section 11.2.1, \\(\\mathbf{c}_1\\), representing the difference between setosa and the other species is labeled Species1 and the comparison of versicolor with virginica is Species2. The coefficients for these in \\(\\mathbf{B}\\) give the differences in the means. The line for (Intercept) gives grand means of the variables.\n\ncoef(iris.mod)\n#&gt;             Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; (Intercept)        5.843       3.057        3.758       1.199\n#&gt; Species1          -0.837       0.371       -2.296      -0.953\n#&gt; Species2          -0.326      -0.102       -0.646      -0.350\n\nNumerical tests of hypotheses using linearHypothesis() can be specified in a very general way: A matrix (or vector) \\(\\mathbf{C}\\) giving linear combinations of coefficients by rows, or a character vector giving the hypothesis in symbolic form. A character variable or vector tests whether the named coefficients are different from zero for all responses.\n\nlinearHypothesis(iris.mod, \"Species1\") |&gt; print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1      0.97     1064      4    144 &lt;2e-16 ***\n#&gt; Wilks             1      0.03     1064      4    144 &lt;2e-16 ***\n#&gt; Hotelling-Lawley  1     29.55     1064      4    144 &lt;2e-16 ***\n#&gt; Roy               1     29.55     1064      4    144 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlinearHypothesis(iris.mod, \"Species2\") |&gt; print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1     0.745      105      4    144 &lt;2e-16 ***\n#&gt; Wilks             1     0.255      105      4    144 &lt;2e-16 ***\n#&gt; Hotelling-Lawley  1     2.925      105      4    144 &lt;2e-16 ***\n#&gt; Roy               1     2.925      105      4    144 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe various test statistics are all equivalent here—they give the same \\(F\\) statistics— because they they have 1 degree of freedom.\nIn passing, from Equation 11.3, note that the joint test of these contrasts is exactly equivalent to the overall test of Species (results not shown).\n\nlinearHypothesis(iris.mod, c(\"Species1\", \"Species2\"))\n\nWe can show these contrasts in an HE plot by supplying a named list for the hypotheses argument. The names are used as labels in the plot. In the case of a 1-df multivariate test, the \\(\\mathbf{H}\\) ellipses plot as a degenerate line.\n\nhyp &lt;- list(\"S:Vv\" = \"Species1\", \"V:v\" = \"Species2\")\nheplot(iris.mod, hypotheses=hyp,\n       cex = 1.5, cex.lab = 1.5,\n       fill = TRUE, fill.alpha = c(0.3, 0.1),\n       col = c(\"red\", \"blue\", \"darkgreen\", \"darkgreen\"),\n       lty = c(0,0,1,1), label.pos = c(3, 1, 2, 1),\n       xlim = c(2, 10), ylim = c(1.4, 4.6))\n\n\n\n\n\n\nFigure 11.7: HE plot for sepal length and width in the iris data showing the tests of the two contrasts, using significance scaling.\n\n\n\n\nThis HE plot shows that, for the two sepal variables, the greatest between-species variation is accounted for by the contrast (S:Vv) between setosa and the others, for which the effect is very large in relation to error variation. The second contrast (V:v), between the versicolor and virginica species is relatively smaller, but still explains significant variation of the sepal variables among the species.\nThe directions of these hypotheses in a given plot show how the group means differ in terms of a given contrast.5 For example, the contrast S:Vv is the line that separates setosa from the others and indicates that setosa flowers have shorter but wider sepals.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-HEplot-matrices",
    "href": "11-mlm-viz.html#sec-HEplot-matrices",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.6 HE plot matrices",
    "text": "11.6 HE plot matrices\nIn base R graphics, 2D scatterplots are extended to all pairwise views of multivariate data with a pairs() method. For multivariate linear models, the heplots package defines a pairs.mlm() method to display HE plots for all pairs of the response variables.\n\npairs(iris.mod,\n      fill=TRUE, fill.alpha=c(0.3, 0.1))\n\n\n\n\n\n\nFigure 11.8: All pairwise HE plots for the iris data.\n\n\n\n\nFigure 11.8 provides a fairly complete visualization of the results of the multivariate tests and answers the question: how do the species differ? Sepal length and the two petal variables have the group means nearly perfectly correlated, in the order setosa &lt; versicolor &lt; virginica. For Sepal width, however, setosa has the largest mean, and so the \\(\\mathbf{H}\\) ellipses show a negative correlation in the second row and column.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-candisc",
    "href": "11-mlm-viz.html#sec-candisc",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.7 Low-D views: Canonical analysis",
    "text": "11.7 Low-D views: Canonical analysis\nThe HE plot framework so far provides views of all the effects in a MLM in variable space. We can view this in 2D for selected pairs of response variables, or for all pairwise views in scatterplot matrix format, as in Figure 11.8.\nThere is also an heplot3d() function giving plots for three response variables together. The 3D plots are interactive, in that they can be rotated and zoomed by mouse control, and dynamic, in that they can be made to spin and saved as movies. To save space, these plots are not shown here.\nHowever in a one-way MANOVA design with more than response three variables, it is difficult to visualize how the groups vary on all responses together, and how the different variables contribute to discrimination among groups. In this situation, canonical discriminant analysis (CDA) is often used, to provide a low-D visualization of between-group variation.\nWhen the predictors are also continuous, the analogous term is canonical correlation analysis (CCA), described in Section 11.10. The advantage in both cases is that we can also show the relations of the response variables to these dimensions, similar to a biplot (Section 4.3) for a PCA of purely quantitative variables.\nBut, to be clear: dimensions and variance accounted for in canonical space describe the relationships between response variables and factors or quantitative predictors, whereas PCA is only striving to account for the total variation in the space of all variables.\nThe key to CDA is the eigenvalue decomposition of the \\(\\mathbf{H}\\) relative to \\(\\mathbf{E}\\) \\(\\mathbf{H}\\mathbf{E}^{-1} \\lambda_i = \\lambda_i \\mathbf{v}_i\\) (Equation 10.9) . The eigenvalues, \\(\\lambda_i\\), give the “size” of each \\(s\\) orthogonal dimensions on which the multivariate tests are based (Section 10.2.3). But the corresponding eigenvectors, \\(\\mathbf{v}_i\\), give the weights for the response variables in \\(s\\) linear combinations that maximally discriminate among the groups or equivalently maximize the (canonical) \\(R^2\\) of a linear combination of the predictor \\(\\mathbf{X}\\)s with a linear combination of the response \\(\\mathbf{Y}\\)s.\nThus, CDA amounts to a transformation of the \\(p\\) responses, \\(\\mathbf{Y}_{n \\times p}\\) into scores \\(\\mathbf{Z}\\) in the canonical space,\n\\[\n\\mathbf{Z}_{n \\times s} = \\mathbf{Y} \\; \\mathbf{E}^{-1/2} \\; \\mathbf{V} \\;,\n\\tag{11.4}\\]\nwhere \\(\\mathbf{V}\\) contains the eigenvectors of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) and \\(s=\\min ( p, df_h )\\) dimensions, the degrees of freedom for the hypothesis. It is well-known (e.g., Gittins (1985)) that canonical discriminant plots of the first two (or three, in 3D) columns of \\(\\mathbf{Z}\\) corresponding to the largest canonical correlations provide an optimal low-D display of the variation between groups relative to variation within groups.\nCanonical discriminant analysis is typically carried out in conjunction with a one-way MANOVA design. The candisc package package (Friendly & Fox, 2025) generalizes this to multi-factor designs in the candisc() function. For any given term in a \"mlm\", the generalized canonical discriminant analysis amounts to a standard discriminant analysis based on the \\(\\mathbf{H}\\) matrix for that term in relation to the full-model \\(\\mathbf{E}\\) matrix.6\nTests based on the eigenvalues \\(\\lambda_i\\), initially stated by Bartlett (1938), use Wilks’ \\(\\Lambda\\) likelihood ratio tests of these. This allow you to determine the number of significant canonical dimensions, or the number of different aspects to consider for the relations between the responses and predictors. This is a big win over univariate analyses for each dependent variable separately as follow-ups for a significant MANOVA result.\nThese take the form of sequential global tests of the hypothesis that the canonical correlation in the current row and all that follow are zero. Thus, if you find that the second dimension is insignificant, there is no need to look at any further down the list. The canonical \\(R^2\\), CanRsq, gives the R-squared value of fitting the \\(i\\)th response canonical variate to the corresponding \\(i\\)th canonical variate for the predictors.\nFor the iris data, we get the following printed summary:\n\niris.can &lt;- candisc(iris.mod) |&gt; \n  print()\n#&gt; \n#&gt; Canonical Discriminant Analysis for Species:\n#&gt; \n#&gt;   CanRsq Eigenvalue Difference Percent Cumulative\n#&gt; 1  0.970     32.192       31.9  99.121       99.1\n#&gt; 2  0.222      0.285       31.9   0.879      100.0\n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;   LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1        0.023    199.1     8   288 &lt; 2e-16 ***\n#&gt; 2        0.778     13.8     3   145 5.8e-08 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis analysis shows a very simple result: The differences among the iris species can be nearly entirely accounted for by the first canonical dimension (99.1%). Interestingly, the second dimension is also highly significant, even though it accounts for only 0.88%.\n\n11.7.1 Coeficients\nThe coef() method for “candisc” objects returns a matrix of weights for the response variables in the canonical dimensions. By default, these are given for the response variables standardized to \\(\\bar{y}=0\\) and \\(s^2_y = 1\\).\nThe type argument also allows for raw score weights (type = \"raw\") used to compute scores for the observations on the canonical variables Can1, Can2, … . Using type = \"structure\" gives the canonical structure coefficients, which are the correlations between each response and the canonical scores.\n\ncoef(iris.can, type = \"std\")\n#&gt;                Can1    Can2\n#&gt; Sepal.Length  0.427  0.0124\n#&gt; Sepal.Width   0.521  0.7353\n#&gt; Petal.Length -0.947 -0.4010\n#&gt; Petal.Width  -0.575  0.5810\ncoef(iris.can, type = \"structure\")\n#&gt;                Can1  Can2\n#&gt; Sepal.Length -0.792 0.218\n#&gt; Sepal.Width   0.531 0.758\n#&gt; Petal.Length -0.985 0.046\n#&gt; Petal.Width  -0.973 0.223\n\nThe standardized (or raw score) weights are interpreted in terms of their signs and magnitudes, just as in coefficient weights in a multiple regression. From the numbers, Can1 seems to be a contrast between the sepal and petal variables. For Can2, sepal length doesn’t matter and the result contrasts the two width variables against petal length.\nI find it easier to interpret the correlations between the observed and canonical variables, given as the canonical structure coefficients. These are easily visualized as vectors in canonical space (similar to biplots for a PCA), as shown below (Figure 11.9).\n\n\n11.7.2 Canonical scores plot\nThe plot() method for \"candisc\" objects gives a plot of these observation scores in any two dimensions. The argument ellipse=TRUE overlays this with their standard data ellipses for each species, as shown in Figure 11.9.\nAnalogous to the biplot (Section 4.3), the response variables are shown as vectors, using the structure coefficients. Thus, the relative size of the projection of these vectors on the canonical axes reflects the correlation of the observed response on the canonical dimension. For ease of interpretation I flipped the sign of the first canonical dimension (rev.axes), so that the positive Can1 direction corresponds to larger flowers.\n\n\nvars &lt;- names(iris)[1:4] |&gt; \n  stringr::str_replace(\"\\\\.\", \"\\n\")\nplot(iris.can,\n     var.labels = vars,\n     var.col = \"black\",\n     var.lwd = 2,\n     ellipse=TRUE,\n     scale = 9,\n     col = iris_colors,\n     pch = 15:17,\n     cex = 0.7, var.cex = 1.25,\n     rev.axes = c(TRUE, FALSE),\n     xlim = c(-10, 10),\n     cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.9: Plot of canonical scores for the iris data. Ellipses give 68% coverage data ellipses for the canonical scores. Variable vectors make angles with the Can1 and Can2 axes indicating their correlations.\n\n\n\n\nThe interpretation of this plot is simple: in canonical space, variation of the means for the iris species is essentially one-dimensional (99.1% of the effect of Species), and this dimension corresponds to overall size of the iris flowers: The Setosas have smaller flowers.\nAll variables except for Sepal.Width are positively aligned with this axis, but the two petal variables show the greatest discrimination. The negative direction for Sepal.Width reflects the pattern seen in Figure 11.8, where setosa have wider sepals.\nFor the second dimension, look at the projections of the variable vectors on the Can2 axis. All are positive, but this is dominated by Sepal.Width. We could call this a flower shape dimension.\n\n11.7.3 Canonical HE plot\nFor a one-way design, the canonical HE plot is simply the HE plot of the canonical scores in the analogous MLM model that substitutes \\(\\mathbf{Z}\\) for \\(\\mathbf{Y}\\). In effect, it is a more compact visual summary of the plot shown of canonical scores in Figure 11.9.\nThis is shown in Figure 11.10 for the iris data. In canonical space, the residuals are always uncorrelated, so the \\(\\mathbf{E}\\) ellipse plots as a circle. The \\(\\mathbf{H}\\) ellipse for Species here reflects a data ellipse for the fitted values— group means— shown as labeled points in the plot. The differences among Species are so large that this plot uses size = \"effect\" scaling, making the axes comparable to those in Figure 11.9.7\nThe vectors for each predictor are the same structure coefficients as in the ordinary canonical plot. They can again be reflected for easier interpretation and scaled in length to fill the plot window.\n\n\nheplot(iris.can,\n       size = \"effect\",\n       scale = 8,\n       var.labels = vars, var.col = \"black\",\n       var.lwd = 2, var.cex = 1.25,\n       fill = TRUE, fill.alpha = 0.2,\n       rev.axes = c(TRUE, FALSE),\n       xlim = c(-10, 10),\n       cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.10: Canonical HE plot for the iris data. Compared with Figure 11.9, it substutes canonical \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) ellipses for the canonical scores shown there.\n\n\n\n\nThe collection of plots shown for the iris data here can be seen as progressive visual summaries of the data and increased visual understanding of the morphology of Anderson’s Iris flowers:\n\nThe scatterplot matrix in Figure 11.4 shows the iris flowers in the data space of the sepal and petal variables.\nCanonical analysis substitutes for these the two linear combinations reflected in Can1 and Can2. The plot in Figure 11.9 portrays exactly the same relations among the species, but in the reduced canonical space of only two dimensions.\nThe HE plot version, shown in Figure 11.10 summarizes the separate data ellipses for the species with pooled, within-group variance of the \\(\\mathbf{E}\\) matrix for the canonical variables, which are always uncorrelated. The variation among the group means is reflected in the size and shape of the ellipse for the \\(\\mathbf{E}\\) matrix.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-HE-factorial",
    "href": "11-mlm-viz.html#sec-HE-factorial",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.8 Factorial MANOVA",
    "text": "11.8 Factorial MANOVA\nWhen there are two or more factors, the overall model is comprised of main effects and possible interactions as shown in Equation 11.1. A significant advantage of HE plots is that they show how the response variables are related in their effects. Moreover, the main effects and interactions can be overlaid in the same plot showing how each term contributes to assessment of differences among the groups.\n\nExample 11.2 Plastic film data\nI illustrate these points below for the Plastic film data analyzed earlier in Example 10.1. The model contemplated there examined how the three response variables, resistance to tear, film gloss and the opacity of the film varied with the two experimental factors, rate of extrusion and amount of some additive, both at two levels, labeled High and Low. Both main effects and the interaction of rate and additive were fit in the model plastic.mod:\n\nplastic.mod &lt;- lm(cbind(tear, gloss, opacity) ~ rate * additive, \n                  data=Plastic)\n\nWe can visualize all these effects for pairs of variables in an HE plot, showing the “size” and orientation of hypothesis variation (\\(\\mathbf{H}\\)) for each model term, in relation to error variation (\\(\\mathbf{E}\\)), as ellipsoids. When, as here, the model terms have 1 degree of freedom, the \\(\\mathbf{H}\\) ellipsoids for rate, additive and rate:additive each degenerate to a line.\nFigure 11.11 shows the HE plot for the responses tear and gloss, the strongest by univariate tests. This plot takes advantage of another feature of heplot(): You can overlay plots using add = TRUE, as is done here to show both significance and effect size scaling in a single plot.\n\ncolors = c(\"red\", \"darkblue\", \"darkgreen\", \"brown4\")\nheplot(plastic.mod, size=\"significance\", \n       col=colors, cex=1.5,  cex.lab = 1.5,\n       fill=TRUE, fill.alpha=0.1)\nheplot(plastic.mod, size=\"effect\", \n       col=colors, lwd=6,\n       add=TRUE, term.labels=FALSE)\n\n\n\n\n\n\nFigure 11.11: HE plot for effects on tear and gloss according to the factors rate, additive and their interaction, rate:additive. The thicker lines show effect size scaling; thinner lines show significance scaling.\n\n\n\n\nIn this view, the main effect of extrusion rate is highly significant, with the high level giving larger tear resistance and lower gloss on average. High level of additive produces greater tear resistance and higher gloss. The interaction effect, rate:additive, while non-significant, points nearly entirely in the direction of gloss. You can see this more directly in Figure 10.9, where the lines for gloss diverge.\nBut what if you also wanted to show the means for the combinations of rate and additive in an HE plot? By design, means for the levels of interaction terms are not shown in the HE plot, because doing so in general can lead to messy displays.\nWe can add them here for the term rate:additive as shown in Figure 11.12. This uses heplots::termMeans() to find the cell means for the combinations of the two factors and then lines() to connect the pairs of points for the low and high levels of additive.\n\npar(mar = c(4,4,1,1)+.1)\nheplot(plastic.mod, size=\"evidence\", \n       col=colors, cex=1.5, cex.lab = 1.5, \n       lwd = c(1, 5),\n       fill=TRUE, fill.alpha=0.05)\n\n# add interaction means\nintMeans &lt;- termMeans(plastic.mod, 'rate:additive', \n                      abbrev.levels=3)\npoints(intMeans[,1], intMeans[,2], pch=18, cex=1.2, col=\"brown4\")\ntext(intMeans[,1], intMeans[,2], rownames(intMeans), \n     adj=c(0.5, 1), col=\"brown4\")\nlines(intMeans[c(1,3),1], intMeans[c(1,3),2], \n      col=\"brown4\", lwd = 3)\nlines(intMeans[c(2,4),1], intMeans[c(2,4),2], \n      col=\"brown4\", lwd = 3)\n\n\n\n\n\n\nFigure 11.12: HE plot for effects on tear and gloss using significance scaling. To this is added points showing the means for the combinations of rate and additive.\n\n\n\n\nFigure 11.12 is somewhat more complicated to interpret than the simple line plots in Figure 10.9, but has the advantage that it shows effects on the two response variables jointly.\n\n\nExample 11.3 MockJury: Manipulation check\nIn Example 10.2, I examined the effects of the attractiveness of photos of hypothetical women defendants and the nature of a crime on the judgments made by members of a mock jury, on the rated seriousness of the crime and the length of a prison sentence participants would give to a guilty defendant. This analysis used the following model, fitting Serious and Years of sentence to the combinations of Attr and Crime:\n\ndata(MockJury, package = \"heplots\")\njury.mod &lt;- lm(cbind(Serious, Years) ~ Attr * Crime, \n                data=MockJury)\n\nThe photos were classified as “beautiful”, of “average” beauty or “unattractive”, and as a validity check on this experimental manipulation, 1–9 ratings on twelve attributes were also collected. Of these, the direct rating of physical attractiveness, physattr, was most important, but it was also of interest to see how other ratings differentiated the photos. The rating scales are the following:\n\nnames(MockJury)[-(1:4)] \n#&gt;  [1] \"exciting\"      \"calm\"          \"independent\"   \"sincere\"      \n#&gt;  [5] \"warm\"          \"phyattr\"       \"sociable\"      \"kind\"         \n#&gt;  [9] \"intelligent\"   \"strong\"        \"sophisticated\" \"happy\"        \n#&gt; [13] \"ownPA\"\n\nTo keep the graphs below simple, I consider only a subset of the ratings here, and fit the full \\(3 \\times 2\\) MANOVA model for Attr and Crime and their interaction. The main interest is in the attractiveness of the photo, but the other terms are included in the model to control for them.8\n\njury.mod1 &lt;- lm(cbind(phyattr, exciting, sociable, happy, independent) ~ Attr * Crime, \n                  data=MockJury)\nuniStats(jury.mod1)\n#&gt; Univariate tests for responses in the multivariate linear model jury.mod1 \n#&gt; \n#&gt;               R^2     F df1 df2 Pr(&gt;F)    \n#&gt; phyattr     0.639 38.24   5 108 &lt;2e-16 ***\n#&gt; exciting    0.139  3.49   5 108 0.0058 ** \n#&gt; sociable    0.134  3.33   5 108 0.0078 ** \n#&gt; happy       0.112  2.73   5 108 0.0233 *  \n#&gt; independent 0.120  2.94   5 108 0.0159 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe pairs.mlm() plot for this model would be a \\(5 \\times 5\\) display showing all pairwise HE plots for this model. Figure 11.13 selects two of these showing phyattr against exciting and independent. Although the model includes the full factorial of Attr and Crime, I only want to show the effect of Attr here, so I do this using the terms and factor.means arguments. Because the ratings are on the same 1–9 scale, I also use asp = 1 to make response variables visually comparable.\n\nheplot(jury.mod1, \n       terms = \"Attr\", factor.means = \"Attr\",\n       fill = TRUE, fill.alpha = 0.2,\n       cex = 1.2, cex.lab = 1.6, asp = 1)\n\nheplot(jury.mod1,\n       variables = c(1,5),\n       terms = \"Attr\", factor.means = \"Attr\",\n       fill = TRUE, fill.alpha = 0.2,\n       cex = 1.2, cex.lab = 1.6, asp = 1)\n\n\n\n\n\n\nFigure 11.13: Two pairwise HE plots showing the effect of the classified attractivenes of the photo and ratings of those photos. Left: exciting vs. phyattr ratings; right: independent vs. phyattr ratings.\n\n\n\n\nThese plots show that the means of the ratings of phyattr of the photos are in the expected order (Beautiful &gt; Average &gt; Unattractive), though the largest difference is between Beautiful and the others in both panels.9\nIn the left panel of Figure 11.13, the means for exciting are nearly perfectly correlated with those for phyattr, and there is little difference between the Beautiful photos and the others. The means for independent are slightly positively correlated with those for phyattr, but there is a wider separation between Average and Unattractive photos. The right panel shows the same order of the means for phyattr, but the photos of Average attractiveness are rated as highest on independence.\nCanonical analysis\nAs before, a canonical analysis squeezes the juice in a collection of responses into fewer dimensions, allowing you to see relationships not not apparent in all pairwise views. With 2 degrees of freedom for Attr, the space for all the ratings is 2D. candisc() for this term10 shows that 91% of the variation between photo groups is accounted for in one dimension, but there is still some significant variation associated with the 2\\(^\\text{nd}\\) dimension.\n\njury.can1 &lt;- candisc(jury.mod1, term = \"Attr\") |&gt;\n  print()\n#&gt; \n#&gt; Canonical Discriminant Analysis for Attr:\n#&gt; \n#&gt;   CanRsq Eigenvalue Difference Percent Cumulative\n#&gt; 1  0.642       1.79       1.61   90.87       90.9\n#&gt; 2  0.153       0.18       1.61    9.13      100.0\n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;   LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1        0.303    17.46    10   214  &lt;2e-16 ***\n#&gt; 2        0.847     4.87     4   108  0.0012 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe 2D plot of canonical scores in Figure 11.14 gives a very simple description for this model. Dimension Can1 widely separates the photo groups, and this dimension is nearly perfectly aligned with the phyattr ratings. Ratings for exciting are also strongly associated with this. This finding is sufficient to claim the validity of the classification used in the experiment.\n\ncol &lt;- c(\"blue\", \"darkgreen\", \"red\")\nplot(jury.can1, rev.axes = c(TRUE, TRUE),\n     col = col,\n     ellipse = TRUE, ellipse.prob = 0.5,\n     lwd = 3,\n     var.lwd = 2,\n     var.cex = 1.4,\n     var.col = \"black\",\n     pch = 15:17,\n     cex = 1.4,\n     cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.14: Canonical discriminant plot for the factor Attr in the model jury.mod1 for the ratings of the photos classified as Beautiful, Average orUnattractive. Ellipses have 50% coverage for the canonical scores. Variable vectors reflect the correlations of the rating scales with the canonical dimensions.\n\n\n\n\nThe second canonical dimension, Can2 is also of some interest. The photo means differ here mainly between those considered of Average beauty and those classified as Unattractive. The ratings for independent and happy are most strongly associated with this dimension. Ratings on sociable are related to both dimensions, but more so with Can2.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-he-mmra",
    "href": "11-mlm-viz.html#sec-he-mmra",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.9 Quantitative predictors: MMRA",
    "text": "11.9 Quantitative predictors: MMRA\nThe ideas behind HE plots extend naturally to multivariate multiple regression (MMRA).  A purely visual feature of HE plots in these cases is that the \\(\\mathbf{H}\\) ellipse for a quantitative predictor with 1 df appears as a degenerate line. But consequently, the angles between these for different predictors has a simple interpretation as as the correlation between their predicted effects. Moreover, it is easy to show visual overall tests of joint linear hypotheses for two or more predictors together.\nTODO: Use for an exercise heplots::Hernior: Recovery from Elective Herniorrhaphy -&gt; HE_mmra vignette\n\n\nExample 11.4 NLSY data\nHere I’ll continue the analysis of the NLSY data from Section 10.6.1. In the model NLSY.mod1, I used only father’s income and education to predict scores in reading and math, and both of these demographic variables were highly significant. Figure 11.15 shows what this looks like in an HE plot.\n\ndata(NLSY, package = \"heplots\")\nNLSY.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY)\n\nheplot(NLSY.mod1, \n  fill=TRUE, fill.alpha = 0.2, \n  cex = 1.5, cex.lab = 1.5,\n  lwd=c(2, 3, 3),\n  label.pos = c(\"bottom\", \"top\", \"top\")\n  )\n\n\n\n\n\n\nFigure 11.15: HE plot for the simple model for the NLSY data fitting reading and math scores from income and education.\n\n\n\n\nFathers income and education are positively correlated in their effects on the outcome scores. From the angles in the plot, income is most related to the math score, while education is related to both, but slightly more to the reading score.\nThe overall joint test for both predictors can then be visualized as the test of the linear hypothesis \\(\\mathcal{H}_0 : \\mathbf{B} = [\\boldsymbol{\\beta}_\\text{income}, \\boldsymbol{\\beta}_\\text{educ}] = \\mathbf{0}\\). For heplot(), we specify the names of the coefficients to be tested with the hypotheses argument.\n\ncoefs &lt;- rownames(coef(NLSY.mod1))[-1] |&gt; print()\n#&gt; [1] \"income\" \"educ\"\n\nheplot(NLSY.mod1, \n       hypotheses = list(\"Overall\" = coefs),\n       fill=TRUE, fill.alpha = 0.2, \n       cex = 1.5, cex.lab = 1.5,\n       lwd=c(2, 3, 3, 2),\n       label.pos = c(\"bottom\", \"top\"))\n\n\n\n\n\n\nFigure 11.16: HE plot adding the \\(\\mathbf{H}\\) ellipse for the overall test that both predictors have no effect on the outcome scores.\n\n\n\n\nThe geometric relations of the \\(\\mathbf{H}\\) ellipses for the overall test and the individual predictors in Figure 11.16 are worth noting here. Those for the separate coefficients always lie within the overall ellipse. The contribution for income makes the overall ellipse larger in the direction of the math score, while the contribution of education makes it larger in both directions.\n\n\n\nExample 11.5 School data: HE plots\nThe schooldata dataset analyzed in Section 10.6.2 can also be illuminated by the methods of this chapter. There I fit the multivariate regression model predicting students scores on reading, mathematics and a measure of self-esteem using as predictors measures of parents’ education, occupation, school visits, counseling help with school assignments and number of teachers per school.\nBut I also found two highly influential observations (cases 44, 59; see Figure 10.19) whose effect on the coefficients is rather large; so, I remove them from the analysis here.11\n\ndata(schooldata, package = \"heplots\")\n\nbad &lt;- c(44, 59)\nOK &lt;- (1:nrow(schooldata)) |&gt; setdiff(bad)\nschool.mod2 &lt;- lm(cbind(reading, mathematics, selfesteem) ~ ., \n                  data=schooldata[OK, ])\n\nIn this model, parent’s education and occupation and their visits to the schools were highly predictive of student’s outcomes but their counseling efforts and the number of teachers in the schools did not contribute much. However, the nature of these relationships was largely uninterpreted in that analysis.\nHere is where HE plots can help. You can think of this as a way to visualize what is entailed in the coefficients for this model by showing the magnitude of the predictor effects by their size and their relations to the outcome variable by their direction. The table of raw score coefficients isn’t very helpful in this regard.\n\ncoef(school.mod2)\n#&gt;             reading mathematics selfesteem\n#&gt; (Intercept)  2.7096       3.561    0.39751\n#&gt; education    0.2233       0.132   -0.01088\n#&gt; occupation   3.3336       4.284    1.79574\n#&gt; visit        0.0101      -0.123    0.20005\n#&gt; counseling  -0.3953      -0.293    0.00868\n#&gt; teacher     -0.1945      -0.360    0.01129\n\nFigure 11.17 shows the HE plot for reading and mathematics scores in this model, using the default significance scaling.\n\nheplot(school.mod2, \n       fill=TRUE, fill.alpha=0.1,\n       cex = 1.5,\n       cex.lab = 1.5,\n       label.pos = c(rep(\"top\", 4), \"bottom\", \"bottom\"))\n\n\n\n\n\n\nFigure 11.17: HE plot for reading and mathematics scores in the multivariate regression model for the school dataset. Predictor effects appear as lines whose lenght indicates the magnitude of the relationship and whose orientation reflects their correlations with the outcome variables shown in the plot.\n\n\n\n\nIn Figure 11.17, you can readily see:\n\nParent’s occupation and education are both significant in this view, but what is more important is their orientation. Both are positively associated with reading and math scores, but education is somewhat more related to reading than to mathematics.\nNumber of teachers and degree of parental counseling have a similar orientation, with teachers having a greater relation to mathematics scores.\nVisits to school and number of teachers are not significant in this plot, but both are positively correlated with reading and math and are coincident in the plot.\nThe parent time counseling measure, while also insignificant, tilts in the opposite direction, having different signs for reading and math.\n\nIn the pairs() plot for all three responses (Figure 11.18), we see something different in the relations for self-esteem. While occupation has a large positive relation in all the plots in the third row and column, education, counseling and teachers have negative relations in these plots, particularly with mathematics scores.\n\npairs(school.mod2, \n      fill=TRUE, fill.alpha=0.1,\n      var.cex = 2.5,\n      cex = 1.3)\n\n\n\n\n\n\nFigure 11.18: Pairwise HE plots for the three outcome variables in the multivariate regression model for the school dataset.\n\n\n\n\nThe analysis of this data is continued below in Example 11.6.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-cancor",
    "href": "11-mlm-viz.html#sec-cancor",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.10 Canonical correlation analysis",
    "text": "11.10 Canonical correlation analysis\nJust as we saw for MANOVA designs, a canonical analysis for multivariate regression involves finding a low-D view of the relations between predictors and outcomes that maximally explains their relations in terms of linear combinations of each. That is, the goal is to find weights for one set of variables, say \\(\\mathbf{X}\\) not to predict each of the other set \\(\\mathbf{Y} =[\\mathbf{y}_1, \\mathbf{y}_2, \\dots]\\) individually, but rather to also find weights for the \\(\\mathbf{y}\\)s which is most highly correlated with the linear combination of the \\(\\mathbf{x}\\)s.\nIn this sense, canonical correlation analysis (CCA) is symmetric in the \\(x\\) and \\(y\\) variables: the \\(y\\) set is not considered responses. Rather the goal is simply to explain the correlations between the two sets. For a thorough treatment of this topic, see Gittins (1985).\nGeometrically, these linear combinations are vectors representing projections in the observation space of the \\(x\\) and \\(y\\) variables, and CCA can also be thought of as minimizing the angle between these vectors or maximizing the cosine of this angle. This is illustrated in Figure 11.19.\n\n\n\n\n\n\n\nFigure 11.19: Diagram illustrating canonical correlation. For two \\(y\\) variables, all linear combinations are vectors in their plane, and similarly for the \\(x\\) variables. Maximizing the correlation between linear combinations of each is equivalent to making the angle \\(\\phi\\) between them as small as possible, or maximizing \\(\\cos({\\theta})\\), shown in the diagram at the right. The thick grey arrow indignates that the two planes should be overlaid at a common origin. Source: Re-drawn by Udi Alter following a Cross-Validated discussion by user ‘ttnphns’, https://bit.ly/4dgq2cp\n\n\n\n\nSpecifically, we want to find one set of weights \\(\\mathbf{a}_1\\) for the \\(x\\) variables and another for the \\(y\\) variables to give the linear combinations \\(\\mathbf{u}_1\\) and \\(\\mathbf{v}_1\\),\n\\[\\begin{aligned}\n\\mathbf{u}_1 & = \\mathbf{X} \\ \\mathbf{a}_1 = a_{11} \\mathbf{x}_1 + a_{12} \\mathbf{x}_2 + \\cdots + a_{11} \\mathbf{x}_q \\\\\n\\mathbf{v}_1 & = \\mathbf{Y} \\ \\mathbf{b}_1 = b_{11} \\mathbf{y}_1 + b_{12} \\mathbf{y}_1 + \\cdots + b_{11} \\mathbf{y}_p \\; ,\n\\end{aligned}\\]\nsuch that the correlation \\(\\rho_1 = \\textrm{corr}(\\mathbf{u}_1, \\mathbf{v}_1)\\) is maximized, or equivalently, minimizing the angle between them.\nUsing \\(\\mathbf{S}_{xx}\\), \\(\\mathbf{S}_{yy}\\) to represent the covariance matrices of the \\(x\\) and \\(y\\) variables, and \\(\\mathbf{S}_{xy}\\) for the cross-covariances between the two sets, the correlation between the linear combinations of each can be expressed as\n\\[\\begin{aligned}\n\\rho_1 & = \\textrm{corr}(\\mathbf{u}_1, \\mathbf{v}_1)\n         = \\textrm{corr}(\\mathbf{X} \\ \\mathbf{a}_1, \\mathbf{Y} \\ \\mathbf{b}_1) \\\\\n       & = \\frac{\\mathbf{a}_1^\\top \\ \\mathbf{S}_{xy} \\ \\mathbf{b}_1 }{\\sqrt{\\mathbf{a}_1^\\top \\ \\mathbf{S}_{xx} \\  \\mathbf{a}_1 } \\sqrt{\\mathbf{b}_1^\\top \\ \\mathbf{S}_{yy} \\ \\mathbf{b}_1 }}\n\\end{aligned}\\]\nBut, the \\(y\\) variables lie in a \\(p\\)-dimensional (observation) space, and the \\(x\\) in \\(q\\) dimensions, so what they have common is a space of \\(s = \\min(p, q)\\) dimensions. Therefore, we can find additional pairs of canonical variables,\n\\[\\begin{aligned}\n\\mathbf{u}_2 = \\mathbf{X} \\ \\mathbf{a}_2 & \\quad\\quad \\mathbf{v}_2 = \\mathbf{Y} \\ \\mathbf{b}_2 \\\\\n                                         & \\vdots \\\\\n\\mathbf{u}_s = \\mathbf{X} \\ \\mathbf{a}_s & \\quad\\quad \\mathbf{v}_s = \\mathbf{Y} \\ \\mathbf{b}_s \\\\\n\\end{aligned}\\]\nsuch that each pair \\((\\mathbf{u}_i, \\mathbf{v}_i)\\) has the maximum possible correlation and all distinct pairs are uncorrelated:\n\\[\\begin{aligned}\n\\rho_i & =\\max _{\\mathbf{a}_i, \\mathbf{b}_i}\\left\\{\\mathbf{u}_i^{\\top} \\mathbf{v}_i\\right\\} = \\\\\n\\left\\|\\mathbf{u}_i\\right\\| & =1, \\quad\\left\\|\\mathbf{v}_i\\right\\|=1, \\\\\n\\mathbf{u}_i^{{\\top}} \\mathbf{u}_j & =0, \\quad \\mathbf{v}_i^{\\top} \\mathbf{v}_j=0 \\quad\\quad \\forall j \\neq i: i, j \\in\\{1,2, \\ldots, s\\} \\ .\n\\end{aligned}\\]\nIn words, the correlations among canonical variables are zero except when when they are associated with the same canonical correlation or the weights \\((\\mathbf{a}_i, \\mathbf{b}_i)\\) for the same pair. Alternatively, all \\(p \\times q\\) correlations the variables in \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\) are fully summarized in the \\(s = \\min(p, q)\\) canonical correlations \\(\\rho_i\\) for \\(i = 1, 2, \\dots, s\\).\nThe solution, developed by Hotelling (1936), is a form of a generalized eigenvalue problem, that can be stated in two equivalent ways,\n\n\n\n\\[\n\\begin{aligned}\n& \\left(\\mathbf{S}_{y x} \\ \\mathbf{S}_{x x}^{-1} \\ \\mathbf{S}_{x y} - \\rho^2 \\ \\mathbf{S}_{y y}\\right) \\mathbf{b} = \\mathbf{0} \\\\\n& \\left(\\mathbf{S}_{x y} \\ \\mathbf{S}_{y y}^{-1} \\ \\mathbf{S}_{y x} - \\rho^2 \\ \\mathbf{S}_{x x}\\right) \\mathbf{a} = \\mathbf{0} \\ .\n\\end{aligned}\n\\] Both equations have the same form and have the same eigenvalues. And, given the eigenvectors for one of these equations, we can find the eigenvectors for the other.\n\n\n\nExample 11.6 School data: Canonical analysis\nFor the school data, with \\(p = 3\\) responses and \\(q = 5\\) predictors there are three possible sets of canonical variables. Together these account for 100% of the total linear relations between them. heplots::cancor() gives the percentage associated with each of the eigenvalues and the canonical correlations.\nFor this dataset, the first canonical variates, with Can \\(R = 0.995\\), accounts for 98.6%, so you might think that that is sufficient. Yet the likelihood ratio tests show that the second set, with Can \\(R = 0.774\\), is also significant, even though it only accounts for 1.3%.\n\nschool.can2 &lt;- cancor(\n  cbind(reading, mathematics, selfesteem) ~\n        education + occupation + visit + counseling + teacher,\n  data=schooldata[OK, ])\nschool.can2\n#&gt; \n#&gt; Canonical correlation analysis of:\n#&gt;   5   X  variables:  education, occupation, visit, counseling, teacher \n#&gt;   with  3   Y  variables:  reading, mathematics, selfesteem \n#&gt; \n#&gt;     CanR CanRSQ    Eigen  percent    cum\n#&gt; 1 0.9946 0.9892 91.41999 98.57540  98.58\n#&gt; 2 0.7444 0.5541  1.24267  1.33994  99.92\n#&gt; 3 0.2698 0.0728  0.07852  0.08466 100.00\n#&gt;                          scree\n#&gt; 1 ****************************\n#&gt; 2                             \n#&gt; 3                             \n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;    CanR LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1 0.995        0.004     67.5    15   166 &lt; 2e-16 ***\n#&gt; 2 0.744        0.413      8.5     8   122 4.1e-09 ***\n#&gt; 3 0.270        0.927      1.6     3    62    0.19    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe virtue of CCA is that all correlations between the X and Y variables are completely captured in the correlations between the pairs of canonical scores: The \\(p \\times q\\) correlations between the sets are entirely represented by the \\(s = \\min(p, q)\\) canonical ones. Whether the second dimension is useful here depends on whether it adds some interpretable increment to what is going on in these relations. One could be justifiably happy with an explanation based on the first dimension that accounts for nearly all the total association between the sets.\nThe class \"cancor\" object returned by cancor() contains the canonical coefficients, for which there is a coef() method as in candisc(), and also a scores() method to return the scores on the canonical variables, called Xcan1, Xcan2, … and Ycan1, Ycan2.\n\nnames(school.can2)\n#&gt;  [1] \"cancor\"    \"names\"     \"ndim\"      \"dim\"       \"coef\"     \n#&gt;  [6] \"scores\"    \"X\"         \"Y\"         \"weights\"   \"structure\"\n#&gt; [11] \"call\"      \"terms\"\n\nYou can use the plot() method or heplot() method to visualize and help interpret the results. The plot() method plots the canonical scores$X against the scores$Y for a given dimension (selected by the which argument). The id.n argument gives a way to flag noteworthy observations.\n\nplot(school.can2, \n     pch=16, id.n = 3,\n     cex.lab = 1.5, id.cex = 1.5,\n     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))\ntext(-2, 1.5, paste(\"Can R =\", round(school.can2$cancor[1], 3)), \n     cex = 1.4, pos = 4)\n\nplot(school.can2, which = 2, \n     pch=16, id.n = 3,\n     cex.lab = 1.5, id.cex = 1.5,\n     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))\ntext(-3, 3, paste(\"Can R =\", round(school.can2$cancor[2], 3)), \n     cex = 1.4, pos = 4)\npar(op)\n\n\n\n\n\n\nFigure 11.20: Plots of canonical scores for the first two canonical dimensions of the schooldata dataset, omitting the two highly influential cases.\n\n\n\n\nIt is worthwhile to look at an analogous plot of canonical scores for the original dataset including the two highly influential cases. As you can see in Figure 11.21, cases 44 and 59 are way outside the range of the rest of the data. Their influence increases the canonical correlation to a near perfect \\(\\rho = 0.997\\).\n\nschool.can &lt;- cancor(cbind(reading, mathematics, selfesteem) ~\n                       education + occupation + visit + counseling + teacher,\n                     data=schooldata)\nplot(school.can, \n     pch=16, id.n = 3,\n     cex.lab = 1.5, id.cex = 1.5,\n     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))\ntext(-5, 1, paste(\"Can R =\", round(school.can$cancor[1], 3)), \n     cex = 1.4, pos = 4)\n\n\n\n\n\n\nFigure 11.21: Plots of canonical scores on the first canonical dimension for the schooldata, including the influential cases, which stand out as so far frome the rest of the observations.\n\n\n\n\nPlots of canonical scores tell us of the strength of the canonical dimensions, but do not help interpreting the analysis in relation to the original variables. The HE plot version for canonical correlation analysis re-fits a multivariate regression model for the Y variables against the Xs, but substitutes the canonical scores for each, essentially projecting the data into canonical space.\nTODO: Check out signs of structure coefs from cancor(). Would be better to reflect the vectors for Ycan1.\n\nheplot(school.can2,\n       fill = TRUE, fill.alpha = 0.2,\n       var.col = \"red\", \n       asp = NA, scale = 0.25,\n       cex.lab = 1.5, cex = 1.25,\n       prefix=\"Y canonical dimension \")\n\n\n\n\n\n\nFigure 11.22: HE plot for the canonical correlation analysis of the schooldata. Vectors for the variables indicate their correlations with the canonical dimensions.\n\n\n\n\nThe red variable vectors shown in these plots are intended only to show the correlations of Y variables with the canonical dimensions. The fact that they are so closely aligned reflects the fact that the first dimension accounts for nearly all of their associations with the predictors. The orientation of the \\(\\mathbf{H}\\) ellipses/lines reflects the projection of those from Figure 11.18 into canonical space\nOnly their relative lengths and angles with respect to the Y canonical dimensions have meaning in these plots. Relative lengths correspond to proportions of variance accounted for in the Y canonical dimensions plotted; angles between the variable vectors and the canonical axes correspond to the structure correlations. The absolute lengths of these vectors are arbitrary and are typically manipulated by the scale argument to provide better visual resolution and labeling for the variables.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-he-mancova",
    "href": "11-mlm-viz.html#sec-he-mancova",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.11 MANCOVA models",
    "text": "11.11 MANCOVA models\nHE plots for designs containing a collection of quantitative predictors and one or more factors are quite simple in MANCOVA models where the effects are additive, i.e., don’t involve interactions. They are a bit more challenging when you allow separate slopes for groups on all quantitative variables, because there get to be too many terms to usefully display. But these models are more complicated!\nIf the evidence for heterogeneity of regressions is not very strong, it is still useful to fit the MANCOVA model and display it in an HE plot.\nAn alternative is to fit separate models for the groups and display these as HE plots. As noted earlier (Section 10.8.1), this is not ideal for testing hypotheses, but provides a useful and informative display of the relations between the predictors and responses and the groups effect. I illustrate these approaches for the Rohwer data, encountered in Section 10.8.1, below.\n\n\nExample 11.7 Rowher data\nIn Section 10.8.1 I fit several models for Rohwer’s data on the relations between paired-associate tasks and scholastic performance. The first model was the MANCOVA model testing the difference between the high and low SES groups, controlling for, or taking into account differences on the paired-associate task.\n\nRohwer.mod1 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES + n + s + ns + na + ss, \n                 data=Rohwer)\n\nHE plots for this model for the pairs (SAT, PPVT) and (SAT, Raven) is shown in Figure 11.23. The result of an overall test for all predictors, \\(\\mathcal{H}_0 : \\mathbf{B} = \\mathbf{0}\\), is added to the basic plot using the hypotheses argument.\n\ncolors &lt;- c(\"red\", \"blue\", rep(\"black\",5), \"#969696\")\ncovariates &lt;- rownames(coef(Rohwer.mod1))[-(1:2)]\npairs(Rohwer.mod1, \n       col=colors,\n       hypotheses=list(\"Regr\" = covariates),\n       fill = TRUE, fill.alpha = 0.1,\n       cex=1.5, cex.lab = 1.5, var.cex = 3,\n       lwd=c(2, rep(3,5), 4))\n\n\n\n\n\n\nFigure 11.23: All-pairs HE plot for SAT, PPVT and Raven using the MANCOVA model. The ellipses labeled ‘Regr’ show the test of the overall effect of the quantitative predictors.\n\n\n\n\nThe positive effect of SES on the outcome measures is seen in all pairwise plots: the high SES group is better on all responses. The positive orientation of the Regr ellipses for the covariates shows that the predicted values for all three responses are positively correlated (more so for SAT and PPVT): higher performance on the paired associate tasks, in general, is associated with higher academic performance. The two significant predictors, na and ns are the only ones that extend outside the error ellipses, but their orientations differ.\nHomogeneity of regression\nA second model tested the assumption of homogeneity of regression by adding interactions of SES with the PA tasks, allowing separate slopes for the two groups on each of the other predictors.\n\nRohwer.mod2 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES * (n + s + ns + na + ss),\n                  data=Rohwer)\n\nThis model has 11 terms, excluding the intercept: SES, plus 5 main effects (\\(x\\)s) for the predictors and 5 interactions (slope differences), too many for an understandable display. To visualize this in an HE plot (Figure 11.24), I simplify, by showing the interaction terms collectively by a single ellipse, representing their joint effect, and specified as a linear hypothesis called slopes that picks out the interaction effects.\nThe argument terms limits the \\(\\mathbf{H}\\) ellipses for the right-hand-side of the model which are shown to just those terms specified. The combined effect of the interaction terms is specified as an hypothesis (slopes) testing the interaction terms (which have a “:” in their name). Because SES is “treatment-coded” in this model, the interaction terms reflect the difference in slopes for the high SES group compared to the low.\n\n(coefs &lt;- rownames(coef(Rohwer.mod2)))\n#&gt;  [1] \"(Intercept)\" \"SESLo\"       \"n\"           \"s\"          \n#&gt;  [5] \"ns\"          \"na\"          \"ss\"          \"SESLo:n\"    \n#&gt;  [9] \"SESLo:s\"     \"SESLo:ns\"    \"SESLo:na\"    \"SESLo:ss\"\n\ncolors &lt;- c(\"red\", \"blue\", rep(\"black\",5), \"#969696\")\nheplot(Rohwer.mod2, col=c(colors, \"darkgreen\"), \n       terms=c(\"SES\", \"n\", \"s\", \"ns\", \"na\", \"ss\"), \n       hypotheses=list(\"Regr\" = c(\"n\", \"s\", \"ns\", \"na\", \"ss\"),\n                       \"Slopes\" = coefs[grep(\":\", coefs)]),\n       fill = TRUE, fill.alpha = 0.2, cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.24: HE plot for SAT and PPVT using the heterogeneous regression model. The ellipse labeled ‘Regr’ shows the test of the covariates combined, and the ellipse labeled ‘slopes’ shows the combined difference in slopes between the two groups.\n\n\n\n\nSeparate models\nWhen there is heterogeneity of regressions, using submodels for each of the groups has the advantage that you can easily visualize the slopes for the predictors in each of the groups, particularly if you overlay the individual HE plots. In this example, I’m using the models Rohwer.sesLo and Rohwer.sesLo fit to each of the groups.\n\nRohwer.sesLo &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Lo\")\nRohwer.sesHi &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Hi\")\n\nHere I make use of the fact that several HE plots can be overlaid using the option add=TRUE as shown in Figure 11.25. The axis limits may need adjustment in the first plot so that the second one will fit.\n\nheplot(Rohwer.sesLo, \n       xlim = c(0,100),               # adjust axis limits\n       ylim = c(40,110), \n       col=c(\"red\", \"black\"), \n       fill = TRUE, fill.alpha = 0.1,\n       lwd=2, cex=1.2, cex.lab = 1.5)\nheplot(Rohwer.sesHi, \n       add=TRUE, \n       col=c(\"brown\", \"black\"), \n       grand.mean=TRUE, \n       error.ellipse=TRUE,            # not shown by default when add=TRUE\n       fill = TRUE, fill.alpha = 0.1,\n       lwd=2, cex=1.2)\n\n\n\n\n\n\nFigure 11.25: Overlaid HE plots for SAT and PPVT, for the low and high SES groups, when each group is fit separately.\n\n\n\n\nWe can readily see the difference in means for the two SES groups (Hi has greater scores on both variables) and it also appears that the slopes of the s and n predictor ellipses are shallower for the High than the Low group, indicating greater relation with the SAT score. As well, the error ellipses show that on these measures, error variation is somewhat smaller in the low SES group.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#what-have-we-learned",
    "href": "11-mlm-viz.html#what-have-we-learned",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.12 What have we learned?",
    "text": "11.12 What have we learned?\n\nHE plots clarify complex multivariate models through direct visualization - The HE plot framework solves the interpretability problem of multivariate models by visualizing hypothesis (H) ellipses against error (E) ellipses. Rather than navigate a confusing maze of tables of coefficients and test statistics, with HE plots you can see which effects matter, how they relate to each other, and whether they’re statistically significant. All of these benefits are given in a single, intuitive plot that reveals the geometric structure underlying your multivariate analysis.\nVisual hypothesis testing beats \\(p\\)-value hunting - HE plots make hypothesis testing immediate and intuitive: if a hypothesis ellipse extends outside the error ellipse (under significance scaling), the effect is significant. No more scanning tables of p-values or wrestling with multiple comparisons—the geometry tells the story directly. You can even decompose overall effects into meaningful contrasts and see their individual contributions as separate ellipses.\nEllipse orientations reveal the hidden correlational structure of your effects - The angles between hypothesis ellipses in HE plots directly show how different predictors relate to your response variables and to each other. When effect ellipses point in similar directions, those predictors have similar multivariate signatures; when they’re orthogonal, they capture independent aspects of variation. This geometric insight goes far beyond what correlation matrices can reveal.\nCanonical space is your secret weapon for high-dimensional visualization - When you have many response variables, canonical discriminant analysis and canonical correlation analysis project the complex multivariate relationships into a lower-dimensional space that captures the essential patterns. This isn’t just dimension reduction—it’s insight amplification, revealing the fundamental directions of variation that matter most while showing how your original variables contribute to these meaningful dimensions.\nMultivariate models become tractable through progressive visual summaries - The chapter demonstrates a powerful visualization strategy: start with scatterplot matrices to see the raw data structure, move to HE plots to understand model effects, then project to canonical space for the clearest possible view of multivariate relationships. Each step preserves the essential information while making it more interpretable, turning the complexity of multivariate analysis into a comprehensible visual narrative.\n\n\n\n\n\nAnderson, E. (1935). The irises of the Gaspé peninsula. Bulletin of the American Iris Society, 35, 2–5.\n\n\nBartlett, M. S. (1938). Further aspects of the theory of multiple regression. Mathematical Proceedings of the Cambridge Philosophical Society, 34(1), 33–40. https://doi.org/10.1017/s0305004100019897\n\n\nBodmer, W., Bailey, R. A., Charlesworth, B., Eyre-Walker, A., Farewell, V., Mead, A., & Senn, S. (2021). The outstanding scientist, r.a. Fisher: His views on eugenics and race. Heredity, 126(4), 565–576. https://doi.org/10.1038/s41437-020-00394-6\n\n\nFisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x\n\n\nFriendly, M., & Fox, J. (2025). Candisc: Visualizing generalized canonical discriminant and canonical correlation analysis. https://github.com/friendly/candisc/\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGayan De Silva. (2020). Exploring the world of artificial neural networks -a beginner’s overview. https://doi.org/10.13140/RG.2.2.14790.14406\n\n\nGittins, R. (1985). Canonical analysis: A review with applications in ecology. Springer-Verlag.\n\n\nHotelling, H. (1936). Relations between two sets of variates. Biometrika, 28(3/4), 321. https://doi.org/10.2307/2333955\n\n\nHuang, F. L. (2019). MANOVA: A procedure whose time has passed? Gifted Child Quarterly, 64(1), 56–60. https://doi.org/10.1177/0016986219887200",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#footnotes",
    "href": "11-mlm-viz.html#footnotes",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "See the heplots vignettes vignette(\"HE_manova\", package=\"heplots\") and vignette(\"HE_mmra\", package=\"heplots\"). ↩︎\nIn smallish samples (\\(n &lt; 30\\)) we use the better approximations, \\(c = \\sqrt{2 F_{2, n-2}^{0.68}}\\) for 2D plots and \\(c = \\sqrt{3 F_{3, n-3}^{0.68}}\\) for 3D. The difference between these is usually invisible in plots.↩︎\nFor example, Megan Stodel in a blog post Stop using iris says, “It is clear to me that knowingly using work that was itself used in pursuit of racist ideals is totally unacceptable.” A Reddit discussion on this topic, Is it socially acceptable to use the Iris dataset? has some interesting replies.↩︎\nRecall that \\(R^2\\) for a linear model is the the proportion of variation in the response that is explained by the model, calculated as \\(R^2 = \\text{SS}_H / \\text{SS}_T = \\text{SS}_H / (\\text{SS}_H + \\text{SS}_E )\\). For a multivariate model, these are obtained from the diagonal elements of \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\).↩︎\nThat the \\(\\mathbf{H}\\) ellipses for the contrasts subtend that for the overall test of Species is no accident. In fact, this is true in \\(p\\)-dimensional space for any linear hypothesis, and orthogonal contrasts have the additional geometric property that they form conjugate axes for the overall \\(\\mathbf{H}\\) ellipsoid relative to the \\(\\mathbf{E}\\) ellipsoid (Friendly et al., 2013).↩︎\nThe related candiscList() function performs a generalized canonical discriminant analysis for all terms in a multivariate linear model, returning a list of the results for each factor.↩︎\nIf significance scaling was used the interpretation of the canonical HE plot plot would the same as before: if the hypothesis ellipse extends beyond the error ellipse, then that dimension is significant.↩︎\nAs in univariate linear models, any factors or covariates ignored in the model formula have their effects pooled with the error term, reducing the sensitivity of their tests.↩︎\nYou could test this comparison with a more focused 1 df linear hypothesis using the contrast \\((1, -\\frac12, -\\frac12)\\) for the levels of Attr.↩︎\nFor MANOVA designs there is a separate analysis for each term in the model because there are different weights for the response variables.↩︎\nAn alternative to fitting the model removing specific cases deemed troublesome is to use a robust method, such as heplots::roblm(). This uses re-weighted least squares to down-weight observations with large residuals or other problems. These methods are illustrated in Section 13.5.↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  }
]