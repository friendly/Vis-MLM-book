[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "",
    "text": "Preface\nThis book is about graphical methods developed recently for multivariate data, and their uses in understanding relationships when there are several aspects to be considered together. Data visualization methods for statistical analysis are well-developed and widely available in R for simple linear models with a single outcome variable, as well as for more complex models with nonlinear effects, hierarchical data with observations grouped within larger units and so forth.\nHowever, with applied research in the sciences, social and behavioral in particular, it is often the case that the phenomena of interest (e.g., depression, job satisfaction, academic achievement, childhood ADHD disorders, etc.) can be measured in several different ways or related aspects. Understanding how these different aspects are related can be crucial to our knowledge of the general phenomenon.\nFor example, if academic achievement can be measured for adolescents by reading, mathematics, science and history scores, how do predictors such as parental encouragement, school environment and socioeconomic status affect all these outcomes? In a similar way? In different ways? In such cases, much more can be understood from a multivariate approach that considers the correlations among the outcomes. Yet, sadly, researchers typically examine the outcomes one by one which often only tells part of the data story.\nHowever, to do this it is useful to set the stage for multivariate thinking, with a grand scheme for statistics and data visualization, a parable, and an example of multivariate discovery.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Features",
    "text": "Features\nSome key substantive and pedagogical features of the book are:\n\nThe writing style is purposely pedagogical (hopefully not too pedantic), in that it aims to teach how to think about analysis and graphics for multivariate data. That is, I try to convey how you can achieve understanding of statistical concepts and data visualization through ways of representing those ideas in diagrams and R functions and packages.\nStatistical data visualization is cast in a general framework by their goal for communicating information, either to your self or others (such as see the data, visualize a model, diagnose problems), rather than a categorization by graphic types. It is best informed by principles and goals of communication, for example making graphic comparison easy and ordering factors and variables according to what should be seen (effect ordering).\nData visualization is seen as a combination of exposure—plotting the raw data—and summarization— plotting statistical summaries—to highlight what should be noticed. For example, data ellipses and confidence ellipses are widely used as simple, effective summaries of data and fitted model parameters. When the data is complex, the idea of visual thinning can be used to balance the tradeoff.\nThe book exploits the rich connections among statistics, geometry and data visualization. Statistical ideas, particularly for multivariate data, can be more easily understood in terms of geometrical ones that can be seen in diagrams and data displays. Moreover, ideas from one domain can amplify what we can understand from another.\nThese graphical tools can be used to understand or explain a wide variety of statistical concepts, phenomena, and paradoxes such as Simpson’s paradox, effects of measurement error, and so forth.\nThe HE (“hypothesis - error”) plot framework provides a simple way to understand the results of statistical tests and the relations among response outcomes in the multivariate linear model.\nDimension reduction techniques such as PCA and discriminant analysis are presented as “multivariate juicers,” able to squeeze the important information in high-dimensional data into informative two-dimensional views.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-i-assume",
    "href": "index.html#what-i-assume",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "What I assume",
    "text": "What I assume\nIt is assumed that the reader has at least a basic background in applied, intermediate statistics. This would normally include material on simple regression and multiple regression as well as simple analysis of variance (ANOVA) designs. It also means that you should be familiar with the basic ideas of statistical inference including hypothesis tests and confidence intervals. \nThere will also be some mathematics in the book where words and diagrams are not enough. The mathematical level will be intermediate, mostly consisting of simple algebra. No derivations, proofs, theorems here!\nFor multivariate methods, it will be useful to express ideas using matrix notation to simplify presentation. It will be enough for you to recognize that a single symbol \\(\\mathbf{y}\\) can be a shorthand for \\(n\\) scores on a variable like weight, and the symbol \\(\\mathbf{X}\\) can represent an entire data table, with, say \\(n\\) observations on \\(p\\) variables like height, body mass index, diet components, and so forth. Then, the notation \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta}\\) represents an entire linear model to relate weight to these other variables. I’m using this math notation to express ideas, and all you will need is a reading-level of understanding.\nFor this, the first chapter of Fox (2021), A Mathematical Primer for Social Statistics, is excellent, and the rest is well worth reading. If you want to learn something of using matrix algebra for data analysis and statistics in R, I recommend our package matlib (Friendly et al., 2024).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-resources",
    "href": "index.html#r-resources",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "R Resources",
    "text": "R Resources\nI also assume the reader to have at least a basic familiarity with statistical analysis in R. While R fundamentals are outside the scope of the book, I believe that this language provides a rich set of resources, far beyond that offered by other statistical software packages, and is well worth learning. For those not familiar with R or wish to learn new skills, I recommend:\n\nCotton (2013), Learning R (online) provides a well-rounded basic introduction to R, covering data types, lists and data frames, functions, packages and workflow for data analysis and graphics;\nMatloff(2011), The Art of R Programming (online) is devoted to learning the programming features of R …\nWickham(2019), Advanced R (online) is aimed at intermediate R programmers who want to dive deeper into R and learn how things work,\nLong & Teetor (2019), R Cookbook 2\\(^{nd}\\) Ed (online) provides how-to recipies for basic tasks from working with RStudio, to input and output, general statistics, graphics, and regression / ANOVA;\nFox & Weisberg (2018), An R Companion to Applied Regression is a fantastic resource for learning how to perform statistical analyses in R and visualize results with insightful graphics. It is the companion book to Fox’s (2016), Applied Regression Analysis and Generalized Linear Models, which I consider the best intermediate-level modern treatment of these topics. I make heavy use of the accompanying car package (Fox & Weisberg, 2019) which provides important and convenient graphical methods.\n\nWhen you work with R, it may be useful to have this collection of R and RStudio cheatsheets I prepared for my graduate data visualization course.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-graphics-resources",
    "href": "index.html#r-graphics-resources",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "R graphics resources",
    "text": "R graphics resources\nIn this book, I create a large number of graphs in R, and have aimed to present and describe how I do them using R packages and code to manipulate the data or numerical output from analysis function, so that you can learn from these examples to apply these ideas to your own data.\nIn writing this, I’ve also tried to exemplify graphical principles that underlie effective graphic communication. You might find the lecture notes, extensive resources and R examples for my course, Psychology of Data Visualization useful.\nIn addition, there are a few books I recommend:\n\n            \n\n \n\nClaus Wilke (2019), Fundamentals of Data Visualization (online) A Well thought out presentation of important ideas of graphic presentation; it covers a wide range of topics, with good practical advice and lots of examples. How to do these in R is covered in his course notes\nKeiran Healy (2019), Data Visualization: A Practical Introduction (online). A highly accessible, hands-on primer on how to create effective graphics from data using ggplot2, with a focus on how to think about the information you want to show.\nAntony Unwin (2024), Getting (more out of) Graphics. This book offers a collection of 25 case studies of interesting datasets, exemplifying desirable features of graphs use to understand them and using ggplot2 graphics. A second part provides useful advice on graphical practice, drawing on the lessons of the examples from the first part. The R code for all chapters is available online.\nNicola Rennie (2025), The Art of Data Visualization with ggplot2 (online). Rennie offers a kind of master class in designing effective, attractive graphics using ggplot2. The examples chosen stem from the weeklyTidy Tuesday challenges that invite graphic programmers and designers to to work on a shared dataset to see what they can do.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-coding-style-used-here",
    "href": "index.html#r-coding-style-used-here",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "R coding style used here",
    "text": "R coding style used here\n\n\n\n\n\n\nNote\n\n\n\nThe coding style for computing and graphics used in this book are expressed using both the traditional functional syntax, f(g(x)) and the newer approach using pipes (|&gt;) of the tidyverse. Similiarly, I use both base R graphics and plots based on the “ggverse” of ggplot2 and it’s large family of add-on and extension packages.1\nHow and why it is this way should be explained to the reader. The material below is just a start.\n\n\nLike natural language and the graphic methods used in this book, R syntax and the programming style for graphics has evolved considerably since R was first introduced by Ross Ihaka and Robert Gentleman in 1992. It was originally based on the S programming language (Becker et al., 1988) and designed as a functional language. This means that programs are constructed by applying and composing functions, like log(x), exp(x), which return a value. …\n\npipes\ntidyverse\nR graphics: plot() -&gt; ggplot()\n\n\nRather than being dogmatic about using the newest, most politically correct style,2 in this book I have taken the view that what is most important about programming and graphics software is that they serve as a route—as short and direct as possible—between having an idea in your head of what you want to do, and seeing the result on your screen or in your report, as illustrated in Figure 1.\n\n\n\n\n\n\n\nFigure 1: The expressive power of graphics software can be considered as minimizing the path from an idea in your head to finished graphic.\n\n\n\n\nConsequently, for nearly every graph in this book, I used what I considered to be the most effective style to produce an admirable graphic, but—perhaps more importantly–to be able to describe how I coded that to the reader.\nFor example, the car package and my heplots and related packages use base-R graphics, but I could customize their use in examples by using the conventions of points(), lines(), text() and even par() when necessary. However, if I was starting this project anew, I would now use tinyplot (McDermott et al., 2025), which has removed many of the cringeworthy features of base-R graphics.\nOn the other hand, ggplot2 package was designed to be an elegant language, based on the grammar of graphics (Wilkinson, 1999). It allows you to think of building plots logically and coherently, layer by layer. Instead of memorizing specific function calls and their arguments for every type of chart, you learn a flexible, high-level language for describing what you want your graphic to look like. This promotes a more structured thinking about data visualization, making it easier for you to iterate3, as we always do, to create beautiful, publication-quality graphics.\nThis is great in theory, but as you will see here, in many code examples, beyond the basic geom_* elements, a good deal of the effort to produce them required multiple steps to (a) get my data into a tidy format, (b) assign proper scale_*s to data variables and (c) use theme() arguments to control the large and small aspects of graphic design that contribute to the elegance and potential beauty of finished product you see.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#typographic-conventions-used-in-this-book",
    "href": "index.html#typographic-conventions-used-in-this-book",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Typographic conventions used in this book",
    "text": "Typographic conventions used in this book\nThe following typographic conventions are used in this book:\n\nitalic : indicates terms or phrases to be emphasized or defined in the text; bold : is used for terms to be given strong emphasis, particularly for their first mention.\nPackage names are printed in bold and colored brown, for example ggplot2, car and the matlib package. These uses generate citations like ggplot2 (Wickham, 2016) on their first use. Package references in the text are automatically indexed, individually and under a “Packages” heading.\nDatasets are rendered as their name in monospaced font, like Prestige or indicating the package from which they come, as in carData::Prestige. These too are automatically indexed.\nA monospaced typewriter font is used in the text to refer to variable and function names, such as education and plot(). This font is also for R statement elements, keywords and code fragments as well.\nR code in program listings and output is presented in a monospaced (typewriter) font, fira mono\n\n\nFor R functions in packages, I use the notation package::function(), such as car::Anova(), to identify that the Anova() function is defined in the car package. This also means you can get help on a function by typing ?car::Anova in the console, or a list of its arguments and default values from args(car::Anova).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI am profoundly grateful to my friends and colleagues John Fox, Georges Monette and Phil Chalmers at York University who have inspired me with their insights into statistical thinking and visualization of statistical ideas over many years. They also contributed greatly to the R packages that help make the methods of this book accessible and easy to use.\nThere is also a host of graduate students I have taught, supervised and worked with over my 50+ year career. Among these, Ernest Kwan and Matthew Sigal were important contributors to the development of data visualization ideas and techniques reflected here. Agnieska Kopinska, Gigi Luk and Touraj Amiri were TAs and RAs who contributed to my teaching and research. Most recently, Udi Alter and Michael Truong worked as research assistants and helped me in numerous ways with work on this book.\nWriting this book using Quarto within the RStudio (now Posit) development environment has presented technical challenges I had not encountered in previous books. I am grateful to Mickaël Canouil, Christophe Dervieux, Felix Benning and others in the quarto-dev community who graciously helped me solve many issues. The book also relies heavily on the graphic ideas and software of many R developers, including Cory Brunson, Vincent Arel-Bundock, Di Cook, John Fox, Duncan Murdoch, who replied to issues and feature requests on their packages.\n\n\n\n\n\nBecker, R. A., Chambers, J. M., & Wilks, A. R. (1988). The new S language. Wadsworth & Brooks/Cole.\n\n\nCotton, R. (2013). Learning R. O’Reilly Media.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (Third edition.). SAGE.\n\n\nFox, J. (2021). A mathematical primer for social statistics (2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878835\n\n\nFox, J., & Weisberg, S. (2018). An R companion to applied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFox, J., & Weisberg, S. (2019). An R companion to applied regression (Third). Sage. https://www.john-fox.ca/Companion/\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix functions for teaching and learning linear algebra and multivariate statistics. https://github.com/friendly/matlib\n\n\nHealy, K. (2019). Data visualization: A practical introduction. Princeton University Press. http://www.socviz.co\n\n\nLong, J. D., & Teetor, P. (2019). R cookbook: Proven recipes for data analysis, statistics, and graphics. O’Reilly. https://rc2e.com/\n\n\nMatloff, N. (2011). The art of R programming: A tour of statistical software design. No Starch Press.\n\n\nMcDermott, G., Arel-Bundock, V., & Zeileis, A. (2025). tinyplot: Lightweight extension of the base r graphics system. https://doi.org/10.32614/CRAN.package.tinyplot\n\n\nRennie, N. (2025). The art of data visualization with ggplot2. CRC Press. https://nrennie.rbind.io/art-of-viz/\n\n\nUnwin, A. (2024). Getting (more out of) graphics: Practice and principles of data visualisation. Chapman; Hall/CRC. https://doi.org/10.1201/9781003131212\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWickham, H. (2019). Advanced r. Chapman; Hall/CRC. https://doi.org/10.1201/9781351201315\n\n\nWilke, C. O. (2019). Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. clauswilke.com/dataviz\n\n\nWilkinson, L. (1999). The grammar of graphics. Springer.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "",
    "text": "The official ggplot2 extensions gallery website, reports 151 registered extensions available for exploration as of this writing.↩︎\nSee Norm Matloff’s essay Tidyverse Skeptic. He argues that the tidyverse is not a good vehicle for teaching novice, noncoders, and that using base-R as the vehicle of instruction brings students to a more skilled level, in shorter time.↩︎\nYou should think of the “80-20” graphics rule when you work. This says you can produce 80% of your finished graphic with 20% of your total effort. But the corollary is that it takes you 80% of your time to fix the limitations of the remaining 20%.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "04-pca-biplot.html",
    "href": "04-pca-biplot.html",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "4.1 Flatland and Spaceland\nThere was a cloud in the sky above Flatland one day. But it was a huge, multidimensional cloud of sparkly points that might contain some important message, perhaps like the hidden EUREKA (Figure 5), or perhaps forecasting the upcoming harvest, if only Flatlanders could appreciate it.\nA leading citizen, A SQUARE, who had traveled once to Spaceland and therefore had an inkling of its majesty beyond the simple world of his life in the plane looked at that cloud and had a brilliant thought, an OMG moment:\nAs it happened, our Square friend, although he could never really see in three dimensions, he could now at least think of a world described by height as well as breadth and width, and think of the shadow cast by a cloud as something mutable, changing size and shape depending on its’ orientation over Flatland.\nAnd what a world it was, inhabited by Pyramids, Cubes and wondrous creatures called Polyhedrons with many \\(C\\)orners, \\(F\\)aces and \\(E\\)dges. Not only that, but all those Polyhedra were forced in Spaceland to obey a magic formula: \\(C + F - E = 2\\).1 How cool was that!\nIndeed, there were even exalted Spheres, having so many faces that its surface became as smooth as a baby’s bottom with no need for pointed corners or edges, just as Circles were the smoothest occupants of his world with far too many sides to count. It was his dream of a Sphere passing through Flatland (Figure 1) that first awakened him to a third dimension.\nHe also marveled at Ellipsoids, as smooth as Spheres, but in Spaceland having three natural axes of different extent and capable of being appearing fatter or slimmer when rotated from different views. An Ellipsoid had magical properties: it could appear as so thin in one or more dimensions that it became a simple 2D ellipse, or a 1D line, or even a 0D point (Friendly et al., 2013).\nAll of these now arose in Square’s richer 3D imagination. And, all of this came from just one more dimension than his life in Flatland.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-spaceland",
    "href": "04-pca-biplot.html#sec-spaceland",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "It is high time that I should pass from these brief and discursive notes about Flatland to the central event of this book, my initiation into the mysteries of Space. THAT is my subject; all that has gone before is merely preface — Edwin Abbott, Flatland, p. 57.\n\n\n\n\nOh, can I, in my imagination, rotate that cloud and squeeze its juice so that it rains down on Flatland with greatest joy?”\n\n\n\n\n\n\n\n4.1.1 Multivariate juicers\nUp to now, we have also been living in Flatland. We have been trying to understand data in data space of possibly many dimensions, but confined to the 2D plane of a graph window. Scatterplot matrices and parallel coordinate plots provided some relief. The former did so by projecting the data into sets of 2D views in the coordinates of data space; the latter did so by providing multiple axes in a 2D space along which we could trace the paths of individual observations.\nThis chapter is about seeing data in a different projection, a low-dimensional (usually 2D) space that squeezes out the most juice from multidimensional data for a particular purpose (Figure 4.1), where what we want to understand can be more easily seen.\n\n\n\n\n\n\n\nFigure 4.1: A multivariate juicer takes data from possibly high-dimensional data space and transforms it to a lower-dimenional space in which important effects can be more easily seen.\n\n\n\n\nHere, I concentrate on principal components analysis (PCA), whose goal reflects A Square’s desire to see that sparkly cloud of points in \\(nD\\) space in the plane showing the greatest variation (squeezing the most juice) among all other possible views. This appealed to his sense of geometry, but left him wondering how the variables in that high-D cloud were related to the dimensions he could see in a best-fitting plane.\nThe idea of a biplot, showing the data points in the plane, together with thick pointed arrows—variable vectors— in one view is the other topic explained in this chapter (Section 4.3). The biplot is the simplest example of a multivariate juicer. The essential idea is to project the cloud of data points in \\(n\\) dimensions into the 2D space of principal components and simultaneously show how the original variables relate to this space. For exploratory analysis to get an initial, incisive view of a multivariate dataset, a biplot is often my first choice.\n\n\n\n\n\n\nLooking ahead\n\n\n\nI’m using the term multivariate juicer here to refer the wider class of dimension reduction techniques, used for various purposes in data analysis and visualization. PCA is the simplest example and illustrates the general ideas.\nThe key point is that these methods are designed to transform the data into a low-dimensional space for a particular goal or purpose. In PCA, the goal is to extract the greatest amount of total variability in the data. In the context of univariate multiple regression, the goal is often to reduce the number of predictors necessary to account for an outcome variable, called feature extraction in the machine learning literature.\nWhen the goal is to best distinguish among groups discriminant analysis finds uncorrelated weighted sums of predictors on which the means of groups are most widely separated in a reduced space of hopefully fewer dimensions.\nThe methods I cover in this book are all linear methods, but there is also a wide variety of non-linear dimension reduction techniques.\n\n\nPackages\nIn this chapter I use the following packages. Load them now:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(ggbiplot)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(car)\nlibrary(ggpubr)\nlibrary(matlib)",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-pca",
    "href": "04-pca-biplot.html#sec-pca",
    "title": "4  Dimension Reduction",
    "section": "\n4.2 Principal components analysis (PCA)",
    "text": "4.2 Principal components analysis (PCA)\nWhen Francis Galton (1886) first discovered the idea of regression toward the mean and presented his famous diagram (Figure 3.10), he had little thought that he had provided a window to a higher-dimensional world, beyond what even A Square could imagine. His friend, Karl Pearson (1896) took that idea and developed it into a theory of regression and a measure of correlation that would bear his name, Pearson’s \\(r\\).\nBut then Pearson (1901) had a further inspiration, akin to that of A Square. If he also had a cloud of sparkly points in \\(2, 3, 4, ..., p\\) dimensions, could he find a point (\\(0D\\)), or line (\\(1D\\)), or plane (\\(2D\\)), or even a hyperplane (\\(nD\\)) that best summarized — squeezed out the most juice—from multivariate data? This was the first truly multivariate problem in the history of statistics (Friendly & Wainer, 2021, p. 186).\nThe best \\(0D\\) point was easy— it was simply the centroid, the means of each of the variables in the data, \\((\\bar{x}_1, \\bar{x}_2, ..., \\bar{x}_p)\\), because that was “closest” to the data in the sense of minimizing the sum of squared differences, \\(\\Sigma_i\\Sigma_j (x_{ij} - \\bar{x}_j)^2\\). In higher dimensions, his solution was also an application of the method of least squares, but he argued it geometrically and visually as shown in Figure 4.2.\n\n\n\n\n\n\n\nFigure 4.2: Karl Pearson’s (1901) geometric, visual argument for finding the line or plane of closest fit to a collection of points, P1, P2, P3, …\n\n\n\n\nFor a \\(1D\\) summary, the line of best fit to the points \\(P_1, P_2, \\dots P_n\\) is the line that goes through the centroid and made the average squared length of the perpendicular segments from those points to a line as small as possible. This was different from the case in linear regression, for fitting \\(y\\) from \\(x\\), where the average squared length of the vertical segments, \\(\\Sigma_i (y_i - \\hat{y}_i)^2\\) was minimized by least squares.\nHe went on to prove the visual insights from simple smoothing of Galton (1886) (shown in Figure 3.10) regarding the regression lines of y ~ x and x ~ y. More importantly, he proved that the cloud of points is captured, for the purpose of finding a best line, plane or hyperplane, by the ellipsoid that encloses it, as seen in his diagram, Figure 4.3. The major axis of the 2D ellipse is the line of best fit, along which the data points have the smallest average squared distance from the line. The axis at right angles to that—the minor axis— is labeled “line of worst fit” with the largest average squared distance.\n\n\n\n\n\n\n\nFigure 4.3: Karl Pearson’s diagram showing the elliptical geometry of regression and principal components analysis … Source: Pearson (1901), p. 566.\n\n\n\n\nEven more importantly— and this is the basis for PCA — he recognized that the two orthogonal axes of the ellipse gave new coordinates for the data which were uncorrelated, whatever the correlation of \\(x\\) and \\(y\\).\n\nPhysically, the axes of the correlation type-ellipse are the directions of independent and uncorrelated variation. — Pearson (1901), p. 566.\n\nIt was but a small step to recognize that for two variables, \\(x\\) and \\(y\\):\n\nThe line of best fit, the major axis (PC1) had the greatest variance of points projected onto it.\nThe line of worst fit, the minor axis (PC2), had the least variance.\nThese could be seen as a rotation of the data space of \\((x, y)\\) to a new space (PC1, PC2) with uncorrelated variables.\nThe total variation of the points in data space, \\(\\text{Var}(x) + \\text{Var}(y)\\), being unchanged by rotation, was equally well expressed as the total variation \\(\\text{Var}(PC1) + \\text{Var}(PC2)\\) of the scores on what are now called the principal component axes.\n\nIt would have appealed to Pearson (and also to A Square) to see these observations demonstrated in a 3D video.  Figure 4.4 shows a 3D plot of the variables Sepal.Length, Sepal.Width and Petal.Length in Edgar Anderson’s iris data, with points colored by species and the 95% data ellipsoid. This is rotated smoothly by interpolation until the first two principal axes, PC1 and PC2 are aligned with the horizontal and vertical dimensions. Because this is a rigid rotation of the cloud of points, the total variability is obviously unchanged.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Animation of PCA as a rotation in 3D space. The plot shows three variables for the iris data, initially in data space and its’ data ellipsoid, with points colored according to species of the iris flowers. This is rotated smoothly until the first two principal axes are aligned with the horizontal and vertical directions in the final frame.\n\n\n\n\n\n4.2.1 PCA by springs\nBefore delving into the mathematics of PCA, it is useful to see how Pearson’s problem, and fitting by least squares generally, could be solved in a physical realization.\nFrom elementary statistics, you may be familiar with a physical demonstration that the mean, \\(\\bar{x}\\), of a sample is the value for which the sum of deviations, \\(\\Sigma_i (x_i - \\bar{x})\\) is zero, so the mean can be visualized as the point of balance on a line where those differences \\((x_i - \\bar{x})\\) are placed. Equally well, there is a physical realization of the mean as the point along an axis where weights connected by springs will minimize the sum of squared differences, because springs with a constant stiffness, \\(k\\), exert forces proportional to \\(k (x_i - \\bar{x}) ^2\\). That’s the reason it is useful as a measure of central tendency: it minimizes the average squared error.\nIn two dimensions, imagine that we have points, \\((x_i, y_i)\\) and these are attached by springs of equal stiffness \\(k\\), to a line anchored at the centroid, \\((\\bar{x}, \\bar{y})\\) as shown in Figure 4.5. If we rotate the line to some initial position and release it, the springs will pull the line clockwise or counterclockwise and the line will bounce around until the forces, proportional to the squares of the lengths of the springs, will eventually balance out at the position (shown by the red fixed line segments at the ends). This is the position that minimizes the the sum of squared lengths of the connecting springs, and also minimizes the kinetic energy in the system.\nIf you look closely at Figure 4.5 you will see something else: When the line is at its final position of minimum squared length and energy, the positions of the red points on this line are spread out furthest, i.e., have maximum variance. Conversely, when the line is at right angles to its final position (shown by the black line at 90\\(^o\\)) the projected points have the smallest possible variance.\n\n\n\n\n\n\n\n\nFigure 4.5: Animation of PCA fitted by springs. The blue data points are connected to their projections on the red line by springs perpendicular to that line. From an initial position, the springs pull that line in proportion to their squared distances, until the line finally settles down to the position where the forces are balanced and the minimum is achieved. Source: Amoeba, https://bit.ly/46tAicu.\n\n\n\n\n4.2.2 Mathematics and geometry of PCA\nAs the ideas of principal components developed, there was a happy marriage of Galton’s geometrical intuition and Pearson’s mathematical analysis. The best men at the wedding were ellipses and higher-dimensional ellipsoids. The bridesmaids were eigenvectors, pointing in as many different directions as space would allow, each sized according to their associated eigenvalues. Attending the wedding were the ghosts of uncles, Leonhard Euler, Jean-Louis Lagrange, Augustin-Louis Cauchy and others who had earlier discovered the mathematical properties of ellipses and quadratic forms in relation to problems in physics.\nThe key idea in the statistical application was that, for a set of variables \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_p\\), the \\(p \\times p\\) covariance matrix \\(\\mathbf{S}\\) could be expressed exactly as a matrix product involving a matrix \\(\\mathbf{V}\\), whose columns are eigenvectors (\\(\\mathbf{v}_i\\)) and a diagonal matrix \\(\\boldsymbol{\\Lambda}\\), whose diagonal elements (\\(\\lambda_i\\)) are the corresponding eigenvalues.\nTo explain this, it is helpful to use a bit of matrix math:\n\\[\n\\begin{aligned}\n\\mathbf{S}_{p \\times p} & = \\mathbf{V}_{p \\times p} \\phantom{0000000000}\n                            \\boldsymbol{\\Lambda}_{p \\times p} \\phantom{00000000000000}\n                            \\mathbf{V}_{p \\times p}^\\mathsf{T} \\\\\n           & = \\left( \\mathbf{v}_1, \\, \\mathbf{v}_2, \\,\\dots, \\, \\mathbf{v}_p \\right)\n           \\begin{pmatrix}\n             \\lambda_1 &  &  & \\\\\n             & \\lambda_2  &   & \\\\\n             &  & \\ddots & \\\\\n             &  &  & \\lambda_p\n            \\end{pmatrix}\n            \\;\n            \\begin{pmatrix}\n            \\mathbf{v}_1^\\mathsf{T}\\\\\n            \\mathbf{v}_2^\\mathsf{T}\\\\\n            \\vdots\\\\\n            \\mathbf{v}_p^\\mathsf{T}\\\\\n            \\end{pmatrix}\n           \\\\\n           & = \\lambda_1 \\mathbf{v}_1 \\mathbf{v}_1^\\mathsf{T} + \\lambda_2 \\mathbf{v}_2 \\mathbf{v}_2^\\mathsf{T} + \\cdots + \\lambda_p \\mathbf{v}_p \\mathbf{v}_p^\\mathsf{T}\n\\end{aligned}\n\\tag{4.1}\\]\nIn this equation,\n\nThe last line follows because \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix, so \\(\\mathbf{S}\\) is expressed as a sum of outer products of each \\(\\mathbf{v}_i\\) with itself, times the eigenvalue \\(\\lambda_i\\).\nThe columns of \\(\\mathbf{V}\\) are the eigenvectors of \\(\\mathbf{S}\\). They are orthogonal and of unit length, so \\(\\mathbf{V}^\\mathsf{T} \\mathbf{V} = \\mathbf{I}\\). Thus they represent orthogonal (uncorrelated) directions in data space.\nThe column \\(\\mathbf{v}_i\\) gives the weights applied to the variables to produce the scores on the \\(i\\)-th principal component _i$. For example, the first principal component is the weighted sum:\n\n\\[\n\\text{PC}_1 = v_{11} \\mathbf{x}_1 + v_{12} \\mathbf{x}_2 + \\cdots + v_{1p} \\mathbf{x}_p \\:\\: .\n\\]\n\nThe matrix of all scores on the principal components can be calculated by multiplying the data matrix \\(\\mathbf{X}\\) by the eigenvectors, \\(\\mathbf{PC} = \\mathbf{X} \\mathbf{V}\\).\nThe eigenvalues, \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_p\\) are the variances of the the components, because \\(\\mathbf{v}_i^\\mathsf{T} \\;\\mathbf{S} \\; \\mathbf{v}_i = \\lambda_i\\).\nIt is usually the case that the variables \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_p\\) are linearly independent, which means that none of these is an exact linear combination of the others. In this case, all eigenvalues \\(\\lambda_i\\) are positive and the covariance matrix \\(\\mathbf{S}\\) is said to have rank \\(p\\). (Rank is the number of non-zero eigenvalues.)\nHere is a key fact: If, as usual, the eigenvalues are arranged in order, so that \\(\\lambda_1 &gt; \\lambda_2 &gt; \\dots &gt; \\lambda_p\\), then the first \\(d\\) components give a \\(d\\)-dimensional approximation to \\(\\mathbf{S}\\). This approximation accounts for \\(\\Sigma_i^d \\lambda_i\\) of the \\(\\Sigma_i^p \\lambda_i\\) total variance, and is usually interpreted as the proportion, \\((\\Sigma_i^d \\lambda_i) / (\\Sigma_i^p \\lambda_i)\\).\n\nFor the case of two variables, \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) Figure 4.6 shows the transformation from data space to component space. The eigenvectors, \\(\\mathbf{v}_1, \\mathbf{v}_2\\) are the major and minor axes of the data ellipse, whose lengths are the square roots \\(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}\\) of the eigenvalues.\n\n\n\n\n\n\n\nFigure 4.6: Geometry of PCA as a rotation from data space to principal component space, defined by the eigenvectors v1 and v2 of a covariance matrix\n\n\n\n\n\n\nExample 4.1 Workers’ experience and income\nFor a small example, consider the relation between years of experience and income in a small (contrived) sample (\\(n = 10\\)) of workers in a factory. The dataset workers contains these and other variables. In a wider context, we might want to fit a regression model to predict Income, but here we focus on a PCA of just these two variables.\n\ndata(workers, package = \"matlib\") \nhead(workers)\n#         Income Experience Skill Gender\n# Abby        20          0     2 Female\n# Betty       35          5     5 Female\n# Charles     40          5     8   Male\n# Doreen      30         10     6 Female\n# Ethan       50         10    10   Male\n# Francie     50         15     7 Female\n\n\nLet’s start with a simple scatterplot of Income vs. Experience, with points labeled by Name (and colored by Gender). There’s a fairly strong correlation (\\(r\\) = 0.853). How does a PCA capture this?\n\n\nvars &lt;- c(\"Experience\", \"Income\")\nplot(workers[, vars],\n     pch = 16, cex = 1.5,\n     cex.lab = 1.5)\ntext(workers[, vars], \n     labels = rownames(workers),\n     col = ifelse(workers$Gender == \"Female\", \"red\", \"blue\"),\n     pos = 3, xpd = TRUE)\n\n\n\n\n\n\nFigure 4.7: Scatterplot of Income vs. Experience for the workers data.\n\n\n\n\nTo carry out a PCA of these variables, first calculate the vector of means (\\(\\bar{\\mathbf{x}}\\)) and covariance matrix \\(\\mathbf{S}\\).\n\nmu &lt;- colMeans(workers[, vars]) |&gt; print()\n# Experience     Income \n#       15.5       46.5\nS &lt;- cov(workers[, vars]) |&gt; print()\n#            Experience Income\n# Experience        136    152\n# Income            152    234\n\nThe eigenvalues and eigenvectors of S are calculated by eigen(). This returns a list with components values for the \\(\\lambda_i\\) and vectors for \\(\\mathbf{V}\\).\n\nS.eig &lt;- eigen(S)\nLambda &lt;- S.eig$values |&gt; print()\n# [1] 344.3  25.1\nV &lt;- S.eig$vectors |&gt; print()\n#       [,1]   [,2]\n# [1,] 0.589 -0.808\n# [2,] 0.808  0.589\n\nFrom this, you can verify the points above regarding the relations between variances of the variables and the eigenvalues:\n\n#total variances of the variables = sum of eigenvalues\nsum(diag(S))\n# [1] 369\nsum(Lambda)\n# [1] 369\n\n# percent of variance of each PC\n100 * Lambda / sum(Lambda)\n# [1] 93.2  6.8\n\nUsing these, you can express the eigenvalue decomposition of \\(\\mathbf{S}\\) in Equation 4.1 with latexMatrix() and Eqn() from the matlib package (Friendly et al., 2024) as:\n\n\noptions(digits = 3)\nrownames(S) &lt;- colnames(S) &lt;- c(\"\\\\small \\\\text{Exp}\", \n                                \"\\\\small \\\\text{Inc}\")\nspacer &lt;- \"\\\\phantom{00000000000000}\"\nEqn(\"\\\\mathbf{S} & = \\\\mathbf{V}\", spacer,\n    \"\\\\mathbf{\\\\Lambda}\", spacer,  \n    \"\\\\mathbf{V}^\\\\top\", Eqn_newline(),\n    latexMatrix(S), \"& =\", \n    latexMatrix(V), \"  \", diag(Lambda), \"  \", latexMatrix(V, transpose=TRUE),\n    align = TRUE)\n\n\n\\[\\begin{aligned}\n\\mathbf{S} & = \\mathbf{V} \\phantom{00000000000000}\n     \\boldsymbol{\\Lambda} \\phantom{00000000000000}  \n     \\mathbf{V}^\\top \\\\\n\\begin{matrix}\n  &  \\begin{matrix} \\phantom{i} Exp & Inc\n  \\end{matrix} \\\\\n\\begin{matrix}  \n   Exp\\\\\n   Inc\\\\\n\\end{matrix}  &\n\\begin{pmatrix}  \n136 & 152 \\\\\n152 & 234 \\\\\n\\end{pmatrix}\n\\\\\n\\end{matrix}\n& =\\begin{pmatrix}\n0.589 & -0.808 \\\\\n0.808 &  0.589 \\\\\n\\end{pmatrix}\n  \\begin{pmatrix}\n344.3 &   0.0 \\\\\n  0.0 &  25.1 \\\\\n\\end{pmatrix}\n  \\begin{pmatrix}\n0.589 & -0.808 \\\\\n0.808 &  0.589 \\\\\n\\end{pmatrix}^\\top\n\\end{aligned}\\]\nThe “scores” on the principal components can be calculated (point (5) above) as \\(\\mathbf{PC} = \\mathbf{X} \\mathbf{V}\\):\n\nPC &lt;- as.matrix(workers[, vars]) %*% V\ncolnames(PC) &lt;- paste0(\"PC\", 1:2)\nhead(PC)\n#          PC1   PC2\n# Abby    16.2 11.78\n# Betty   31.2 16.57\n# Charles 35.3 19.52\n# Doreen  30.1  9.59\n# Ethan   46.3 21.37\n# Francie 49.2 17.32\n\nThen, you can visualize the geometry of PCA as in Figure 4.6 (left) by plotting the data ellipse for the points, along with the PCA axes (heplots::ellipse.axes()). Figure 4.8 also shows the bounding box of the data ellipse, which are parallel to the PC axes and scaled to have the same “radius” as the data ellipse.\n\n\nCode# calculate conjugate axes for PCA factorization\npca.fac &lt;- function(x) {\n  xx &lt;- svd(x)\n  ret &lt;- t(xx$v) * sqrt(pmax( xx$d,0))\n  ret\n}\n\ndataEllipse(Income ~ Experience, data=workers,\n    pch = 16, cex = 1.5, \n    center.pch = \"+\", center.cex = 2,\n    cex.lab = 1.5,\n    levels = 0.68,\n    grid = FALSE,\n    xlim = c(-10, 40),\n    ylim = c(10, 80),\n    asp = 1)\nabline(h = mu[2], v = mu[1], \n       lty = 2, col = \"grey\")\n\n# axes of the ellipse = PC1, PC2\nradius &lt;- sqrt(2 * qf(0.68, 2, nrow(workers)-1 ))\nheplots::ellipse.axes(S, mu, \n     radius = radius,\n     labels = TRUE,\n     col = \"red\", lwd = 2,\n     cex = 1.8)\n\n# bounding box of the ellipse\nlines(spida2::ellplus(mu, S, radius = radius,\n              box = TRUE, fac = pca.fac),\n      col = \"darkgreen\",\n      lwd = 2, lty=\"longdash\")\n\n\n\n\n\n\nFigure 4.8: Geometry of the PCA for the workers data, showing the data ellipse, the eigenvectors of \\(\\mathbf{S}\\), whose half-lengths are the square roots \\(\\sqrt{\\lambda_i}\\) of the eigenvalues, and the bounding box of the ellipse.\n\n\n\n\nFinally, to preview the methods of the next section, the results calculated “by hand” above can be obtained using prcomp(). The values labeled \"Standard deviations\" are the square roots \\(\\sqrt{\\lambda}_i\\) of the two eigenvalues. The eigenvectors are labeled \"Rotation\" because \\(\\mathbf{V}\\) is the matrix that rotates the data matrix to produce the component scores.\n\nworkers.pca &lt;- prcomp(workers[, vars]) |&gt; \n  print()\n# Standard deviations (1, .., p=2):\n# [1] 18.56  5.01\n# \n# Rotation (n x k) = (2 x 2):\n#              PC1    PC2\n# Experience 0.589  0.808\n# Income     0.808 -0.589\n\n\n\n4.2.3 Finding principal components\nIn R, PCA is most easily carried out using stats::prcomp() or stats::princomp() or similar functions in other packages such as FactomineR::PCA(). The FactoMineR package (Husson et al., 2017; Lê et al., 2008) has extensive capabilities for exploratory analysis of multivariate data (PCA, correspondence analysis, cluster analysis).\nA particular strength of FactoMineR for PCA is that it allows the inclusion of supplementary variables (which can be categorical or quantitative) and supplementary points for individuals. These are not used in the analysis, but are projected into the plots to facilitate interpretation. For example, in the analysis of the crime data described below, it would be useful to have measures of other characteristics of the U.S. states, such as poverty and average level of education (Section 4.3.5).\nUnfortunately, although all of these functions perform similar calculations, the options for analysis and the details of the result they return differ.\nThe important options for analysis include:\n\nwhether or not the data variables are centered, to a mean of \\(\\bar{x}_j =0\\)\n\nwhether or not the data variables are scaled, to a variance of \\(\\text{Var}(x_j) =1\\).\n\nIt nearly always makes sense to center and scale the variables. This choice of scaling determines whether the correlation matrix is analyzed, so that each variable contributes equally to the total variance that is to be accounted for. The unscaled choice analyzes the covariance matrix, where each variable contributes its own variance to the total. Analysis of the covariance matrix makes little sense when the variables are measured in different units2, unless you want to interpret total variance on the scales of the different variables.\nYou don’t need to scale your data in advance, but be aware of the options: prcomp() has default options center = TRUE, scale. = FALSE3 so in most cases you should specify scale. = TRUE. I mostly use this. The older princomp() has only the option cor = FALSE which centers the data and uses the covariance matrix, so in most cases you should set cor = TRUE.\nTo illustrate, the analysis of the workers data presented above used scale. = FALSE by default, so the eigenvalues reflected the variances of Experience and Income. The analogous result, using standardized variables (\\(z\\)-scores) can be computed in any of the forms shown below, using either scale. = FALSE or standardizing first using scale():\n\nprcomp(workers[, vars], scale. = TRUE)\n# Standard deviations (1, .., p=2):\n# [1] 1.361 0.383\n# \n# Rotation (n x k) = (2 x 2):\n#              PC1    PC2\n# Experience 0.707  0.707\n# Income     0.707 -0.707\n\n# same as (output suppressed):\nworkers[, vars] |&gt; prcomp(scale. = TRUE) |&gt; invisible()\nworkers[, vars] |&gt; scale() |&gt; prcomp() |&gt; invisible()\n\nIn this form, each of Experience and Income have variance = 1, and the \"Standard deviations\" reported are the square roots (\\(\\sqrt{\\lambda}_i\\)) of the eigenvalues \\(\\lambda_i\\) of the correlation matrix \\(\\mathbf{R}\\). The eigenvalues of a correlation matrix always sum to \\(p\\), the number of variables. This fact prompted the rough rule of thumb to extract principal components whose eigenvalues exceed 1.0, which is their average value, \\(\\bar{\\lambda} = (\\Sigma^p \\lambda_i) / p = p / p\\).\n\nprcomp(workers[, vars], scale. = TRUE)$sdev\n# [1] 1.361 0.383\n\n# eiven values of correlation matrix\nR &lt;- cor(workers[, vars])\nR.eig &lt;- eigen(R)\nLambda &lt;- R.eig$values |&gt; print()\n# [1] 1.853 0.147\nsum(Lambda)\n# [1] 2\n\nExample: Crime data\nThe dataset crime, analysed in Section 3.8, showed all positive correlations among the rates of various crimes in the corrgram, Figure 3.34. What can we see from a PCA? Is it possible that a few dimensions can account for most of the juice in this data?\nIn this example, you can easily find the PCA solution using prcomp() in a single line in base-R. You need to specify the numeric variables to analyze by their columns in the data frame. The most important option here is scale. = TRUE.\n\ndata(crime, package = \"ggbiplot\")\ncrime.pca &lt;- prcomp(crime[, 2:8], scale. = TRUE)\n\nThe tidy equivalent is more verbose, but also more expressive about what is being done. It selects the variables to analyze by a function, is.numeric() applied to each of the columns and feeds the result to prcomp().\n\ncrime.pca &lt;- \n  crime |&gt; \n  dplyr::select(where(is.numeric)) |&gt;\n  prcomp(scale. = TRUE)\n\nAs is typical with models in R, the result, crime.pca of prcomp() is an object of class \"prcomp\", a list of components, and there are a variety of methods for \"prcomp\" objects. Among the simplest is summary(), which gives the contributions of each component to the total variance in the dataset.\n\nsummary(crime.pca) |&gt; print(digits=2)\n# Importance of components:\n#                         PC1  PC2  PC3   PC4   PC5   PC6   PC7\n# Standard deviation     2.03 1.11 0.85 0.563 0.508 0.471 0.352\n# Proportion of Variance 0.59 0.18 0.10 0.045 0.037 0.032 0.018\n# Cumulative Proportion  0.59 0.76 0.87 0.914 0.951 0.982 1.000\n\nThe object, crime.pca returned by prcomp() is a list of the following the following elements:\n\nnames(crime.pca)\n# [1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"\n\nOf these, for \\(n\\) observations and \\(p\\) variables,\n\n\nsdev is the length \\(p\\) vector of the standard deviations of the principal components (i.e., the square roots \\(\\sqrt{\\lambda_i}\\) of the eigenvalues of the covariance/correlation matrix). When the variables are standardized, the sum of squares of the eigenvalues is equal to \\(p\\).\n\nrotation is the \\(p \\times p\\) matrix of weights or loadings of the variables on the components; the columns are the eigenvectors of the covariance or correlation matrix of the data;\n\nx is the \\(n \\times p\\) matrix of scores for the observations on the components, the result of multiplying (rotating) the data matrix by the loadings. These are uncorrelated, so cov(x) is a \\(p \\times p\\) diagonal matrix whose diagonal elements are the eigenvalues \\(\\lambda_i\\) = sdev^2.\n\ncenter gives the means of the variables when the option center. = TRUE (the default)\n\n4.2.4 Visualizing variance proportions: screeplots\nFor a high-D dataset, such as the crime data in seven dimensions, a natural question is how much of the variation in the data can be captured in 1D, 2D, 3D, … summaries and views. This is answered by considering the proportions of variance accounted by each of the dimensions, or their cumulative values. The components returned by various PCA methods have (confusingly) different names, so broom::tidy() provides methods to unify extraction of these values.\n\n(crime.eig &lt;- crime.pca |&gt; \n  broom::tidy(matrix = \"eigenvalues\"))\n# # A tibble: 7 × 4\n#      PC std.dev percent cumulative\n#   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n# 1     1   2.03   0.588       0.588\n# 2     2   1.11   0.177       0.765\n# 3     3   0.852  0.104       0.868\n# 4     4   0.563  0.0452      0.914\n# 5     5   0.508  0.0368      0.951\n# 6     6   0.471  0.0317      0.982\n# 7     7   0.352  0.0177      1\n\nThen, a simple visualization is a plot of the proportion of variance for each component (or cumulative proportion) against the component number, usually called a screeplot. The idea, introduced by Cattell (1966), is that after the largest, dominant components, the remainder should resemble the rubble, or scree formed by rocks falling from a cliff.\nFrom this plot, imagine drawing a straight line through the plotted eigenvalues, starting with the largest one. The typical rough guidance is that the last point to fall on this line represents the last component to extract, the idea being that beyond this, the amount of additional variance explained is non-meaningful. Another rule of thumb is to choose the number of components to extract a desired proportion of total variance, usually in the range of 80 - 90%.\nstats::plot(crime.pca) would give a bar plot of the variances of the components, however ggbiplot::ggscreeplot() gives nicer and more flexible displays as shown in Figure 4.9.\n\np1 &lt;- ggscreeplot(crime.pca) +\n  stat_smooth(data = crime.eig |&gt; filter(PC&gt;=4), \n              aes(x=PC, y=percent), method = \"lm\", \n              se = FALSE,\n              fullrange = TRUE) +\n  theme_bw(base_size = 14)\n\np2 &lt;- ggscreeplot(crime.pca, type = \"cev\") +\n  geom_hline(yintercept = c(0.8, 0.9), color = \"blue\") +\n  theme_bw(base_size = 14)\n\np1 + p2\n\n\n\n\n\n\nFigure 4.9: Screeplots for the PCA of the crime data. The left panel shows the traditional version, plotting variance proportions against component number, with linear guideline for the scree rule of thumb. The right panel plots cumulative proportions, showing cutoffs of 80%, 90%.\n\n\n\n\nFrom this we might conclude that four components are necessary to satisfy the scree criterion or to account for 90% of the total variation in these crime statistics. However two components, giving 76.5%, might be enough juice to tell a reasonable story.\n\n4.2.5 Visualizing PCA scores and variable vectors\nTo see and attempt to understand PCA results, it is useful to plot both the scores for the observations on a few of the largest components and also the loadings or variable vectors that give the weights for the variables in determining the principal components.\nIn Section 4.3 I discuss the biplot technique that plots both in a single display. However, I do this directly here, using tidy processing to explain what is going on in PCA and in these graphical displays.\nScores\nThe (uncorrelated) principal component scores can be extracted as crime.pca$x or using purrr::pluck(\"x\"). As noted above, these are uncorrelated and have variances equal to the eigenvalues of the correlation matrix.\n\nscores &lt;- crime.pca |&gt; purrr::pluck(\"x\") \ncov(scores) |&gt; zapsmall()\n#      PC1  PC2  PC3  PC4  PC5  PC6  PC7\n# PC1 4.11 0.00 0.00 0.00 0.00 0.00 0.00\n# PC2 0.00 1.24 0.00 0.00 0.00 0.00 0.00\n# PC3 0.00 0.00 0.73 0.00 0.00 0.00 0.00\n# PC4 0.00 0.00 0.00 0.32 0.00 0.00 0.00\n# PC5 0.00 0.00 0.00 0.00 0.26 0.00 0.00\n# PC6 0.00 0.00 0.00 0.00 0.00 0.22 0.00\n# PC7 0.00 0.00 0.00 0.00 0.00 0.00 0.12\n\nFor plotting, it is more convenient to use broom::augment() which extracts the scores (named .fittedPC*) and appends these to the variables in the dataset.\n\ncrime.pca |&gt;\n  broom::augment(crime) |&gt; head()\n# # A tibble: 6 × 18\n#   .rownames state      murder  rape robbery assault burglary larceny\n#   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n# 1 1         Alabama      14.2  25.2    96.8    278.    1136.   1882.\n# 2 2         Alaska       10.8  51.6    96.8    284     1332.   3370.\n# 3 3         Arizona       9.5  34.2   138.     312.    2346.   4467.\n# 4 4         Arkansas      8.8  27.6    83.2    203.     973.   1862.\n# 5 5         California   11.5  49.4   287      358     2139.   3500.\n# 6 6         Colorado      6.3  42     171.     293.    1935.   3903.\n# # ℹ 10 more variables: auto &lt;dbl&gt;, st &lt;chr&gt;, region &lt;fct&gt;,\n# #   .fittedPC1 &lt;dbl&gt;, .fittedPC2 &lt;dbl&gt;, .fittedPC3 &lt;dbl&gt;,\n# #   .fittedPC4 &lt;dbl&gt;, .fittedPC5 &lt;dbl&gt;, .fittedPC6 &lt;dbl&gt;,\n# #   .fittedPC7 &lt;dbl&gt;\n\nThen, we can use ggplot() to plot any pair of components. To aid interpretation, I label the points by their state abbreviation and color them by region of the U.S.. A geometric interpretation of the plot requires an aspect ratio of 1.0 (via coord_fixed()) so that a unit distance on the horizontal axis is the same length as a unit distance on the vertical. To demonstrate that the components are uncorrelated, I also added their data ellipse.\n\ncrime.pca |&gt;\n  broom::augment(crime) |&gt; # add original dataset back in\n  ggplot(aes(.fittedPC1, .fittedPC2, color = region)) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_point(size = 1.5) +\n  geom_text(aes(label = st), nudge_x = 0.2) +\n  stat_ellipse(color = \"grey\") +\n  coord_fixed(ylim = c(-3,3), ratio = 1) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"PC Dimension 1\", y = \"PC Dimension 2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"top\") \n\n\n\n\n\n\nFigure 4.10: Plot of component scores on the first two principal components for the crime data. States are colored by region.\n\n\n\n\nTo interpret such plots, it is useful consider the observations that are a high and low on each of the axes as well as other information, such as region here, and ask how these differ on the crime statistics. The first component, PC1, contrasts Nevada and California with North Dakota, South Dakota and West Virginia. The second component has most of the southern states on the low end and Massachusetts, Rhode Island and Hawaii on the high end. However, interpretation is easier when we also consider how the various crimes contribute to these dimensions.\nWhen, as here, there are more than two components that seem important in the scree plot, we could obviously go further and plot other pairs.\nVariable vectors\nYou can extract the variable loadings using either crime.pca$rotation or purrr::pluck(\"rotation\"), similar to what I did with the scores.\n\ncrime.pca |&gt; purrr::pluck(\"rotation\")\n#             PC1     PC2     PC3     PC4     PC5     PC6     PC7\n# murder   -0.300 -0.6292 -0.1782  0.2321  0.5381  0.2591  0.2676\n# rape     -0.432 -0.1694  0.2442 -0.0622  0.1885 -0.7733 -0.2965\n# robbery  -0.397  0.0422 -0.4959  0.5580 -0.5200 -0.1144 -0.0039\n# assault  -0.397 -0.3435  0.0695 -0.6298 -0.5067  0.1724  0.1917\n# burglary -0.440  0.2033  0.2099  0.0576  0.1010  0.5360 -0.6481\n# larceny  -0.357  0.4023  0.5392  0.2349  0.0301  0.0394  0.6017\n# auto     -0.295  0.5024 -0.5684 -0.4192  0.3698 -0.0573  0.1470\n\nBut note something important in this output: All of the weights for the first component are negative. In PCA, the directions of the eigenvectors are completely arbitrary, in the sense that the vector \\(-\\mathbf{v}_i\\) gives the same linear combination as \\(\\mathbf{v}_i\\), but with its’ sign reversed. For interpretation, it is useful (and usually recommended) to reflect the loadings to a positive orientation by multiplying them by -1. In general, you are free to reflect any of the components for ease of interpretation, and not necessarily if all the signs are negative.\nTo reflect the PCA loadings (multiplying PC1 and PC2 by -1) and get them into a convenient format for plotting with ggplot(), it is necessary to do a bit of processing, including making the row.names() into an explicit variable for the purpose of labeling.\n\n\n\n\n\n\n\nrownames in R\n\n\n\nR software evolved over many years, particularly in conventions for labeling cases in printed output and graphics. In base-R, the convention was that the row.names() of a matrix or data.frame served as observation labels in all printed output and plots, with a default to use numbers 1:n if there were no rownames.\nIn ggplot2 and the tidyverse framework, the decision was made that observation labels had to be an explicit variable in a “tidy” dataset, so it could be used as a variable in constructs like geom_text(aes(label = label)) as in this example. This change often requires extra steps in pre-tidy software that uses the rownames convention.\n\n\n\nvectors &lt;- crime.pca |&gt; \n  purrr::pluck(\"rotation\") |&gt;\n  as.data.frame() |&gt;\n  mutate(PC1 = -1 * PC1, PC2 = -1 * PC2) |&gt;      # reflect axes\n  tibble::rownames_to_column(var = \"label\") \n\nvectors[, 1:3]\n#      label   PC1     PC2\n# 1   murder 0.300  0.6292\n# 2     rape 0.432  0.1694\n# 3  robbery 0.397 -0.0422\n# 4  assault 0.397  0.3435\n# 5 burglary 0.440 -0.2033\n# 6  larceny 0.357 -0.4023\n# 7     auto 0.295 -0.5024\n\nThen, I plot these using geom_segment(), taking some care to use arrows from the origin with a nice shape and add geom_text() labels for the variables positioned slightly to the right. Again, coord_fixed() ensures equal scales for the axes, which is important because we want to interpret the angles between the variable vectors and the PCA coordinate axes.\n\narrow_style &lt;- arrow(\n  angle = 20, ends = \"first\", type = \"closed\", \n  length = grid::unit(8, \"pt\")\n)\n\nvectors |&gt;\n  ggplot(aes(PC1, PC2)) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(xend = 0, yend = 0, \n               linewidth = 1, \n               arrow = arrow_style,\n               color = \"brown\") +\n  geom_text(aes(label = label), \n            size = 5,\n            hjust = \"outward\",\n            nudge_x = 0.05, \n            color = \"brown\") +\n  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 0.5),  color = gray(.50)) +\n  xlim(-0.5, 0.9) + \n  ylim(-0.8, 0.8) +\n  coord_fixed() +         # fix aspect ratio to 1:1\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\nFigure 4.11: Plot of component loadings the first two principal components for the crime data. These are interpreted as the contributions of the variables to the components.\n\n\n\n\nThe variable vectors (arrows) shown in Figure 4.11 have the following interpretations:\n\nThe lengths of the variable vectors, \\(\\lVert\\mathbf{v}_i\\rVert = \\sqrt{\\Sigma_{j} \\; v_{ij}^2}\\) give the relative proportion of variance of each variable accounted for in a two-dimensional display.\nEach vector points in the direction in component space with which that variable is most highly correlated: the value, \\(v_{ij}\\), of the vector for variable \\(\\mathbf{x}_i\\) on component \\(j\\) reflects the correlation of that variable with the \\(j\\)th principal component. Thus,\n\n* A variable that is perfectly correlated with a component is parallel to it.\n* A variable uncorrelated with an component is perpendicular to it.\n\nThe angle between vectors shows the strength and direction of the correlation between those variables: the cosine of the angle \\(\\theta\\) between two variable vectors, \\(\\mathbf{v}_i\\) and \\(\\mathbf{v}_j\\), which is \\(\\cos(\\theta) = \\mathbf{v}_i^\\prime \\; \\mathbf{v}_j \\;/ \\; \\| \\mathbf{v}_i \\| \\cdot \\| \\mathbf{v}_j \\|\\) gives the approximation of the correlation \\(r_{ij}\\) between \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) that is shown in this space. This means that:\n\n* two variable vectors that point in the same direction are highly correlated; $r = 1$ if they are completely aligned.\n* Variable vectors at right angles are approximately uncorrelated, while those\npointing in opposite directions are negatively correlated; \\(r = -1\\) if they are at 180\\(^o\\).\nTo illustrate point (1), the following indicates that almost 70% of the variance of murder is represented in the the 2D plot shown in Figure 4.10, but only 40% of the variance of robbery is captured. For point (2), the correlation of murder with the dimensions is 0.3 for PC1 and 0.63 for PC2. For point (3), the angle between murder and burglary looks to be about 90\\(^o\\), but the actual correlation is 0.39.\n\n\n\n\n\n\n\n\n\nvectors |&gt; select(label, PC1, PC2) |&gt; \n  mutate(length = sqrt(PC1^2 + PC2^2))\n#      label   PC1     PC2 length\n# 1   murder 0.300  0.6292  0.697\n# 2     rape 0.432  0.1694  0.464\n# 3  robbery 0.397 -0.0422  0.399\n# 4  assault 0.397  0.3435  0.525\n# 5 burglary 0.440 -0.2033  0.485\n# 6  larceny 0.357 -0.4023  0.538\n# 7     auto 0.295 -0.5024  0.583",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-biplot",
    "href": "04-pca-biplot.html#sec-biplot",
    "title": "4  Dimension Reduction",
    "section": "\n4.3 Biplots",
    "text": "4.3 Biplots\nThe biplot is a visual multivariate juicer. It is the simple and powerful idea that came from the recognition that you can overlay a plot of observation scores in a principal components analysis with the information of the variable loadings (weights) to give a simultaneous display that is easy to interpret. In this sense, a biplot is generalization of a scatterplot, projecting from data space to PCA space, where the observations are shown by points, as in the plots of component scores in Figure 4.10, but with the variables also shown by vectors (or scaled linear axes aligned with those vectors).\nThe idea of the biplot was introduced by Ruben Gabriel (1971, 1981) and later expanded in scope by Gower & Hand (1996). The book by Greenacre (2010) gives a practical overview of the many variety of biplots. Gower et al. (2011) Understanding biplots provides a full treatment of many topics, including how to calibrate biplot axes, 3D plots, and so forth.\nBiplot methodolgy is far more general than I cover here. Categorical variables can be incorporated in PCA using points that represent the levels of discrete categories. Two-way frequency tables of categorical variables can be analysed using correspondence analysis, which is similar to PCA, but designed to account for the maximum amount of the \\(\\chi^2\\) statistic for association; multiple correspondence analysis extends this to method to multi-way tables (Friendly & Meyer, 2016; Greenacre, 1984).\n\n4.3.1 Constructing a biplot\nThe biplot is constructed by using the singular value decomposition (SVD) to obtain a low-rank approximation to the data matrix \\(\\mathbf{X}_{n \\times p}\\) (centered, and optionally scaled to unit variances) whose \\(n\\) rows are the observations and whose \\(p\\) columns are the variables.\n\n\n\n\n\n\n\nFigure 4.12: The singular value decomposition expresses a data matrix X as the product of a matrix U of observation scores, a diagonal matrix \\(\\Lambda\\) of singular values and a matrix V of variable weights.\n\n\n\n\nUsing the SVD, the matrix \\(\\mathbf{X}\\), of rank \\(r \\le p\\) can be expressed exactly as:\n\\[\n\\mathbf{X} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{V}'\n                 = \\sum_i^r \\lambda_i \\mathbf{u}_i \\mathbf{v}_i' \\; ,\n\\tag{4.2}\\]\nwhere\n\n\n\\(\\mathbf{U}\\) is an \\(n \\times r\\) orthonormal matrix of uncorrelated observation scores; these are also the eigenvectors of \\(\\mathbf{X} \\mathbf{X}'\\),\n\n\\(\\boldsymbol{\\Lambda}\\) is an \\(r \\times r\\) diagonal matrix of singular values, \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\lambda_r\\), which are also the square roots of the eigenvalues of \\(\\mathbf{X} \\mathbf{X}'\\).\n\n\\(\\mathbf{V}\\) is an \\(r \\times p\\) orthonormal matrix of variable weights and also the eigenvectors of \\(\\mathbf{X}' \\mathbf{X}\\).\n\nThen, a rank 2 (or 3) PCA approximation \\(\\widehat{\\mathbf{X}}\\) to the data matrix used in the biplot can be obtained from the first 2 (or 3) singular values \\(\\lambda_i\\) and the corresponding \\(\\mathbf{u}_i, \\mathbf{v}_i\\) as:\n\\[\n\\mathbf{X} \\approx \\widehat{\\mathbf{X}} = \\lambda_1 \\mathbf{u}_1 \\mathbf{v}_1' + \\lambda_2 \\mathbf{u}_2 \\mathbf{v}_2' \\; .\n\\]\nThe variance of \\(\\mathbf{X}\\) accounted for by each term is \\(\\lambda_i^2\\).\nA biplot is then obtained by overlaying two scatterplots that share a common set of axes and have a between-set scalar product interpretation. Typically, the observations (rows of \\(\\mathbf{X}\\)) are represented as points and the variables (columns of \\(\\mathbf{X}\\)) are represented as vectors from the origin.\nThe scale factor, \\(\\alpha\\) allows the variances of the components to be apportioned between the row points and column vectors, with different interpretations, by representing the approximation \\(\\widehat{\\mathbf{X}}\\) as the product of two matrices,\n\\[\n\\widehat{\\mathbf{X}} = (\\mathbf{U} \\boldsymbol{\\Lambda}^\\alpha) (\\boldsymbol{\\Lambda}^{1-\\alpha} \\mathbf{V}') = \\mathbf{A} \\mathbf{B}'\n\\]\nThis notation uses a little math trick involving a power, \\(0 \\le \\alpha \\le 1\\): When \\(\\alpha = 1\\), \\(\\boldsymbol{\\Lambda}^\\alpha = \\boldsymbol{\\Lambda}^1  =\\boldsymbol{\\Lambda}\\), and \\(\\boldsymbol{\\Lambda}^{1-\\alpha} = \\boldsymbol{\\Lambda}^0  =\\mathbf{I}\\). \\(\\alpha = 1/2\\) gives the diagonal matrix \\(\\boldsymbol{\\Lambda}^{1/2}\\) whose elements are the square roots of the singular values.\nThe choice \\(\\alpha = 1\\) assigns the singular values totally to the left factor; then, the angle between two variable vectors, reflecting the inner product \\(\\mathbf{x}_j^\\mathsf{T}, \\mathbf{x}_{j'}\\) approximates their correlation or covariance, and the distance between the points approximates their Mahalanobis distances. \\(\\alpha = 0\\) gives a distance interpretation to the column display. \\(\\alpha = 1/2\\) gives a symmetrically scaled biplot. TODO: Explain this better.\nWhen the singular values are assigned totally to the left or to the right factor, the resultant coordinates are called principal coordinates and the sum of squared coordinates on each dimension equal the corresponding singular value. The other matrix, to which no part of the singular values is assigned, contains the so-called standard coordinates and have sum of squared values equal to 1.0.\n\n4.3.2 Biplots in R\nThere are a large number of R packages providing biplots. The most basic, stats::biplot(), provides methods for \"prcomp\" and \"princomp\" objects. Among other packages, factoextra (Kassambara & Mundt, 2020), an extension of FactoMineR (Lê et al., 2008), is perhaps the most comprehensive and provides ggplot2 graphics. In addition to biplot methods for quantitative data using PCA (fviz_pca()), it offers biplots for categorical data using correspondence analysis (fviz_ca()) and multiple correspondence analysis (fviz_mca()); factor analysis with mixed quantitative and categorical variables (fviz_famd()) and cluster analysis (fviz_cluster()). The adegraphics package (Dray & Siberchicot, 2025) produces lovely biplots using lattice graphics, but with its own analytic framework.\nHere, I use the ggbiplot package (Vu & Friendly, 2024), which aims to provide a simple interface to biplots within the ggplot2 framework. I also use some convenient utility functions from factoextra.\n\n4.3.3 Example: Crime data\nA basic biplot of the crime data, using standardized principal components and labeling the observation by their state abbreviation is shown in Figure 4.13. The correlation circle reflects the data ellipse of the standardized components. This reminds us that these components are uncorrelated and have equal variance in the display.\n\n\ncrime.pca &lt;- reflect(crime.pca) # reflect the axes\n\nggbiplot(crime.pca,\n   obs.scale = 1, var.scale = 1,\n   labels = crime$st ,\n   circle = TRUE,\n   varname.size = 4,\n   varname.color = \"brown\") +\n  theme_minimal(base_size = 14) \n\n\n\n\n\n\nFigure 4.13: Basic biplot of the crime data. State abbreviations are shown at their standardized scores on the first two dimensions. The variable vectors reflect the correlations of the variables with the biplot dimensions.\n\n\n\n\nIn this dataset the states are grouped by region and we saw some differences among regions in the plot (Figure 4.10) of component scores. ggbiplot() provides options to include a groups = variable, used to color the observation points and also to draw their data ellipses, facilitating interpretation.\n\nggbiplot(crime.pca,\n   obs.scale = 1, var.scale = 1,\n   groups = crime$region,\n   labels = crime$st,\n   labels.size = 4,\n   var.factor = 1.4,\n   ellipse = TRUE, \n   ellipse.prob = 0.5, ellipse.alpha = 0.1,\n   circle = TRUE,\n   varname.size = 4,\n   varname.color = \"black\",\n   clip = \"off\") +\n  labs(fill = \"Region\", color = \"Region\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.direction = 'horizontal', legend.position = 'top')\n\n\n\n\n\n\nFigure 4.14: Enhanced biplot of the crime data, grouping the states by region and adding data ellipses.\n\n\n\n\nThis plot provides what is necessary to interpret the nature of the components and also the variation of the states in relation to these. In this, the data ellipses for the regions provide a visual summary that aids interpretation.\n\nFrom the variable vectors, it seems that PC1, having all positive and nearly equal loadings, reflects a total or overall index of crimes. Nevada, California, New York and Florida are highest on this, while North Dakota, South Dakota and West Virginia are lowest.\nThe second component, PC2, shows a contrast between crimes against persons (murder, assault, rape) at the top and property crimes (auto theft, larceny) at the bottom. Nearly all the Southern states are high on personal crimes; states in the North East are generally higher on property crimes.\nWestern states tend to be somewhat higher on overall crime rate, while North Central are lower on average. In these states there is not much variation in the relative proportions of personal vs. property crimes.\n\nMoreover, in this biplot you can interpret the the value for a particular state on a given crime by considering its projection on the variable vector, where the origin corresponds to the mean, positions along the vector have greater than average values on that crime, and the opposite direction have lower than average values. For example, Massachusetts has the highest value on auto theft, but a value less than the mean. Louisiana and South Carolina on the other hand are highest in the rate of murder and slightly less than average on auto theft.\nThese 2D plots account for only 76.5% of the total variance of crimes, so it is useful to also examine the third principal component, which accounts for an additional 10.4%. The choices = option controls which dimensions are plotted.\n\nggbiplot(crime.pca,\n         choices = c(1,3),\n         obs.scale = 1, var.scale = 1,\n         groups = crime$region,\n         labels = crime$st,\n         labels.size = 4,\n         var.factor = 2,\n         ellipse = TRUE, \n         ellipse.prob = 0.5, ellipse.alpha = 0.1,\n         circle = TRUE,\n         varname.size = 4,\n         varname.color = \"black\",\n         clip = \"off\") +\n  labs(fill = \"Region\", color = \"Region\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.direction = 'horizontal', legend.position = 'top')\n\n\n\n\n\n\nFigure 4.15: Biplot of dimensions 1 & 3 of the crime data, with data ellipses for the regions.\n\n\n\n\nDimension 3 in Figure 4.15 is more subtle. One interpretation is a contrast between larceny, which is a larceny (simple theft) and robbery, which involves stealing something from a person and is considered a more serious crime with an element of possible violence. In this plot, murder has a relatively short variable vector, so does not contribute very much to differences among the states.\n\n4.3.4 Biplot contributions and quality\nTo better understand how much each variable contributes to the biplot dimensions, it is helpful to see information about the variance of variables along each dimension. Graphically, this is nothing more than a measure of the lengths of projections of the variables on each of the dimensions. factoextra::get_pca_var() calculates a number of tables from a \"prcomp\" or similar object.\n\nvar_info &lt;- factoextra::get_pca_var(crime.pca)\nnames(var_info)\n# [1] \"coord\"   \"cor\"     \"cos2\"    \"contrib\"\n\nThe component cor gives correlations of the variables with the dimensions and contrib gives their variance contributions as percents, where each row and column sums to 100.\n\ncontrib &lt;- var_info$contrib\ncbind(contrib, Total = rowSums(contrib)) |&gt;\n  rbind(Total = c(colSums(contrib), NA)) |&gt; \n  round(digits=2)\n#           Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7 Total\n# murder     9.02  39.59   3.18   5.39  28.96   6.71   7.16   100\n# rape      18.64   2.87   5.96   0.39   3.55  59.79   8.79   100\n# robbery   15.75   0.18  24.59  31.14  27.04   1.31   0.00   100\n# assault   15.73  11.80   0.48  39.67  25.67   2.97   3.68   100\n# burglary  19.37   4.13   4.41   0.33   1.02  28.73  42.01   100\n# larceny   12.77  16.19  29.08   5.52   0.09   0.16  36.20   100\n# auto       8.71  25.24  32.31  17.58  13.67   0.33   2.16   100\n# Total    100.00 100.00 100.00 100.00 100.00 100.00 100.00    NA\n\nThese contributions can be visualized as sorted barcharts for a given axis using factoextra::fviz_contrib(). The dashed horizontal lines are at the average value for each dimension.\n\np1 &lt;- fviz_contrib(crime.pca, choice = \"var\", axes = 1,\n                   fill = \"lightgreen\", color = \"black\")\np2 &lt;- fviz_contrib(crime.pca, choice = \"var\", axes = 2,\n                   fill = \"lightgreen\", color = \"black\")\np1 + p2\n\n\n\n\n\n\nFigure 4.16: Contributions of the crime variables to dimensions 1 (left) & 2 (right) of the PCA solution\n\n\n\n\nA simple rubric for interpreting the dimensions in terms of the variable contributions is to mention those that are largest or above average on each dimension. So, burglary and rape contribute most to the first dimension, while murder and auto theft contribute most to the second.\nAnother useful measure is called cos2, the quality of representation, meaning how much of a variable is represented in a given component. The columns sum to the eigenvalue for each dimension. The rows each sum to 1.0, meaning each variable is completely represented on all components, but we can find the quality of a \\(k\\)-D solution by summing the values in the first \\(k\\) columns. These can be plotted in a style similar to Figure 4.16 using factoextra::fviz_cos2().\n\nquality &lt;- var_info$cos2\nrowSums(quality)\n#   murder     rape  robbery  assault burglary  larceny     auto \n#        1        1        1        1        1        1        1\n\ncolSums(quality)\n# Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 \n# 4.115 1.239 0.726 0.316 0.258 0.222 0.124\n\ncbind(quality[, 1:2], \n      Total = rowSums(quality[, 1:2])) |&gt;\n  round(digits = 2)\n#          Dim.1 Dim.2 Total\n# murder    0.37  0.49  0.86\n# rape      0.77  0.04  0.80\n# robbery   0.65  0.00  0.65\n# assault   0.65  0.15  0.79\n# burglary  0.80  0.05  0.85\n# larceny   0.53  0.20  0.73\n# auto      0.36  0.31  0.67\n\nIn two dimensions, murder and burglary are best represented; robbery and larceny are the worst, but as we saw above (Figure 4.15), these crimes are implicated in the third dimension.\n\n4.3.5 Supplementary variables\nAn important feature of biplot methodology is that once you have a reduced-rank display of the relations among a set of variables, you can use other available data to help interpret what is shown in the biplot. In a sense, this is what I did above in Figure 4.14 and Figure 4.15 using region as a grouping variable and summarizing the variability in the scores for states with their data ellipses by region.\nWhen we have other quantitative variables on the same observations, these can be represented as supplementary variables in the same space. Geometrically, this amounts to projecting the new variables on the space of the principal components. It is carried out by regressions of these supplementary variables on the scores for the principal component dimensions.\nFor example, the left panel of Figure 4.17 depicts the vector geometry of a regression of a variable \\(\\mathbf{y}\\) on two predictors, \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). The fitted vector, \\(\\widehat{\\mathbf{y}}\\), is the perpendicular projection of \\(\\mathbf{y}\\) onto the plane of \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). In the same way, in the right panel, a supplementary variable is projected into the plane of two principal component axes shown as an ellipse. The black fitted vector shows how that additional variable relates to the biplot dimensions.\n\n\n\n\n\n\n\nFigure 4.17: Fitting supplementary variables in a biplot is analogous (right) to regression on the principal component dimensions (left). Source: Aluja et al. (2018), Figure 2.11\n\n\n\n\nFor this example, it happens that some suitable supplementary variables to aid interpretation of crime rates are available in the dataset datsets::state.x77, which was obtained from the U.S. Bureau of the Census Statistical Abstract of the United States for 1977. I select a few of these below and make the state name a column variable so it can be merged with the crime data.\n\nsupp_data &lt;- state.x77 |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(var = \"state\") |&gt;\n  rename(Life_Exp = `Life Exp`,\n         HS_Grad = `HS Grad`) |&gt;\n  select(state, Income:Life_Exp, HS_Grad) \n\nhead(supp_data)\n#        state Income Illiteracy Life_Exp HS_Grad\n# 1    Alabama   3624        2.1     69.0    41.3\n# 2     Alaska   6315        1.5     69.3    66.7\n# 3    Arizona   4530        1.8     70.5    58.1\n# 4   Arkansas   3378        1.9     70.7    39.9\n# 5 California   5114        1.1     71.7    62.6\n# 6   Colorado   4884        0.7     72.1    63.9\n\nThen, we can merge the crime data with the supp_data dataset to produce something suitable for analysis using factoMineR::PCA().\n\ncrime_joined &lt;-\n  dplyr::left_join(crime[, 1:8], supp_data, by = \"state\")\nnames(crime_joined)\n#  [1] \"state\"      \"murder\"     \"rape\"       \"robbery\"   \n#  [5] \"assault\"    \"burglary\"   \"larceny\"    \"auto\"      \n#  [9] \"Income\"     \"Illiteracy\" \"Life_Exp\"   \"HS_Grad\"\n\nPCA() can only get the labels for the observations from the row.names() of the dataset, so I assign them explicitly. The supplementary variables are specified by the argument quanti.sup as the indices of the columns in what is passed as the data argument.\n\nrow.names(crime_joined) &lt;- crime$st\ncrime.PCA_sup &lt;- PCA(crime_joined[,c(2:8, 9:12)], \n                     quanti.sup = 8:11,\n                     scale.unit=TRUE, \n                     ncp=3, \n                     graph = FALSE)\n\nThe essential difference between the result of prcomp() used earlier to get the crime.pca object and the result of PCA() with supplementary variables is that the crime.PCA_sup object now contains a quanti.sup component containing the coordinates for the supplementary variables in PCA space.\nThese can be calculated directly as a the coefficients of a multivariate regression of the standardized supplementary variables on the PCA scores for the dimensions, with no intercept—which forces the fitted vectors to go through the origin. For example, in the plot below (Figure 4.18), the vector for Income has coordinates (0.192, -0.530) on the first two PCA dimensions.\n\nreg.data &lt;- cbind(scale(supp_data[, -1]), \n                  crime.PCA_sup$ind$coord) |&gt;\n  as.data.frame()\n\nsup.mod &lt;- lm(cbind(Income, Illiteracy, Life_Exp, HS_Grad) ~ \n                    0 + Dim.1 + Dim.2 + Dim.3, \n              data = reg.data )\n\n(coefs &lt;- t(coef(sup.mod)))\n#             Dim.1  Dim.2   Dim.3\n# Income      0.192  0.530  0.0482\n# Illiteracy  0.112 -0.536  0.1689\n# Life_Exp   -0.131  0.649 -0.2158\n# HS_Grad     0.103  0.610 -0.4095\n\nNote that, because the supplementary variables are standardized, these coefficients are the same as the correlations between the supplementary variables and the scores on the principal components, up to a scaling factor for each dimension. This provides a general way to relate dimensions found in other methods to the original data variables using vectors as in biplot techniques.\n\n(R &lt;- cor(reg.data[, 1:4], reg.data[, 5:7]))\n#             Dim.1  Dim.2   Dim.3\n# Income      0.393  0.596  0.0415\n# Illiteracy  0.230 -0.602  0.1453\n# Life_Exp   -0.268  0.730 -0.1857\n# HS_Grad     0.211  0.686 -0.3524\n\nR / coefs\n#            Dim.1 Dim.2 Dim.3\n# Income      2.05  1.12 0.861\n# Illiteracy  2.05  1.12 0.861\n# Life_Exp    2.05  1.12 0.861\n# HS_Grad     2.05  1.12 0.861\n\nThe PCA() result can then be plotted using FactoMiner::plot() or various factoextra functions like fviz_pca_var() for a plot of the variable vectors or fviz_pca_biplot() for a biplot. When a quanti.sup component is present, supplementary variables are also shown in the displays.\nFor simplicity I use FactoMiner::plot() here and only show the variable vectors. For consistency with earlier plots, I first reflect the orientation of the 2nd PCA dimension so that crimes of personal violence are at the top, as in Figure 4.11.\n\n# reverse coordinates of Dim 2\ncrime.PCA_sup &lt;- ggbiplot::reflect(crime.PCA_sup, columns = 2)\nplot(crime.PCA_sup, choix = \"var\")\n\n\n\n\n\n\nFigure 4.18: PCA plot of variables for the crime data, with vectors for the supplementary variables showing their association with the principal component dimensions.\n\n\n\n\nRecall that from earlier analyses, I interpreted the the dominant PC1 dimension as reflecting overall rate of crime. The contributions to this dimension, which are the projections of the variable vectors on the horizontal axis in Figure 4.11 and Figure 4.14 were shown graphically by barcharts in the left panel of Figure 4.16.\nBut now in Figure 4.18, with the addition of variable vectors for the supplementary variables, you can see how income, rate of illiteracy, life expectancy and proportion of high school graduates are related to the variation in rates of crimes for the U.S. states.\nOn dimension 1, what stands out is that life expectancy is associated with lower overall crime, while other supplementary variable have positive associations. On dimension 2, crimes against persons (murder, assault, rape) are associated with greater rates of illiteracy among the states, which as we earlier saw (Figure 4.14) were more often Southern states. Crimes against property (auto theft, larceny) at the bottom of this dimension are associated with higher levels of income and high school graduates.\n\n4.3.6 Example: Diabetes data\nAs another example, consider the data from Reaven & Miller (1979) on measures of insulin and glucose shown in Figure 6 and that led to the discovery of two distinct types of development of Type 2 diabetes (Section 5). This dataset is available as Diabetes. The three groups are Normal, Chemical_Diabetic and Overt_Diabetic, and the (numerical) diagnostic variables are:\n\n\nrelwt: relative weight, the ratio of actual to expected weight, given the person’s height,\n\nglufast: fasting blood plasma glucose level\n\nglutest: test blood plasma glucose level, a measure of glucose intolerance\n\ninstest: plasma insulin during test, a measure of insulin response to oral glucose\n\nsspg: steady state plasma glucose, a measure of insulin resistance\n\nTODO: Should introduce 3D plots earlier, in Ch3 before Section 3.7.\nFirst, let’s try to create a 3D plot, analogous to the artist’s drawing from PRIM-9 shown in Figure 7. For this, I use car::scatter3d() which can show data ellipsoids summarizing each group. The formula notation, z ~ x + y assigns the z variable to the vertical direction in the plot, and the x and y variable form a base plane.\n\ncols &lt;- c(\"darkgreen\", \"blue\", \"red\")\nscatter3d(sspg ~ instest + glutest, data=Diabetes,\n          groups = Diabetes$group,\n          ellipsoid = TRUE,\n          surface = FALSE,\n          col = cols,\n          surface.col = cols)\n\ncar::scatter3d() uses the rgl package (Murdoch & Adler, 2025) to render 3D graphics on a display device, which means that it has facilities for perspective, lighting and other visual properties. You can interactively zoom in or out or rotate the display in any of the three dimensions and use rgl::spin3d() to animate rotations around any axes and record this a a movie3d(). Figure 4.19 shows two views of this plot, one from the front and one from the back. The data ellipsoids are not as evocative as the artist’s rendering, but they give a sense of the relative sizes and shapes of the clouds of points for the three diagnostic groups.\n\n\n\n\n\n\n\nFigure 4.19: Two views of a 3D scatterplot of three main diagnostic variables in the Diabetes dataset. The left panel shows an orientation similar to that of Figure 7; the right panel shows a view from the back.\n\n\n\n\nThe normal group is concentrated near the origin, with relatively low values on all three diagnostic measures. The chemical diabetic group forms a wing with higher values on insulin response to oral glucose (instest), while the overt diabetics form the other wing, with higher values on glucose intolerance (glutest). The relative sizes and orientations of the data ellipsoids are also informative.\nGiven this, what can we see in a biplot view based on PCA? The PCA of these data shows that 83% of the variance is captured in two dimensions and 96% in three. The result for 3D is interesting, in that the view from PRIM-9 shown in Figure 7 and Figure 4.19 nearly captured all available information.\n\ndata(Diabetes, package=\"heplots\")\n\ndiab.pca &lt;- \n  Diabetes |&gt; \n  dplyr::select(where(is.numeric)) |&gt;\n  prcomp(scale. = TRUE)\nsummary(diab.pca)\n# Importance of components:\n#                          PC1   PC2   PC3    PC4     PC5\n# Standard deviation     1.662 1.177 0.818 0.3934 0.17589\n# Proportion of Variance 0.552 0.277 0.134 0.0309 0.00619\n# Cumulative Proportion  0.552 0.829 0.963 0.9938 1.00000\n\nA 2D biplot, with data ellipses for the groups, can be produced as before, but I also want to illustrate labeling the groups directly, rather than in a legend.\n\nplt &lt;- ggbiplot(diab.pca,\n     obs.scale = 1, var.scale = 1,\n     groups = Diabetes$group,\n     var.factor = 1.4,\n     ellipse = TRUE, \n     ellipse.prob = 0.5, ellipse.alpha = 0.1,\n     circle = TRUE,\n     point.size = 2,\n     varname.size = 4) +\n  labs(fill = \"Group\", color = \"Group\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\nThen, find the centroids of the component scores and use geom_label() to plot the group labels.\n\nscores &lt;- data.frame(diab.pca$x[, 1:2], group = Diabetes$group)\ncentroids &lt;- scores |&gt;\n  group_by(group) |&gt;\n  summarize(PC1 = mean(PC1),\n            PC2 = mean(PC2))\n\nplt + geom_label(data = centroids, \n                 aes(x = PC1, y = PC2, \n                     label=group, color = group),\n                 nudge_y = 0.2)\n\n\n\n\n\n\nFigure 4.20: 2D biplot of the Diabetes data\n\n\n\n\nWhat can we see here, and how does it relate to the artist’s depiction in Figure 7? The variables instest, sspg and glutest correspond approximately to the coordinate axes in the artist’s drawing. glutest and glufast primarily separate the overt diabetics from the others. The chemical diabetics are distinguished by having larger values of insulin response (instest) and are also higher in relative weight (relwt).",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-nonlinear",
    "href": "04-pca-biplot.html#sec-nonlinear",
    "title": "4  Dimension Reduction",
    "section": "\n4.4 Nonlinear dimension reduction",
    "text": "4.4 Nonlinear dimension reduction\nThe world of dimension reduction methods reflected by PCA is a simple and attractive one in which relationships among variable are at least approximately linear, and can be made visible in a lower-dimensional view by linear transformations and projections. PCA does an optimal job of capturing global linear relationships in the data. But many phenomena defy linear description or involve local nonlinear relationships and clusters within the data. Our understanding of high-D data can sometimes be improved by nonlinear dimension reduction techniques.\nTo see why, consider the data shown in the left panel of Figure 4.21 and suppose we want to be able to separate the two classes by a line. The groups are readily seen in this simple 2D example, but there is no linear combination or projection that shows them as distinct categories. The right panel shows the same data after a nonlinear transformation to polar coordinates, where the two groups are readily distinguished by radius. Such problems arise in higher dimensions where direct visualization is far more difficult and nonlinear methods become attractive.\n\n\n\n\n\n\n\nFigure 4.21: *Nonlinear patterns**: Two representations of the same data are shown. In the plot at the left, the clusters are clear to the eye, but there is no linear relation that separates them. Transforming the data nonlinearly, to polar coordinates in the plot at the right, makes the two groups distinct.\n\n\n\n\n\n4.4.1 Multidimensional scaling\nOne way to break out of the “linear-combination, maximize-variance PCA” mold is to consider a more intrinsic property of points in Spaceland: similarity or distance. The earliest expression of this idea was in multidimensional scaling (MDS) by Torgerson (1952), which involved trying to determine a metric low-D representation of objects from their interpoint distances via an application of the SVD.\nThe break-through for nonlinear methods came from Roger Shepard and William Kruskal (Kruskal, 1964; Shepard, 1962a, 1962b) who recognized that a more general, nonmetric version (nMDS) could be achieved using only the rank order of input distances \\(d_{ij}\\) among objects. nMDS maps these into a low-D spatial representation of points, \\(\\mathbf{x}_i, \\mathbf{x}_j\\) whose fitted distances, \\(\\hat{d}_{ij} = \\lVert\\mathbf{x}_i - \\mathbf{x}_j\\rVert\\) matches the order of the \\(d_{ij}\\) as closely as possible. That is, rather than assume that the observed distances are linearly related to the fitted \\(\\hat{d}_{ij}\\), nMDS assumes only that their order is the same. Borg & Groenen (2005) and Borg et al. (2018) give a comprehensive overview of modern developments in MDS.\nThe impetus for MDS stemmed largely from psychology and the behavioral sciences, where simple experimental measures of similarity or dissimilarity of psychological objects (color names, facial expressions, words, Morse code symbols) could be obtained by direct ratings, confusions, or other tasks (Shepard et al., 1972b, 1972a). MDS was revolutionary in that it provided a coherent method to study the dimensions of perceptual and cognitive space in applications where the explanation of a cognitive process was derived directly from an MDS solution (Shoben, 1983).\nTo perform nMDS, you need to calculate the matrix of distances between all pairs of observations (dist()). The basic function is MASS::isoMDS().4 In the call, you can specify the number of dimensions (k) desired, with k=2 as default. It returns the coordinates in a dataset called points.\n\ndiab.dist &lt;- dist(Diabetes[, 1:5])\nmds &lt;- diab.dist |&gt;\n  MASS::isoMDS(k = 2, trace = FALSE) |&gt;\n  purrr::pluck(\"points\") \n\ncolnames(mds) &lt;- c(\"Dim1\", \"Dim2\")\nmds &lt;- bind_cols(mds, group = Diabetes$group)\nmds |&gt; sample_n(6)\n# # A tibble: 6 × 3\n#     Dim1   Dim2 group            \n#    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;            \n# 1 -213.  -42.1  Normal           \n# 2  191.   47.3  Overt_Diabetic   \n# 3   12.0 -63.2  Overt_Diabetic   \n# 4   25.0 -38.1  Chemical_Diabetic\n# 5  774.    9.44 Overt_Diabetic   \n# 6   79.0 136.   Overt_Diabetic\n\nThe method works by trying to minimize a measure, “Stress”, of the average difference between the fitted distances \\(\\hat{d}_{ij}\\) and an optimal monotonic (order-preserving) transformation, \\(f_{\\text{mon}}(d_{ij})\\), of the distances in the data. Values of Stress around 5-8% and smaller are generally considered adequate.\nUnlike PCA, where you can fit all possible dimensions once and choose the number of components to retain by examining the eigenvalues or variance proportions, in MDS it is necessary to fit the data for several values of k and consider the trade-off between goodness of fit and complexity.\n\nstress &lt;- vector(length = 5)\nfor(k in 1:5){\n  res &lt;- MASS::isoMDS(diab.dist, k=k, trace = FALSE)\n  stress[k] &lt;- res$stress\n}\nround(stress, 3)\n# [1] 17.755  3.525  0.256  0.000  0.000\n\nPlotting these shows that a 3D solution is nearly perfect, while a 2D solution is certainly adequate. This plot is the MDS analog of a screeplot for PCA.\n\nplot(stress, type = \"b\", pch = 16, cex = 2,\n     xlab = \"Number of dimensions\",\n     ylab = \"Stress (%)\")\n\n\n\n\n\n\nFigure 4.22: Badness of fit (Stress) of the MDS solution in relation to number of dimensions.\n\n\n\n\nTo plot the 2D solution, I’ll use ggpubr::ggscatter() here because it handles grouping, provides concentration ellipses and other graphical features.\n\nlibrary(ggpubr)\ncols &lt;- scales::hue_pal()(3) |&gt; rev()\nmplot &lt;-\nggscatter(mds, x = \"Dim1\", y = \"Dim2\", \n          color = \"group\",\n          shape = \"group\",\n          palette = cols,\n          size = 2,\n          ellipse = TRUE,\n          ellipse.level = 0.5,\n          ellipse.type = \"t\") +\n  geom_hline(yintercept = 0, color = \"gray\") +\n  geom_vline(xintercept = 0, color = \"gray\") \n\nFor this and other examples using MDS, it would be nice to also show how the dimensions of this space relate to the original variables, as in a biplot. Using the idea of correlations between variables and dimensions from Section 4.3.5, I do this as shown below. Only the relative directions and lengths of the variable vectors matter, so you can choose any convenient scale factor to make the vectors fill the plot region.\n\nvectors &lt;- cor(Diabetes[, 1:5], mds[, 1:2])\nscale_fac &lt;- 500\nmplot + \n  coord_fixed() +\n  geom_segment(data=vectors,\n               aes(x=0, xend=scale_fac*Dim1, y=0, yend=scale_fac*Dim2),\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"),\n               linewidth = 1.1) +\n  geom_text(data = vectors,\n            aes(x = 1.15*scale_fac*Dim1, y = 1.07*scale_fac*Dim2, \n                label=row.names(vectors)),\n            nudge_x = 4,\n            size = 4) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(.8, .8))\n\n\n\n\n\n\nFigure 4.23: Nonmetric MDS representation of the Diabetes data. The vectors reflect the correlations of the variables with the MDS dimensions.\n\n\n\n\nThe configuration of the groups in Figure 4.23 is similar to that of the biplot in Figure 4.20, but the groups are more widely separated along the first MDS dimension. The variable vectors are also similar, except that relwt is not well-represented in the MDS solution.\n\n4.4.2 t-SNE\nWith the rise of “machine learning” methods for “feature extraction” in “supervised” vs. “unsupervised” settings, a variety of new algorithms have been proposed for the task of finding low-D representations of high-D data. Among these, t-distributed Stochastic Neighbor Embedding (t-SNE) developed by Maaten & Hinton (2008) is touted as method for revealing local structure and clustering better in possibly complex high-D data and at different scales.\nt-SNE differs from MDS in what it tries to preserve in the mapping to low-D space: Multidimensional scaling aims to preserve the distances between pairs of data points, focusing on pairs of distant points in the original space. t-SNE, on the other hand focuses on preserving neighboring data points. Data points that are close in the original data space will be tight in the t-SNE embeddings.\n\n“The t-SNE algorithm models the probability distribution of neighbors around each point. Here, the term neighbors refers to the set of points which are closest to each point. In the original, high-dimensional space, this is modeled as a Gaussian distribution. In the 2-dimensional output space this is modeled as a \\(t\\)-distribution. The goal of the procedure is to find a mapping onto the 2-dimensional space that minimizes the differences between these two distributions over all points. The fatter tails of a \\(t\\)-distribution compared to a Gaussian help to spread the points more evenly in the 2-dimensional space.” (Jake Hoare, How t-SNE works and Dimensionality Reduction).\nt-SNE also uses the idea of mapping distance measures into a low-D space, but converts Euclidean distances into conditional probabilities. Stochastic neighbor embedding means that t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability.\nAs van der Maaten and Hinton explained: “The similarity of datapoint \\(\\mathbf{x}_{j}\\) to datapoint \\(\\mathbf{x}_{i}\\) is the conditional probability, \\(p_{j|i}\\), that \\(\\mathbf{x}_{i}\\) would pick \\(\\mathbf{x}_{j}\\) as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian distribution centered at \\(\\mathbf{x}_{i}\\).” For \\(i \\ne j\\), they define:\n\\[\np_{j\\mid i} = \\frac{\\exp(-\\lVert\\mathbf{x}_i - \\mathbf{x}_j\\rVert^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\lVert\\mathbf{x}_i - \\mathbf{x}_k\\rVert^2 / 2\\sigma_i^2)} \\;.\n\\] and set \\(p_{i\\mid i} = 0\\). \\(\\sigma^2_i\\) is the variance of the normal distribution that centered on datapoint \\(\\mathbf{x}_{i}\\) and serves as a tuning bandwidth so smaller values of \\(\\sigma _{i}\\) are used in denser parts of the data space. These conditional probabilities are made symmetric via averaging, giving \\(p_{ij} = \\frac{p_{j\\mid i} + p_{i\\mid j}}{2n}\\).\nt-SNE defines a similar probability distribution \\(q_{ij}\\) over the points \\(\\mathbf{y}_i\\) in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map,\n\\[\nD_\\mathrm{KL}\\left(P \\parallel Q\\right) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} \\; ,\n\\] a measure of how different the distribution of \\(P\\) in the data is from that of \\(Q\\) in the low-D representation. The t in t-SNE comes from the fact that the probability distribution of the points \\(\\mathbf{y}_i\\) in the embedding space is taken to be a heavy-tailed \\(t_{(1)}\\) distribution with one degree of freedom to spread the points more evenly in the 2-dimensional space, rather than the Gaussian distribution for the points in the high-D data space.\nt-SNE is implemented in the Rtsne package (Krijthe, 2023) which is capable of handling thousands of points in very high dimensions. It uses a tuning parameter, “perplexity” to choose the bandwidth \\(\\sigma^2_i\\) for each point. This value effectively controls how many nearest neighbors are taken into account when constructing the embedding in the low-dimensional space. It can be thought of as the means to balance between preserving the global and the local structure of the data.5\nRtsne::Rtsne() finds the locations of the points in the low-D space, of dimension k=2 by default. It returns the coordinates in a component named Y. The package has no print(), summary() or plot methods, so you’re on your own.\n\nlibrary(Rtsne)\nset.seed(123) \ndiab.tsne &lt;- Rtsne(Diabetes[, 1:5], scale = TRUE)\ndf2 &lt;- data.frame(diab.tsne$Y, group = Diabetes$group) \ncolnames(df2) &lt;- c(\"Dim1\", \"Dim2\", \"group\")\n\nYou can plot this as shown below:\n\n\np2 &lt;- ggplot(df2, aes(x=Dim1, y=Dim2, color = group, shape=group)) + \n  geom_point(size = 3) + \n  stat_ellipse(level = 0.68, linewidth=1.1) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  scale_color_manual(values = cols) +\n  labs(x = \"Dimension 1\",\n       y = \"Dimension 2\") + \n  ggtitle(\"tSNE\") +\n  theme_bw(base_size = 16) +\n  theme(legend.position = \"bottom\") \np2\n\n\n\n\n\n\nFigure 4.24: t-SNE representation of the Diabetes data.\n\n\n\n\n\n4.4.2.1 Comparing solutions\nFor the Diabetes data, I’ve shown the results of three different dimension reduction techniques, PCA (Figure 4.20), MDS (Figure 4.23), and t-SNE (Figure 4.24). How are these similar, and how do they differ?\nOne way is to view them side by side as shown in Figure 4.25. To an initial glance, the t-SNE solution looks like a rotated version of the PCA solution, but there are differences in the shapes of the clusters as well.\n\n\n\n\n\n\n\n\nFigure 4.25: Comparison of the PCA and t-SNE 2D representations of the Diabetes data.\n\n\n\n\nAnother way to compare these two views is to animate the transition from the PCA to the t-SNE representation by a series of smooth interpolated views. This is a more generally useful visualization technique, so it is useful to spell out the details.\nThe essential idea is calculate interpolated views as a weighted average of the two endpoints using a weight \\(\\gamma\\) that is varied from 0 to 1.\n\\[\n\\mathbf{X}_{\\text{View}} = \\gamma \\;\\mathbf{X}_{\\text{PCA}} + (1-\\gamma) \\;\\mathbf{X}_{\\text{t-SNE}}\n\\] The same idea can be applied to other graphical features: lines, paths (ellipses), and so forth. These methods are implemented in the gganimate package (Pedersen & Robinson, 2025).\nIn this case, to create an animation you can extract the coordinates for the PCA, \\(\\mathbf{X}_{\\text{PCA}}\\), as a data.frame df1, and those for the t-SNE, \\(\\mathbf{X}_{\\text{t-SNE}}\\) as df2, each with a constant method variable. These two are then stacked (using rbind()) to give a combined df3. The animation can then interpolate over method going from pure PCA to pure t-SNE.\n\ndiab.pca &lt;- prcomp(Diabetes[, 1:5], scale = TRUE, rank.=2) \ndf1 &lt;- data.frame(diab.pca$x, group = Diabetes$group) \ncolnames(df1) &lt;- c(\"Dim1\", \"Dim2\", \"group\")\ndf1 &lt;- cbind(df1, method=\"PCA\")\n\nset.seed(123) \ndiab.tsne &lt;- Rtsne(Diabetes[, 1:5], scale = TRUE)\ndf2 &lt;- data.frame(diab.tsne$Y, group = Diabetes$group) \ncolnames(df2) &lt;- c(\"Dim1\", \"Dim2\", \"group\")\ndf2 &lt;- cbind(df2, method=\"tSNE\")\n\n# stack the PCA and t-SNE solutions\ndf3 &lt;- rbind(df1, df2) \n\nThen, plot the configuration of the points and add data ellipses as before. The key thing for animating the difference between the solutions is to add transition_states(method, ...), tweening from PCA to t-SNE. The state_length argument transition_states() controls the relative length of the pause between states.\nThis animated graphic is shown only in the online version of the book.\n\nlibrary(gganimate)\nanimated_plot &lt;- \n  ggplot(df3, aes(x=Dim1, y=Dim2, color=group, shape=group)) + \n  geom_point(size = 3) + \n  stat_ellipse(level = 0.68, linewidth=1.1) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  scale_color_manual(values = cols) +\n  labs(title = \"PCA vs. tSNE Dimension Reduction: {closest_state}\",\n       subtitle = \"Frame {frame} of {nframes}\",\n       x = \"Dimension 1\",\n       y = \"Dimension 2\") + \n  transition_states( method, transition_length = 3, state_length = 2 ) + \n  view_follow() + \n  theme_bw(base_size = 16) +\n  theme(legend.position = \"bottom\") \n\nanimated_plot\n\n\n\n\n\n\n\n\nFigure 4.26: Animation of the relationship of PCA to the t-SNE embedding for the Diabetes data. The method name in the title reflects the closest state\n\n\n\n\nYou can see that the PCA configuration is morphed into the that for t-SNE largely by rotation 90\\(^o\\) clockwise, so that dimension 1 in PCA becomes dimension 2 in t-SNE. This is not unexpected, because PCA finds the dimensions in to order of maximum variance, whereas t-SNE is only trying to match the distances in the data to those in the solution. To interpret the result from t-SNE you are free to interchange the axes, or indeed to rotate the solution arbitrarily.\nIt is more interesting that the sizes and shapes of the group clusters change from one solution to the other. The normal group is most compact in the PCA solution, but becomes the least compact in t-SNE.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-var-order",
    "href": "04-pca-biplot.html#sec-var-order",
    "title": "4  Dimension Reduction",
    "section": "\n4.5 Application: Variable ordering for data displays",
    "text": "4.5 Application: Variable ordering for data displays\nIn many multivariate data displays, such as scatterplot matrices, parallel coordinate plots and others reviewed in Chapter 3, the order of different variables might seem arbitrary. They might appear in alphabetic order, or more often in the order they appear in your dataset, for example when you use pairs(mydata). Yet, the principle of effect ordering (Friendly & Kwan (2003)) for variables says you should try to arrange the variables so that adjacent ones are as similar as possible.6\nFor example, the mtcars dataset contains data on 32 automobiles from the 1974 U.S. magazine Motor Trend and consists of fuel comsumption (mpg) and 10 aspects of automobile design (cyl: number of cyliners; hp: horsepower, wt: weight) and performance (qsec: time to drive a quarter-mile). What can we see from a simple corrplot() of their correlations? No coherent pattern stands out in Figure 4.27.\n\ndata(mtcars)\nlibrary(corrplot)\nR &lt;- cor(mtcars)\ncorrplot(R, \n         method = 'ellipse',\n         title = \"Dataset variable order\",\n         tl.srt = 0, tl.col = \"black\", tl.pos = 'd',\n         mar = c(0,0,1,0))\n\n\n\n\n\n\nFigure 4.27: Corrplot of mtcars data, with the variables arranged in the order they appear in the dataset.\n\n\n\n\nIn this display you can scan the rows and columns to “look up” the sign and approximate magnitude of a given correlation; for example, the correlation between mpg and cyl appears to be about -0.9, while that between mpg and gear is about 0.5. Of course, you could print the correlation matrix to find the actual values (-0.86 and 0.48 respectively):\n\nprint(floor(100*R))\n#      mpg cyl disp  hp drat  wt qsec  vs  am gear carb\n# mpg  100 -86  -85 -78   68 -87   41  66  59   48  -56\n# cyl  -86 100   90  83  -70  78  -60 -82 -53  -50   52\n# disp -85  90  100  79  -72  88  -44 -72 -60  -56   39\n# hp   -78  83   79 100  -45  65  -71 -73 -25  -13   74\n# drat  68 -70  -72 -45  100 -72    9  44  71   69  -10\n# wt   -87  78   88  65  -72 100  -18 -56 -70  -59   42\n# qsec  41 -60  -44 -71    9 -18  100  74 -23  -22  -66\n# vs    66 -82  -72 -73   44 -56   74 100  16   20  -57\n# am    59 -53  -60 -25   71 -70  -23  16 100   79    5\n# gear  48 -50  -56 -13   69 -59  -22  20  79  100   27\n# carb -56  52   39  74  -10  42  -66 -57   5   27  100\n\nBecause the angles between variable vectors in the biplot reflect their correlations, Friendly & Kwan (2003) defined principal component variable ordering as the order of angles, \\(a_i\\) of the first two eigenvectors, \\(\\mathbf{v}_1, \\mathbf{v}_2\\) around the unit circle. These values are calculated going counter-clockwise from the 12:00 position as:\n\\[\na_i =\n  \\begin{cases}\n    \\tan^{-1} (v_{i2}/v_{i1}), & \\text{if $v_{i1}&gt;0$;}\n     \\\\\n    \\tan^{-1} (v_{i2}/v_{i1}) + \\pi, & \\text{otherwise.}\n  \\end{cases}     \n\\tag{4.3}\\]\nIn Equation 4.3 \\(\\tan^{-1}(x)\\) is read as “the angle whose tangent is \\(x\\)”, and so the angles are determined by the tangent ratios “opposite” / “adjacent” = \\(v_{i2} / v_{i1}\\) in the right triangle defined by the vector and the horizontal axis.\n\nFor the mtcars data the biplot in Figure 4.28 accounts for 84% of the total variance so a 2D representation is fairly good. The plot shows the variables as widely dispersed. There is a collection at the left of positively correlated variables and another positively correlated set at the right.\n\nmtcars.pca &lt;- prcomp(mtcars, scale. = TRUE)\nggbiplot(mtcars.pca,\n         circle = TRUE,\n         point.size = 2.5,\n         varname.size = 6,\n         varname.color = \"brown\") +\n  theme_minimal(base_size = 14) \n\n\n\n\n\n\nFigure 4.28: Biplot of the mtcars data. The order of the variables around the circle, starting from “gear” (say) arranges them so that the most similar variables are adjacent in graphical displays.\n\n\n\n\nIn corrplot() principal component variable ordering is implemented using the order = \"AOE\" option. There are a variety of other methods based on hierarchical cluster analysis described in the package vignette.\nFigure 4.29 shows the result of ordering the variables by this method. A nice feature of corrplot() is the ability to manually highlight blocks of variables that have a similar pattern of signs by outlining them with rectangles. From the biplot, the two main clusters of positively correlated variables seemed clear, and are outlined in the plot using corrplot::corrRect(). What became clear in the corrplot is that qsec, the time to drive a quarter-mile from a dead start didn’t quite fit this pattern, so I highlighted it separately.\n\ncorrplot(R, \n         method = 'ellipse', \n         order = \"AOE\",\n         title = \"PCA variable order\",\n         tl.srt = 0, tl.col = \"black\", tl.pos = 'd',\n         mar = c(0,0,1,0)) |&gt;\n  corrRect(c(1, 6, 7, 11))\n\n\n\n\n\n\nFigure 4.29: Corrplot of mtcars data, with the variables ordered according to the variable vectors in the biplot.\n\n\n\n\nBut wait, there is something else to be seen in Figure 4.29. Can you see one cell that doesn’t fit with the rest?\nYes, the correlation of number of forward gears (gear) and number of carburators (carb) in the upper left and lower right corners stands out as moderately positive (0.27) while all the others in their off-diagonal blocks are negative. This is another benefit of effect ordering: when you arrange the variables so that the most highly related variable are together, features that deviate from dominant pattern become visible.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-eigenfaces",
    "href": "04-pca-biplot.html#sec-eigenfaces",
    "title": "4  Dimension Reduction",
    "section": "\n4.6 Application: Eigenfaces",
    "text": "4.6 Application: Eigenfaces\nThere are many applications of principal components analysis beyond the use for visualization for multivariate data covered here, that rely on its’ ability as a dimension reduction technique, that is, to find a low-dimensional approximation to a high-dimensional dataset.\n\n\n\n\n\n\nMachine learning uses\n\n\n\nIn machine learning, for example, PCA is a method used to reduce model complexity and avoid overfitting by feature extraction, which amounts to fitting a response variable in a low-D space of the predictors. This is just another name for principal components regression, where, instead of regressing the dependent variable on all the explanatory variables directly, a smaller number principal components of the explanatory variables is used as predictors. This has the added benefit that it avoids problems of collinearity (section-ref) due to high correlations of the predictors, because the principal component scores are necessarily uncorrelated. When the goal is model explanation rather than pure prediction, it has the disadvantage that the components may be hard to interpret.\n\n\nAn interesting class of problems have to do with image processing, where an image of size width \\(\\times\\) height in pixels can be represented by a \\(w \\times h\\) array of greyscale values \\(x_{ij}\\) in the range of [0, 1] or \\(h \\times w \\times 3\\) array \\(x_{ijk}\\) of (red, green, blue) color values. For example a single \\(640 \\times 640\\) photo is comprised of about 400K pixels in B/W and 1200K pixels in color.\nThe uses here include\n\n\nImage compression: a process applied to a graphics file to minimize its size in bytes for storage or transmission, without degrading image quality below an acceptable threshold\n\nimage enhancement: improving the quality of an image, with applications in Computer Vision tasks, remote sensing, and satellite imagery.\n\nfacial recognition: classifying or matching a facial image against a large corpus of stored images.\n\nWhen PCA is used on facial images, you can think of the process as generating eigenfaces (Turk & Pentland (1991)) a representation of the pixels in the image in terms of an eigenvalue decomposition. Dimension reduction means that a facial image can be considerably compressed by removing the components associated with small dimensions.\nAs an example, consider the black and white version of the Mona Lisa shown in Figure 4.30. The idea and code for this example is adapted from this blog post by Kieran Healy.7\n\n\n\n\n\n\n\n\nFigure 4.30: 640 x 954 black and white image of the Mona Lisa. Source: Wikipedia\n\n\n\n\nIt would take too long to explain the entire method, so I’ll just sketch the essential parts here. The complete script for this example is contained in PCA-MonaLisa.R. …\n\nAn image can be imported using imager::load.image() which creates a \"cimg\" object, a 4-dimensional array with dimensions named x,y,z,c. x and y are the usual spatial dimensions, z is a depth dimension (which would correspond to time in a movie), and c is a color dimension containing R, G, B values.\n\nlibrary(imager)\nimg &lt;- imager::load.image(here::here(\"images\", \"MonaLisa-BW.jpg\"))\ndim(img)\n# [1] 640 954   1   1\n\n\nAn as.data.frame() method converts this to a data frame with x and y coordinates. Each x-y pair is a location in the 640 by 954 pixel grid, and the value is a grayscale value ranging from zero to one.\n\nimg_df_long &lt;- as.data.frame(img)\nhead(img_df_long)\n#   x y value\n# 1 1 1 0.431\n# 2 2 1 0.337\n# 3 3 1 0.467\n# 4 4 1 0.337\n# 5 5 1 0.376\n# 6 6 1 0.361\n\nHowever, to do a PCA we will need a matrix of data in wide format containing the grayscale pixel values. We can do this using tidyr::pivot_wider(), giving a result with 640 rows and 954 columns.\n\nimg_df &lt;- pivot_wider(img_df_long, \n                     names_from = y, \n                     values_from = value) |&gt;\n  select(-x)\ndim(img_df)\n# [1] 640 954\n\nMona’s PCA is produced from this img_df with prcomp():\n\nimg_pca &lt;- img_df |&gt;\n  prcomp(scale = TRUE, center = TRUE)\n\nWith 955 columns, the PCA comprises 955 eigenvalue/eigenvector pairs. However, the rank of a matrix is the smaller of the number of rows and columns, so only 640 eigenvalues can be non-zero. Printing the first 10 shows that the first three dimensions account for 46% of the variance and we only get to 63% with 10 components.\n\nimg_pca |&gt;\n  broom::tidy(matrix = \"eigenvalues\") |&gt; head(10)\n# # A tibble: 10 × 4\n#       PC std.dev percent cumulative\n#    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#  1     1   14.1  0.209        0.209\n#  2     2   11.6  0.141        0.350\n#  3     3   10.1  0.107        0.457\n#  4     4    7.83 0.0643       0.522\n#  5     5    6.11 0.0392       0.561\n#  6     6    4.75 0.0237       0.585\n#  7     7    3.70 0.0143       0.599\n#  8     8    3.52 0.0130       0.612\n#  9     9    3.12 0.0102       0.622\n# 10    10    2.86 0.00855      0.631\n\nFigure 4.31 shows a screeplot of proportions of variance. Because there are so many components and most of the information is concentrated in the largest dimensions, I’ve used a \\(\\log_{10}()\\) scale on the horizontal axis. Beyond 10 or so dimensions, the variance of additional components looks quite tiny.\n\nggscreeplot(img_pca) +\n  scale_x_log10()\n\n\n\n\n\n\nFigure 4.31: Screeplot of the variance proportions in the Mona Lisa PCA.\n\n\n\n\nThen, if \\(\\mathbf{M}\\) is the \\(640 \\times 955\\) matrix of pixel values, a best approximation \\(\\widehat{\\mathbf{M}}_k\\) using \\(k\\) dimensions can be obtained as \\(\\widehat{\\mathbf{M}}_k = \\mathbf{X}_k\\;\\mathbf{V}_k^\\mathsf{T}\\) where \\(\\mathbf{X}_k\\) are the principal component scores and \\(\\mathbf{V}_k\\) are the eigenvectors corresponding to the \\(k\\) largest eigenvalues. The function approx_pca() does this, and also undoes the scaling and centering carried out in PCA.\n\n\nCodeapprox_pca &lt;- function(n_comp = 20, pca_object = img_pca){\n  ## Multiply the matrix of rotated data (component scores) by the transpose of \n  ## the matrix of eigenvectors (i.e. the component loadings) to get back to a \n  ## matrix of original data values\n\n  recon &lt;- pca_object$x[, 1:n_comp] %*% t(pca_object$rotation[, 1:n_comp])\n  \n  ## Reverse any scaling and centering that was done by prcomp()\n  if(all(pca_object$scale != FALSE)){\n    ## Rescale by the reciprocal of the scaling factor, i.e. back to\n    ## original range.\n    recon &lt;- scale(recon, center = FALSE, scale = 1/pca_object$scale)\n  }\n  if(all(pca_object$center != FALSE)){\n    ## Remove any mean centering by adding the subtracted mean back in\n    recon &lt;- scale(recon, scale = FALSE, center = -1 * pca_object$center)\n  }\n  \n  ## Make it a data frame that we can easily pivot to long format\n  ## for drawing with ggplot\n  recon_df &lt;- data.frame(cbind(1:nrow(recon), recon))\n  colnames(recon_df) &lt;- c(\"x\", 1:(ncol(recon_df)-1))\n\n  ## Return the data to long form \n  recon_df_long &lt;- recon_df |&gt;\n    tidyr::pivot_longer(cols = -x, \n                        names_to = \"y\", \n                        values_to = \"value\") |&gt;\n    mutate(y = as.numeric(y)) |&gt;\n    arrange(y) |&gt;\n    as.data.frame()\n  \n  recon_df_long\n}\n\n\nFinally, the recovered images, using 2, 3 , 4, 5, 10, 15, 20, 50, and 100 principal components can be plotted using ggplot. In the code below, the approx_pca() function is run for each of the 9 values specified by n_pcs giving a data frame recovered_imgs containing all reconstructed images, with variables x, y and value (the greyscale pixel value).\n\nn_pcs &lt;- c(2:5, 10, 15, 20, 50, 100)\nnames(n_pcs) &lt;- paste(\"First\", n_pcs, \"Components\", sep = \"_\")\n\nrecovered_imgs &lt;- map_dfr(n_pcs, \n                          approx_pca, \n                          .id = \"pcs\") |&gt;\n  mutate(pcs = stringr::str_replace_all(pcs, \"_\", \" \"), \n         pcs = factor(pcs, levels = unique(pcs), ordered = TRUE))\n\nIn ggplot(), each is plotted using geom_raster(), using value to as the fill color. A quirk of images imported to R is that origin is taken as the upper left corner, so the Y axis scale needs to be reversed. The 9 images are then plotted together using facet_wrap().\n\np &lt;- ggplot(data = recovered_imgs, \n            mapping = aes(x = x, y = y, fill = value))\np_out &lt;- p + geom_raster() + \n  scale_y_reverse() + \n  scale_fill_gradient(low = \"black\", high = \"white\") +\n  facet_wrap(~ pcs, ncol = 3) + \n  guides(fill = \"none\") + \n  labs(title = \"Recovering Mona Lisa from PCA of her pixels\") + \n  theme(strip.text = element_text(face = \"bold\", size = rel(1.2)),\n        plot.title = element_text(size = rel(1.5)))\n\np_out\n\nThe result, in Figure 4.32 is instructive about how much visual information is contained in lower-dimensional reconstructions, or conversely, how much the image can be compressed by omitting the many small dimensions.\n\n\n\n\n\n\n\nFigure 4.32: Re-construction of the Mona Lisa using 2, 3 , 4, 5, 10, 15, 20, 50, and 100 principal components.\n\n\n\n\nIn this figure, with 4-5 components most people would recognize this as a blury image of the world’s most famous portrait. It is certainly clear that this is the Mona Lisa with 10–15 components. Details of the portrait and backgound features become recognizable with 20–50 components, and with 100 components it compares favorably with the original in Figure 4.30. In numbers, the original \\(640 \\times 955\\)) image is of size 600 Kb. The 100 component version is only 93 Kb, 15.6% of this.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-outlier-detection",
    "href": "04-pca-biplot.html#sec-outlier-detection",
    "title": "4  Dimension Reduction",
    "section": "\n4.7 Elliptical insights: Outlier detection",
    "text": "4.7 Elliptical insights: Outlier detection\nThe data ellipse (Section 3.2), or ellipsoid in more than 2D is fundamental in regression. But, as Pearson showed, it is also key to understanding principal components analysis, where the principal component directions are simply the axes of the ellipsoid of the data. As such, observations that are unusual in data space may not stand out in univariate views of the variables, but will stand out in principal component space, usually on the smallest dimension.\nAs an illustration, I created a dataset of \\(n = 100\\) observations with a linear relation, \\(y = x + \\mathcal{N}(0, 1)\\) and then added two discrepant points at (1.5, -1.5), (-1.5, 1.5).\n\nset.seed(123345)\nx &lt;- c(rnorm(100),             1.5, -1.5)\ny &lt;- c(x[1:100] + rnorm(100), -1.5, 1.5)\n\nWhen these are plotted with a data ellipse in Figure 4.33 (left), you can see the discrepant points labeled 101 and 102, but they do not stand out as unusual on either \\(x\\) or \\(y\\). The transformation to from data space to principal components space, shown in Figure 4.33 (right), is simply a rotation of \\((x, y)\\) to a space whose coordinate axes are the major and minor axes of the data ellipse, \\((PC_1, PC_2)\\). In this view, the additional points appear a univariate outliers on the smallest dimension, \\(PC_2\\).\n\n\n\n\n\n\n\nFigure 4.33: Outlier demonstration: The left panel shows the original data and highlights the two discrepant points, which do not appear to be unusual on either x or y. The right panel shows the data rotated to principal components, where the labeled points stand out on the smallest PCA dimension.\n\n\n\n\nTo see this more clearly, Figure 4.34 shows an animation of the rotation from data space to PCA space. This uses heplots::interpPlot() to interpolate linearly from the positions of the points in data space to their locations in PCA space.\n\n\n\n\n\n\n\nFigure 4.34: Animation of rotation from data space to PCA space. Source: R/outlier-demo.R\n\n\n\n\n\n4.7.1 Example: Penguin data\nIn Section 3.6.2 we examined the questions of multivariate normality and outliers for the penguin data. From a \\(\\chi^2\\) QQ plot (Figure 3.26) of the Mahalanobis \\(D^2\\) values, three Penguins (283, 10, 35) were identified as noteworthy, deserving a closer look to see why they are unusual. It was pointed out (Figure 3.27) that 2D plots of the data variables were only partially revealing. Let’s see where they appear in biplots.\nFirst, I find the noteworthy points with the three the largest \\(D^2\\) values as before:\n\ndata(peng, package=\"heplots\")\n\n# find potential multivariate outliers\nDSQ &lt;- heplots::Mahalanobis(peng[, 3:6])\nnoteworthy &lt;- order(DSQ, decreasing = TRUE)[1:3] |&gt; print()\n# [1] 283  10  35\n\nThe PCA shows that the first two components account for 88% of variance, so this is probably an adequate representation of the overall structure of our penguins:\n\npeng.pca &lt;- prcomp(\n  ~ bill_length + bill_depth + flipper_length + body_mass,\n                   data=peng, scale. = TRUE\n  )\nsummary(peng.pca)\n# Importance of components:\n#                          PC1   PC2    PC3   PC4\n# Standard deviation     1.657 0.882 0.6072 0.328\n# Proportion of Variance 0.686 0.195 0.0922 0.027\n# Cumulative Proportion  0.686 0.881 0.9730 1.000\n\n\n\n\n\n\n\n\nFigure 4.35: Biplot of the first two dimensions of the Penguin data. The points for the three noteworthy cases are labeled, but none of these appear to be unusual in this view.\n\n\n\n\nFigure 4.35 gives the biplot for the first two dimensions. It can be seen that:\n\nPC1 is largely determined by flipper length and body mass. We can interpret this as an overall measure of penguin size. On this dimension, Gentoos are the largest, by quite a lot, compared with Adelie and Chinstrap.\nPC2 is mainly determined by variation in the two beak variables: bill length and depth. Chinstrap are lower than the other two species on bill length and depth, but bill length further distinguishes the Gentoos from the Adelies. A penguin biologist could almost certainly provide an explanation, but I’ll call this beak shape.\nBut, our three suspected outliers are well-within the bulk of their species.\n\nThat’s the point of this exercise. The projection of the data into the space that accounts for the greatest total variance usually does not reveal a few unusual points.\n\nShow the codesource(\"R/penguin/penguin-colors.R\")\n# create vector of labels, blank except for the noteworthy\nlab &lt;- 1:nrow(peng)\nlab &lt;- ifelse(lab %in% noteworthy, lab, \"\")\n\nggbiplot(peng.pca,\n         choices = 1:2,\n         groups = peng$species, \n         ellipse = TRUE, ellipse.alpha = 0.1,\n         circle = TRUE,\n         var.factor = 1,\n         geom.ind = c(\"point\", \"text\"),\n         point.size = 1,\n         labels = lab, labels.size = 6,\n         varname.size = 5,\n         clip = \"off\") +\n  theme_minimal(base_size = 14) +\n  theme_penguins(\"dark\") +\n  scale_shape_discrete() +\n  theme(legend.direction = 'horizontal', legend.position = 'top') \n\n\nNow, plotting dimensions 3 and 4 gives Figure 4.36. Dimension 3, accounting for 9%, is largely determined by a contrast of bill length with bill depth and body mass, while dimension 4 involves a contrast between body mass and flipper length. The Chinstraps here have the longest, straightest beaks.\n\n\n\n\n\n\n\nFigure 4.36: Biplot of dimensions 3-4 of the Penguin data. The three noteworthy birds stand out in this view.\n\n\n\n\nRecall that the perpendicular projection of observation \\(i\\) on the vector for variable \\(j\\) gives an approximation of \\(\\hat{x}_{ij}\\) shown in that space. Our friend Cyrano (case 283), the only true multivariate outlier, lies at the extreme ends of both dimensions, with his exceptionally long, straight bill. Case 10 (Hook Nose) stands out at the high end of dimension 3 with a highly curved beak. case 35 is at the high end of dimension 4, so probably is much heavier than most and has short flippers.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#what-have-we-learned",
    "href": "04-pca-biplot.html#what-have-we-learned",
    "title": "4  Dimension Reduction",
    "section": "\n4.8 What have we learned?",
    "text": "4.8 What have we learned?\nWelcome to the world of multivariate juicers–—those magical tools that squeeze the most meaningful information from high-dimensional data clouds! This chapter has taken us on a journey from Flatland to Spaceland, revealing how dimension reduction methods can transform overwhelming complexity into interpretable insights.\n• PCA is your geometric friend, helping you compress N-dimensional data: Principal Components Analysis finds the directions of maximum variance in your data, creating uncorrelated orthogonal dimensions that capture the most “juice” from your multivariate cloud. Think of it as finding the best viewpoint to see a 3D sculpture when you can only look at a 2D photograph—PCA rotates and compresses your data to show you the best 2D viewing angle.\n• Biplots are visualization gold, helping you view compressed N-dimensional data: These elegant displays build off of PCA by simultaneously showing both your observations (as points) and your variables (as vectors) in the same reduced space. The magic lies in the interpretation: variable vectors pointing in similar directions are correlated, and you can read approximate values by projecting points onto variable vectors. It’s like having X-ray vision for multivariate relationships!\n• Eigenvalues tell the variance story: The screeplot becomes your guide for deciding how many dimensions to keep. Look for the “elbow” where the eigenvalues start to resemble scree (rubble) rather than meaningful signal. Generally, 80-90% cumulative variance gives you a solid foundation for interpretation.\n• Supplementary variables enhance interpretation: Once you’ve found your reduced-dimension view, you can project additional variables into the space to aid interpretation—like adding helpful annotations to a map. This technique bridges the gap between statistical discovery and domain knowledge.\n• Nonlinear methods reveal hidden structures: When relationships aren’t linear, techniques like multidimensional scaling (MDS) and t-SNE can uncover patterns that PCA might miss. These methods focus on preserving local neighborhoods and distances rather than global variance, often revealing clusters and nonlinear manifolds lurking in your data.\n• Variable ordering creates visual clarity: When similar variables are placed adjacent to each other, patterns emerge and anomalies become visible—it’s like organizing a messy bookshelf by subject. In biplots, the angles of variable vectors provide a natural ordering that are used to make correlation matrices and other displays much more interpretable.\n• Outlier detection gets multidimensional power: Points that seem normal in individual variables can reveal themselves as true multivariate outliers when viewed in principal component space, especially along the smallest dimensions. The data ellipse becomes your guide to understanding what’s typical versus what deserves a closer look.\n• Real applications abound: From compressing the Mona Lisa using eigenfaces to understanding crime patterns across U.S. states, dimension reduction methods bridge the gap between statistical technique and practical insight. These aren’t just mathematical curiosities—they’re essential tools for making sense of our increasingly high-dimensional world.\n\n\n\n\n\nAluja, T., Morineau, A., & Sanchez, G. (2018). Principal component analysis for data science. https://pca4ds.github.io/\n\n\nBorg, I., & Groenen, P. J. F. (2005). Modern Multidimensional Scaling: Theory and Applications. Springer.\n\n\nBorg, I., Groenen, P. J. F., & Mair, P. (2018). Applied multidimensional scaling and unfolding. In SpringerBriefs in Statistics. Springer International Publishing. https://doi.org/10.1007/978-3-319-73471-2\n\n\nCattell, R. B. (1966). The scree test for the number of factors. Multivariate Behavioral Research, 1(2), 245–276. https://doi.org/10.1207/s15327906mbr0102_10\n\n\nDray, S., & Siberchicot, A. (2025). Adegraphics: An S4 lattice-based package for the representation of multivariate data. http://pbil.univ-lyon1.fr/ADE-4/\n\n\nEuler, L. (1758). Elementa doctrinae solidorum. Novi Commentarii Academiae Scientiarum Petropolitanae, 4, 109–140. https://scholarlycommons.pacific.edu/euler-works/230/\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix functions for teaching and learning linear algebra and multivariate statistics. https://github.com/friendly/matlib\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays. Computational Statistics and Data Analysis, 43(4), 509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with application to principal components analysis. Biometrics, 58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGabriel, K. R. (1981). Biplot display of multivariate matrices for inspection of data and diagnosis. In V. Barnett (Ed.), Interpreting multivariate data (pp. 147–173). John Wiley; Sons.\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman & Hall.\n\n\nGower, J. C., Lubbe, S. G., & Roux, N. J. L. (2011). Understanding biplots. Wiley. http://books.google.ca/books?id=66gQCi5JOKYC\n\n\nGreenacre, M. (1984). Theory and applications of correspondence analysis. Academic Press.\n\n\nGreenacre, M. (2010). Biplots in practice. Fundación BBVA. https://books.google.ca/books?id=dv4LrFP7U\\_EC\n\n\nHahsler, M., Buchta, C., & Hornik, K. (2024). Seriation: Infrastructure for ordering objects using seriation. https://github.com/mhahsler/seriation\n\n\nHusson, F., Le, S., & Pagès, J. (2017). Exploratory multivariate analysis by example using r. Chapman & Hall. https://doi.org/10.1201/b21874\n\n\nKassambara, A., & Mundt, F. (2020). Factoextra: Extract and visualize the results of multivariate data analyses. https://doi.org/10.32614/CRAN.package.factoextra\n\n\nKrijthe, J. (2023). Rtsne: T-distributed stochastic neighbor embedding using a barnes-hut implementation. https://github.com/jkrijthe/Rtsne\n\n\nKruskal, J. B. (1964). Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29(1), 1–27. https://doi.org/10.1007/bf02289565\n\n\nLê, S., Josse, J., & Husson, F. (2008). FactoMineR: A package for multivariate analysis. Journal of Statistical Software, 25(1), 1–18. https://doi.org/10.18637/jss.v025.i01\n\n\nMaaten, L. van der, & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9, 2579–2605. http://www.jmlr.org/papers/v9/vandermaaten08a.html\n\n\nMurdoch, D., & Adler, D. (2025). Rgl: 3D visualization using OpenGL. https://doi.org/10.32614/CRAN.package.rgl\n\n\nOksanen, J., Simpson, G. L., Blanchet, F. G., Kindt, R., Legendre, P., Minchin, P. R., O’Hara, R. B., Solymos, P., Stevens, M. H. H., Szoecs, E., Wagner, H., Barbour, M., Bedward, M., Bolker, B., Borcard, D., Borman, T., Carvalho, G., Chirico, M., De Caceres, M., … Weedon, J. (2025). Vegan: Community ecology package. https://doi.org/10.32614/CRAN.package.vegan\n\n\nPearson, K. (1896). Contributions to the mathematical theory of evolution—III, regression, heredity and panmixia. Philosophical Transactions of the Royal Society of London, 187, 253–318.\n\n\nPearson, K. (1901). On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 6(2), 559–572.\n\n\nPedersen, T. L., & Robinson, D. (2025). Gganimate: A grammar of animated graphics. https://doi.org/10.32614/CRAN.package.gganimate\n\n\nReaven, G. M., & Miller, R. G. (1979). An attempt to define the nature of chemical diabetes using a multidimensional analysis. Diabetologia, 16, 17–24.\n\n\nShepard, R. N. (1962a). The analysis of proximities: Multidimensional scaling with an unknown distance function. i. Psychometrika, 27(2), 125–140. https://doi.org/10.1007/bf02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities: Multidimensional scaling with an unknown distance function. II. Psychometrika, 27(3), 219–246. https://doi.org/10.1007/bf02289621\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S. (1972a). Multidimensional scaling; theory and applications in the behavioral sciences: Vols. II. Applications. Seminar Press. https://books.google.ca/books?id=PpFAAQAAIAAJ\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S. (1972b). Multidimensional scaling: Theory and applications in the behavioral sciences: Vols. I. Theory. Seminar Press. https://books.google.ca/books?id=pJRAAQAAIAAJ\n\n\nShoben, E. J. (1983). Applications of multidimensional scaling in cognitive psychology. Applied Psychological Measurement, 7(4), 473–490. https://doi.org/10.1177/014662168300700406\n\n\nTorgerson, W. S. (1952). Multidimensional scaling: I. Theory and method. Psychometrika, 17(4), 401–419. https://doi.org/10.1007/bf02288916\n\n\nTurk, M., & Pentland, A. (1991). Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1), 71–86. https://doi.org/10.1162/jocn.1991.3.1.71\n\n\nVu, V. Q., & Friendly, M. (2024). Ggbiplot: A grammar of graphics implementation of biplots. https://doi.org/10.32614/CRAN.package.ggbiplot",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#footnotes",
    "href": "04-pca-biplot.html#footnotes",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "This is Euler’s (1758) formula, which states that any convex polyhedron must obey the formula \\(V + F - E = 2\\) where \\(V\\) is the number of vertexes (corners), \\(F\\) is the number of faces and \\(E\\) is the number of edges. For example, a tetrahedron or pyramid has \\((V, F, E) = (4, 4, 6)\\) and a cube has \\((V, F, E) = (8, 6, 12)\\). Stated in words, for all solid bodies confined by planes, the sum of the number of vertexes and the number of faces is two less than the number of edges.↩︎\nFor example, if two variables in the analysis are height and weight, changing the unit of height from inches to centimeters would multiply its variance by \\(2.54^2\\); changing weight from pounds to kilograms would divide its variance by \\(2.2^2\\).↩︎\nThe unfortunate default scale. = FALSE was for consistency with S, the precursor to R but in general scaling is usually advisable.↩︎\nThe vegan package (Oksanen et al., 2025) provides vegan::metaMDS() which allows a wide range of distance measures …↩︎\nThe usual default, perplexity = 30 focuses on preserving the distances to the 30 nearest neighbors and puts virtually no weight on preserving distances to the remaining points. For data sets with a small number of points e.g. \\(n=100\\), this will uncover the global structure quite well since each point will preserve distances to a third of the data set. For larger problems, e.g., \\(n = 10,000\\) points, using a higher perplexity value e.g. 500, will do a much better job for of uncovering the global structure. (This description comes from https://opentsne.readthedocs.io/en/latest/parameters.html)↩︎\nThe general topic of arranging items (variables, factor values) in an orderly sequence is called seriation, and stems from methods of dating in archaeology, used to arrange stone tools, pottery fragments, and other artifacts in time order. In R, the seriation package (Hahsler et al., 2024) provides a wide range of techniques. …↩︎\nhttps://kieranhealy.org/blog/archives/2019/10/27/reconstructing-images-using-pca/↩︎",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "00-Author.html",
    "href": "00-Author.html",
    "title": "Author",
    "section": "",
    "text": "Michael Friendly\n\n datavis.ca [Legacy web site]\n dataVisFriendly [BlueSky]\n friendly [Github]\n vis.social/@datavisFriendly [Mastodon]\n\n\nMichael Friendly is a Fellow of the American Statistical Association, Professor of Psychology and coordinator of the Statistical Consulting Service at York University, He is an associate editor of the Journal of Graphical and Computational Statistics and has served as an editorial collaborator for many other journals. He received his Ph.D. in psychometrics and cognitive psychology from Princeton University.\nHis current research work includes the development of graphical methods for data visualization, where he is a principal innovator of novel methods for relatively “un-vizzed” problems, including visualizing categorical (Friendly, 2000; Friendly & Meyer, 2016,) and those for multivariate data, the subject of this book. Along with this, he is the author and maintainer of many R packages, described on his GitHub R packages page.\nHis passion for the deep roots of modern graphical methods led him to become an world-known amateur historian of data visualization. In the latter, he directs The Milestones Project, a comprehensive catalog and database of the principal developments in the histories of thematic cartography, statistical graphics and data visualization. He is a founder of an international group, Les Chevaliers des Albums de Statistique Graphique, devoted to this history and is author of multiple books and research papers on these topics. His most recent book in this area is Friendly & Wainer (2021) A History of Data Visualization and Graphic Communication, Harvard University Press.\n\n\n\n\nFriendly, M. (2000). Visualizing categorical data. SAS Institute.\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034",
    "crumbs": [
      "Author"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html",
    "href": "07-lin-mod-topics.html",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "7.1 Ellipsoids in data space and \\(\\boldsymbol{\\beta}\\) space\nThe geometric and graphical approach of earlier chapters has already introduced some new ideas for thinking about multivariate data, models for explaining them, and graphical methods for understanding their results. These can be applied to better understand common problems that arise in data analysis.\nIn Section 7.1 I explore the geometric relationships between ellipses in data space and how these appear in the space of the estimated coefficients of linear models, called \\(\\beta\\) space. It turns out that points in one space correspond to lines in the other, a reflection that each one is in some sense the inverse and dual of the other.\nThis geometry can also be used to clarify the effect of measurement errors in the predictor variables, as illustrated in Section 7.2.\nPackages\nIn this chapter I use the following packages. Load them now:\nIt is most common to look at data and fitted models in our familiar “data space”. Here, axes correspond to variables, points represent observations, and fitted models can be plotted as lines (or planes) in this space. As we’ve suggested, data ellipsoids provide informative summaries of relationships in data space.\nFor linear models, particularly regression models with quantitative predictors, there is another space—“\\(\\boldsymbol{\\beta}\\) space”—that provides deeper views of models and the relationships among them. This discussion extends Friendly et al. (2013), Sec. 4.6.\nIn \\(\\boldsymbol{\\beta}\\) space, the axes pertain to coefficients, for example \\((\\beta_0, \\beta_1)\\) in a simple linear regression corresponds to the model \\(y = \\beta_0 + \\beta_1 x\\). Points in this space are models (true, hypothesized, fitted) whose coordinates represent values of these parameters. For example,",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#sec-betaspace",
    "href": "07-lin-mod-topics.html#sec-betaspace",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "In simple regression, one point plotted with the coefficients \\(\\widehat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\hat{\\beta}_0, \\hat{\\beta}_1)\\) represents the least squares estimate.\nOther points, representing different fitting methods can be shown in the same plot. For instance, \\(\\widehat{\\boldsymbol{\\beta}}_{\\text{WLS}}\\) and \\(\\widehat{\\boldsymbol{\\beta}}_{\\text{ML}}\\) would give weighted least squares and maximum likelihood estimates.\nThe line \\(\\beta_1 = 0\\) represents the null hypothesis that the slope is zero and the point \\((0, 0)\\) corresponds to the joint hypothesis \\(\\mathcal{H}_0: \\beta_0 = 0, \\beta_1 = 0\\).\n\n\n7.1.1 Dual and inverse spaces\nAs illustrated below, the data space of \\(\\mathbf{X}\\) and that of \\(\\boldsymbol{\\beta}\\) space are each dual and inverse to the other. To make this explicit for simple linear regression:\n\neach line, like \\(\\mathbf{y} = \\beta_0 + \\beta_1 \\mathbf{x}\\) with intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) in data space corresponds to a point \\((\\beta_0,\\beta_1)\\) in \\(\\boldsymbol{\\beta}\\) space, and conversely;\nthe set of points on any line \\(\\beta_1 = x + y \\beta_0\\) in \\(\\boldsymbol{\\beta}\\) space corresponds to a set of lines through a given point \\((x, y)\\) in data space, and conversely;\nthe geometric proposition that “every pair of points defines a line in one space” corresponds to the proposition that “every two lines intersect in a point in the other space”.\n\n\nExample 7.1 Dual points and lines\nThis duality of points and lines is illustrated in Figure 7.1. The left panel shows three lines in data space, which can be expressed as linear equations in \\(\\mathbf{z} = (x, y)\\) of the form \\(\\mathbf{A} \\mathbf{z} = \\mathbf{d}\\). The function matlib::showEqn(A, d) prints these as equations in the data coordinates \\(x\\) and \\(y\\).\n\nA &lt;- matrix(c( 1, 1, 0,\n              -1, 1, 1), nrow = 3, ncol = 2) \nd &lt;- c(2, 1/2, 1)\nshowEqn(A, d, vars = c(\"x\", \"y\"), simplify = TRUE)\n#   x - 1*y  =    2 \n#   x   + y  =  0.5 \n# 0*x   + y  =    1\n\n\n\n\n\n\n\n\nFigure 7.1: Duality of \\((x, y)\\) lines in data space (left) and points in \\(\\beta\\)-space (right). Each line in data space corresponds to a point, whose intercept and slope are shown in \\(\\beta\\)-space. Labels in both plots identify the points and lines.\n\n\n\n\nThe first equation, \\(x - y = 2\\) can be expressed as the red line \\(y = x - 2\\) labeled in the left panel of Figure 7.1. This corresponds to the red point \\((\\beta_0, \\beta_1) = (-2, 1)\\) in \\(\\beta\\) space, and similarly for the other two equations shown in blue and green.\nThe second equation, \\(x + y = \\frac{1}{2}\\), or \\(y = 0.5 - x\\) intersects the first at the point \\((x, y) = (1.25, 0.75)\\). This corresponds to the line connecting \\((-2, 1)\\) and \\((0.5, -1)\\) in \\(\\beta\\) space. All solutions to this pair of equations lie along this line.\n\n\nExample 7.2 Inverses\nThis lovely interchange between points and lines is an example of an important general principle of duality in modern mathematics, which translates concepts and structures from one perspective to another and back again. We get two views of the same thing, whose dual nature provides greater insight from the combination of perspectives.\nWe have seen (Section 3.2) how ellipsoids in data space summarize variance (lack of precision) and correlation of our data. For the purpose of understanding linear models, ellipsoids in \\(\\beta\\) space do the same thing for the estimates of parameters. These ellipsoids are dual and inversely related to each other, a point first made clear by Dempster (1969, Ch. 6):\n\nIn data space, joint confidence intervals for the mean vector or joint prediction regions for the data are given by the ellipsoids \\((\\bar{x}_1, \\bar{x}_2)^\\mathsf{T} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}}\\), where the covariance matrix \\(\\mathbf{S}_{\\mathbf{X}}\\) depends on \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) (\\(\\oplus\\) here shifts the ellipsoid to one centered at \\((\\bar{x}_1, \\bar{x}_2)\\) here, as in Equation 3.2).\nIn the dual \\(\\mathbf{\\beta}\\) space, joint confidence regions for the coefficients of a response variable \\(y\\) on \\((x_1, x_2)\\) are given by ellipsoids of the form \\(\\widehat{\\boldsymbol{\\beta}} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}^{-1}}\\), and depend on \\(\\mathbf{(\\mathbf{X}^\\mathsf{T}\\mathbf{X})}^{-1}\\).\n\nIt is useful to understand the underlying geometry here connecting the ellipses for a matrix and its inverse. This can be seen in Figure 7.2, which shows an ellipse for a covariance matrix \\(\\mathbf{S}\\), whose axes, as we saw in Chapter 4 are the eigenvectors \\(\\mathbf{v}_i\\) of \\(\\mathbf{S}\\) and whose radii are the square roots \\(\\sqrt{\\lambda_i}\\) of the corresponding eigenvalues. The comparable ellipse for \\(2 \\mathbf{S}\\) has radii multiplied by \\(\\sqrt{2}\\).\n\n\n\n\n\n\n\nFigure 7.2: Geometric properties of an ellipse \\(\\mathbf{S}\\) and its inverse, \\(\\mathbf{S}^{-1}\\). The principal axes (dotted lines) are given by the eigenvectors, which are the same for \\(\\mathbf{S}\\) and \\(\\mathbf{S}^{-1}\\). Multiplying \\(\\mathbf{S}\\) by 2 makes it’s ellipse larger by \\(\\sqrt{2}\\), while the same factor makes the ellipse for \\((2 \\mathbf{S})^{-1}\\) smaller by the same factor.\n\n\n\n\nAs long as \\(\\mathbf{S}\\) is of full rank, the eigenvectors of \\(\\mathbf{S}^{-1}\\) are identical, while the eigenvalues are \\(1 / \\lambda_i\\), so the radii are the reciprocals \\(1 / \\sqrt{\\lambda_i}\\). The analogous ellipse for \\((2 \\mathbf{S}^{-1})\\) is smaller by a factor of \\(\\sqrt{2}\\).\nThus, in two dimensions, the ellipse for \\(\\mathbf{S}^{-1}\\) is a \\(90^o\\) rotation of that for \\(\\mathbf{S}\\). The ellipse for \\(\\mathbf{S}^{-1}\\) is small in directions where the ellipse for \\(\\mathbf{S}\\) is large, and vice-versa. In our statistical applications, this translates as:\n\nParameter estimates in \\(\\beta\\) space are more precise (have less variance) in the directions where the data are more widely dispersed, giving more information about the relationship.\n\n\nI illustrate these ideas in the example below.\n\n7.1.2 Data ellipse and confidence ellipse\nIn Section 6.4 I used the data (heplots::coffee) on coffee consumption, stress and heart disease to illustrate added variable plots. These explained the perplexing result that increased coffee seemed strongly positively related to heart damage when considered alone (see Figure 6.10), but had a negative coefficient (\\(\\beta_{\\text{Coffee}} = -0.41\\)) in the model (fit.both) that also included the effect of stress on heart damage. The AV plots (Figure 6.11) clarified this by showing the conditional relations of each predictor, when the other was controlled or adjusted for.\nHere, I use this dataset to illustrate the inverse relation between a data ellipse based on the covariance matrix \\(\\mathbf{S}_X\\) and the corresponding confidence ellipse for the coefficients based on \\(\\mathbf{S}_X^{-1}\\). This is shown in Figure 7.3 for the data space relation between the predictors coffee and stress, and the \\(\\beta\\) space of the confidence ellipse for their coefficients.\n\n\n\n\n\n\n\nFigure 7.3: Data space and \\(\\boldsymbol{\\beta}\\) space representations of Coffee and Stress. Left: 40% and 68% data ellipses. Right: Joint 95% confidence ellipse (blue) for (\\(\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}}\\)), confidence interval generating ellipse (red) with 95% univariate shadows. \\(H_0\\) marks the joint hypothesis that both coefficients equal zero.\n\n\n\n\n\nThe left panel in Figure 7.3 is the same as that in the (3,2) cell of Figure 6.10 for the relation Stress ~ Coffee but with data ellipses of 40% and 68% coverage. The shadows of the 40% ellipse on any axis give univariate intervals of the mean \\(\\bar{x} \\pm 1 s_x\\) (standard deviation) shown by the thick red lines; the shadow of the 68% ellipse corresponds to an interval \\(\\bar{x} \\pm 1.5 s_x\\).1\nThe right panel shows the joint 95% confidence region for the coefficients \\((\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}})\\) and individual confidence intervals in \\(\\boldsymbol{\\beta}\\) space. These are determined as\n\\[\n\\widehat{\\mathbf{\\beta}} \\oplus \\sqrt{d F^{.95}_{d, \\nu}} \\times s_e \\times \\mathbf{S}_X^{-1/2} \\:\\: .\n\\] where \\(d\\) is the number of dimensions for which we want coverage, \\(\\nu\\) is the residual degrees of freedom for \\(s_e\\), and \\(\\mathbf{S}_X\\) is the covariance matrix of the predictors.\nThus, the blue ellipse in Figure 7.3 (right) is the ellipse of joint 95% coverage, using the factor \\(\\sqrt{2 F^{.95}_{2, \\nu}}\\), which covers the true values of (\\(\\beta_{\\mathrm{Stress}}, \\beta_{\\mathrm{Coffee}}\\)) in 95% of samples. Moreover:\n\nAny joint hypothesis (e.g., \\(\\mathcal{H}_0:\\beta_{\\mathrm{Stress}}=0, \\beta_{\\mathrm{Coffee}}=0\\)) can be tested visually, simply by observing whether the hypothesized point, \\((0, 0)\\) here, lies inside or outside the joint confidence ellipse. That hypothesis is rejected\nThe shadows of this ellipse on the horizontal and vertical axes give Scheff'e joint 95% confidence intervals for the parameters, with protection for simultaneous inference (“fishing”) in a 2-dimensional space.\nSimilarly, using the factor \\(\\sqrt{F^{1-\\alpha/d}_{1, \\nu}} = t^{1-\\alpha/2d}_\\nu\\) would give an ellipse whose 1D shadows are \\(1-\\alpha\\) Bonferroni confidence intervals for \\(d\\) posterior hypotheses.\n\nVisual hypothesis tests and \\(d=1\\) confidence intervals for the parameters separately are obtained from the red ellipse in Figure 7.3, which is scaled by \\(\\sqrt{F^{.95}_{1, \\nu}} = t^{.975}_\\nu\\). We call this the confidence-interval generating ellipse (or, more compactly, the “confidence-interval ellipse”). The shadows of the confidence-interval ellipse on the axes (thick red lines) give the corresponding individual 95% confidence intervals, which are equivalent to the (partial, Type III) \\(t\\)-tests for each coefficient given in the standard multiple regression output shown above.\nThus, controlling for Stress, the confidence interval for the slope for Coffee includes 0, so we cannot reject the hypothesis that \\(\\beta_{\\mathrm{Coffee}}=0\\) in the multiple regression model, as we saw above in the numerical output. On the other hand, the interval for the slope for Stress excludes the origin, so we reject the null hypothesis that \\(\\beta_{\\mathrm{Stress}}=0\\), controlling for Coffee consumption.\nFinally, consider the relationship between the data ellipse and the confidence ellipse. These have exactly the same shape, but (with equal coordinate scaling of the axes), the confidence ellipse is exactly a \\(90^o\\) rotation and rescaling of the data ellipse. In directions in data space where the slice of the data ellipse is wide—where we have more information about the relationship between Coffee and Stress—the projection of the confidence ellipse is narrow, reflecting greater precision of the estimates of coefficients. Conversely, where slice of the the data ellipse is narrow (less information), the projection of the confidence ellipse is wide (less precision).\nTODO: Maybe include this code only in the HTML version?\nConfidence ellipses are drawn using car::confidenceEllipse(). Click the button to show the code.\n\nCode for confidence ellipsesconfidenceEllipse(coffee.mod, \n    grid = FALSE,\n    xlim = c(-2, 1), ylim = c(-0.5, 2.5),\n    xlab = expression(paste(\"Coffee coefficient,  \", beta[\"Coffee\"])),\n    ylab = expression(paste(\"Stress coefficient,  \", beta[\"Stress\"])),\n    cex.lab = 1.5)\nconfidenceEllipse(coffee.mod, add=TRUE, draw = TRUE,\n    col = \"red\", fill = TRUE, fill.alpha = 0.1,\n    dfn = 1)\nabline(h = 0, v = 0, lwd = 2)\n\n# confidence intervals\nbeta &lt;- coef( coffee.mod )[-1]\nCI &lt;- confint(coffee.mod)\nlines( y = c(0,0), x = CI[\"Coffee\",] , lwd = 6, col = 'red')\nlines( x = c(0,0), y = CI[\"Stress\",] , lwd = 6, col = 'red')\npoints( diag( beta ), col = 'black', pch = 16, cex=1.8)\n\nabline(v = CI[\"Coffee\",], col = \"red\", lty = 2)\nabline(h = CI[\"Stress\",], col = \"red\", lty = 2)\n\ntext(-2.1, 2.35, \"Beta space\", cex=2, pos = 4)\narrows(beta[1], beta[2], beta[1], 0, angle=8, len=0.2)\narrows(beta[1], beta[2], 0, beta[2], angle=8, len=0.2)\n\ntext( -1.5, 1.85, \"df = 2\", col = 'blue', adj = 0, cex=1.2)\ntext( 0.2, .85, \"df = 1\", col = 'red', adj = 0, cex=1.2)\n\nheplots::mark.H0(col = \"darkgreen\", pch = \"+\", lty = 0, pos = 4, cex = 3)",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#sec-meas-error",
    "href": "07-lin-mod-topics.html#sec-meas-error",
    "title": "\n7  Topics in Linear Models\n",
    "section": "\n7.2 Measurement error",
    "text": "7.2 Measurement error\n\nIn classical linear models, the predictors are often considered to be fixed variables, statistical constants as in a designed experiment where the \\(x\\)s are set to given values. If they are random variables, they are to be measured without error and independent of the regression errors in \\(y\\). Either condition, along with the assumption of linearity, guarantees that the standard OLS estimators are unbiased.\nThat is, in a simple linear regression, \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), the estimated slope \\(\\hat{\\beta}_1\\) will have an average, expected value \\(\\mathcal{E} (\\hat{\\beta}_1)\\) equal to the true population value \\(\\beta_1\\) over repeated samples.\nNot only this, but the Gauss-Markov theorem (https://bit.ly/4njKnBo) guarantees that under these conditions the OLS estimator is also the most efficient because it has the least variance (most precise) among all linear and unbiased estimators. The classical OLS estimator is then said to be BLUE: It is the Best (lowest variance), Linear (among linear estimators), Unbiased, Estimator.\nBut these happy results may go out the window when the predictor variables are subject to measurement errors, for example with rating scales of less than perfect reliability. This section illustrates how error in predictors affects bias and precision using the graphical methods of this book.\n\n7.2.1 Errors in predictors\nErrors in the response \\(y\\) are accounted for in the model and measured by the mean squared error, \\(\\text{MSE} = \\hat{\\sigma}_\\epsilon^2\\). But in practice, of course, predictor variables are often only observed indicators, subject to their own error. Indeed, in the behavioral sciences it is rare that predictors are perfectly reliable and measured exactly. In economics, measures of employment, or cost of your groceries cannot be assumed to be error-free.\nThis fact that is recognized in errors-in-variables regression models (Fuller, 2006) and in more general structural equation models, but often ignored otherwise. Ellipsoids in data space and \\(\\beta\\) space are well suited to showing the effect of measurement error in predictors on OLS estimates.\nThe statistical facts are well known, though perhaps counter-intuitive in certain details:\n\nmeasurement error in a predictor biases regression coefficients (towards 0), while\nerror in the measurement in \\(y\\) increases the MSE and thus standard errors of the regression coefficients but does not introduce bias in the coefficients.\n\n7.2.2 Example: Measurement Error Quartet\nAn illuminating example can be constructed by starting with the simple linear regression\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\; ,\n\\] where \\(x_i\\) is the true, fully reliable predictor and \\(y\\) is the response, with error variance \\(\\sigma_\\epsilon^2\\). Now consider that we don’t measure \\(x_i\\) exactly, but instead observe \\(x^\\star_i\\).\n\\[\nx^\\star_i = x_i + \\eta_i \\; ,\n\\] where the measurement error \\(\\eta_i\\) is independent of the true \\(x_i\\) with variance \\(\\sigma^2_\\eta\\). We can extend this example to also consider the effect of adding additional, independent error variance to \\(y\\), so that instead of \\(y_i\\) we observe\n\\[\ny^\\star_i = y_i + \\nu_i\n\\] with variance \\(\\sigma^2_\\nu\\).\nLet’s simulate an example where the true relation is \\(y = 0.2 + 0.3 x\\) with error standard deviation \\(\\sigma = 0.5\\). I’ll take \\(x\\) to be uniformly distributed in [0, 10] and calculate \\(y\\) as normally distributed around that linear relation.\n\n\nset.seed(123)\nn &lt;- 300\n\na &lt;- 0.2    # true intercept\nb &lt;- 0.3    # true slope\nsigma &lt;- 0.5 # baseline error standard deviation\n\nx &lt;- runif(n, 0, 10)\ny &lt;- rnorm(n, a + b*x, sigma)\ndemo &lt;- data.frame(x,y)\n\nThen, generate alternative values \\(x^\\star\\) and \\(y^\\star\\) with additional error standard deviations around \\(x\\) given by \\(\\sigma_\\eta = 4\\) and around \\(y\\) given by \\(\\sigma_\\nu = 1\\).\n\nerr_y &lt;- 1   # additional error stdev for y\nerr_x &lt;- 4   # additional error stdev for x\ndemo  &lt;- demo |&gt;\n  mutate(y_star = rnorm(n, y, err_y),\n         x_star = rnorm(n, x, err_x))\n\nThere are four possible models we could fit and compare, using the combinations of \\((x, x^\\star)\\) and \\((y, y^\\star)\\)\n\nfit_1 &lt;- lm(y ~ x,           data = demo)   # no additional error\nfit_2 &lt;- lm(y_star ~ x,      data = demo)   # error in y\nfit_3 &lt;- lm(y ~ x_star,      data = demo)   # error in x\nfit_4 &lt;- lm(y_star ~ x_star, data = demo)   # error in x and y\n\nHowever, to show the differences visually, we can simply plot the data for each pair and show the regression lines (with confidence bands) and the data ellipses. To do this efficiently with ggplot2, it is necessary to transform the demo data to long format with columns x and y, distinguished by name for the four combinations.\n\n# make the demo dataset long, with names for the four conditions\ndf &lt;- bind_rows(\n  data.frame(x=demo$x,      y=demo$y,      name=\"No measurement error\"),\n  data.frame(x=demo$x,      y=demo$y_star, name=\"Measurement error on y\"),\n  data.frame(x=demo$x_star, y=demo$y,      name=\"Measurement error on x\"),\n  data.frame(x=demo$x_star, y=demo$y_star, name=\"Measurement error on x and y\")) |&gt;\n  mutate(name = fct_inorder(name)) \n\nThen, we can plot the data in df with points, regression lines and a data ellipse, faceting by name to give the measurement error quartet. \n\n\nggplot(df, aes(x, y)) +\n  geom_point(alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", \n               color = \"blue\",fill= \"blue\", \n               alpha=0.05, linewidth = 1.1) +\n  geom_smooth(method=\"lm\", formula = y~x, fullrange=TRUE, level=0.995,\n              color = \"red\", fill = \"red\", alpha = 0.2) +\n  facet_wrap(~name) \n\n\n\n\n\n\nFigure 7.4: The measurement error quartet: Each plot shows the linear regression of y on x, but where additional error variance has been added to y or x or both. The widths of the confidence bands and the vertical extent of the data ellipses show the effect on precision.\n\n\n\n\nComparing the plots in the first row, you can see that when additional error is added to \\(y\\), the regression slope remains essentially unchanged, illustrating that the estimate is unbiased. However, the confidence bounds on the regression line become wider, and the data ellipse becomes fatter in the \\(y\\) direction, illustrating the loss of precision.\nThe effect of error in \\(x\\) is less kind. Comparing the first row of plots with the second row, you can see that the estimated slope decreases when errors are added to \\(x\\). This is called attenuation bias, and it can be shown that \\[\n\\widehat{\\beta}_{x^\\star} \\longrightarrow \\frac{\\beta}{1+\\sigma^2_\\eta /\\sigma^2_x} \\; ,\n\\] where \\(\\beta\\) here refers to the regression slope and \\(\\longrightarrow\\) means “converges to”, as the sample size gets large. Thus, as \\(\\sigma^2_\\eta\\) increases, \\(\\widehat{\\beta}_{x^\\star}\\) becomes less than \\(\\beta\\).\nBeyond plots like Figure 7.4, we can see the effects of error in \\(x\\) or \\(y\\) on the model summary statistics such as the correlation \\(r_{xy}\\) or MSE by extracting these from the fitted models. This is easily done using dplyr::nest_by(name) and fitting the regression model to each subset, from which we can obtain the model statistics using sigma(), coef() and so forth. A bit of dplyr::mutate() magic is used to construct indicators errX and errY giving whether or not error was added to \\(x\\) and/or \\(y\\).\n\nmodel_stats &lt;- df |&gt;\n  dplyr::nest_by(name) |&gt;\n  mutate(model = list(lm(y ~ x, data = data)),\n         sigma = sigma(model),\n         intercept = coef(model)[1],\n         slope = coef(model)[2],\n         r = sqrt(summary(model)$r.squared)) |&gt;\n  mutate(errX = stringr::str_detect(name, \" x\"),\n         errY = stringr::str_detect(name, \" y\")) |&gt;\n  mutate(errX = factor(errX, levels = c(\"TRUE\", \"FALSE\")),\n         errY = factor(errY, levels = c(\"TRUE\", \"FALSE\"))) |&gt;\n  relocate(errX, errY, r, .after = name) |&gt;\n  select(-data) |&gt;\n  print()\n# # A tibble: 4 × 8\n# # Rowwise:  name\n#   name                errX  errY      r model sigma intercept  slope\n#   &lt;fct&gt;               &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lis&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n# 1 No measurement err… FALSE FALSE 0.858 &lt;lm&gt;  0.495    0.244  0.294 \n# 2 Measurement error … FALSE TRUE  0.648 &lt;lm&gt;  1.09     0.0838 0.329 \n# 3 Measurement error … TRUE  FALSE 0.481 &lt;lm&gt;  0.844    1.22   0.0946\n# 4 Measurement error … TRUE  TRUE  0.401 &lt;lm&gt;  1.31     1.12   0.117\n\nWe plot the model \\(R = r_{xy}\\) and the estimated residual standard error in Figure 7.5 below. The lines connecting the points are approximately parallel, indicating that errors of measurement in \\(x\\) and \\(y\\) have nearly additive effects on model summaries.\n\n\np1 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = r, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model R \",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") +\n  legend_inside(c(0.27, 0.8))\n\np2 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = sigma, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model residual standard error\",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\nFigure 7.5: Model statistics for the combinations of additional error variance in x or y or both. Left: model R; right: Residual standard error.\n\n\n\n\n\n7.2.3 Coffee data: Bias and precision\nIn multiple regression the effects of measurement error in a predictor become more complex, because error variance in one predictor, \\(x_1\\), say, can affect the coefficients of other terms in the model.\nConsider the marginal relation between Heart disease and Stress in the coffee data. Figure 7.6 shows this with data ellipses in data space and the corresponding confidence ellipses in \\(\\beta\\) space. Each panel starts with the observed data (the darkest ellipse, marked \\(0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0.75, 1.0, 1.5\\}\\), to the value of Stress, while keeping the mean of Stress the same. All of the data ellipses have the same vertical shadows (\\(\\text{SD}_{\\textrm{Heart}}\\)), while the horizontal shadows increase with \\(\\delta\\), driving the slope for Stress toward 0.\nIn \\(\\beta\\) space, it can be seen that the estimated coefficients, \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) vary along a line and approach \\(\\beta_{\\textrm{Stress}}=0\\) as \\(\\delta\\) gets sufficiently large. The shadows of ellipses for \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) along the \\(\\beta_{\\textrm{Stress}}\\) axis also demonstrate the effects of measurement error on the standard error of \\(\\beta_{\\textrm{Stress}}\\).\n\n\n\n\n\n\n\nFigure 7.6: Effects of measurement error in Stress on the marginal relationship between Heart disease and Stress. Each panel starts with the observed data (\\(\\delta = 0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\text{SD}_\\text{Stress})\\) with standard deviations multiplied by \\(\\delta\\) = 0.75, 1.0, 1.5, to the value of Stress. Increasing measurement error biases the slope for Stress toward 0. Left: 50% data ellipses; right: 50% confidence ellipses.\n\n\n\n\nPerhaps less well-known, but both more surprising and interesting, is the effect that measurement error in one variable, \\(x_1\\), has on the estimate of the coefficient for an other variable, \\(x_2\\), in a multiple regression model. Figure 7.7 shows the confidence ellipses for \\((\\beta_{\\textrm{Coffee}}, \\beta_{\\textrm{Stress}})\\) in the multiple regression predicting Heart disease, adding random normal error \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0, 0.2, 0.4, 0.8\\}\\), to the value of Stress alone.\nAs can be plainly seen, while this measurement error in Stress attenuates its coefficient, it also has the effect of biasing the coefficient for Coffee toward that in the marginal regression of Heart disease on Coffee alone.\n\n\n\n\n\n\n\nFigure 7.7: Biasing effect of measurement error in one variable (Stress) on on the coefficient of another variable (Coffee) in a multiple regression. The coefficient for Coffee is driven towards its value in the marginal model using Coffee alone, as measurement error in Stress makes it less informative in the joint model.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#what-have-we-learned",
    "href": "07-lin-mod-topics.html#what-have-we-learned",
    "title": "\n7  Topics in Linear Models\n",
    "section": "\n7.3 What have we learned?",
    "text": "7.3 What have we learned?\n\nData space and \\(\\beta\\) space are dualities of each other - While we typically visualize regression models in data space (where points are observations), there’s a parallel \\(\\beta\\) space where points represent models and their coefficients. These spaces mirror each other in elegant ways: lines in one space become points in the other, and confidence ellipses in \\(\\beta\\) space are 90\\(^\\circ\\) rotations of data ellipses. This duality reveals that we gain precision in estimating coefficients precisely where our data spread the most.\nConfidence ellipses make hypothesis testing visual and intuitive - Instead of squinting at p-values in regression output, we can literally see whether hypotheses are supported by checking whether null hypothesis points fall inside or outside confidence ellipses. The shadows of these ellipses automatically give us individual confidence intervals, while the full ellipse captures joint uncertainty about multiple coefficients.\nMeasurement error in predictors is far more dangerous than measurement error in responses - While errors in your response variable (y) simply inflate standard errors without biasing coefficients, errors in predictors create attenuation bias that systematically pulls slope estimates toward zero. This “errors-in-variables” problem means that unreliable measurements of your predictors can make real effects appear weaker than they actually are.\nIn multiple regression, measurement error in one predictor contaminates estimates of other predictors - Perhaps most surprisingly, when one predictor in your model suffers from measurement error, it doesn’t just bias its own coefficient—it also distorts the coefficients of other variables in unpredictable ways. This distortion will cascade, meaning that measurement quality affects your entire model, not just individual variables.\nEllipses reveal the hidden geometry behind familiar statistical concepts - Data ellipses, confidence ellipses, and their mathematical relationships provide a geometric foundation for understanding correlation, regression coefficients, confidence intervals, and hypothesis tests. This visual approach transforms abstract statistical concepts into concrete geometric relationships that you can literally see and manipulate.\n\n\nPackages used here:\n\n\n\n\n\nDempster, A. P. (1969). Elements of continuous multivariate analysis. Addison-Wesley.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFuller, W. (2006). Measurement error models (2nd ed.). John Wiley & Sons.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#footnotes",
    "href": "07-lin-mod-topics.html#footnotes",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "I use 40% and 68% for the data ellipses and 95% for the confidence ellipse only for ease of interpretation and convenience in plotting. What is important is the relative shape and orientation of the ellipses in the two panels of Figure 7.3.↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html",
    "href": "08-collinearity-ridge.html",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "8.1 What is collinearity?\nIn univariate multiple regression models, we usually hope to have high correlations between the outcome \\(y\\) and each of the predictors, \\(\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x_2}, \\dots]\\). But high correlations among the predictors can cause problems in estimating and testing their effects. Exactly the same problems can exist in multivariate response models, because they involve only the relations among the predictor variables, so the problems and solutions discussed here apply equally to MLMs.\nThe problem of high correlations among the predictors in a model is called collinearity (or multicollinearity), referring to the situation when two or more predictors are very nearly linearly related to each other (collinear). This chapter illustrates the nature of collinearity geometrically, using data and confidence ellipsoids (Section 8.1) It describes diagnostic measures to asses these effects (Section 8.2) and presents some novel visual tools for these purposes using the VisCollin package. These include tableplots (Section 8.3) and collinearity biplots (Section 8.4).\nOne class of solutions for collinearity involves regularization methods such as ridge regression (Section 8.6). Another collection of graphical methods, generalized ridge trace plots, implemented in the genridge package, sheds further light on what is accomplished by this technique. In Section 8.9 we see that, once again, PCA-related techniques like the biplot can be insightful—here, to understand the nature of collinearity and ridge regression. More generally, the methods of this chapter are further examples of how data and confidence ellipsoids can be used to visualize bias and precision of regression estimates.\nPackages\nIn this chapter I use the following packages. Load them now.\nResearchers who have studies standard treatments of linear models (e.g, Graybill (1961); Hocking (2013)) are often less than clear about what collinearity is, how to find its sources and how to take steps to resolve them. There are a number of important diagnostic measures that can help, but these are usually presented in a tabular display like Figure 8.1, which prompted this query on an online forum:\nFigure 8.1: Collinearity diagnostics for a multiple regression model from SPSS. Source: Arndt Regorz, How to interpret a Collinearity Diagnostics table in SPSS, https://bit.ly/3YRB82b\nThe trouble with displays like Figure 8.1 is that the important information is hidden in a sea of numbers, some of which are bad when large, others bad when they are small and a large bunch which are irrelevant to interpretation.\nIn Friendly & Kwan (2009), we liken this problem to that of the reader of Martin Hansford’s successful series of books, Where’s Waldo. These consist of a series of full-page illustrations of hundreds of people and things and a few Waldos— a character wearing a red and white striped shirt and hat, glasses, and carrying a walking stick or other paraphernalia. Waldo was never disguised, yet the complex arrangement of misleading visual cues in the pictures made him very hard to find. Collinearity diagnostics often provide a similar puzzle: where should you look in traditional tabular displays?1\nFigure 8.2: A scene from one of the Where’s Waldo books. Waldo wears a red-striped shirt, but far too many of the other figures in the scene have horizontal red stripes, making it very difficult to find him among all the distractors. This is often the problem with collinearity diagnostics. Source: Modified from https://bit.ly/48KPcOo\nRecall the standard classical linear model for a response variable \\(y\\) with a collection of predictors in \\(\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_p)\\)\n\\[\n\\begin{aligned}\n\\mathbf{y}  & =  \\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\cdots + \\beta_p \\mathbf{x}_p + \\boldsymbol{\\epsilon} \\\\\n            & =  \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\; ,\n\\end{aligned}\n\\]\nfor which the ordinary least squares solution is:\n\\[\n\\widehat{\\mathbf{b}} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\; \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\; .\n\\] The sampling variances and covariances of the estimated coefficients is \\(\\text{Var} (\\widehat{\\mathbf{b}}) = \\sigma_\\epsilon^2 \\times (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1}\\) and \\(\\sigma_\\epsilon^2\\) is the variance of the residuals \\(\\boldsymbol{\\epsilon}\\), estimated by the mean squared error (MSE).\nIn the limiting case, collinearity becomes particularly problematic when one \\(x_i\\) is perfectly predictable from the other \\(x\\)s, i.e., \\(R^2 (x_i | \\text{other }x) = 1\\). This is problematic because:\nThis extreme case reflects a situation when one or more predictors are effectively redundant, for example when you include two variables \\(x\\) and \\(y\\) and their sum \\(z = x + y\\) in a model. For instance, a dataset may include variables for income, expenses, and savings. But income is the sum of expenses and savings, so not all three should be used as predictors.\nA more subtle case is the use ipsatized, defined as scores that sum to a constant, such as proportions of a total. You might have scores on tests of reading, math, spelling and geography. With ipsatized scores, any one of these is necessarily 1 \\(-\\) sum of the others, i.e., if reading is 0.5, math and geography are both 0.15, then geography must be 0.2. Once thre of the four scores are known, the last provides no new information.\nMore generally, collinearity refers to the case when there are very high multiple correlations among the predictors, such as \\(R^2 (x_i | \\text{other }x) \\ge 0.9\\). Note that you can’t tell simply by looking at the simple correlations. A large correlation \\(r_{ij}\\) is sufficient for collinearity, but not necessary—you can have variables \\(x_1, x_2, x_3\\) for which the pairwise correlation are low, but the multiple correlation is high.\nThe consequences are:",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-what-is-collin",
    "href": "08-collinearity-ridge.html#sec-what-is-collin",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "Some of my collinearity diagnostics have large values, or small values, or whatever they are not supposed to be.\n\nWhat is bad?\nIf bad, what can I do about it?\n\n\n\n\n\n\n\n\n\n\n\n\n\nthere is no unique solution for the regression coefficients \\(\\mathbf{b} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X} \\mathbf{y}\\);\nthe standard errors \\(s (b_i)\\) of the estimated coefficients are infinite and t statistics \\(t_i = b_i / s (b_i)\\) are 0.\n\n\n\n\n\n\nThe estimated coefficients have large standard errors, \\(s(\\hat{b}_j)\\). They are multiplied by the square root of the variance inflation factor, \\(\\sqrt{\\text{VIF}}\\), discussed below.\nThe large standard errors deflate the \\(t\\)-statistics, \\(t = \\hat{b}_j / s(\\hat{b}_j)\\), by the same factor, so a coefficient that would significant if the predictors were uncorrelated becomes insignificant when collinearity is present.\nThus you may find a situation where an overall model is highly significant (large \\(F\\)-statistic), while no (or few) of the individual predictors are. This is a puzzlement!\nBeyond this, the least squares solution may have poor numerical accuracy (Longley, 1967), because the solution depends inversely on the determinant \\(|\\,\\mathbf{X}^\\mathsf{T} \\mathbf{X}\\,|\\), which approaches 0 as multiple correlations increase.\nThere is an interpretive problem as well. Recall that the coefficients \\(\\hat{b}\\) are partial coefficients, meaning that they estimate change \\(\\Delta y\\) in \\(y\\) when \\(x\\) changes by one unit \\(\\Delta x\\), but holding all other variables constant. Then, the model may be trying to estimate something that does not occur in the data. (For example: predicting strength from the highly correlated height and weight)\n\n\n8.1.1 Visualizing collinearity\nCollinearity can be illustrated in data space for two predictors in terms of the stability of the regression plane for a linear model Y = X1 + X2. Figure 8.3 shows three cases as 3D plots of \\((X_1, X_2, Y)\\), where the correlation of predictors can be observed in the \\((X_1, X_2)\\) plane.\n\nshows a case where \\(X_1\\) and \\(X_2\\) are uncorrelated as can be seen in their scatter in the horizontal plane (+ symbols). The gray regression plane is well-supported; a small change in Y for one observation won’t make much difference.\nIn panel (b), \\(X_1\\) and \\(X_2\\) have a perfect correlation, \\(r (x_1, x_2) = 1.0\\). The regression plane is not unique; in fact there are an infinite number of planes that fit the data equally well. Note that, if all we care about is prediction (not the coefficients), we could use \\(X_1\\) or \\(X_2\\), or both, or any weighted sum of them in a model and get the same predicted values.\nShows a typical case where there is a strong correlation between \\(X_1\\) and \\(X_2\\). The regression plane here is unique, but is not well determined. A small change in Y can make quite a difference in the fitted value or coefficients, depending on the values of \\(X_1\\) and \\(X_2\\). Where \\(X_1\\) and \\(X_2\\) are far from their near linear relation in the botom plane, you can imagine that it is easy to tilt the plane substantially by a small change in \\(Y\\).\n\n\n\n\n\n\n\n\nFigure 8.3: Effect of collinearity on the least squares regression plane. (a) Small correlation between predictors; (b) Perfect correlation ; (c) Very strong correlation. The black points show the data Y values, white points are the fitted values in the regression plane, and + signs represent the values of X1 and X2. Source: Adapted from Fox (2016), Fig. 13.2\n\n\n\n\n\n8.1.2 Data space and \\(\\beta\\) space\nIt is also useful to visualize collinearity by comparing the representation in data space with the analogous view of the confidence ellipses for coefficients in beta space. To do so in this example, I generate data from a known model \\(y = 3 x_1 + 3 x_2 + \\epsilon\\) with \\(\\epsilon \\sim \\mathcal{N} (0, 100)\\) and various true correlations between \\(x_1\\) and \\(x_2\\), \\(\\rho_{12} = (0, 0.8, 0.97)\\) 2.\n\n\nFirst, I use MASS:mvrnorm() to construct a list of three data frames XY with the same means and standard deviations, but with different correlations. In each case, the variable \\(y\\) is generated with true coefficients beta \\(=(3, 3)\\), and the fitted model for that value of rho is added to a corresponding list of models, mods.\n\nCodelibrary(MASS)\nlibrary(car)\n\nset.seed(421)            # reproducibility\nN &lt;- 200                 # sample size\nmu &lt;- c(0, 0)            # means\ns &lt;- c(1, 1)             # standard deviations\nrho &lt;- c(0, 0.8, 0.97)   # correlations\nbeta &lt;- c(3, 3)          # true coefficients\n\n# Specify a covariance matrix, with standard deviations\n#   s[1], s[2] and correlation r\nCov &lt;- function(s, r){\n  matrix(c(s[1],        r * s[1]*s[2],\n         r * s[1]*s[2], s[2]), nrow = 2, ncol = 2)\n}\n\n# Generate a dataframe of X, y for each rho\n# Fit the model for each\nXY &lt;- vector(mode =\"list\", length = length(rho))\nmods &lt;- vector(mode =\"list\", length = length(rho))\nfor (i in seq_along(rho)) {\n  r &lt;- rho[i]\n  X &lt;- mvrnorm(N, mu, Sigma = Cov(s, r))\n  colnames(X) &lt;- c(\"x1\", \"x2\")\n  y &lt;- beta[1] * X[,1] + beta[2] * X[,2] + rnorm(N, 0, 10)\n\n  XY[[i]] &lt;- data.frame(X, y=y)\n  mods[[i]] &lt;- lm(y ~ x1 + x2, data=XY[[i]])\n}\n\n\nThe estimated coefficients can then be extracted using coef() applied to each model:\n\ncoefs &lt;- sapply(mods, coef)\ncolnames(coefs) &lt;- paste0(\"mod\", 1:3, \" (rho=\", rho, \")\")\ncoefs\n#             mod1 (rho=0) mod2 (rho=0.8) mod3 (rho=0.97)\n# (Intercept)         1.01        -0.0535           0.141\n# x1                  3.18         3.4719           3.053\n# x2                  1.68         2.9734           2.059\n\nThen, I define a function to plot the data ellipse (car::dataEllipse()) for each data frame and confidence ellipse (car::confidenceEllipse()) for the coefficients in the corresponding fitted model. In the plots in Figure 8.4, I specify the x, y limits for each plot so that the relative sizes of these ellipses are comparable, so that variance inflation can be assessed visually.\n\nCodedo_plots &lt;- function(XY, mod, r) {\n  X &lt;- as.matrix(XY[, 1:2])\n  dataEllipse(X,\n    levels= 0.95,\n    col = \"darkgreen\",\n    fill = TRUE, fill.alpha = 0.05,\n    xlim = c(-3, 3),\n    ylim = c(-3, 3), asp = 1)\n  text(0, 3, bquote(rho == .(r)), cex = 2, pos = NULL)\n\n  confidenceEllipse(mod,\n    col = \"red\",\n    fill = TRUE, fill.alpha = 0.1,\n    xlab = expression(paste(\"x1 coefficient, \", beta[1])),\n    ylab = expression(paste(\"x2 coefficient, \", beta[2])),\n    xlim = c(-5, 10),\n    ylim = c(-5, 10),\n    asp = 1)\n  points(beta[1], beta[2], pch = \"+\", cex=2)\n  abline(v=0, h=0, lwd=2)\n}\n\nop &lt;- par(mar = c(4,4,1,1)+0.1,\n          mfcol = c(2, 3),\n          cex.lab = 1.5)\nfor (i in seq_along(rho)) {\n  do_plots(XY[[i]], mods[[i]], rho[i])\n}\npar(op)\n\n\n\n\n\n\nFigure 8.4: 95% Data ellipses for x1, x2 and the corresponding 95% confidence ellipses for their coefficients in the model predicting y. In the confidence ellipse plots, reference lines show the value (0,0) for the null hypothesis and “+” marks the true values for the coefficients. This figure adapts an example by John Fox (2022).\n\n\n\n\nRecall (Section 7.1) that the confidence ellipse for \\((\\beta_1, \\beta_2)\\) is just a 90 degree rotation (and rescaling) of the data ellipse for \\((x_1, x_2)\\): it is wide (more variance) in any direction where the data ellipse is narrow.\nThe shadows of the confidence ellipses on the coordinate axes in Figure 8.4 represent the standard errors of the coefficients, and get larger with increasing \\(\\rho\\). This is the effect of variance inflation, described in the following section.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-measure-collin",
    "href": "08-collinearity-ridge.html#sec-measure-collin",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.2 Measuring collinearity",
    "text": "8.2 Measuring collinearity\nThis section first describes the variance inflation factor (VIF) used to measure the effect of possible collinearity on each predictor and a collection of diagnostic measures designed to help interpret these. Then I describe some novel graphical methods to make these effects more readily understandable, to answer the “Where’s Waldo” question posed at the outset.\n\n8.2.1 Variance inflation factors\nHow can we measure the effect of collinearity? The essential idea is to compare, for each predictor the variance \\(s^2 (\\widehat{b_j})\\) that the coefficient that \\(x_j\\) would have if it was totally unrelated to the other predictors to the actual variance it has in the given model.\nFor two predictors such as shown in Figure 8.4 the sampling variance of \\(x_1\\) can be expressed as\n\\[\ns^2 (\\widehat{b_1}) = \\frac{MSE}{(n-1) \\; s^2(x_1)} \\; \\times \\; \\left[ \\frac{1}{1-r^2_{12}} \\right]\n\\] The first term here is the variance of \\(b_1\\) when the two predictors are uncorrelated. The term in brackets represents the variance inflation factor (Marquardt, 1970), the amount by which the variance of the coefficient is multiplied as a consequence of the correlation \\(r_{12}\\) of the predictors. As \\(r_{12} \\rightarrow 1\\), the variances approaches infinity.\nMore generally, with any number of predictors, this relation has a similar form, replacing the simple correlation \\(r_{12}\\) with the multiple correlation predicting \\(x_j\\) from all others,\n\\[\ns^2 (\\widehat{b_j}) = \\frac{MSE}{(n-1) \\; s^2(x_j)} \\; \\times \\; \\left[ \\frac{1}{1-R^2_{j | \\text{others}}} \\right]\n\\] So, we have that the variance inflation factors are:\n\\[\n\\text{VIF}_j = \\frac{1}{1-R^2_{j \\,|\\, \\text{others}}}\n\\] In practice, it is often easier to think in terms of the square root, \\(\\sqrt{\\text{VIF}_j}\\) as the multiplier of the standard errors. The denominator, \\(1-R^2_{j | \\text{others}}\\) is sometimes called tolerance, a term I don’t find particularly useful, but it is just the proportion of the variance of \\(x_j\\) that is not explainable from the others.3\nFor the cases shown in Figure 8.4 the VIFs and their square roots are:\n\nvifs &lt;- sapply(mods, car::vif)\ncolnames(vifs) &lt;- paste(\"rho:\", rho)\nvifs\n#    rho: 0 rho: 0.8 rho: 0.97\n# x1      1     3.09      18.6\n# x2      1     3.09      18.6\n\nsqrt(vifs)\n#    rho: 0 rho: 0.8 rho: 0.97\n# x1      1     1.76      4.31\n# x2      1     1.76      4.31\n\nGeneralized VIF\nNote that when there are terms in the model with more than one degree of freedom, such as education with four levels (and hence 3 df) or a polynomial term specified as poly(age, 3), that variable, education or age is represented by three separate \\(x\\)s in the model matrix, and the standard VIF calculation gives results that vary with how those terms are coded in the model.\nTo allow for these cases, Fox & Monette (1992) define generalized, GVIFs as the inflation in the squared area of the confidence ellipse for the coefficients of such terms, relative to what would be obtained with uncorrelated data. Visually, this can be seen by comparing the areas of the ellipses in the bottom row of Figure 8.4. Because the magnitude of the GVIF increases with the number of degrees of freedom for the set of parameters, Fox & Monette suggest the analog \\(\\sqrt{\\text{GVIF}^{1/2 \\text{df}}}\\) as the measure of impact on standard errors. This is what car::vif() calculates for a factor or other term with more than 1 df.\n\nExample 8.1 Cars data\n\nThis example uses the cars dataset in the VisCollin package which contains various measures of size and performance on 406 models of automobiles from 1982. Interest is focused on predicting gas mileage, mpg.\n\ndata(cars, package = \"VisCollin\")\nstr(cars)\n# 'data.frame': 406 obs. of  10 variables:\n#  $ make    : Factor w/ 30 levels \"amc\",\"audi\",\"bmw\",..: 6 4 22 1 12 12 6 22 23 1 ...\n#  $ model   : chr  \"chevelle\" \"skylark\" \"satellite\" \"rebel\" ...\n#  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...\n#  $ cylinder: int  8 8 8 8 8 8 8 8 8 8 ...\n#  $ engine  : num  307 350 318 304 302 429 454 440 455 390 ...\n#  $ horse   : int  130 165 150 150 140 198 220 215 225 190 ...\n#  $ weight  : int  3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ...\n#  $ accel   : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n#  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...\n#  $ origin  : Factor w/ 3 levels \"Amer\",\"Eur\",\"Japan\": 1 1 1 1 1 1 1 1 1 1 ...\n\nWe fit a model predicting gas mileage (mpg) from the number of cylinders, engine displacement, horsepower, weight, time to accelerate from 0 – 60 mph and model year (1970–1982). Perhaps surprisingly, only weight and year appear to significantly predict gas mileage. What’s going on here?\n\ncars.mod &lt;- lm (mpg ~ cylinder + engine + horse + \n                      weight + accel + year, \n                data=cars)\nAnova(cars.mod)\n# Anova Table (Type II tests)\n# \n# Response: mpg\n#           Sum Sq  Df F value Pr(&gt;F)    \n# cylinder      12   1    0.99   0.32    \n# engine        13   1    1.09   0.30    \n# horse          0   1    0.00   0.98    \n# weight      1214   1  102.84 &lt;2e-16 ***\n# accel          8   1    0.70   0.40    \n# year        2419   1  204.99 &lt;2e-16 ***\n# Residuals   4543 385                   \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe check the variance inflation factors, using car::vif(). We see that most predictors have very high VIFs, indicating moderately severe multicollinearity.\n\nvif(cars.mod)\n# cylinder   engine    horse   weight    accel     year \n#    10.63    19.64     9.40    10.73     2.63     1.24\n\nsqrt(vif(cars.mod))\n# cylinder   engine    horse   weight    accel     year \n#     3.26     4.43     3.07     3.28     1.62     1.12\n\nAccording to \\(\\sqrt{\\text{VIF}}\\), the standard error of cylinder has been multiplied by \\(\\sqrt{10.63} = 3.26\\) and it’s \\(t\\)-value is divided by this number, compared with the case when all predictors are uncorrelated. engine, horse and weight suffer a similar fate.\nIf we also included the factor origin in the models, we would get the generalized GVIF:\n\ncars.mod2 &lt;- lm (mpg ~ cylinder + engine + horse + \n                       weight + accel + year + origin, \n                 data=cars)\nvif(cars.mod2)\n#           GVIF Df GVIF^(1/(2*Df))\n# cylinder 10.74  1            3.28\n# engine   22.94  1            4.79\n# horse     9.96  1            3.16\n# weight   11.07  1            3.33\n# accel     2.63  1            1.62\n# year      1.30  1            1.14\n# origin    2.10  2            1.20\n\n\n\n\n\n\n\n\nConnection with inverse of correlation matrix\n\n\n\nIn the linear regression model with standardized predictors, the covariance matrix of the estimated intercept-excluded parameter vector \\(\\mathbf{b}^\\star\\) has the simpler form, \\[\n\\mathcal{V} (\\mathbf{b}^\\star) = \\frac{\\sigma^2}{n-1} \\mathbf{R}^{-1}_{X} \\; .\n\\] where \\(\\mathbf{R}_{X}\\) is the correlation matrix among the predictors. It can then be seen that the VIF\\(_j\\) are just the diagonal entries of \\(\\mathbf{R}^{-1}_{X}\\).\nMore generally, the matrix \\(\\mathbf{R}^{-1}_{X} = (r^{ij})\\), when standardized to a correlation matrix as \\(-r^{ij} / \\sqrt{r^{ii} \\; r^{jj}}\\) gives the matrix of all partial correlations, \\(r_{ij} \\,|\\, \\text{others}\\).\nThis inverse connection is analogous to the dual relationship (Section 8.1.2) between ellipses in data space, based on \\(\\mathbf{X}^\\top \\mathbf{X}\\) and in \\(\\beta\\)-space, based on \\((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\).\n\n\n\n8.2.2 VIF displays\nBeyond the console output from car::vif(), the easystats suite of packages has some useful functions for displaying VIFs in helpful tables and plots. performance::check_collinearity() calculates VIFs and their standard errors and returns a \"check_collinearity\" data frame. The plot method for this uses a log scale for VIF because it is multiples of variance that matter here. It uses intervals of 1-5, 5-10, 10+ to highlight low, medium and high variance inflation with colored backgrounds.\n\ncars.collin &lt;- check_collinearity(cars.mod)\n\nplot(cars.collin, \n     linewidth = 1.1,\n     size_point = 5, size_title = 16, base_size = 14)\n\n\n\n\n\n\nFigure 8.5: Variance inflation plot. VIF is plotted on a log scale. Colored bands show regions of low, medium and high variance inflation.\n\n\n\n\nThe graphic properties here help to make the problematic variables more apparent than in a table of numbers, though the underlying message is the same. Number of cylinders, engine displacement and weight are the collinearity bad boys.\nKnowing this helps to pin Waldo down a bit, but to really find him, we need a few more diagnostics and better graphical methods.\n\n8.2.3 Collinearity diagnostics\nOK, we now know that large VIF\\(_j\\) indicate predictor coefficients whose estimation is degraded due to large \\(R^2_{j \\,|\\, \\text{others}}\\). But for this to be useful, we need to determine:\n\nhow many dimensions in the space of the predictors are associated with nearly collinear relations?\nwhich predictors are most strongly implicated in each of these?\n\nAnswers to these questions are provided using measures developed by Belsley and colleagues (Belsley et al., 1980; Belsley, 1991). These measures are based on the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots \\lambda_p\\) of the correlation matrix \\(R_{X}\\) of the predictors (preferably centered and scaled, and not including the constant term for the intercept), and the corresponding eigenvectors in the columns of \\(\\mathbf{V}_{p \\times p}\\), given by the the eigen decomposition\n\\[\n\\mathbf{R}_{X} = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^\\mathsf{T} \\; .\n\\]\nBy elementary matrix algebra, the eigen decomposition of \\(\\mathbf{R}_{XX}^{-1}\\) is then\n\\[\n\\mathbf{R}_{X}^{-1} = \\mathbf{V} \\boldsymbol{\\Lambda}^{-1} \\mathbf{V}^\\mathsf{T} \\; ,\n\\tag{8.1}\\]\nso, \\(\\mathbf{R}_{X}\\) and \\(\\mathbf{R}_{XX}^{-1}\\) have the same eigenvectors, and the eigenvalues of \\(\\mathbf{R}_{X}^{-1}\\) are just \\(\\lambda_i^{-1}\\). Using Equation 8.1, the variance inflation factors may be expressed as\n\\[\n\\text{VIF}_j = \\sum_{k=1}^p \\frac{V^2_{jk}}{\\lambda_k} \\; .\n\\tag{8.2}\\]\nThis shows that (a) only the small eigenvalues contribute to variance inflation, but (b) only for those predictors that have large eigenvector coefficients \\(V_{jk}\\) on those small components.\nThese facts lead to the following diagnostic statistics for collinearity:\n\n\nCondition indices (\\(\\kappa\\)): The smallest of the eigenvalues, those for which \\(\\lambda_j \\approx 0\\), indicate collinearity and the number of small values indicates the number of near collinear relations. Because the sum of the eigenvalues, \\(\\Sigma \\lambda_i = p\\) increases with the number of predictors \\(p\\), it is useful to scale them all inversely in relation to the largest so that larger numbers are worse. This leads to condition indices, defined as \\(\\kappa_j = \\sqrt{ \\lambda_1 / \\lambda_j}\\). These have the property that the resulting numbers have common interpretations regardless of the number of predictors.\n\nFor completely uncorrelated predictors, all \\(\\kappa_j = 1\\).\nAs any \\(\\lambda_k \\rightarrow 0\\) the corresponding \\(\\kappa_j \\rightarrow \\infty\\).\nAs a rule of thumb, Belsley (1991) suggests that values \\(\\kappa_j &gt; 10\\) reflect a moderate problem, while \\(\\kappa_j &gt; 30\\) indicates severe collinearity. Even worse values use bounds of 100, 300, … as collinearity becomes more extreme.\n\n\nVariance decomposition proportions: Large VIFs indicate variables that are involved in some nearly collinear relations, but they don’t indicate which other variable(s) each is involved with. For this purpose, Belsley et al. (1980) and Belsley (1991) proposed calculation of the proportions of variance of each variable associated with each principal component as a decomposition of the coefficient variance for each dimension. These are simply the the terms in Equation 8.2 divided by their sum.\n\nThese measures can be calculated using VisCollin::colldiag(). For the current model, the usual display contains both the condition indices and variance proportions. However, even for a small example, it is often difficult to know what numbers to pay attention to.\n\n(cd &lt;- colldiag(cars.mod, center=TRUE))\n# Condition\n# Index   -- Variance Decomposition Proportions --\n#           cylinder engine horse weight accel year \n# 1   1.000 0.005    0.003  0.005 0.004  0.009 0.010\n# 2   2.252 0.004    0.002  0.000 0.007  0.022 0.787\n# 3   2.515 0.004    0.001  0.002 0.010  0.423 0.142\n# 4   5.660 0.309    0.014  0.306 0.087  0.063 0.005\n# 5   8.342 0.115    0.000  0.654 0.715  0.469 0.052\n# 6  10.818 0.563    0.981  0.032 0.176  0.013 0.004\n\nBelsley (1991) recommends that the sources of collinearity be diagnosed (a) only for those components with large \\(\\kappa_j\\), and (b) for those components for which the variance proportion is large (say, \\(\\ge 0.5\\)) on two or more predictors. The print method for \"colldiag\" objects has a fuzz argument controlling this. The descending = TRUE argument puts the rows with the largest condition indices at the top.\n\nprint(cd, fuzz = 0.5, descending = TRUE)\n# Condition\n# Index   -- Variance Decomposition Proportions --\n#           cylinder engine horse weight accel year \n# 6  10.818 0.563    0.981   .     .      .     .   \n# 5   8.342  .        .     0.654 0.715   .     .   \n# 4   5.660  .        .      .     .      .     .   \n# 3   2.515  .        .      .     .      .     .   \n# 2   2.252  .        .      .     .      .    0.787\n# 1   1.000  .        .      .     .      .     .\n\nThe Waldo mystery is nearly solved, if you can read that table with these recommendations in mind. There are two nearly collinear relations among the predictors, corresponding to the two smallest dimensions with largest condition indices.\n\nDimension 6 reflects the high correlation between number of cylinders and engine displacement.\nDimension 5 reflects the high correlation between horsepower and weight,\n\nNote that the high variance proportion for year (0.787) on the second component creates no problem and should be ignored because (a) the condition index is low and (b) it shares nothing with other predictors.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-tableplot",
    "href": "08-collinearity-ridge.html#sec-tableplot",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.3 Tableplots",
    "text": "8.3 Tableplots\nThe default tabular display of condition indices and variance proportions from colldiag() is what triggered the comparison to “Where’s Waldo”. It suffers from the fact that the important information — (a) how many Waldos? (b) where are they hiding — is disguised by being embedded in a sea of mostly irrelevant numbers, just as Waldo is hiding in Figure 8.2 in a field of stripy things. The simple option of using a principled fuzz factor helps considerably, but not entirely.\nThe simplified tabular display above can be improved to make the patterns of collinearity more visually apparent and to signify warnings directly to the eyes. A tableplot (Kwan et al., 2009) is a semi-graphic display that presents numerical information in a table using shapes proportional to the value in a cell and other visual attributes (shape type, color fill, and so forth) to encode other information.\nFor collinearity diagnostics, these show:\n\nthe condition indices, using squares whose background color is red for condition indices &gt; 10, brown for values &gt; 5 and green otherwise, reflecting danger, warning and OK respectively. The value of the condition index is encoded within this using a white square whose side is proportional to the value (up to some maximum value, cond.max that fills the cell).\nVariance decomposition proportions are shown by filled circles whose radius is proportional to those values and are filled (by default) with shades ranging from white through pink to red. Rounded values of those diagnostics are printed in the cells.\n\nThe tableplot below (Figure 8.6) encodes all the information from the values of colldiag() printed above. To aid perception, it uses prop.col color breaks such that variance proportions &lt; 0.3 are shaded white. The visual message is that one should attend to collinearities with large condition indices and large variance proportions implicating two or more predictors.\n\n\n\n\n\ntableplot(cd, title = \"Tableplot of cars data\", \n          cond.max = 30 )\n\n\n\n\n\n\nFigure 8.6: Tableplot of condition indices and variance proportions for the cars data. In column 1, the square symbols are scaled relative to a maximum condition index of 30. In the remaining columns, variance proportions (times 100) are shown as circles scaled relative to a maximum of 100.\n\n\n\n\nThe information in Figure 8.6 is essentially the same as the fuzzed version of the printed output from colldiag() shown above; however the graphic encoding of the tableplot makes the pattern of the numbers and their",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-collin-biplots",
    "href": "08-collinearity-ridge.html#sec-collin-biplots",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.4 Collinearity biplots",
    "text": "8.4 Collinearity biplots\nAs we have just seen, the collinearity diagnostics are all functions of the eigenvalues and eigenvectors of the correlation matrix of the predictors in the regression model, or alternatively, the SVD of the \\(\\mathbf{X}\\) matrix in the linear model (excluding the constant). We can use our trusty multivariate juicer the biplot (Section 4.3) to see where the problems lie in a space that relates to observations and variables together.\nA standard biplot (Gabriel, 1971; Gower & Hand, 1996) showing the 2 (or 3) largest dimensions in the data is what we usually want to use. By projecting multivariate data into a low-D space, we can see the main variation in the data an how this related to the variables.\nHowever the standard biplot of the largest dimensions is less useful for visualizing the relations among the predictors that lead to nearly collinear relations. Instead, biplots of the smallest dimensions show these relations directly, and can show other features of the data as well, such as outliers and leverage points. I use prcomp(X, scale.=TRUE) to obtain the PCA of the correlation matrix of the predictors in the cars dataset:\n\ncars.X &lt;- cars |&gt;\n  select(where(is.numeric)) |&gt;\n  select(-mpg) |&gt;\n  tidyr::drop_na()\ncars.pca &lt;- prcomp(cars.X, scale. = TRUE)\ncars.pca\n# Standard deviations (1, .., p=6):\n# [1] 2.070 0.911 0.809 0.367 0.245 0.189\n# \n# Rotation (n x k) = (6 x 6):\n#             PC1     PC2    PC3    PC4     PC5     PC6\n# cylinder -0.454 -0.1869  0.168 -0.659 -0.2711 -0.4725\n# engine   -0.467 -0.1628  0.134 -0.193 -0.0109  0.8364\n# horse    -0.462 -0.0177 -0.123  0.620 -0.6123 -0.1067\n# weight   -0.444 -0.2598  0.278  0.350  0.6860 -0.2539\n# accel     0.330 -0.2098  0.865  0.143 -0.2774  0.0337\n# year      0.237 -0.9092 -0.335  0.025 -0.0624  0.0142\n\nThe standard deviations above are the square roots \\(\\sqrt{\\lambda_j}\\) of the eigenvalues of the correlation matrix; these are returned in the sdev component of the \"prcomp\" object. The eigenvectors are returned in the rotation component. Their orientations are arbitrary and can be reversed for ease of interpretation. Because we are interested in seeing the relative magnitude of variable vectors, we are also free to multiply them all by any constant to make them to zoom in or out, making them more visible in relation to the scores for the cars.\nI use factoextra::fviz_pca_biplot() for the biplot in Figure 8.7 because I want to illustrate identification of noteworthy points with geom_text_repel().\n\ncars.pca$rotation &lt;- -2.5 * cars.pca$rotation    # reflect & scale var vectors\n\nggp &lt;- fviz_pca_biplot(\n  cars.pca,\n  axes = 6:5,\n  geom = \"point\",\n  col.var = \"blue\",\n  labelsize = 5,\n  pointsize = 1.5,\n  arrowsize = 1.5,\n  addEllipses = TRUE,\n  ggtheme = ggplot2::theme_bw(base_size = 14),\n  title = \"Collinearity biplot for cars data\")\n\n# add point labels for outlying points\ndsq &lt;- heplots::Mahalanobis(cars.pca$x[, 6:5])\nscores &lt;- as.data.frame(cars.pca$x[, 6:5])\nscores$name &lt;- rownames(scores)\n\nggp + geom_text_repel(data = scores[dsq &gt; qchisq(0.95, df = 6),],\n                aes(x = PC6,\n                    y = PC5,\n                    label = name),\n                vjust = -0.5,\n                size = 5)\n\n\n\n\n\n\nFigure 8.7: Collinearity biplot of the Cars data, showing the last two dimensions. The projections of the variable vectors on the coordinate axes are proportional to their variance proportions. To reduce graphic clutter, only the most outlying observations in predictor space are identified by case labels. An extreme outlier (case 20) appears in the lower right corner.\n\n\n\n\nAs with the tabular display of variance proportions, Waldo is hiding in the dimensions associated with the smallest eigenvalues (largest condition indices). As well, it turns out that outliers in the predictor space (also high leverage observations) can often be seen as observations far from the centroid in the space of the smallest principal components.\nThe projections of the variable vectors in Figure 8.7 on the Dimension 5 and Dimension 6 axes are proportional to their variance proportions shown above. The relative lengths of these variable vectors can be considered to indicate the extent to which each variable contributes to collinearity for these two near-singular dimensions.\nThus, we see again that Dimension 6 is largely determined by engine size, with a substantial (negative) relation to cylinder. Dimension 5 has its strongest relations to weight and horse.\nMoreover, there is one observation, #20, that stands out as an outlier in predictor space, far from the centroid. It turns out that this vehicle, a Buick Estate wagon, is an early-year (1970) American behemoth. It had an 8-cylinder, 455 cu. in, 225 horse-power engine, and able to go from 0 to 60 mph in 10 sec. (Its MPG is only slightly under-predicted from the regression model, however.) \nWith PCA and the biplot, we are used to looking at the dimensions that account for the most variation, but the answer to Where’s Waldo? is that he is hiding in the smallest data dimensions, just as he does in Figure 8.2 where the weak signals of his stripped shirt, hat and glasses are embedded in a visual field of noise. As we just saw, outliers hide there also, hoping to escape detection. These small dimensions are also implicated in ridge regression as we will see shortly (Section 8.6).",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-remedies",
    "href": "08-collinearity-ridge.html#sec-remedies",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.5 Remedies for collinearity: What can I do?",
    "text": "8.5 Remedies for collinearity: What can I do?\nCollinearity is often a data problem, for which there is no magic cure. Nevertheless there are some general guidelines and useful techniques to address this problem.\n\nPure prediction: If we are only interested in predicting / explaining an outcome, and not the model coefficients or which are “significant”, collinearity can be largely ignored. The fitted values are unaffected by collinearity, even in the case of perfect collinearity as shown in Figure 8.3 (b).\n\nStructural collinearity: Sometimes collinearity results from structural relations among the variables that relate to how they have been defined.\n\nFor example, polynomial terms, like \\(x, x^2, x^3\\) or interaction terms like \\(x_1, x_2, x_1 * x_2\\) are necessarily correlated. A simple cure is to center the predictors at their means, using \\(x - \\bar{x}, (x - \\bar{x})^2, (x - \\bar{x})^3\\) or \\((x_1 - \\bar{x}_1), (x_2 - \\bar{x}_2), (x_1 - \\bar{x}_1) * (x_2 - \\bar{x}_2)\\). Centering removes the spurious ill-conditioning, thus reducing the VIFs. Note that in polynomial models, using y ~ poly(x, 3) to specify a cubic model generates orthogonal (uncorrelated) regressors, whereas in y ~ x + I(x^2) + I(x^3) the terms have built-in correlations.\nWhen some predictors share a common cause, as in GNP or population in time-series or cross-national data, you can reduce collinearity by re-defining predictors to reflect per capita measures. In a related example with sports data, when you have cumulative totals (e.g., runs, hits, homeruns in baseball) for players over years, expressing these measures as per year will reduce the common effect of longevity on these measures.\n\n\n\nModel re-specification:\n\nDrop one or more regressors that have a high VIF, if they are not deemed to be essential to understanding the model. Care must be taken here to not omit variables which should be controlled or accounted for in interpretation.\nReplace highly correlated regressors with less correlated linear combination(s) of them. For example, two related variables, \\(x_1\\) and \\(x_2\\) can be replaced without any loss of information by replacing them with their sum and difference, \\(z_1 = x_1 + x_2\\) and \\(z_2 = x_1 - x_2\\). For instance, in a dataset on fitness, we may have correlated predictors of resting pulse rate and pulse rate while running. Transforming these to average pulse rate and their difference gives new variables which are interpretable and less correlated.\n\n\n\nStatistical remedies:\n\nTransform the predictors \\(\\mathbf{X}\\) to uncorrelated principal component scores \\(\\mathbf{Z} = \\mathbf{X} \\mathbf{V}\\), and regress \\(\\mathbf{y}\\) on \\(\\mathbf{Z}\\). These will have the identical overall model fit without loss of information. A related technique is incomplete principal components regression, where some of the smallest dimensions (those causing collinearity) are omitted from the model. The trade-off is that it may be more difficult to interpret what the model means, but this can be countered with a biplot, showing the projections of the original variables into the reduced space of the principal components.\nUse regularization methods such as ridge regression and lasso, which correct for collinearity by introducing shrinking coefficients towards 0, but inducing a small amount of bias. I illustrate ridge regression below (Section 8.6) using the genridge package for visualization methods.\nUse Bayesian regression. If multicollinearity prevents a regression coefficient from being estimated precisely, Bayesian regression (e.g., Pesaran & Smith (2019)) can reduce collinearity by imposing shrinkage priors; these incorporate prior information to regularize the model, making it less sensitive to correlated predictors and reducing its posterior varioance.\n\n\n\n\nExample 8.2 Centering \n\nTo illustrate the effect of centering a predictor in a polynomial model, I generate data with a perfect quadratic relationship, \\(y = x^2\\) and consider the correlations of \\(y\\) with \\(x\\) and with \\((x - \\bar{x})^2\\). The correlation of \\(y\\) with \\(x\\) is 0.97, while the correlation of \\(y\\) with \\((x - \\bar{x})^2\\) is zero.\n\nx &lt;- 1:20\ny1 &lt;- x^2\ny2 &lt;- (x - mean(x))^2\nXY &lt;- data.frame(x, y1, y2)\n\n(R &lt;- cor(XY))\n#        x    y1    y2\n# x  1.000 0.971 0.000\n# y1 0.971 1.000 0.238\n# y2 0.000 0.238 1.000\n\nThe effect of centering here is remove the linear association in what is a purely quadratic relationship. This can be seen in Figure 8.8 by plotting y1 and y2 against x.\n\nr1 &lt;- R[1, 2]\nr2 &lt;- R[1, 3]\n\ngg1 &lt;-\nggplot(XY, aes(x = x, y = y1)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y~x, \n              linewidth = 2, se = FALSE) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_bw(base_size = 16) +\n  annotate(\"text\", x = 5, y = 350, size = 6,\n           label = paste(\"X Uncentered\\nr =\", round(r1, 3)))\n\ngg2 &lt;-\n  ggplot(XY, aes(x = x, y = y2)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y~x, \n              linewidth = 2, se = FALSE) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_bw(base_size = 16) +\n  annotate(\"text\", x = 5, y = 80, size = 6,\n           label = paste(\"X Centered\\nr =\", round(r2, 3)))\n\ngg1 + gg2         # show plots side-by-side\n\n\n\n\n\n\nFigure 8.8: Centering a predictor removes the nessessary correlation in a quadratic regression. Left: linear relatioship fitting \\(y\\) to the uncentered \\(x\\). Right: fitting to the centered \\((x - \\bar{x}\\).\n\n\n\n\nInterpretation\nCentering of predictors has an added benefit: the fitted coefficients are easier to interpret*, particularly the intercept in this example. In the left panel of Figure 8.8, the fitted blue line has an intercept of -77 and slope of 21. But x = 0 is outside the range of the data and it is hard to understand what -77 means.4\n\nlm(y1 ~ x, data = XY) |&gt; coef()\n# (Intercept)           x \n#         -77          21\n\nContrast this with the coefficients for the model using the centered x and shown by the blue line in the right panel. The slope of the line is clearly zero and the intercept 33.25 is the average value of y.\n\nlm(y2 ~ x, data = XY) |&gt; coef() |&gt; zapsmall()\n# (Intercept)           x \n#        33.3         0.0\n\nThis ease of interpretation is more pronounced in polynomial models. For example, in the quadratic model \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) with \\(x\\) uncentered, the slope coefficient \\(\\beta_1\\) gives the slope of the curve at the value \\(x = 0\\), which may be well-outside the range of data. With a centered predictor \\(x^\\star = x - \\bar{x}\\), the analogous coefficient \\(\\beta_1^\\star\\) in the model \\(y = \\beta_0^\\star + \\beta_1^\\star x^\\star + \\beta_2^\\star (x^\\star)^2\\) is gives the slope at the mean value of \\(x\\).\n\n\n\nExample 8.3 Interactions and response surface models\nCentering of numeric predictors becomes even more important in polynomial models that also include interaction effects, such as response surface models this include all quadratic terms of the predictors and all possible pairwise interactions. The simple notation in an R formula5 is y ~ (x1 + x2 + ...)^2. Spelled out for two predictors with uncentered x1 and x2, this would be:\ny ~ x1 + x2 + I(x1^2) + I(x2^2)+ x1:x2\nHere, the product term is necessarily correlated with each of the predictors involved. This gives more opportunities for collinearity and more places for Waldo to hide.\nTo illustrate, I use the the genridge::Acetylene dataset, which gives results from a manufacturing experiment to study the yield of acetylene in relation to reactor temperature (temp), the ratio of two components and the contact time in the reactor. A naive response surface model might suggest that yield is quadratic in time and there are potential interactions among all pairs of predictors. Without centering time the quadratic effect is fit as a term I(time^2), which simply squares the value of time.6\n\n\ndata(Acetylene, package = \"genridge\")\nacetyl.mod0 &lt;- lm(\n  yield ~ temp + ratio + time + I(time^2) + \n          temp:time + temp:ratio + time:ratio,\n  data=Acetylene)\n\n(acetyl.vif0 &lt;- vif(acetyl.mod0))\n#       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#        383      10555      18080        564       9719       9693 \n# ratio:time \n#        225\n\nThese results are horrible! How much does centering help? I first center all three predictors and then use update() to re-fit the same model using the centered data.\n\nAcetylene.centered &lt;-\n  Acetylene |&gt;\n  mutate(temp = temp - mean(temp),\n         time = time - mean(time),\n         ratio = ratio - mean(ratio))\n\nacetyl.mod1 &lt;- update(acetyl.mod0, \n                      data=Acetylene.centered)\n\n(acetyl.vif1 &lt;- vif(acetyl.mod1))\n#       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#      57.09       1.09      81.57      51.49      44.67      30.69 \n# ratio:time \n#      33.33\n\nThis is far better, although still not great in terms of VIF. But, how much have we improved the situation by the simple act of centering the predictors? The square roots of the ratios of VIFs tell us the impact of centering on the standard errors.\n\nsqrt(acetyl.vif0 / acetyl.vif1)\n#       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#       2.59      98.24      14.89       3.31      14.75      17.77 \n# ratio:time \n#       2.60\n\nFinally, I use poly(time, 2) in the model for the centered data. Because this polynomial term has 2 degree of freedom, car::vif() calculates GVIFs here. The final column gives \\(\\sqrt{\\text{GVIF}^{1/2 \\text{df}}}\\), the remaining effect of collinearity on the standard errors of terms in this model.\n\nacetyl.mod2 &lt;- lm(yield ~ temp + ratio + poly(time, 2) + \n                          temp:time + temp:ratio + time:ratio,\n                  data=Acetylene.centered)\n\nvif(acetyl.mod2, type = \"term\")\n#                  GVIF Df GVIF^(1/(2*Df))\n# temp            57.09  1            7.56\n# ratio            1.09  1            1.05\n# poly(time, 2) 1733.56  2            6.45\n# temp:time       44.67  1            6.68\n# temp:ratio      30.69  1            5.54\n# ratio:time      33.33  1            5.77",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge",
    "href": "08-collinearity-ridge.html#sec-ridge",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.6 Ridge regression",
    "text": "8.6 Ridge regression\nWhen the goals of your analysis are thwarted by the constraints of the assumptions and goals of a model, some trade-offs may help you stay closer to your goals. Ridge regression is a simple instance of a class of techniques designed to obtain more favorable predictions at the expense of some increase in bias in the coefficients, compared to ordinary least squares (OLS) estimation. These methods began as a way of solving collinearity problems in OLS regression with highly correlated predictors (Hoerl & Kennard, 1970). \nMore recently, the ideas of ridge regression spawned a larger class of model selection methods, of which the LASSO method of Tibshirani (1996) and LAR method of Efron et al. (2004) are well-known instances. See, for example, the reviews in Vinod (1978) and McDonald (2009) for details and context omitted here. \nThe case of ridge regression has also been extended to the multivariate case of two or more response variables (Brown & Zidek, 1980; Haitovsky, 1987), but there no implementations of these methods in R. \nAn essential idea behind these methods is that the OLS estimates are constrained in some way, shrinking them, on average, toward zero, to achieve increased predictive accuracy at the expense of some increase in bias. Another common characteristic is that they involve some tuning parameter (\\(k\\)) or criterion to quantify the tradeoff between bias and variance. In many cases, analytical or computationally intensive methods have been developed to choose an optimal value of the tuning parameter, for example using generalized cross validation, bootstrap methods.\nVisualization\nA common means to visualize the effects of shrinkage in these problems is to make what are called univariate ridge trace plots (Section 8.7) showing how the estimated coefficients \\(\\widehat{\\boldsymbol{\\beta}}_k\\) change as the shrinkage criterion \\(k\\) increases. (An example is shown in Figure 8.10 below.) But this only provides a view of bias. It is the wrong graphic form for a multivariate problem where we want to visualize bias in the coefficients \\(\\widehat{\\boldsymbol{\\beta}}_k\\) vs. their precision, as reflected in their estimated variances, \\(\\widehat{\\textsf{Var}} (\\widehat{\\boldsymbol{\\beta}}_k)\\). A more useful graphic plots the confidence ellipses for the coefficients, showing both bias and precision (Section 8.8). Some of the material below borrows from Friendly (2011) and Friendly (2013). \n\n8.6.1 Properties of ridge regression\nTo provide some context, I summarize the properties of ridge regression below, comparing the OLS estimates with their ridge counterparts. To avoid unnecessary details related to the intercept, assume the predictors have been centered at their means and the unit vector is omitted from \\(\\mathbf{X}\\). Further, to avoid scaling issues, we standardize the columns of \\(\\mathbf{X}\\) to unit length, so that \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is a also correlation matrix.\nThe ordinary least squares estimates of coefficients and their estimated variance covariance matrix take the (hopefully now) familiar form\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} = &\n    (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\mathbf{y} \\:\\: ,\\\\\n\\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}) = &\n    \\widehat{\\sigma}_{\\epsilon}^2 (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}.\n\\end{aligned}\n\\tag{8.3}\\]\nAs we saw earlier, one signal of the problem of collinearity is that the determinant \\(\\mathrm{det}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})\\) approaches zero as the predictors become more collinear. The inverse \\((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\) then becomes numerically unstable, or worse—does not exist, if the determinant becomes zero as in the case of exact dependency of one variable on the others. You just can’t divide by zero!\nRidge regression uses a cool matrix trick to avoid this. It simply adds a constant, \\(k\\) to the diagonal elements, thus replacing \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) with \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I}\\) in Equation 8.3. This drives the determinant away from zero as \\(k\\) increases. The ridge regression estimates then become,\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k = &\n    (\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T}\\mathbf{y}  \\\\\n                                    = & \\mathbf{G}_k \\, \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} \\:\\: ,\\\\\n\\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k) = &\n     \\widehat{\\sigma}^2  \\mathbf{G}_k (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{G}_k^\\mathsf{T}\\:\\: ,\n\\end{aligned}\n\\tag{8.4}\\]\nwhere \\(\\mathbf{G}_k = \\left[\\mathbf{I} + k (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\right] ^{-1}\\) is the \\((p \\times p)\\) shrinkage matrix. Thus, as \\(k\\) increases, \\(\\mathbf{G}_k\\) decreases, and drives \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k\\) toward \\(\\mathbf{0}\\) (Hoerl & Kennard, 1970).\nAnother insight, from the shrinkage literature, is that ridge regression can be formulated as least squares regression, minimizing a residual sum of squares, \\(\\text{RSS}(k)\\), which adds a penalty for large coefficients,\n\\[\n\\text{RSS}(k) = (\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\beta}) ^\\mathsf{T}(\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\beta}) + k \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} \\quad\\quad (k \\ge 0)\n\\:\\: ,\n\\tag{8.5}\\] where the penalty restrict the coefficients to some squared length \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} = \\Sigma \\beta_i \\le t(k)\\).\nGeometry The geometry of ridge regession is illustrated in Figure 8.9 for two coefficients \\(\\boldsymbol{\\beta} = (\\beta_1, \\beta_2)\\). The blue circles at the origin, having radii \\(\\sqrt{t_k}\\), show the constraint that the sum of squares of coefficients, \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} = \\beta_1^2 + \\beta_2^2\\) be less than \\(k\\). The red ellipses show contours of the covariance ellipse of \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}\\). As the shrinkage constant \\(k\\) increases, the center of these ellipses travel along the path illustrated toward \\(\\boldsymbol{\\beta} = \\mathbf{0}\\) This path is called the locus of osculation, the path along which circles or ellipses first kiss as they expand, like the pattern of ripples from rocks dropped into a pond (Friendly et al., 2013). \n\n\n\n\n\n\n\nFigure 8.9: Geometric interpretation of ridge regression, using elliptical contours of the \\(\\text{RSS}(k)\\) function. The blue circles at the origin show the constraint that the sum of squares of coefficients, \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta}\\) be less than \\(k\\). The red ellipses show the covariance ellipse of two coefficients \\(\\boldsymbol{\\beta}\\). Ridge regression finds the point \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k\\) where the OLS contours just kiss the constraint region. Source: Friendly et al. (2013).\n\n\n\n\n\nEquation 8.4 is computationally expensive, potentially numerically unstable for small \\(k\\), and it is conceptually opaque, in that it sheds little light on the underlying geometry of the data in the column space of \\(\\mathbf{X}\\).\nOnce again, an alternative formulation, highlighting the role of shrinkage here, can be given in terms of the singular value decomposition (SVD) of \\(\\mathbf{X}\\) (Section 4.3.1),\n\\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\mathsf{T}\n\\]\nwhere \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are respectively \\(n\\times p\\) and \\(p\\times p\\) orthonormal matrices, so that \\(\\mathbf{U}^\\mathsf{T}\\mathbf{U} = \\mathbf{V}^\\mathsf{T}\\mathbf{V} = \\mathbf{I}\\), and \\(\\mathbf{D} = \\mathrm{diag}\\, (d_1, d_2, \\dots d_p)\\) is the diagonal matrix of ordered singular values, with entries \\(d_1 \\ge d_2 \\ge \\cdots \\ge d_p \\ge 0\\).\nBecause \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X} = \\mathbf{V} \\mathbf{D}^2 \\mathbf{V}^\\mathsf{T}\\), the eigenvalues of \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) are given by \\(\\mathbf{D}^2\\) and therefore the eigenvalues of \\(\\mathbf{G}_k\\) can be shown (Hoerl & Kennard, 1970) to be the diagonal elements of\n\\[\n\\mathbf{D}(\\mathbf{D}^2 + k \\mathbf{I} )^{-1} \\mathbf{D} = \\mathrm{diag}\\,  \\left(\\frac{d_i^2}{d_i^2 + k}\\right) \\:\\: .\n\\]\nNoting that the eigenvectors, \\(\\mathbf{V}\\) are the principal component vectors, and that \\(\\mathbf{X} \\mathbf{V} = \\mathbf{U} \\mathbf{D}\\), the ridge estimates can be calculated more simply in terms of \\(\\mathbf{U}\\) and \\(\\mathbf{D}\\) as\n\\[\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k = (\\mathbf{D}^2 + k \\mathbf{I})^{-1} \\mathbf{D} \\mathbf{U}^\\mathsf{T}\\mathbf{y} = \\left( \\frac{d_i}{d_i^2 + k}\\right) \\: \\mathbf{u}_i^\\mathsf{T}\\mathbf{y}, \\quad i=1, \\dots p \\:\\: .\n\\]\nThe terms \\(d^2_i / (d_i^2 + k) \\le 1\\) are thus the factors by which the coordinates of \\(\\mathbf{u}_i^\\mathsf{T}\\mathbf{y}\\) are shrunk with respect to the orthonormal basis for the column space of \\(\\mathbf{X}\\). The small singular values \\(d_i\\) correspond to the directions which ridge regression shrinks the most. These are the directions which contribute most to collinearity, discussed earlier.\nThis analysis also provides an alternative and more intuitive characterization of the ridge tuning constant. By analogy with OLS, where the hat matrix, \\(\\mathbf{H} = \\mathbf{X} (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\) reflects degrees of freedom \\(\\text{df} = \\mathrm{tr} (\\mathbf{H}) = p\\) corresponding to the \\(p\\) parameters, the effective degrees of freedom for ridge regression (Hastie et al., 2009) is\n\\[\n\\begin{aligned}\n\\text{df}_k\n    = & \\text{tr}[\\mathbf{X} (\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T}] \\\\\n    = & \\sum_i^p \\text{df}_k(i) = \\sum_i^p \\left( \\frac{d_i^2}{d_i^2 + k} \\right) \\:\\: .\n\\end{aligned}\n\\tag{8.6}\\]\n\\(\\text{df}_k\\) is a monotone decreasing function of \\(k\\), and hence any set of ridge constants can be specified in terms of equivalent \\(\\text{df}_k\\). Greater shrinkage corresponds to fewer coefficients being estimated.\nThere is a close connection with principal components regression mentioned in Section 8.5. Ridge regression shrinks all dimensions in proportion to \\(\\text{df}_k(i)\\), so the low variance dimensions are shrunk more. Principal components regression discards the low variance dimensions and leaves the high variance dimensions unchanged.\n\n8.6.2 The genridge package\nRidge regression and other shrinkage methods are available in several packages including MASS (the lm.ridge() function), glmnet (Friedman et al., 2025), and penalized (Goeman et al., 2022), but none of these provides insightful graphical displays. glmnet::glmnet() also implements a method for multivariate responses with a `family=“mgaussian”.\nHere, I focus in the genridge package (Friendly, 2024), where the ridge() function is the workhorse and pca.ridge() transforms these results to PCA/SVD space. vif.ridge() calculates VIFs for class \"ridge\" objects and precision() calculates precision and shrinkage measures.\nA variety of plotting functions is available for univariate, bivariate and 3D plots:\n\n\ntraceplot() Traditional univariate ridge trace plots\n\nplot.ridge() Bivariate 2D ridge trace plots, showing the covariance ellipse of the estimated coefficients\n\npairs.ridge() All pairwise bivariate ridge trace plots\n\nplot3d.ridge() 3D ridge trace plots with ellipsoids\n\nbiplot.ridge() ridge trace plots in PCA/SVD space\n\nIn addition, the pca() method for \"ridge\" objects transforms the coefficients and covariance matrices of a ridge object from predictor space to the equivalent, but more interesting space of the PCA of \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) or the SVD of \\(\\mathbf{X}\\). biplot.pcaridge() adds variable vectors to the bivariate plots of coefficients in PCA space",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge-univar",
    "href": "08-collinearity-ridge.html#sec-ridge-univar",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.7 Univariate ridge trace plots",
    "text": "8.7 Univariate ridge trace plots\nThe usual idea to visualize the effects of shrinkage of the coefficients in ridge regression is a simple set of line plots showing how the coefficient of each predictor decreases as the ridge constant increases, as shown below in Figure 8.10 and Figure 8.11.\n\nExample 8.4 A classic example for ridge regression is Longley’s (1967) data, consisting of 7 economic variables, observed yearly from 1947 to 1962 (n=16), in the dataset longley. The goal is to predict Employed from GNP, Unemployed, Armed.Forces, Population, Year, and GNP.deflator.\n\n\n\ndata(longley, package=\"datasets\")\nstr(longley)\n# 'data.frame': 16 obs. of  7 variables:\n#  $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...\n#  $ GNP         : num  234 259 258 285 329 ...\n#  $ Unemployed  : num  236 232 368 335 210 ...\n#  $ Armed.Forces: num  159 146 162 165 310 ...\n#  $ Population  : num  108 109 110 111 112 ...\n#  $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...\n#  $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...\n\nThese data were constructed to illustrate numerical problems in least squares software at the time, and they are (purposely) perverse, in that:\n\nEach variable is a time series so that there is clearly a lack of independence among predictors. Year is at least implicitly correlated with most of the others.\nWorse, there is also some structural collinearity among the variables GNP, Year, GNP.deflator, and Population; for example, GNP.deflator is a multiplicative factor to account for inflation.\n\nWe fit the regression model, and sure enough, there are some extremely large VIFs. The largest, for GNP represents a multiplier of \\(\\sqrt{1788.5} = 42.3\\) on the standard errors.\n\nlongley.lm &lt;- lm(Employed ~ GNP + Unemployed + Armed.Forces + \n                            Population + Year + GNP.deflator, \n                 data=longley)\nvif(longley.lm)\n#          GNP   Unemployed Armed.Forces   Population         Year \n#      1788.51        33.62         3.59       399.15       758.98 \n# GNP.deflator \n#       135.53\n\nShrinkage values can be specified using \\(k\\) (where \\(k = 0\\) corresponds to OLS) or the equivalent degrees of freedom \\(\\text{df}_k\\) (Equation 8.6). (The function uses argument lambda, \\(\\lambda \\equiv k\\) for the shrinkage constant.) Among other quantities, ridge() returns a matrix containing the coefficients for each predictor for each shrinkage value and other quantities.\n\nlambda &lt;- c(0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08)\nlridge &lt;- ridge(Employed ~ GNP + Unemployed + Armed.Forces + \n                           Population + Year + GNP.deflator, \n    data=longley, lambda=lambda)\nprint(lridge, digits = 2)\n# Ridge Coefficients:\n#        GNP     Unemployed  Armed.Forces  Population  Year  \n# 0.000  -3.447  -1.828      -0.696        -0.344       8.432\n# 0.002  -2.114  -1.644      -0.658        -0.713       7.466\n# 0.005  -1.042  -1.491      -0.623        -0.936       6.567\n# 0.010  -0.180  -1.361      -0.588        -1.003       5.656\n# 0.020   0.499  -1.245      -0.548        -0.868       4.626\n# 0.040   0.906  -1.155      -0.504        -0.523       3.577\n# 0.080   1.091  -1.086      -0.458        -0.086       2.642\n#        GNP.deflator\n# 0.000   0.157      \n# 0.002   0.022      \n# 0.005  -0.042      \n# 0.010  -0.026      \n# 0.020   0.098      \n# 0.040   0.321      \n# 0.080   0.570\n\nThe standard univariate plot, given by traceplot(), simply plots the estimated coefficients for each predictor against the shrinkage factor \\(k\\).\n\ntraceplot(lridge, \n          X = \"lambda\",\n          xlab = \"Ridge constant (k)\",\n          xlim = c(-0.02, 0.08), cex.lab=1.25)\n\n\n\n\n\n\nFigure 8.10: Univariate ridge trace plot for the coefficients of predictors of Employment in Longley’s data via ridge regression, with ridge constants k = (0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08). The dotted lines show optimal values for shrinkage by two criteria (HKB, LW).\n\n\n\n\nYou can see that the coefficients for Year and GNP are shrunk considerably. Differences from the \\(\\beta\\) value at \\(k =0\\) represent the bias (smaller \\(\\mid \\beta \\mid\\)) needed to achieve more stable estimates.\nThe dotted lines in Figure 8.10 show choices for the ridge constant by two commonly used criteria to balance bias against precision due to Hoerl et al. (1975) (HKB) and Lawless & Wang (1976) (LW). These values (along with a generalized cross-validation value GCV) are also stored in the “ridge” object as a vector criteria.\n\nlridge$criteria\n#    kHKB     kLW    kGCV \n# 0.00428 0.03230 0.00200\n\n\nThe shrinkage constant \\(k\\) doesn’t have much intrinsic meaning, so it is often easier to interpret the plot when coefficients are plotted against the equivalent degrees of freedom, \\(\\text{df}_k\\). OLS corresponds to \\(\\text{df}_k = 6\\) degrees of freedom in the space of six parameters, and the effect of shrinkage is to decrease the degrees of freedom, as if estimating fewer parameters.7 This more natural scale also makes the changes in coefficient with shrinkage more nearly linear.\n\ntraceplot(lridge, \n          X = \"df\",\n          xlim = c(4, 6.2), cex.lab=1.25)\n\n\n\n\n\n\nFigure 8.11: Univariate ridge trace plot using equivalent degrees of freedom, \\(\\text{df}_k\\) to specify shrinkage. This scale is easier to understand and makes the traces of prarameters more nearly linear.\n\n\n\n\n\n\n8.7.1 What’s not to like?\nThe bigger problem is that these univariate plots are the wrong kind of plot! They show the trends in increased bias (toward smaller \\(\\mid \\beta \\mid\\)) associated with larger \\(k\\), but they do not show the accompanying increase in precision (decrease in variance) achieved by allowing a bit of bias.\nFor that, we need to consider the variances and covariances of the estimated coefficients. The univariate trace plot is simply the wrong graphic form for what is essentially a multivariate problem, where we would like to visualize how both coefficients and their variances change with \\(k\\).",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge-bivar",
    "href": "08-collinearity-ridge.html#sec-ridge-bivar",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.8 Bivariate ridge trace plots",
    "text": "8.8 Bivariate ridge trace plots\nThe bivariate analog of the trace plot suggested by Friendly (2013) plots bivariate confidence ellipses for pairs of coefficients. Their centers, \\((\\widehat{\\beta}_i, \\widehat{\\beta}_j)\\) compared to the OLS values show the bias induced for each coefficient, and also how the change in the ridge estimate for one parameter is related to changes for other parameters.\nThe size and shapes of the covariance ellipses show directly the effect on precision of the estimates as a function of the ridge tuning constant; their size and shape indicate sampling variance and covariance, given by \\(\\widehat{\\text{Var}} (\\boldsymbol{\\widehat{\\beta}}_{ij})\\).\nHere (Figure 8.12), I plot those for GNP against four of the other predictors. The plot() method for \"ridge\" objects plots these ellipses for a pair of variables.\n\nclr &lt;-  c(\"black\", \"red\", \"brown\", \"darkgreen\",\"blue\", \"cyan4\", \"magenta\")\npch &lt;- c(15:18, 7, 9, 12)\nlambdaf &lt;- c(expression(~widehat(beta)^OLS), as.character(lambda[-1]))\n\nfor (i in 2:5) {\n  plot(lridge, variables=c(1,i), \n       radius=0.5, cex.lab=1.5, col=clr, \n       labels=NULL, fill=TRUE, fill.alpha=0.2)\n  text(lridge$coef[1,1], lridge$coef[1,i], \n       expression(~widehat(beta)^OLS), \n       cex=1.5, pos=4, offset=.1)\n  text(lridge$coef[-1,c(1,i)], lambdaf[-1], pos=3, cex=1.3)\n}\n\n\n\n\n\n\nFigure 8.12: Bivariate ridge trace plots for the coefficients of four predictors against the coefficient for GNP in Longley’s data, with k = 0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08. In most cases, the coefficients are driven toward zero, but the bivariate plot also makes clear the reduction in variance, as well as the bivariate path of shrinkage.\n\n\n\n\nAs can be seen, the coefficients for each pair of predictors trace a graceful path generally toward the origin (0,0), and the covariance ellipses get smaller, indicating increased precision. Most often these paths are rather direct, but it takes a peculiar curvilinear route in the case of population and GNP here.\nThe pairs() method for \"ridge\" objects shows all pairwise views in scatterplot matrix form. radius sets the base size of the ellipse-generating circle for the covariance ellipses.\n\npairs(lridge, radius=0.5, diag.cex = 2, \n      fill = TRUE, fill.alpha = 0.1)\n\n\n\n\n\n\nFigure 8.13: Scatterplot matrix of bivariate ridge trace plots. Each panel shows the effect of shrinkage on the covariance ellipse for a given pair of predictors.\n\n\n\n\nMost of the shrinkage paths in Figure 8.13 are regular, but those involving population are curvilinear, reflecting more complex behavior in the ridge method.\n\n8.8.1 Visualizing the bias-variance tradeoff\nThe function precision() calculates a number of measures of the effect of shrinkage of the coefficients in relation to the “size” of the covariance matrix \\(\\boldsymbol{\\mathcal{V}}_k \\equiv \\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k)\\). Larger shrinkage \\(k\\) should lead to a smaller ellipsoid for \\(\\boldsymbol{\\mathcal{V}}_k\\), indicating increased precision.\n\npdat &lt;- precision(lridge) |&gt; print()\n#       lambda   df   det  trace max.eig norm.beta norm.diff\n# 0.000  0.000 6.00 -12.9 18.119  15.419     1.000     0.000\n# 0.002  0.002 5.70 -13.6 11.179   8.693     0.857     0.695\n# 0.005  0.005 5.42 -14.4  6.821   4.606     0.741     1.276\n# 0.010  0.010 5.14 -15.4  4.042   2.181     0.637     1.783\n# 0.020  0.020 4.82 -16.8  2.218   1.025     0.528     2.262\n# 0.040  0.040 4.48 -18.7  1.165   0.581     0.423     2.679\n# 0.080  0.080 4.13 -21.1  0.587   0.260     0.337     3.027\n\nHere, the first three terms described below are (inverse) measures of precision; the last two quantify shrinkage:\n\ndet \\(=\\log{| \\mathcal{V}_k |}\\) is an overall measure of variance of the coefficients. It is the (linearized) volume of the covariance ellipsoid and corresponds conceptually to Wilks’ Lambda criterion.\ntrace \\(=\\text{trace} (\\boldsymbol{\\mathcal{V}}_k)\\) is the sum of the variances and also the sum of the eigenvalues of \\(\\boldsymbol{\\mathcal{V}}_k\\), conceptually similar to Pillai’s trace criterion.\nmax.eig is the largest eigenvalue measure of size, an analog of Roy’s maximum root test.\nnorm.beta \\(= \\left \\Vert \\boldsymbol{\\beta}\\right \\Vert / \\max{\\left \\Vert \\boldsymbol{\\beta}\\right \\Vert}\\) is a summary measure of shrinkage, the normalized root mean square of the estimated coefficients. It starts at 1.0 for \\(k=0\\) and decreases with the penalty for large coefficients.\ndiff.beta is the root mean square of the difference from the OLS estimate \\(\\lVert \\boldsymbol{\\beta}_{\\text{OLS}} - \\boldsymbol{\\beta}_k \\rVert\\). This measure is inversely related to norm.beta.\n\nPlotting shrinkage against a measure of variance gives a direct view of the tradeoff between bias and precision. In Figure 8.14 I use the plot() method for \"precision\" objects. By default, this plots norm.beta against det, joins the points with a smooth spline curve and adds labels for the optimum values according to the different criteria. The points are labelled with their equivalent degrees of freedom.\n\n\n\nShow the codepridge &lt;- precision(lridge)\ncriteria &lt;- lridge$criteria\nnames(criteria) &lt;- sub(\"k\", \"\", names(criteria))\nplot(pridge, criteria = criteria, \n     labels=\"df\", label.prefix=\"df:\",\n     cex.lab = 1.5,\n     xlab ='shrinkage: ||b|| / max(||b||)',\n     ylab='variance: log |Var(b)|'\n     )\nwith(pdat, {\n    text(min(norm.beta), max(det), \n       labels = \"log |Variance| vs. Shrinkage\", \n       cex=1.5, pos=4)\n  })\n\n\n\n\n\n\nFigure 8.14: Precision plot showing the tradeoff between bias and precision. Bias increases as we move away from the OLS solution, but precision increases. The slope of the curve indicates the rate at which variance decreases with shrinkage.\n\n\n\n\n\nYou can see that in this example the HKB and CGV criteria prefer a smaller degree of shrinkage, but achieves only a modest decrease in variance. But variance decreases more sharply thereafter and the LW choice achieves greater precision.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge-low-rank",
    "href": "08-collinearity-ridge.html#sec-ridge-low-rank",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.9 Low-rank views",
    "text": "8.9 Low-rank views\nJust as principal components analysis gives low-dimensional views of a data set, PCA can be useful to understand ridge regression, just as it did for the problem of collinearity.\nThe visCollin::pca() method transforms a \"ridge\" object from parameter space, where the estimated coefficients are \\(\\beta_k\\) with covariance matrices \\(\\boldsymbol{\\mathcal{V}}_k\\), to the principal component space defined by the right singular vectors, \\(\\mathbf{V}\\), of the singular value decomposition \\(\\mathbf{U} \\mathbf{D} \\mathbf{V}^\\mathsf{T}\\) of the scaled predictor matrix, \\(\\mathbf{X}\\).\nIn PCA space the total variance of the predictors remains the same, but it is distributed among the linear combinations that account for successively greatest variance.\n\nplridge &lt;- pca(lridge) |&gt;\n  print()\n# Ridge Coefficients:\n#        dim1     dim2     dim3     dim4     dim5     dim6   \n# 0.000  1.51541  0.37939  1.80131  0.34595  5.97391  6.74225\n# 0.002  1.51537  0.37935  1.80021  0.34308  5.69497  5.06243\n# 0.005  1.51531  0.37928  1.79855  0.33886  5.32221  3.68519\n# 0.010  1.51521  0.37918  1.79579  0.33205  4.79871  2.53553\n# 0.020  1.51500  0.37898  1.79031  0.31922  4.00988  1.56135\n# 0.040  1.51459  0.37858  1.77944  0.29633  3.01774  0.88291\n# 0.080  1.51377  0.37778  1.75810  0.25915  2.01876  0.47238\n\nTraceplot\nThen, a traceplot() of the resulting \"pcaridge\" object shows how the dimensions are affected by shrinkage, shown on the scale of degrees of freedom in Figure 8.15.\n\ntraceplot(plridge, X=\"df\", \n          cex.lab = 1.2, lwd=2)\n\n\n\n\n\n\nFigure 8.15: Ridge traceplot for the longley regression viewed in PCA space. The dimensions are the linear combinations of the predictors which account for greatest variance.\n\n\n\n\nWhat may be surprising at first is that the coefficients for the first 4 components are not shrunk at all. These large dimensions are immune to ridge tuning. Rather, the effect of shrinkage is seen only on the last two dimensions. But those also are the directions that contribute most to collinearity as we saw earlier.\n\npairs() plot\nA pairs() plot gives a dramatic representation bivariate effects of shrinkage in PCA space: the principal components of X are uncorrelated, so the ellipses are all aligned with the coordinate axes. The ellipses largely coincide for dimensions 1 to 4 where there is little effect of shrinkage. You can see them shrink in one direction in the last two columns and rows, and in both for the combination of (dim5, dim6).\n\npairs(plridge)\n\n\n\n\n\n\nFigure 8.16: pairs method: All pairwise bivariate ridge plots shown in PCA space.\n\n\n\n\nIf we focus on the plot of dimensions dim5:dim6, we can see where all the shrinkage action is in this representation. Generally, the predictors that are related to the smallest dimension (6) are shrunk quickly at first.\n\nplot(plridge, variables=5:6, \n     fill = TRUE, fill.alpha=0.15, cex.lab = 1.5)\ntext(plridge$coef[, 5:6], \n     label = lambdaf, \n     cex=1.5, pos=4, offset=.1)\n\n\n\n\n\n\nFigure 8.17: Bivariate ridge trace plot for the smallest two dimensions. The coefficients for these two dimensions head smoothly toward zero and their variance also shrinks.\n\n\n\n\n\n8.9.1 Biplot view\nThe question arises how to relate this view of shrinkage in PCA space to the original predictors. The biplot is again your friend. In effect, it adds vectors showing the contributions of the predictors to a plot like Figure 8.17. You can project variable vectors for the predictor variables into the PCA space of the smallest dimensions, where the shrinkage action mostly occurs to see how the predictor variables relate to these dimensions.\nbiplot.pcaridge() supplements the standard display of the covariance ellipsoids for a ridge regression problem in PCA/SVD space with labeled arrows showing the contributions of the original variables to the dimensions plotted. Recall from Section 4.3 that these reflect the correlations of the variables with the PCA dimensions. The lengths of the arrows reflect the proportion of variance that each predictors shares with the components.\n\nbiplot(plridge, radius=0.5, \n       ref=FALSE, asp=1, \n       var.cex=1.15, cex.lab=1.3, col=clr,\n       fill=TRUE, fill.alpha=0.15, \n       prefix=\"Dimension \")\n# Vector scale factor set to  5.25\ntext(plridge$coef[,5:6], lambdaf, pos=2, cex=1.3)\n\n\n\n\n\n\nFigure 8.18: Biplot view of the ridge trace plot for the smallest two dimensions, where the effects of shrinkage are most apparent.\n\n\n\n\nThe biplot view in Figure 8.18 showing the two smallest dimensions is particularly useful for understanding how the predictors contribute to shrinkage in ridge regression. Here, Year and Population largely contribute to dimension 5; a contrast between (Year, Population) and GNP contributes to dimension 6.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#what-have-we-learned",
    "href": "08-collinearity-ridge.html#what-have-we-learned",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.10 What have we learned?",
    "text": "8.10 What have we learned?\nTODO: Consider replacing this with bullet point take-aways.\nThis chapter has considered the problems in regression models which stem from high correlations among the predictors. We saw that collinearity results in unstable estimates of coefficients with larger uncertainty, often dramatically more so than would be the case if the predictors were uncorrelated.\nCollinearity can be seen as merely a “data problem” which can safely be ignored if we are only interested in prediction. When we want to understand a model, ridge regression can tame the collinearity beast by shrinking the coefficients slightly to gain greater precision in the estimates.\nBeyond these statistical considerations, the methods of this chapter highlight the roles of multivariate thinking and visualization in understanding these phenomena and the methods developed for solving them. Data ellipses and confidence ellipses for coefficients again provide tools for visualizing what is concealed in numerical summaries. A perhaps surprising feature of both collinearity and ridge regression is that the important information usually resides in the smallest PCA dimensions and biplots help again to understand these dimensions.\n\n\n\n\n\nBelsley, D. A. (1991). Conditioning diagnostics: Collinearity and weak data in regression. Wiley.\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley; Sons.\n\n\nBrown, P. J., & Zidek, J. V. (1980). Adaptive multivariate ridge regression. The Annals of Statistics, 8(1), 64–74. http://www.jstor.org/stable/2240743\n\n\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. The Annals of Statistics, 32(2), 407–499.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (Third edition.). SAGE.\n\n\nFox, J., & Monette, G. (1992). Generalized collinearity diagnostics. Journal of the American Statistical Association, 87(417), 178–183.\n\n\nFriedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K., Simon, N., & Yang, J. (2025). Glmnet: Lasso and elastic-net regularized generalized linear models. https://glmnet.stanford.edu\n\n\nFriendly, M. (2011). Generalized ridge trace plots: Visualizing bias and precision with the genridge R package. SCS Seminar.\n\n\nFriendly, M. (2013). The generalized ridge trace plot: Visualizing bias and precision. Journal of Computational and Graphical Statistics, 22(1), 50–68. https://doi.org/10.1080/10618600.2012.681237\n\n\nFriendly, M. (2024). Genridge: Generalized ridge trace plots for ridge regression. https://doi.org/10.32614/CRAN.package.genridge\n\n\nFriendly, M., & Kwan, E. (2009). Where’s Waldo: Visualizing collinearity diagnostics. The American Statistician, 63(1), 56–65. https://doi.org/10.1198/tast.2009.0012\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with application to principal components analysis. Biometrics, 58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGoeman, J., Meijer, R., Chaturvedi, N., & Lueder, M. (2022). Penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the cox model. https://doi.org/10.32614/CRAN.package.penalized\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman & Hall.\n\n\nGraybill, F. A. (1961). An introduction to linear statistical models. McGraw-Hill.\n\n\nHaitovsky, Y. (1987). On multivariate ridge regression. Biometrika, 74(3), 563–570. https://doi.org/10.1093/biomet/74.3.563\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference and prediction (2nd ed.). Springer. http://www-stat.stanford.edu/~tibs/ElemStatLearn/\n\n\nHocking, R. R. (2013). Methods and applications of linear models: Regression and the analysis of variance. Wiley. https://books.google.ca/books?id=iq2J-1iS6HcC\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12, 55–67.\n\n\nHoerl, A. E., Kennard, R. W., & Baldwin, K. F. (1975). Ridge regression: Some simulations. Communications in Statistics, 4(2), 105–123. https://doi.org/10.1080/03610927508827232\n\n\nKwan, E., Lu, I. R. R., & Friendly, M. (2009). Tableplot: A new tool for assessing precise predictions. Zeitschrift für Psychologie / Journal of Psychology, 217(1), 38–48. https://doi.org/10.1027/0044-3409.217.1.38\n\n\nLawless, J. F., & Wang, P. (1976). A simulation study of ridge and other regression estimators. Communications in Statistics, 5, 307–323.\n\n\nLongley, J. W. (1967). An appraisal of least squares programs for the electronic computer from the point of view of the user. Journal of the American Statistical Association, 62, 819–841. https://doi.org/https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896\n\n\nMarquardt, D. W. (1970). Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation. Technometrics, 12, 591–612.\n\n\nMcDonald, G. C. (2009). Ridge regression. Wiley Interdisciplinary Reviews: Computational Statistics, 1(1), 93–100. https://doi.org/10.1002/wics.14\n\n\nPesaran, M. H., & Smith, R. P. (2019). A bayesian analysis of linear regression models with highly collinear regressors. Econometrics and Statistics, 11, 1–21. https://doi.org/10.1016/j.ecosta.2018.10.001\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B: Methodological, 58, 267–288.\n\n\nVinod, H. D. (1978). A survey of ridge regression and related techniques for improvements over ordinary least squares. The Review of Economics and Statistics, 60(1), 121–131. http://www.jstor.org/stable/1924340",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#footnotes",
    "href": "08-collinearity-ridge.html#footnotes",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "The “Where’s Waldo” problem has attracted attention in machine learning, AI and computational image analysis circles. One approach uses convolutional neural networks. FindWaldo is one example implemented in Python.↩︎\nThis example is adapted from one by John Fox (2022), Collinearity Diagnostics↩︎\nRecall that in an added-variable plot (Section 6.4), the horizontal axis for predictor \\(x_j\\) is \\(x^\\star_j = x_j  \\,|\\, \\text{others}\\) … TODO complete this thought↩︎\nWell, -77 is just a bit beyond left-most point on the fitted line in this panel of Figure 8.8.↩︎\nThe rsm package provides convenient shortcuts for specifying response surface models. For instance, the SO() shortcut, y ~ SO(x1, x2) automatically generates all linear, interaction, and quadratic terms for the specified variables x1 and x2.↩︎\nThe I() here is the identity function. It is needed because time^2 has a different interpretation in a model formula than in algebra.↩︎\nA related shrinkage method, LASSO (Least Absolute Shrinkage and Selection Operator) (Tibshirani, 1996) uses a penalty term of the sum of absolute values of the coefficients, \\(\\Sigma \\lvert \\beta_i \\rvert \\le t(k)\\) rather than the sum of squares in Equation 8.5. The effect of this change is to shrink some coefficients exactly to zero, effectively eliminating them from the model. This makes LASSO a model selection method, similar in aim to other best subset regression methods. This is widely used in machine learning methods, where interpretation less important than prediction accuracy. ↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  }
]