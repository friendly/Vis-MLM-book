[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "",
    "text": "Preface\nThis book is about graphical methods developed recently for multivariate data, and their uses in understanding relationships when there are several aspects to be considered together. Data visualization methods for statistical analysis are well-developed for simple linear models with a single outcome variable. However, with applied research in the social and behavioral sciences, it is often the case that the phenomena of interest (e.g., depression, job satisfaction, academic achievement, childhood ADHD disorders, etc.) can be measured in several different ways or related aspects.\nFor example, if academic achievement can be measured for adolescents by reading, mathematics, science and history scores, how do predictors such as parental encouragement, school environment and socioeconomic status affect all these outcomes? In a similar way? In different ways? In such cases, much more can be understood from a multivariate approach that considers the correlations among the outcomes. Yet, sadly, researchers typically examine the outcomes one by one which often only tells part of the data story.\nHowever, to do this it is useful to set the stage for multivariate thinking, with a grand scheme for statistics and data visualization, a parable, and an example of multivariate discovery.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#one-two-many",
    "href": "index.html#one-two-many",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "ONE, TWO, MANY",
    "text": "ONE, TWO, MANY\nThere is an old and helpful idea I learned from John Hartigan in my graduate days at Princeton:\n\nIn statistics and data visualization all methods can be classified by the number of dimensions contemplated, on a scale of ONE, TWO, MANY.\n\nBy this, he meant that, at a global level, all data, statistical summaries, and graphical displays could be classified as:\n\n\nunivariate: a single variable, considered in isolation (age, COVID cases, pizzas ordered). Univariate numerical summaries are means, medians, measures of variablilty, and so forth. Univariate displays include dot plots, boxplots, histograms and density estimates.\n\nbivariate: two variables, considered jointly. Numerical summaries include correlations, covariances and two-way tables of frequencies or measures of association for categorical variables. Bivariate displays include scatterplots and mosaic plots.\n\nmultivariate: three or more variables, considered jointly. Numerical summaries include correlation and covariance matrices, consisting of all pairwise values, but also derived measures from the analysis of these matrices (eigenvalues, eigenvectors). Graphical displays of multivariate data can sometimes be shown in 3D, but often involve multiple views of the data projected into 2D plots.\n\nAs a quasi-numerical scale, I refer to these as 1D, 2D and nD. This admits the possibility of half-integer cases, such as 1.5D, where the main focus is on a single variable, but that is classified by a simple factor (e.g., gender), or 2.5D where a 2D scatterplot can show other variables using color, shape or other visual attributes His point in this classification was that once you’ve reached three variables, all higher dimensions involve similar summaries and data displays.\nUnivariate and bivariate methods and displays are well-known. This book is about how these ideas can be extended to an \\(n\\)-dimensional world. Three-dimensional data displays are now fairly easy to produce, even if they are sometimes difficult to understand. But how can we even think about four or more dimensions? The difficulty can be appreciated by considering the tale of Flatland.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#flatland",
    "href": "index.html#flatland",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Flatland",
    "text": "Flatland\n\nTo comport oneself with perfect propriety in Polygonal society, one ought to be a Polygon oneself. — Edwin A. Abbott, Flatland\n\nIn 1884, an English schoolmaster, Edwin Abbott Abbott, shook the world of Victorian culture with a slim volume, Flatland: A Romance of Many Dimensions (Abbott, 1884). He described a two-dimensional world, Flatland, inhabited entirely by geometric figures in the plane. His purpose was satirical, to poke fun at the social and gender class system at the time: Women were mere line segments, while men were represented as polygons with varying numbers of sides— a triangle was a working man, but acute isosceles were soldiers or criminals of very small angle; gentlemen and professionals had more sides. Abbot published this under the pseudonym, “A Square”, suggesting his place in the hierarchy.\n\nTrue, said the Sphere; it appears to you a Plane, because you are not accustomed to light and shade and perspective; just as in Flatland a Hexagon would appear a Straight Line to one who has not the Art of Sight Recognition. But in reality it is a Solid, as you shall learn by the sense of Feeling. — Edwin A. Abbott, Flatland\n\nBut how did it feel to be a member of a flatland society? How could a point (a newborn child?) understand a line (a woman)? How does a Triangle “see” a Hexagon or even a infinitely-sided Circle? Abbott introduces the very idea of different dimensions of existence through dreams and visions:\n\nA Square dreams of visiting a one-dimensional Lineland where men appear as lines, and women are merely “illustrious points”, but the inhabitants can only see the Square as lines.\nIn a vision, the Square is visited by a Sphere, to illustrate what a 2D Flatlander could understand from a 3D sphere (Figure 1) that passes through the plane he inhabits. It is a large circle when seen at the moment of its’ greatest extent. As the Spehere rises, it becomes progressively smaller, until it becomes a point, and then vanishes.\n\n\n\n\n\n\n\n\nFigure 1: A 2D Flatlander seeing a sphere as it passes through Flatland. The line, labeled ‘My Eye’ indicates what the Flatlander would see. Source: Abbott (1884)\n\n\n\n\nAbbott goes on to state what could be considered as a demonstration (or proof) by induction of the difficulties of seeing in 1, 2, 3 dimensions, and how the idea motion over time (one more dimension) could allow citizens of any 1D, 2D, 3D world to contemplate one more dimension.\n\nIn One Dimensions, did not a moving Point produce a Line with two terminal points? In two Dimensions, did not a moving Line produce a Square with four terminal points? In Three Dimensions, did not a moving Square produce - did not the eyes of mine behold it - that blessed being, a Cube, with eight terminal points? And in Four Dimensions, shall not a moving Cube - alas, for Analogy, and alas for the Progress of Truth if it be not so - shall not, I say the motion of a divine Cube result in a still more divine organization with sixteen terminal points? — Edwin A. Abbott\n\nFor Abbot, the way for a citizen of any world to imagine one more dimension was to consider how a higher-dimensional object would change over time.1 A line moved over time could produce a rectangle as shown in Figure 2; that rectangle moving in another direction over time would produce a 3D figure, and so forth.\n\n\n\n\n\n\n\nFigure 2: Geometrical objects in 1 to 4 dimensions. One more dimension can be thought of as the trace of movement over time.\n\n\n\n\nBut wait! Where does that 4D thing (a tesseract) come from? To really see a tesseract it helps to view it in an animation over time (Figure 3). But like the Square, contemplating 3D from a 2D world, it takes some imagination.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Animation of a tesseract, a cube changing over time.\n\n\nYet the deep mathematics of more than three dimensions only emerged in the 19th century. In Newtonian mechanics, space and time were always considered independent of each other. Our familiar three-dimensional space, of length, width, and height had formed the backbone of Euclidean geometry for millenea. However, the idea that space and time are indeed interwoven was first proposed by German mathematician Hermann Minkowski (1864–1909) in 1908. This was a powerful idea. It bore fruit when Albert Einstein revolutionized the Newtonian conceptions of gravity in 1915 when he presented a theory of general relativity which was based primarily on the fact that mass and energy warp the fabric of four-dimensional spacetime.\nThe parable of Flatland can provide inspiration for statistical thinking and data visualization. Once we go beyond bivariate statistics and 2D plots, we are in a multivariate world of possibly MANY dimensions. It takes only some imagination and suitable methods to get there.\nLike Abbott’s Flatland, this book is a romance, in many dimensions, of what we can learn from modern methods of data visualization.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#eureka",
    "href": "index.html#eureka",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "EUREKA!",
    "text": "EUREKA!\nEven modest sized multivariate data can have secrets that can be revealed in the right view. As an example, David Coleman at RCA Laboratories in Princeton, N.J. generated a dataset of five (fictitious) measurements of grains of pollen for the 1986 Data Exposition at the Joint statistical Meetings. The first three variables are the lengths of geometric features 3848 observed sampled pollen grains – in the x, y, and z dimensions: a ridge along x, a nub in the y direction, and a crack in along the z dimension. The fourth variable is pollen grain weight, and the fifth is density. The challenge was to “find something interesting” in this dataset, now available as animation::pollen. \nThose who solved the puzzle were able to find an orientation of this 5-dimensional dataset, such that zooming in revealed a magic word, “EUREKA” spelled in points, as in the following figure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Four views of the pollen data, zooming in, clockwise from the upper left to discover the word “EUREKA”.\n\n\nThis can be seen better in a 3D animation. The rgl package (Adler & Murdoch, 2023) is used to create a 3D scatterplot of the first three variables. Then the animation package (Xie, 2021) is use to record a sequence of images, adjusting the rgl::par3d(zoom) value.\n\nCodelibrary(animation)\nlibrary(rgl)\ndata(pollen, package = \"animation\")\noopt = ani.options(interval = 0.05)\n## adjust the viewpoint\nuM =\n  matrix(c(-0.3709192276, -0.5133571028, -0.7738776206, 0, \n           -0.7305060625,  0.6758151054, -0.0981751680, 0, \n            0.57339602708, 0.5289064049, -0.6256819367, 0, \n           0, 0, 0, 1), 4, 4)\nopen3d(userMatrix = uM, \n       windowRect = c(10, 10, 510, 510))\n\nplot3d(pollen[, 1:3])\n\n# zoom in\nzm = seq(1, 0.045, length = 200)\npar3d(zoom = 1)\nfor (i in 1:length(zm)) {\n  par3d(zoom = zm[i])\n  ani.pause()\n}\nani.options(oopt)\n\n\n\n\n\n\nAnimation of zooming in on the pollen data. This figure only appears in the online version.\n\n\n\nFigure 5\n\n\nMultivariate scientific discoveries\nLest this example seem contrived (which it admittedly is), multivariate visualization has played an important role in quite a few scientific discoveries. Among these, Francis Galton’s (1863) discovery of the anti-cyclonic pattern of wind direction in relation to barometric pressure from many weather measures recorded systematically across all weather stations, lighthouses and observatories in Europe in December 1861 stands out as the best example of a scientific discovery achieved almost entirely through graphical means–– something that was totally unexpected, and purely the product of his use of remarkably novel high-dimensional graphs (Friendly & Wainer, 2021, pp. 170–173).\nA more recent example is the discovery of two general classes in the development of Type 2 diabetes by Reaven & Miller (1979), using PRIM-9 (Fishkeller et al., 1974), the first computer system for high-dimensional visualization2. In an earlier study Reaven & Miller (1968) examined the relation between blood glucose levels and the production of insulin in normal subjects and in patients with varying degrees of hyperglicemia (elevated blood sugar level). They found a peculiar ‘’horse shoe’’ shape in this relation (shown in Figure 6), about which they could only speculate: perhaps individuals with the best glucose tolerance also had the lowest levels of insulin as a response to an oral dose of glucose; perhaps those with low glucose response could secrete higher levels of insulin; perhaps those who were low on both glucose and insulin responses followed some other mechanism. In 2D plots, this was a mystery.\n\ndata(Diabetes, package=\"heplots\")\nplot(instest ~ glutest, data=Diabetes, \n     pch=16,\n     cex.lab=1.25,\n     xlab=\"Glucose response\",\n     ylab=\"Insulin response\")\n\n\n\n\n\n\nFigure 6: Reproduction of a graph similar to that from Reaven & Miller (1968) on the relationship between glucose and insulin response to being given an oral dose of glucose.\n\n\n\n\n\nAn answer to their questions came ten years later, when they were able to visualize similar but new data in 3D using the PRIM-9 system. In a carefully controlled study, they also measured ‘’steady state plasma glucose’’ (SSPG), a measure of the efficiency of use of insulin in the body, where large values mean insulin resistance, as well as other variables. PRIM-9 allowed them to explore various sets of three variables, and, more importantly, to rotate a given plot in three dimensions to search for interesting features. One plot that stood out concerned the relation between plasma glucose response, plasma insulin response and SSPG response, shown in Figure 7.\n\n\n\n\n\n\n\nFigure 7: Artist’s rendition of data from Reaven & Miller (1979) as seen in three dimensions using the PRIM-9 system. Labels for the clusters have been added, identifying the three groups of patients. Source: Reaven & Miller (1979).\n\n\n\n\nFrom this graphical insight, they were able to classify the participants into three groups, based on clinical levels of glucose and insulin. The people in the wing on the left in Figure 7 were considered to have overt diabetes, the most advanced form, characterized by elevated fasting blood glucose concentration and classical diabetic symptoms. Those in the right wing were classified as latent or chemical diabetics, with no symptoms of diabetes but demonstrable abnormality of oral or intravenous glucose tolerance. Those in the central blob were classified as normal.\nPrevious thinking was that Type 2 diabetes (when the body cannot make enough insulin, as opposed to Type I, an autoimmune condition where the pancreatic cells have been destroyed) progressed from the chemical stage to an overt one in a smooth transition. However, it was clear from Figure 7 that the only “path” from one to the other lead through the cluster of normal patients near the origin, so that explanation must be wrong. Instead, this suggested that the chemical and overt diabetics were distinct classes. Indeed, longitudinal studies showed that patients classified as chemical diabetics rarely developed the overt form. The understanding of the etiology of Type 2 diabetes was altered dramatically by the power of high-D interactive graphics.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-i-assume",
    "href": "index.html#what-i-assume",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "What I assume",
    "text": "What I assume\nIt is assumed that the reader has a background in applied intermediate statistics including material on univariate linear models including analysis of variance (ANOVA) and multiple regression. This means you should be familiar with … TODO: Complete this required background\nThere will also be some mathematics in the book where words and diagrams are not enough. The mathematical level will be intermediate, mostly consisting of simple algebra. No derivations, proofs, theorems here! For multivariate methods, it will be useful to express ideas using matrix notation to simplify presentation. The single symbol I’m using math to express ideas, and all you will need is a reading-level of understanding. For this, the first chapter of Fox (2021), A mathematical primer for social statistics, is excellent. If you want to learn something of using matrix algebra for data analysis and statistics, I recommend our package matlib (Friendly et al., 2024).\nI also assume the reader to have at least a basic familiarity with R. While R fundamentals are outside the scope of the book, I believe that this language provides a rich set of resources, far beyond that offered by other statistical software packages, and is well worth learning.\nFor those not familiar with R, I recommend Matloff (2011), Wickham (2014), and Cotton (2013) for introductions to programming in the language. Fox & Weisberg (2018) and Teetor (2011) are great for learning about how to conduct basic statistical analyses in R. TODO: Revise this list.\nTODO: Add stuff on general books about graphics",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#conventions-used-in-this-book",
    "href": "index.html#conventions-used-in-this-book",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Conventions used in this book",
    "text": "Conventions used in this book\nTODO: Some stuff below is just for testing… Revise.\nThe following typographic conventions are used in this book:\n\nitalic : indicates terms to be emphasized or defined in the text, …\n\nbold : is used for names of R packages. Or, better yet: bold monospace, but I’d rather this be in a different color. Perhaps I can use “r colorize(”lattice”, “green”)” inline -&gt; lattice will do this? This does bold & color, but can’t use monospace.\nI can now use inline ‘pkg(“lattice”)’ generating lattice, or also with a citation, pkg(\"lattice\", cite=TRUE) -&gt; lattice (Sarkar, 2024). Can also refer to the matlib package (Friendly et al., 2024), including “package” between the name and citation.\n\nfixed-width : is used in program listings as well as in text to refer to variable and function names, R statement elements and keywords.\nR code in program listings and output is presented in monospaced (typewriter) font, fira mono\nfixed-width italic : isn’t used yet, but probably should be.\n\nFor R functions in packages, we use the notation package::function(), for example: car::Anova() to identify where those functions are defined\n\n\n\n\n\nAbbott, E. A. (1884). Flatland: A romance of many dimensions. Buccaneer Books.\n\n\nAdler, D., & Murdoch, D. (2023). Rgl: 3D visualization using OpenGL. https://CRAN.R-project.org/package=rgl\n\n\nCajori, F. (1926). Origins of fourth dimension concepts. The American Mathematical Monthly, 33(8), 397–406. https://doi.org/10.1080/00029890.1926.11986607\n\n\nCotton, R. (2013). Learning R. O’Reilly Media.\n\n\nFishkeller, M. A., Friedman, J. H., & Tukey, J. W. (1974). PRIM-9, an interactive multidimensional data display and analysis system. Proceedings of the Pacific ACM Regional Conference.\n\n\nFox, J. (2021). A mathematical primer for social statistics (2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878835\n\n\nFox, J., & Weisberg, S. (2018). An R companion to applied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix functions for teaching and learning linear algebra and multivariate statistics. https://github.com/friendly/matlib\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034\n\n\nGalton, F. (1863). Meteorographica, or methods of mapping the weather. Macmillan. http://www.mugu.com/galton/books/meteorographica/index.htm\n\n\nMatloff, N. (2011). The art of R programming: A tour of statistical software design. No Starch Press.\n\n\nReaven, G. M., & Miller, R. G. (1968). Study of the relationship between glucose and insulin responses to an oral glucose load in man. Diabetes, 17(9), 560–569. https://doi.org/10.2337/diab.17.9.560\n\n\nReaven, G. M., & Miller, R. G. (1979). An attempt to define the nature of chemical diabetes using a multidimensional analysis. Diabetologia, 16, 17–24.\n\n\nSarkar, D. (2024). Lattice: Trellis graphics for r. https://lattice.r-forge.r-project.org/\n\n\nTeetor, P. (2011). R cookbook. O’Reilly Media.\n\n\nWickham, H. (2014). Advanced R. Chapman and Hall/CRC.\n\n\nXie, Y. (2021). Animation: A gallery of animations in statistics and utilities to create animations. https://yihui.org/animation/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "",
    "text": "In his famous TV series, Cosmos, Carl Sagan provides an intriguing video presentation Flatland and the 4th dimension. However, as far back as 1754 (Cajori, 1926), the idea of adding a fourth dimension appears in Jean le Rond d’Alembert’s “Dimensions”, and one realization of a four-dimensional object is a tesseract, shown in Figure 2.↩︎\nPRIM-9 is an acronym for Picturing, Rotation, Isolation and Masking in up to 9 dimensions. These operations are fundamental to interactive and dynamic data visualization.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Multivariate vs. multivariable methods\nIn this era of multivitamins, multitools, multifactor authentication and even the multiverse, it is well to understand the distinction between multivariate and multivariable methods as these terms are generally used and as I use them here in relation to statistical methods and data visualization. The distinction is simple:",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#multivariate-vs.-multivariable-methods",
    "href": "01-intro.html#multivariate-vs.-multivariable-methods",
    "title": "1  Introduction",
    "section": "",
    "text": "multivariate \\(\\ne\\) multivariable\n\n\n\nMultivariate methods for linear models such as multivariate regression have more than one dependent, response or outcome variable. Other multivariate methods such as principal components analysis or factor analysis treat all variables on an equal footing.\nMultivariable methods have a single dependent variable and more than one independent variables or covariates.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#why-use-a-multivariate-design",
    "href": "01-intro.html#why-use-a-multivariate-design",
    "title": "1  Introduction",
    "section": "\n1.2 Why use a multivariate design",
    "text": "1.2 Why use a multivariate design\nA particular research outcome (e.g., depression, neuro-cognitive functioning, academic achievement, self-concept, attention deficit hyperactivity disorders) might take on a multivariate form if it has several observed measurement scales or related aspects by which it is quantified, or if there are multiple theoretically distinct outcomes that should be assessed in conjunction with each other (e.g., using depression, generalized anxiety, and stress inventories to model overall happiness). In this situation, the primary concern of the researcher is to ascertain the impact of potential predictors on two or more response variables simultaneously.\nFor example, if academic achievement is measured for adolescents by their reading, mathematics, science, and history scores, the following questions are of interest:\n\nDo predictors such as parent encouragement, socioeconomic status and school environmental variables affect all of these outcomes?\nDo they affect them in the same or different ways?\nHow many different aspects of academic achievement can be distinguished in the predictors? Equivalently, is academic achievement unidimensional or multidimensional in relation to the predictors?\n\nSimilarly, if psychiatric patients in various diagnostic categories are measured on a battery of tests related to social skills and cognitive functioning, we might want to know:\n\nWhich measures best discriminate among the diagnostic groups?\nWhich measures are most predictive of positive outcomes?\nFurther, how are the relationships between the outcomes affected by the predictors?\n\nSuch questions obviously concern more than just the separate univariate relations of each response to the predictors. Equally, or perhaps more importantly, are questions of how the response variables are predicted jointly.\n\n\n\n\n\n\nSEM\n\n\n\nStructural equation modeling (SEM) offers another route to explore and analyze the relationships among multiple predictors and multiple responses. They have the advantage of being able to test potentially complex systems of linear equations in very flexible ways; however, these methods are often far removed from data analysis per se and except for path diagrams offer little in the way of visualization methods to aid in understanding and communicating the results. The graphical methods we describe here can also be useful in a SEM context.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#linear-models-univariate-to-multivariate",
    "href": "01-intro.html#linear-models-univariate-to-multivariate",
    "title": "1  Introduction",
    "section": "\n1.3 Linear models: Univariate to multivariate",
    "text": "1.3 Linear models: Univariate to multivariate\nFor classical linear models for ANOVA and regression, the step from a univariate model for a single response, \\(y\\), to a multivariate one for a collection of \\(p\\) responses, \\(\\mathbf{y}\\) is conceptually very easy. That’s because the univariate model,\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_q x_q + \\epsilon_i , \\]\nor, in matrix terms,\n\\[\\mathbf{y} = \\mathbf{X} \\; \\mathbf{\\beta} + \\mathbf{\\epsilon}, \\quad\\mbox{   with   }\\quad \\mathbf{u} \\sim \\mathcal{N} (0, \\sigma^2 \\mathbf{I}) ,\\]\ngeneralizes directly to an analogous multivariate linear model (MLM),\n\\[\\mathbf{Y} = [\\mathbf{y_1}, \\mathbf{y_2}, \\dots, \\mathbf{y_p}] = \\mathbf{X} \\; \\mathbf{B} + \\Epsilon \\quad\\mbox{   with   }\\quad \\Epsilon \\sim \\mathcal{N} (\\mathbf{0}, \\mathbf{\\Sigma})\\]\nfor multiple responses (as will be discussed in detail). The design matrix, \\(\\mathbf{X}\\) remains the same, and the vector \\(\\beta\\) of coefficients becomes a matrix \\(\\mathbf{B}\\), with one column for each of the \\(p\\) outcome variables.\nHappily as well, hypothesis tests for the MLM are also straight-forward generalizations of the familiar \\(F\\) and \\(t\\)-tests for univariate response models. Moreover, there is a rich geometry underlying these generalizations  which we can exploit for understanding and visualization.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#visualization-is-harder",
    "href": "01-intro.html#visualization-is-harder",
    "title": "1  Introduction",
    "section": "\n1.4 Visualization is harder",
    "text": "1.4 Visualization is harder\nHowever, with two or more response variables, visualizations for multivariate models are not as simple as they are for their univariate counterparts for understanding the effects of predictors, model parameters, or model diagnostics. Consequently, the results of such studies are often explored and discussed solely in terms of coefficients and significance, and visualizations of the relationships are only provided for one response variable at a time, if at all. This tradition can mask important nuances, and lead researchers to draw erroneous conclusions.\nThe aim of this book is to describe and illustrate some central methods that we have developed over the last ten years that aid in the understanding and communication of the results of multivariate linear models (Friendly, 2007; Friendly & Meyer, 2016). These methods rely on data ellipsoids as simple, minimally sufficient visualizations of variance that can be shown in 2D and 3D plots. As will be demonstrated, the Hypothesis-Error (HE) plot framework applies this idea to the results of multivariate tests of linear hypotheses. \nFurther, in the case where there are more than just a few outcome variables, the important nectar of their relationships to predictors can often be distilled in a multivariate juicer— a projection of the multivariate relationships to the predictors in the low-D space that captures most of the flavor. This idea can be applied using canonical correlation plots and with canonical discriminant HE plots. \n\n\nProjection: The cover image from Hofstadter’s Gödel, Bach and Escher illustrates projection of 3D solids onto each 2D plane.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-problems",
    "href": "01-intro.html#sec-problems",
    "title": "1  Introduction",
    "section": "\n1.5 Problems in understanding and communicating MLM results",
    "text": "1.5 Problems in understanding and communicating MLM results\nIn my consulting practice within the Statistical Consulting Service at York University, I see hundreds of clients each year ranging from advanced undergraduate thesis students, to graduate students and faculty from a variety of fields. Over the last two decades, and across each of these groups, I have noticed an increasing desire to utilize multivariate methods. As researchers are exposed to the utility and power of multivariate tests, they see them as an appealing alternative to running many univariate ANOVAs or multiple regressions for each response variable separately.\nHowever, multivariate analyses are more complicated than such approaches, especially when it comes to understanding and communicating results. Output is typically voluminous, and researchers will often get lost in the numbers. While software (SPSS, SAS and R) make tabular summary displays easy, these often obscure the findings that researchers are most interested in. The most common analytic oversights that we have observed are:\n\nAtomistic data screening: Researchers have mostly learned the assumptions (the Holy Trinity of normality, constant variance and independence) of univariate linear models, but then apply univariate tests (e.g., Shapiro-Wilk) and diagnostic plots (normal QQ plots) to every predictor and every response.\nBonferroni everywhere: Faced with the task of reporting the results for multiple response measures and a collection of predictors for each, a common tendency is to run (and sometimes report) each of the separate univariate response models and then apply a correction for multiple testing. Not only is this confusing and awkward to report, but it is largely unnecessary because the multivariate tests provide protection for multiple testing.\nReverting to univariate visualizations: To display results, SPSS and SAS make some visualization methods available through menu choices or syntax, but usually these are the wrong (or at least unhelpful) choices, in that they generate separate univariate graphs for the individual responses.\n\nThis book to discusses a few essential procedures for multivariate linear models, how their interpretation can be aided through the use of well-crafted (though novel) visualizations, and provides replicable sample code in R to showcase their use in applied behaviorial research. A later section [ref?] provides some practical guidelines for analyzing, visualizing and reporting such models to help avoid these and other problems.\nPackage summary:\n\n1 packages used here: knitr\n\n\n\n\n\n\nFriendly, M. (2007). HE plots for multivariate general linear models. Journal of Computational and Graphical Statistics, 16(2), 421–444. https://doi.org/10.1198/106186007X208407\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html",
    "href": "02-getting_started.html",
    "title": "2  Getting Started",
    "section": "",
    "text": "2.1 Why plot your data?\nAt the time the Farhquhar brothers wrote this pithy aphorism, graphical methods for understanding data had advanced considerably, but were not universally practiced, prompting their complaint.\nThe main graphic forms we use today—the pie chart, line graphs and bar—were invented by William Playfair around 1800 (Playfair, 1786, 1801). The scatterplot arrived shortly after (Herschel, 1833) and thematic maps showing the spatial distributions of social variables (crime, suicides, literacy) were used for the first time to reason about important societal questions (Guerry, 1833) such as “is increased education associated with lower rates of crime?”\nIn the last half of the 18th Century, the idea of correlation was developed (Galton, 1886; Pearson, 1896) and the period, roughly 1860–1890, dubbed the “Golden Age of Graphics (Funkhouser, 1937) became the richest period of innovation and beauty in the entire history of data visualization. During this time there was an incredible development of visual thinking, represented by the work of Charles Joseph Minard, advances in the role of visualization within scientific discovery, as illustrated through Francis Galton, and graphical excellence, embodied in state statistical atlases produced in France and elsewhere. See Friendly (2008); Friendly & Wainer (2021) for this history.\nThis chapter introduces the importance of graphing data through three nearly classic stories with the following themes:",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#sec-why_plot",
    "href": "02-getting_started.html#sec-why_plot",
    "title": "2  Getting Started",
    "section": "",
    "text": "Getting information from a table is like extracting sunlight from a cucumber. Farquhar & Farquhar (1891)\n\n\n\n\n\n\nsummary statistics are not enough: Anscombe’s Quartet demonstrates datasets that are indistinguishable by numerical summary statistics (mean, standard deviation, correlation), but whose relationships are vastly different.\none lousy point can ruin your day: A researcher is mystified by a difference between a correlation for men and women until she plots the data.\nfinding the signal in noise: The story of the US 1970 Draft Lottery shows how a weak, but reliable signal, reflecting bias in a process can be revealed by graphical enhancement and summarization.\n\n\n2.1.1 Anscombe’s Quartet\nIn 1973, Francis Anscombe (Anscombe, 1973) famously constructed a set of four datasets illustrate the importance of plotting the graphs before analyzing and model building, and the effect of unusual observations on fitted models. Now known as Anscombe’s Quartet, these datasets had identical statistical properties: the same means, standard deviations, correlations and regression lines.\nHis purpose was to debunk three notions that had been prevalent at the time:\n\nNumerical calculations are exact, but graphs are coarse and limited by perception and resolution;\nFor any particular kind of statistical data there is just one set of calculations constituting a correct statistical analysis;\nPerforming intricate calculations is virtuous, whereas actually looking at the data is cheating.\n\nThe dataset datasets::anscombe has 11 observations, recorded in wide format, with variables x1:x4 and y1:y4.\n\ndata(anscombe) \nhead(anscombe)\n#&gt;   x1 x2 x3 x4   y1   y2    y3   y4\n#&gt; 1 10 10 10  8 8.04 9.14  7.46 6.58\n#&gt; 2  8  8  8  8 6.95 8.14  6.77 5.76\n#&gt; 3 13 13 13  8 7.58 8.74 12.74 7.71\n#&gt; 4  9  9  9  8 8.81 8.77  7.11 8.84\n#&gt; 5 11 11 11  8 8.33 9.26  7.81 8.47\n#&gt; 6 14 14 14  8 9.96 8.10  8.84 7.04\n\nThe following code transforms this data to long format and calculates some summary statistics for each dataset.\n\nanscombe_long &lt;- anscombe |&gt; \n  pivot_longer(everything(), \n               names_to = c(\".value\", \"dataset\"), \n               names_pattern = \"(.)(.)\"\n  ) |&gt;\n  arrange(dataset)\n\nanscombe_long |&gt;\n  group_by(dataset) |&gt;\n  summarise(xbar      = mean(x),\n            ybar      = mean(y),\n            r         = cor(x, y),\n            intercept = coef(lm(y ~ x))[1],\n            slope     = coef(lm(y ~ x))[2]\n         )\n#&gt; # A tibble: 4 × 6\n#&gt;   dataset  xbar  ybar     r intercept slope\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 1           9  7.50 0.816      3.00 0.500\n#&gt; 2 2           9  7.50 0.816      3.00 0.5  \n#&gt; 3 3           9  7.5  0.816      3.00 0.500\n#&gt; 4 4           9  7.50 0.817      3.00 0.500\n\nAs we can see, all four datasets have nearly identical univariate and bivariate statistical measures. You can only see how they differ in graphs, which show their true natures to be vastly different.\nFigure 2.1 is an enhanced version of Anscombe’s plot of these data, adding helpful annotations to show visually the underlying statistical summaries.\n\n\n\n\n\n\n\nFigure 2.1: Scatterplots of Anscombe’s Quartet. Each plot shows the fitted regression line and a 68% data ellipse representing the correlation between \\(x\\) and \\(y\\).\n\n\n\n\nThis figure is produced as follows, using a single call to ggplot(), faceted by dataset. As we will see later (Section 3.2), the data ellipse (produced by stat_ellipse()) reflects the correlation between the variables.\n\ndesc &lt;- tibble(\n  dataset = 1:4,\n  label = c(\"Pure error\", \"Lack of fit\", \"Outlier\", \"Influence\")\n)\n\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 4) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"red\", linewidth = 1.5) +\n  scale_x_continuous(breaks = seq(0,20,2)) +\n  scale_y_continuous(breaks = seq(0,12,2)) +\n  stat_ellipse(level = 0.5, color=col, type=\"norm\") +\n  geom_label(data=desc, aes(label = label), x=6, y=12) +\n  facet_wrap(~dataset, labeller = label_both) \n\nThe subplots are labeled with the statistical idea they reflect:\n\ndataset 1: Pure error. This is the typical case with well-behaved data. Variation of the points around the line reflect only measurement error or unreliability in the response, \\(y\\).\ndataset 2: Lack of fit. The data is clearly curvilinear, and would be very well described by a quadratic, y ~ poly(x, 2). This violates the assumption of linear regression that the fitted model has the correct form.\ndataset 3: Outlier. One point, second from the right, has a very large residual. Because this point is near the extreme of \\(x\\), it pulls the regression line towards it, as you can see by imagining a line through the remaining points.\ndataset 4: Influence. All but one of the points have the same \\(x\\) value. The one unusual point has sufficient influence to force the regression line to fit it exactly.\n\nOne moral from this example:\n\nLinear regression only “sees” a line. It does its’ best when the data are really linear. Because the line is fit by least squares, it pulls the line toward discrepant points to minimize the sum of squared residuals.\n\n\n\n\n\n\n\nDatasaurus Dozen\n\n\n\nThe method Anscombe used to compose his quartet is unknown, but it turns out that that there is a method to construct a wider collection of datasets with identical statistical properties. After all, in a bivariate dataset with \\(n\\) observations, the correlation has \\((n-2)\\) degrees of freedom, so it is possible to choose \\(n-2\\) of the \\((x, y)\\) pairs to yield any given value. As it happens, it is also possible to create any number of datasets with the same means, standard deviations and correlations with nearly any shape you like — even a dinosaur!\nThe Datasaurus Dozen was first publicized by Alberto Cairo in a blog post and are available in the datasauRus package (Davies et al., 2022). As shown in Figure 2.2, the sets include a star, cross, circle, bullseye, horizontal and vertical lines, and, of course the “dino”. The method (Matejka & Fitzmaurice, 2017) uses simulated annealing, an iterative process that perturbs the points in a scatterplot, moving them towards a given shape while keeping the statistical summaries close to the fixed target value.\nThe datasauRus package just contains the datasets, but a general method, called statistical metamers, for producing such datasets has been described by Elio Campitelli and implemented in the metamer package.\n\n\n\n\n\n\n\n\n\nFigure 2.2: Animation of the Dinosaur Dozen datasets. Source: https://youtu.be/It4UA75z_KQ\n\n\n\n\n\n\n\n\n\n\nQuartets\n\n\n\nThe essential idea of a statistical “quartet” is to illustrate four quite different datasets or circumstances that seem superficially the same, but yet are paradoxically very different when you look behind the scenes. For example, in the context of causal analysis Gelman et al. (2023), illustrated sets of four graphs, within each of which all four represent the same average (latent) causal effect but with much different patterns of individual effects; McGowan et al. (2023) provide another illustration with four seemingly identical data sets each generated by a different causal mechanism. As an example of machine learning models, Biecek et al. (2023), introduced the “Rashamon Quartet”, a synthetic dataset for which four models from different classes (linear model, regression tree, random forest, neural network) have practically identical predictive performance. In all cases, the paradox is solved when their visualization reveals the distinct ways of understanding structure in the data. The quartets package contains these and other variations on this theme.\n\n\n\n\n2.1.2 One lousy point can ruin your day\nIn the mid 1980s, a consulting client had a strange problem.1 She was conducting a study of the relation between body image and weight preoccupation in exercising and non-exercising people (Davis, 1990). As part of the design, the researcher wanted to know if self-reported weight could be taken as a reliable indicator of true weight measured on a scale. It was expected that the correlations between reported and measured weight should be close to 1.0, and the slope of the regression lines for men and women should also be close to 1.0. The dataset is car::Davis.\nShe was therefore very surprise to see the following numerical results: For men, the correlation was nearly perfect, but not so for women. \n\ndata(Davis, package=\"carData\")\nDavis &lt;- Davis |&gt;\n  drop_na()          # drop missing cases\nDavis |&gt;\n  group_by(sex) |&gt;\n  select(sex, weight, repwt) |&gt;\n  summarise(r = cor(weight, repwt))\n#&gt; # A tibble: 2 × 2\n#&gt;   sex       r\n#&gt;   &lt;fct&gt; &lt;dbl&gt;\n#&gt; 1 F     0.501\n#&gt; 2 M     0.979\n\nSimilarly, the regression lines showed the expected slope for men, but that for women was only 0.26.\n\nDavis |&gt;\n  nest(data = -sex) |&gt;\n  mutate(model = map(data, ~ lm(repwt ~ weight, data = .)),\n         tidied = map(model, tidy)) |&gt;\n  unnest(tidied) |&gt;\n  filter(term == \"weight\") |&gt;\n  select(sex, term, estimate, std.error)\n#&gt; # A tibble: 2 × 4\n#&gt;   sex   term   estimate std.error\n#&gt;   &lt;fct&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 M     weight    0.990    0.0229\n#&gt; 2 F     weight    0.262    0.0459\n\n“What could be wrong here?”, the client asked. The consultant replied with the obvious question:\n\nDid you plot your data?\n\nThe answer turned out to be one discrepant point, a female, whose measured weight was 166 kg (366 lbs!). This single point exerted so much influence that it pulled the fitted regression line down to a slope of only 0.26.\n\n# shorthand to position legend inside the figure\nlegend_inside &lt;- function(position) {\n  theme(legend.position = \"inside\",\n        legend.position.inside = position)\n}\n\nDavis |&gt;\n  ggplot(aes(x = weight, y = repwt, \n             color = sex, shape = sex, linetype = sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  labs(x = \"Measured weight (kg)\", y = \"Reported weight (kg)\") +\n  scale_linetype_manual(values = c(F = \"longdash\", M = \"solid\")) +\n  legend_inside(c(.8, .8))\n\n\n\n\n\n\nFigure 2.3: Regression for Davis’ data on reported weight and measures weight for men and women. Separate regression lines, predicting reported weight from measured weight are shown for males and females. One highly unusual point is highlighted.\n\n\n\n\nIn this example, it was arguable that \\(x\\) and \\(y\\) axes should be reversed, to determine how well measured weight can be predicted from reported weight. In ggplot this can easily be done by reversing the x and y aesthetics.\n\nDavis |&gt;\n  ggplot(aes(y = weight, x = repwt, color = sex, shape=sex)) +\n  geom_point(size = ifelse(Davis$weight==166, 6, 2)) +\n  labs(y = \"Measured weight (kg)\", x = \"Reported weight (kg)\") +\n    geom_smooth(method = \"lm\", formula = y~x, se = FALSE) +\n  legend_inside(c(.8, .8))\n\n\n\n\n\n\nFigure 2.4: Regression for Davis’ data on reported weight and measures weight for men and women. Separate regression lines, predicting measured weight from reported weight are shown for males and females. The highly unusual point no longer has an effect on the fitted lines.\n\n\n\n\nIn Figure 2.4, this discrepant observation again stands out like a sore thumb, but it makes very little difference in the fitted line for females. The reason is that this point is well within the range of the \\(x\\) variable (repwt). To impact the slope of the regression line, an observation must be unusual in_both_ \\(x\\) and \\(y\\). We take up the topic of how to detect influential observations and what to do about them in Chapter 6.\nThe value of such plots is not only that they can reveal possible problems with an analysis, but also help identify their reasons and suggest corrective action. What went wrong here? Examination of the original data showed that this person switched the values, recording her reported weight in the box for measured weight and vice versa.\n\n2.1.3 Shaken, not stirred: The 1970 Draft Lottery\n\nAlthough we often hear that data speak for themselves, their voices can be soft and sly.—Frederick Mosteller (1983), Beginning Statistics with Data Analysis, p. 234.\n\nThe power of graphics is particularly evident when data contains a weak signal embedded in a field of noise. To the casual glance, there may seem to be nothing going on, but the signal can be made apparent in an incisive graph.\nA dramatic example of this occurred in 1969 when the U.S. military conducted a lottery, the first since World War II, to determine which young men would be called up to serve in the Vietnam War for 1970. The U.S. Selective Service had devised a system to rank eligible men according to a random drawing of their birthdays. There were 366 blue plastic capsules containing birth dates placed in a transparent glass container and drawn by hand to assign ranked order-of-call numbers to all men within the 18-26 age range.\n\n\n\n\nCongressman Alexander Pirnie (R-NY) drawing the first capsule for the Selective Service draft, Dec 1, 1969. Source: https://en.wikipedia.org/wiki/Draft_lottery_(1969)#/media/File:1969_draft_lottery_photo.jpg\n\n\n\nIn an attempt to make the selection process also transparent, the proceeding was covered on radio, TV and film and the dates posted in order on a large display board. The first capsule—drawn by Congressman Alexander Pirnie (R-NY) of the House Armed Services Committee—contained the date September 14, so all men born on September 14 in any year between 1944 and 1950 were assigned lottery number 1, and would be drafted first. April 24 was drawn next, then December 30, February 14, and so on until June 8, selected last. At the time of the drawing, US officials stated that those with birthdays drawn in the first third would almost certainly be drafted, while those in the last third would probably avoid the draft (Fienberg, 1971).\nI watched this unfold with considerable interest because I was eligible for the Draft that year. I was dismayed when my birthday, May 7, came up ranked 35. Ugh!\nThe data, from the official Selective Service listing are contained in the dataset vcdExtra::Draft1970, ordered by Month and birthdate (Day), with Rank as the order in which the birthdates were drawn.\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\ndata(Draft1970, package = \"vcdExtra\")\ndplyr::glimpse(Draft1970)\n#&gt; Rows: 366\n#&gt; Columns: 3\n#&gt; $ Day   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n#&gt; $ Rank  &lt;int&gt; 305, 159, 251, 215, 101, 224, 306, 199, 194, 325, 32…\n#&gt; $ Month &lt;ord&gt; Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Ja…\n\nA basic scatterplot, slightly prettified, is shown in Figure 2.5. The points are colored by month, and month labels are shown at the bottom.\n\nShow the code# make markers for months at their mid points\nmonths &lt;- data.frame(\n  month =unique(Draft1970$Month),\n  mid = seq(15, 365-15, by = 30))\n\nggplot2:: theme_set(theme_bw(base_size = 16))\ngg &lt;- ggplot(Draft1970, aes(x = Day, y = Rank)) +\n  geom_point(size = 2.5, shape = 21, \n             alpha = 0.3, \n             color = \"black\", \n             aes(fill=Month)\n  ) +\n  scale_fill_manual(values = rainbow(12)) +\n  geom_text(data=months, aes(x=mid, y=0, label=month), nudge_x = 5) +\n  geom_smooth(method = \"lm\", formula = y ~ 1,\n              col = \"black\", fill=\"grey\", linetype = \"dashed\", alpha=0.6) +\n  labs(x = \"Day of the year\",\n       y = \"Lottery rank\") +\n  theme(legend.position = \"none\") \ngg\n\n\n\n\n\n\nFigure 2.5: Basic scatterplot of 1970 Draft Lottery data plotting rank order of selection against birthdates in the year. Points are colored by month. The horizontal line is at the average rank.\n\n\n\n\nThe ranks do seem to be essentially random. Is there any reason to suspect a flaw in the selection process, as I firmly hoped at the time?\nIf you stare at the graph in @fig-draft-gg1 long enough, you can make out a sparsity of points in the\nupper right corner and also in the lower left corner compared to the opposite corners.\nVisual smoothers\nFitting a linear regression line or a smoothed (loess) curve can bring out the signal lurking in the background of a field of nearly random points. Figure 2.6 shows a definite trend to lower ranks for birthdays toward the end of the year. Those born earlier in the year were more likely to be given lower ranks, calling them up sooner for the draft.\n\nShow the codeggplot(Draft1970, aes(x = Day, y = Rank)) +\n  geom_point(size = 2.5, shape = 21, \n             alpha = 0.3, \n             color = \"black\", \n             aes(fill=Month)) +\n  scale_fill_manual(values = rainbow(12)) +\n  geom_smooth(method = \"lm\", formula = y~1,\n              se = FALSE,\n              col = \"black\", fill=\"grey\", linetype = \"dashed\", alpha=0.6) +\n  geom_smooth(method = \"loess\", formula = y~x,\n              color = \"blue\", se = FALSE,\n              alpha=0.25) +\n  geom_smooth(method = \"lm\", formula = y~x,\n              color = \"darkgreen\",\n              fill = \"darkgreen\", \n              alpha=0.25) +\n  geom_text(data=months, aes(x=mid, y=0, label=month), nudge_x = 5) +\n  labs(x = \"Day of the year\",\n       y = \"Lottery rank\") +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\nFigure 2.6: Enhanced scatterplot of 1970 Draft Lottery data adding a linear regression line and loess smooth.\n\n\n\n\nIs this a real effect? Even though the points seem to be random over the year, linear regression of Rank on Day shows a highly significant negative effect even though the correlation2 is small (\\(r = -0.226\\)). The slope, -0.226, means that for each additional day in the year the lottery rank decreases about 1/4 toward the front of the draft line; that’s nearly 7 ranks per month.\n\ndraft.mod &lt;- lm(Rank ~ Day, data=Draft1970)\nwith(Draft1970, cor(Day, Rank))\n#&gt; [1] -0.226\ncoef(draft.mod)\n#&gt; (Intercept)         Day \n#&gt;     224.913      -0.226\n\nSo, smoothing the data, using either the linear regression line or a nonparametric smoother is one important technique for seeing a weak signal in a noisy background.\nStatistical summaries\nAnother way to enhance the signal-to-noise ratio of a graph is to plot summaries of the messy data points. For example, you might make boxplots of the ranks by month, or calculate and plot the mean or median rank by month and plot those together with some indication of variability within month.\nFigure 2.7 plots the average Rank for each month with error bars showing the mean \\(\\pm 1\\) standard errors against the average Day. The message of rank decreasing nearly linearly with month is now more dramatic. The correlation between the means is \\(r = -0.867\\).\n\nCodemeans &lt;- Draft1970 |&gt;\n  group_by(Month) |&gt;\n  summarize(Day = mean(Day),\n            se = sd(Rank/ sqrt(n())),\n            Rank = mean(Rank)) \n\nggplot(aes(x = Day, y = Rank), data=means) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", formula = y~x,\n              color = \"blue\", fill = \"blue\", alpha = 0.1) +\n  geom_errorbar(aes(ymin = Rank-se, ymax = Rank+se), width = 8) +\n  geom_text(data=months, aes(x=mid, y=0, label=month), nudge_x = 5) +\n  labs(x = \"Average day of the year\",\n       y = \"Average lottery rank\")\n\n\n\n\n\n\nFigure 2.7: Plot of the average rank per month with \\(\\pm 1\\) standard error bars. The line shows the least squares regression line, treating months as equally spaced.\n\n\n\n\nThe visual impression of a linearly decreasing trend in lottery rank is much stronger in Figure 2.7 than in Figure 2.6 for two reasons:\n\nReplacing the data points with their means strengthens the signal in relation to noise.\nThe narrower vertical range (100–250) in the plot of means makes the slope of the line appear steeper. (However, the correlation of the means, \\(r = -0.231\\) is nearly the same as the correlation of the data points.)\nWhat happened here?\nPrevious lotteries carried out by drawing capsules from a container had occasionally suffered the embarrassment that an empty capsule was selected because of vigorous mixing (Fienberg, 1971). So for the 1970 lottery, the birthdate capsules were put in cardboard boxes, one for each month and these were carefully emptied into the glass container in order of month: Jan., Feb., … Dec., gently shaken in atop the pile already there. All might have been well had the persons drawing the capsules put their hand in truly randomly, but generally they picked from toward the top of the container. Consequently, those born later in the year had a greater chance of being picked earlier.\nThere was considerable criticism of this procedure once the flaw had been revealed by analyses such as described here. In the following year, the Selective Service called upon the National Bureau of Standards to devise a better procedure. In 1971 they used two drums, one with the dates of the year and another with the rank numbers 1-366. As a date capsule was drawn randomly from the first drum, another from the numbers drum was picked simultaneously, giving a doubly-randomized sequence.\nOf course, if they had R, the entire process could have been done using sample():\n\nset.seed(42)\ndate = seq(as.Date(\"1971-01-01\"), as.Date(\"1971-12-31\"), by=\"+1 day\")\nrank = sample(seq_along(date))\ndraft1971 &lt;- data.frame(date, rank)\n\nhead(draft1971, 4)\n#&gt;         date rank\n#&gt; 1 1971-01-01   49\n#&gt; 2 1971-01-02  321\n#&gt; 3 1971-01-03  153\n#&gt; 4 1971-01-04   74\ntail(draft1971, 4)\n#&gt;           date rank\n#&gt; 362 1971-12-28  247\n#&gt; 363 1971-12-29    8\n#&gt; 364 1971-12-30  333\n#&gt; 365 1971-12-31  132\n\nAnd, what would have happened to me and all others born on a May 7th, if they did it this way? My lottery rank would have 274!\n\nme &lt;- as.Date(\"1971-05-07\")\ndraft1971[draft1971$date == me,]\n#&gt;           date rank\n#&gt; 127 1971-05-07  274",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#plots-for-data-analysis",
    "href": "02-getting_started.html#plots-for-data-analysis",
    "title": "2  Getting Started",
    "section": "\n2.2 Plots for data analysis",
    "text": "2.2 Plots for data analysis\nVisualization methods take an enormous variety of forms, but it is useful to distinguish several broad categories according to their use in data analysis:\n\ndata plots : primarily plot the raw data, often with annotations to aid interpretation (regression lines and smooths, data ellipses, marginal distributions)\nreconnaissance plots : with more than a few variables, reconnaissance plots provide a high-level, bird’s-eye overview of the data, allowing you to see patterns that might not be visible in a set of separate plots. Some examples are scatterplot matrices, showing all bivariate plots of variables in a dataset; correlation diagrams, using visual glyphs to represent the correlations between all pairs of variables and “trellis” or faceted plots that show how a focal relation of one or more variables differs across values of other variables.\nmodel plots : plot the results of a fitted model, such as a regression line or curve to show uncertainty, or a regression surface in 3D, or a plot of coefficients in model together with confidence intervals. Other model plots try to take into account that a fitted model may involve more variables than can be shown in a static 2D plot. Some examples of these are added variable plots, and marginal effect plots, both of which attempt to show the net relation of two focal variables, controlling or adjusting for other variables in a model.\ndiagnostic plots : indicating potential problems with the fitted model. These include residual plots, influence plots, plots for testing homogeneity of variance and so forth.\ndimension reduction plots : plot representations of the data into a space of fewer dimensions than the number of variables in the dataset. Simple examples include principal components analysis (PCA) and the related biplots, and multidimensional scaling (MDS) methods.\n\nWe give some more details and a few examples in the sections that follow.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#data-plots",
    "href": "02-getting_started.html#data-plots",
    "title": "2  Getting Started",
    "section": "\n2.3 Data plots",
    "text": "2.3 Data plots\nData plots portray the data in a space where the coordinate axes are the observed variables.\n\n1D plots include line plots, histograms and density estimates.\n2D plots are most often scatterplots, but contour plots or hex-binned plots are also useful when the sample size is large.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#model-plots",
    "href": "02-getting_started.html#model-plots",
    "title": "2  Getting Started",
    "section": "\n2.4 Model plots",
    "text": "2.4 Model plots\nModel plots show the fitted or predicted values from a statistical model and provide visual summaries…",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#diagnostic-plots",
    "href": "02-getting_started.html#diagnostic-plots",
    "title": "2  Getting Started",
    "section": "\n2.5 Diagnostic plots",
    "text": "2.5 Diagnostic plots",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#principles-of-graphical-display",
    "href": "02-getting_started.html#principles-of-graphical-display",
    "title": "2  Getting Started",
    "section": "\n2.6 Principles of graphical display",
    "text": "2.6 Principles of graphical display\n[This could be a separate chapter]\n\nCriteria for assessing graphs: communication goals\nEffective data display:\n\nMake the data stand out\nMake graphical comparison easy\nEffect ordering: For variables and unordered factors, arrange them according to the effects to be seen\n\n\nVisual thinning: As the data becomes more complex, focus more on impactful summaries\n\nPackage summary\n\n12 packages used here: broom, dplyr, forcats, ggplot2, knitr, lubridate, purrr, readr, stringr, tibble, tidyr, tidyverse\n\n\n\n\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. The American Statistician, 27, 17–21.\n\n\nBiecek, P., Baniecki, H., Krzyzinski, M., & Cook, D. (2023). Performance is not enough: A story of the rashomon’s quartet. https://arxiv.org/abs/2302.13356\n\n\nDavies, R., Locke, S., & D’Agostino McGowan, L. (2022). datasauRus: Datasets from the datasaurus dozen. https://CRAN.R-project.org/package=datasauRus\n\n\nDavis, C. (1990). Body image and weight preoccupation: A comparison between exercising and non-exercising women. Appetite, 16(1), 84. https://doi.org/10.1016/0195-6663(91)90115-9\n\n\nFarquhar, A. B., & Farquhar, H. (1891). Economic and industrial delusions: A discourse of the case for protection. Putnam.\n\n\nFienberg, S. E. (1971). Randomization and social affairs: The 1970 draft lottery. Science, 171, 255–261.\n\n\nFriendly, M. (2008). The Golden Age of statistical graphics. Statistical Science, 23(4), 502–535. https://doi.org/10.1214/08-STS268\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034\n\n\nFunkhouser, H. G. (1937). Historical development of the graphical representation of statistical data. Osiris, 3(1), 269–405. http://tinyurl.com/32ema9\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGelman, A., Hullman, J., & Kennedy, L. (2023). Causal quartets: Different ways to attain the same average treatment effect. http://www.stat.columbia.edu/~gelman/research/unpublished/causal_quartets.pdf\n\n\nGuerry, A.-M. (1833). Essai sur la statistique morale de la France. Crochard.\n\n\nHerschel, J. F. W. (1833). On the investigation of the orbits of revolving double stars: Being a supplement to a paper entitled \"micrometrical measures of 364 double stars\". Memoirs of the Royal Astronomical Society, 5, 171–222.\n\n\nMatejka, J., & Fitzmaurice, G. (2017, May). Same stats, different graphs. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3025453.3025912\n\n\nMcGowan, L. D., Gerke, T., & Barrett, M. (2023). Causal inference is not just a statistics problem. Journal of Statistics and Data Science Education, 1–9. https://doi.org/10.1080/26939169.2023.2276446\n\n\nPearson, K. (1896). Contributions to the mathematical theory of evolution—III, regression, heredity and panmixia. Philosophical Transactions of the Royal Society of London, 187, 253–318.\n\n\nPlayfair, W. (1786). Commercial and political atlas: Representing, by copper-plate charts, the progress of the commerce, revenues, expenditure, and debts of england, during the whole of the eighteenth century. Debrett; Robinson;; Sewell. http://ucpj.uchicago.edu/Isis/journal/demo/v000n000/000000/000000.fg4.html\n\n\nPlayfair, W. (1801). Statistical breviary; shewing, on a principle entirely new, the resources of every state and kingdom in Europe. Wallis.",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-getting_started.html#footnotes",
    "href": "02-getting_started.html#footnotes",
    "title": "2  Getting Started",
    "section": "",
    "text": "This story is told apocryphally. The consulting client actually did plot the data, but needed help making better graphs.↩︎\nBecause both days of the year and rank in the lottery are the integers, 1 to 366, the Pearson correlation and Spearman rank order correlation are identical.↩︎",
    "crumbs": [
      "Orienting Ideas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html",
    "href": "03-multivariate_plots.html",
    "title": "3  Plots of Multivariate Data",
    "section": "",
    "text": "3.1 Bivariate summaries\nThe basic scatterplot is the workhorse of multivariate data visualization, showing how one variable, \\(y\\), often an outcome to be explained by or varies with another, \\(x\\). It is a building block for many useful techniques, so it is helpful to understand how it can be used as a tool for thinking in a wider, multivariate context.\nThe essential idea is that we can start with a simple version of the scatterplot and add annotations to show interesting features more clearly. We consider the following here:\nExample: Academic salaries\nLet’s start with data on the academic salaries of faculty members collected at a U.S. college for the purpose of assessing salary differences between male and female faculty members, and perhaps address anomalies in compensation. The dataset carData::Salaries gives data on nine-month salaries and other variables for 397 faculty members in the 2008-2009 academic year.\ndata(Salaries, package = \"carData\")\nstr(Salaries)\n#&gt; 'data.frame':  397 obs. of  6 variables:\n#&gt;  $ rank         : Factor w/ 3 levels \"AsstProf\",\"AssocProf\",..: 3 3 1 3 3 2 3 3 3 3 ...\n#&gt;  $ discipline   : Factor w/ 2 levels \"A\",\"B\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;  $ yrs.since.phd: int  19 20 4 45 40 6 30 45 21 18 ...\n#&gt;  $ yrs.service  : int  18 16 3 39 41 6 23 45 20 18 ...\n#&gt;  $ sex          : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 1 ...\n#&gt;  $ salary       : int  139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ...\nThe most obvious, but perhaps naive, predictor of salary is years.since.phd. For simplicity, I’ll refer to this as years of “experience.” Before looking at differences between males and females, we would want consider faculty rank (related also to yrs.service) and discipline, recorded here as \"A\" (“theoretical” departments) or \"B\" (“applied” departments). But, for a basic plot, we will ignore these for now to focus on what can be learned from plot annotations.\nlibrary(ggplot2)\ngg1 &lt;- ggplot(Salaries, \n       aes(x = yrs.since.phd, y = salary)) +\n  geom_jitter(size = 2) +\n  scale_y_continuous(labels = scales::dollar_format(\n    prefix=\"$\", scale = 0.001, suffix = \"K\")) +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") \n\ngg1 + geom_rug(position = \"jitter\", alpha = 1/4)\n\n\n\n\n\n\nFigure 3.1: Naive scatterplot of Salary vs. years since PhD, ignoring other variables, and without graphical annotations.\nThere is quite a lot we can see “just by looking” at this simple plot, but the main things are:",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#sec-bivariate_summaries",
    "href": "03-multivariate_plots.html#sec-bivariate_summaries",
    "title": "3  Plots of Multivariate Data",
    "section": "",
    "text": "Smoothers: Showing overall trends, perhaps in several forms, as visual summaries such as fitted regression lines or curves and nonparametric smoothers.\n\nStratifiers: Using color, shape or other features to identify subgroups; more generally, conditioning on other variables in multi-panel displays;\n\nData ellipses: A compact 2D visual summary of bivariate linear relations and uncertainty assuming normality; more generally, contour plots of bivariate density.\n\n\n\n\n\n\n\n\n\nSalary increases generally from 0 - 40 years since the PhD, but then maybe begins to drop off (partial retirement?);\nVariability in salary increases among those with the same experience, a “fan-shaped” pattern that signals a violation of homogeneity of variance in simple regression;\nData beyond 50 years is thin, but there are some quite low salaries there. Adding rug plots to the X and Y axes is a simple but effective way to show the marginal distributions of the observations. Jitter and transparency helps to avoid overplotting due to discrete values.\n\n\n3.1.1 Smoothers\nSmoothers are among the most useful graphical annotations you can add to such plots, giving a visual summary of how \\(y\\) changes with \\(x\\). The most common smoother is a line showing the linear regression for \\(y\\) given \\(x\\), expressed in math notation as \\(\\mathbb{E} (y | x) = b_0 + b_1 x\\). If there is doubt that a linear relation is an adequate summary, you can try a quadratic or other polynomial smoothers.\nIn ggplot2, these are easily added to a plot using geom_smooth() with method = \"lm\", and a model formula, which (by default) is y ~ x for a linear relation or y ~ poly(x, k) for a polynomial of degree \\(k\\).\n\nCodegg1 + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\", \n              color = \"red\", fill= \"pink\",\n              linewidth = 2) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x,2)\", \n              color = \"darkgreen\", fill = \"lightgreen\",\n              linewidth = 2) \n\n\n\n\n\n\nFigure 3.2: Scatterplot of Salary vs. years since PhD, showing linear and quadratic smooths with 95% confidence bands.\n\n\n\n\n\nThis serves to highlight some of our impressions from the basic scatterplot shown in Figure 3.1, making them more apparent. And that’s precisely the point: the regression smoother draws attention to a possible pattern that we can consider as a visual summary of the data. You can think of this as showing what a linear (or quadratic) regression “sees” in the data. Statistical tests  can help you decide if there is more evidence for a quadratic fit compared to the simpler linear relation. \nIt is useful to also show some indication of uncertainty (or inversely, precision) associated with the predicted values. Both the linear and quadratic trends are shown in Figure 3.2 with 95% pointwise confidence bands.1 These are necessarily narrower in the center of the range of \\(x\\) where there is typically more data; they get wider toward the highest values of experience where the data are thinner.\nNon-parametric smoothers\nThe most generally useful idea is a smoother that tracks an average value, \\(\\mathbb{E} (y | x)\\), of \\(y\\) as \\(x\\) varies across its’ range without assuming any particular functional form, and so avoiding the necessity to choose among y ~ poly(x, 1), or y ~ poly(x, 2), or y ~ poly(x, 3), etc.\nNon-parametric smoothers attempt to estimate \\(\\mathbb{E} (y | x) = f(x)\\) where \\(f(x)\\) is some smooth function. These typically use a collection of weighted local regressions for each \\(x_i\\) within a window centered at that value. In the method called lowess or loess (Cleveland, 1979; Cleveland & Devlin, 1988), a weight function is applied, giving greatest weight to \\(x_i\\) and a weight of 0 outside a window containing a certain fraction, \\(s\\), called span, of the nearest neighbors of \\(x_i\\). The fraction, \\(s\\), is usually within the range \\(1/3 \\le s \\le 2/3\\), and it determines the smoothness of the resulting curve; smaller values produce a wigglier curve and larger values giving a smoother fit (an optimal span can be determined by \\(k\\)-fold cross-validation to minimize a measure of overall error of approximation).\nNon-parametric regression is a broad topic; see Fox (2016), Ch. 18 for a more general treatment including smoothing splines, and Wood (2006) for generalized additive models, fit using method = \"gam\" in ggplot2, which is the default when the largest group has more than 1,000 observations.\nFigure 3.3 shows the addition of a loess smooth to the plot in Figure 3.2, suppressing the confidence band for the linear regression. The loess fit is nearly coincident with the quadratic fit, but has a slightly wider confidence band.\n\nCodegg1 + \n  geom_smooth(method = \"loess\", formula = \"y ~ x\", \n              color = \"blue\", fill = scales::muted(\"blue\"),\n              linewidth = 2) +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE,\n              color = \"red\",\n              linewidth = 2) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x,2)\", \n              color = \"darkgreen\", fill = \"lightgreen\",\n              linewidth = 2) \n\n\n\n\n\n\nFigure 3.3: Scatterplot of Salary vs. years since PhD, adding the loess smooth. The loess smooth curve and confidence band in green is nearly indistinguishable from a quadratic fit in blue.\n\n\n\n\nBut now comes an important question: is it reasonable that academic salary should increase up to about 40 years since the PhD degree and then decline? The predicted salary for someone still working 50 years after earning their degree is about the same as a person at 15 years. What else is going on here?\n\n3.1.2 Stratifiers\nVery often, we have a main relationship of interest, but various groups in the data are identified by discrete factors (like faculty rank and sex, their type of discipline here), or there are quantitative predictors for which the main relation might vary. In the language of statistical models such effects are interaction terms, as in y ~ group + x + group:x, where the term group:x fits a different slope for each group and the grouping variable is often called a moderator variable. Common moderator variables are ethnicity, health status, social class and level of education. Moderators can also be continuous variables as in y ~ x1 + x2 + x1:x2.\nI call these stratifiers, recognizing that we should consider breaking down the overall relation to see whether and how it changes over such “other” variables. Such variables are most often factors, but we can cut a continuous variable into ranges (shingles) and do the same graphically. There are two general stratifying graphical techniques:\n\nGrouping: Identify subgroups in the data by assigning different visual attributes, such as color, shape, line style, etc. within a single plot. This is quite natural for factors; quantitative predictors can be accommodated by cutting their range into ordered intervals. Grouping has the advantage that the levels of a grouping variable can be shown within the same plot, facilitating direct comparison.\nConditioning: Showing subgroups in different plot panels. This has the advantages that relations for the individual groups more easily discerned and one can easily stratify by two (or more) other variables jointly, but visual comparison is more difficult because the eye must scan from one panel to another.\n\n\n\n\n\n\n\nHistory Corner\n\n\n\nRecognition of the roles of visual grouping by factors within a panel and conditioning in multi-panel displays was an important advance in the development of modern statistical graphics. It began at A.T.&T. Bell Labs in Murray Hill, NJ in conjunction with the S language, the mother of R.\nConditioning displays (originally called coplots (Chambers & Hastie, 1991)) are simply a collection of 1D, 2D or 3D plots separate panels for subsets of the data broken down by one or more factors, or, for quantitative variables, subdivided into a factor with several overlapping intervals (shingles). The first implementation was in Trellis plots (Becker et al., 1996; Cleveland, 1985).\nTrellis displays were extended in the lattice package (Sarkar, 2024), which offered:\n\nA graphing syntax similar to that used in statistical model formulas: y ~ x | g conditions the data by the levels of g, with | read as “given”; two or more conditioning are specified as y ~ x | g1 + g2 + ..., with + read as “and”.\n\nPanel functions define what is plotted in a given panel. panel.xyplot() is the default for scatterplots, plotting points, but you can add panel.lmline() for regression lines, latticeExtra::panel.smoother() for loess smooths and a wide variety of others.\n\nThe car package (Fox et al., 2023) supports this graphing syntax in many of its functions. ggplot2 does not; it uses aesthetics (aes()), which map variables in the data to visual characteristics in displays.\n\n\nThe most obvious variable that affects academic salary is rank, because faculty typically get an increase in salary with a promotion that carries through in their future salary. What can we see if we group by rank and fit a separate smoothed curve for each?\nIn ggplot2 thinking, grouping is accomplished simply by adding an aesthetic, such as color = rank. What happens then is that points, lines, smooths and other geom_*() inherit the feature that they are differentiated by color. In the case of geom_smooth(), we get a separate fit for each subset of the data, according to rank.\n\nCode# make some re-useable pieces to avoid repetitions\nscale_salary &lt;-   scale_y_continuous(\n  labels = scales::dollar_format(prefix=\"$\", \n                                 scale = 0.001, \n                                 suffix = \"K\")) \n# position the legend inside the plot\nlegend_pos &lt;- theme(legend.position = \"inside\",\n                    legend.position.inside = c(.1, 0.95), \n                    legend.justification = c(0, 1))\n\nggplot(Salaries, \n       aes(x = yrs.since.phd, y = salary, \n           color = rank, shape = rank)) +\n  geom_point() +\n  scale_salary +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  geom_smooth(aes(fill = rank),\n                  method = \"loess\", formula = \"y ~ x\", \n                  linewidth = 2)  +\n  legend_pos\n\n\n\n\n\n\nFigure 3.4: Scatterplot of Salary vs. years since PhD, grouped by rank.\n\n\n\n\nWell, there is a different story here. Salaries generally occupy separate vertical levels, increasing with academic rank. The horizontal extents of the smoothed curves show their ranges. Within each rank there is some initial increase after promotion, and then some tendency to decline with increasing years. But, by and large, years since the PhD doesn’t make as much difference once we’ve taken academic rank into account.\nWhat about the discipline which is classified, perhaps peculiarly, as “theoretical” vs. “applied”? The values are just \"A\" and \"B\", so I map these to more meaningful labels before making the plot.\n\nCodeSalaries &lt;- Salaries |&gt;\n  mutate(discipline = \n           factor(discipline, \n                  labels = c(\"A: Theoretical\", \"B: Applied\")))\n\nSalaries |&gt;\n  ggplot(aes(x = yrs.since.phd, y = salary, color = discipline)) +\n    geom_point() +\n  scale_salary +\n  geom_smooth(aes(fill = discipline ),\n                method = \"loess\", formula = \"y ~ x\", \n                linewidth = 2) + \n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  legend_pos \n\n\n\n\n\n\nFigure 3.5: Scatterplot of Salary vs. years since PhD, grouped by discipline.\n\n\n\n\nThe story in Figure 3.5 is again different. Faculty in applied disciplines on average earn about 10,000$ more per year on average than their theoretical colleagues.\n\nSalaries |&gt;\n  group_by(discipline) |&gt;\n  summarize(mean = mean(salary)) \n#&gt; # A tibble: 2 × 2\n#&gt;   discipline        mean\n#&gt;   &lt;fct&gt;            &lt;dbl&gt;\n#&gt; 1 A: Theoretical 108548.\n#&gt; 2 B: Applied     118029.\n\nFor both groups, there is an approximately linear relation up to about 30–40 years, but the smoothed curves then diverge into the region where the data is thinner.\nThis result is more surprising than differences among faculty ranks. The effect of annotation with smoothed curves as visual summaries is apparent, and provides a stimulus to think about why these differences (if they are real) exist between theoretical and applied professors, and maybe should theoreticians be paid more!\n\n3.1.3 Conditioning\nThe previous plots use grouping by color to plot the data for different subsets inside the same plot window, making comparison among groups easier, because they can be directly compared along a common vertical scale 2. This gets messy, however, when there are more than just a few levels, or worse—when there are two (or more) variables for which we want to show separate effects. In such cases, we can plot separate panels using the ggplot2 concept of faceting. There are two options: facet_wrap() takes one or more conditioning variables and produces a ribbon of plots for each combination of levels; facet_grid(row ~ col) takes two or more conditioning variables and arranges the plots in a 2D array identified by the row and col variables.\nLet’s look at salary broken down by the combinations of discipline and rank. Here, I chose to stratify using color by rank within each of panels faceting by discipline. Because there is more going on in this plot, a linear smooth is used to represent the trend.\n\nCodeSalaries |&gt;\n  ggplot(aes(x = yrs.since.phd, y = salary, \n             color = rank, shape = rank)) +\n  geom_point() +\n  scale_salary +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  geom_smooth(aes(fill = rank),\n              method = \"lm\", formula = \"y ~ x\", \n              linewidth = 2, alpha = 1/4) +\n  facet_wrap(~ discipline) +\n  legend_pos\n\n\n\n\n\n\nFigure 3.6: Scatterplot of Salary vs. years since PhD, grouped by rank, with separate panels for discipline.\n\n\n\n\nOnce both of these factors are taken into account, there does not seem to be much impact of years of service. Salaries in theoretical disciplines are noticeably greater than those in applied disciplines at all ranks, and there are even greater differences among ranks.\nFinally, to shed light on the question that motivated this example— are there anomalous differences in salary for men and women— we can look at differences in salary according to sex, when discipline and rank are taken into account. To do this graphically, condition by both variables, but use facet_grid(discipline ~ rank) to arrange their combinations in a grid whose rows are the levels of discipline and columns are those of rank. I want to make the comparison of males and females most direct, so I use color = sex to stratify the panels. The smoothed regression lines and error bands are calculated separately for each combination of discipline, rank and sex.\n\nCodeSalaries |&gt;\n  ggplot(aes(x = yrs.since.phd, y = salary, color = sex)) +\n  geom_point() +\n  scale_salary +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  geom_smooth(aes(fill = sex),\n              method = \"lm\", formula = \"y ~ x\",\n              linewidth = 2, alpha = 1/4) +\n  facet_grid(discipline ~ rank) +\n  theme_bw(base_size = 14) + \n  legend_pos\n\n\n\n\n\n\nFigure 3.7: Scatterplot of Salary vs. years since PhD, grouped by sex, faceted by discipline and rank.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#sec-data-ellipse",
    "href": "03-multivariate_plots.html#sec-data-ellipse",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.2 Data Ellipses",
    "text": "3.2 Data Ellipses\nThe data ellipse (Monette, 1990), or concentration ellipse (Dempster, 1969) is a remarkably simple and effective display for viewing and understanding bivariate relationships in multivariate data. The data ellipse is typically used to add a visual summary to a scatterplot, that shows all together the means, standard deviations, correlation, and slope of the regression line for two variables, perhaps stratified by another variable. Under the classical assumption that the data are bivariate normally distributed, the data ellipse is also a sufficient visual summary, in the sense that it captures all relevant features of the data. See Friendly et al. (2013) for a complete discussion of the role of ellipsoids in statistical data visualization.\nIt is based on the idea that in a bivariate normal distribution, the contours of equal probability form a series of concentric ellipses. If the variables were uncorrelated and had the same variances, these would be circles, and Euclidean distance would measure the distance of each observation from the mean. When the variables are correlated, a different measure, Mahalanobis distance is the proper measure of how far a point is from the mean, taking the correlation into account.\n\n\n\n\n\n\n\nFigure 3.8: 2D data with curves of constant distance from the centroid. The blue solid ellipse shows a contour of constant Mahalanobis distance, taking the correlation into account; the dashed red circle is a contour of equal Euclidean distance. Given the data points, Which of the points A and B is further from the mean (X)? Source: Re-drawn from Ou Zhang\n\n\n\n\n\nTo illustrate, Figure 3.8 shows a scatterplot with labels for two points, “A” and “B”. Which is further from the mean, “X”? A contour of constant Euclidean distance, shown by the red dashed circle, ignores the apparent negative correlation, so point “A” is further. The blue ellipse for Mahalanobis distance takes the correlation into account, so point “B” has a greater distance from the mean.\nMathematically, Euclidean (squared) distance for \\(p\\) variables, \\(j = 1, 2, \\dots , p\\), is just a generalization of the square of a univariate standardized (\\(z\\)) score, \\(z^2 = [(y - \\bar{y}) / s]^2\\),\n\\[\nD_E^2 (\\mathbf{y}) = \\sum_j^p z_j^2 = \\mathbf{z}^\\textsf{T}  \\mathbf{z} = (\\mathbf{y} - \\bar{\\mathbf{y}})^\\textsf{T} \\operatorname{diag}(\\mathbf{S})^{-1} (\\mathbf{y} - \\bar{\\mathbf{y}}) \\; ,\n\\] where \\(\\mathbf{S}\\) is the sample variance-covariance matrix, \\(\\mathbf{S} = ({n-1})^{-1} \\sum_{i=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})^\\textsf{T} (\\mathbf{y}_i - \\bar{\\mathbf{y}})\\).\nMahalanobis’ distance takes the correlations into account simply by using the covariances as well as the variances, \\[\nD_M^2 (\\mathbf{y}) = (\\mathbf{y} - \\bar{\\mathbf{y}})^\\mathsf{T} S^{-1} (\\mathbf{y} - \\bar{\\mathbf{y}}) \\; .\n\\tag{3.1}\\]\nIn Equation 3.1, the inverse \\(S^{-1}\\) serves to “divide” the matrix \\((\\mathbf{y} - \\bar{\\mathbf{y}})^\\mathsf{T} (\\mathbf{y} - \\bar{\\mathbf{y}})\\) of squared distances by the variances (and covariances) of the variables, as in the univariate case.\nFor \\(p\\) variables, the data ellipsoid \\(\\mathcal{E}_c\\) of size \\(c\\) is a \\(p\\)-dimensional ellipse, defined as the set of points \\(\\mathbf{y} = (y_1, y_2, \\dots y_p)\\) whose squared Mahalanobis distance, \\(D_M^2 ( \\mathbf{y} )\\) is less than or equal to \\(c^2\\), \\[\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{S}) := \\{ D_M^2 (\\mathbf{y}) \\le c^2 \\} \\; .\n\\] A computational definition recognizes that the boundary of the ellipsoid can be found by transforming a unit sphere \\(\\mathcal{P}\\) centered at the origin, \\(\\mathcal{P} : \\{ \\mathbf{x}^\\textsf{T} \\mathbf{x}= 1\\}\\), by \\(\\mathbf{S}^{1/2}\\) and then shifting that to centroid of the data,\n\\[\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{S}) = \\bar{\\mathbf{y}} \\; \\oplus \\; \\mathbf{S}^{1/2} \\, \\mathcal{P} \\:\\: ,\n\\] where \\(\\mathbf{S}^{1/2}\\) represents a rotation and scaling and the notation \\(\\oplus\\) represents translation to a new centroid, \\(\\bar{\\mathbf{y}}\\) here. The matrix \\(\\mathbf{S}^{1/2}\\) is commonly computed as the Choleski factor of \\(\\mathbf{S}\\). Slightly abusing notation and taking the unit sphere as given (like an identity matrix \\(\\mathbf{I}\\)), we can write the data ellipsoid as simply:\n\\[\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{S}) = \\bar{\\mathbf{y}} \\; \\oplus \\; \\sqrt{\\mathbf{S}} \\:\\: .\n\\tag{3.2}\\]\nWhen \\(\\mathbf{y}\\) is (at least approximately) bivariate normal, \\(D_M^2(\\mathbf{y})\\) has a large-sample \\(\\chi^2_2\\) distribution (\\(\\chi^2\\) with 2 df), so\n\n\n\\(c^2 = \\chi^2_2 (0.68) = 2.28\\) gives a “1 standard deviation bivariate ellipse,” an analog of the standard interval \\(\\bar{y} \\pm 1 s\\), while\n\n\\(c^2 = \\chi^2_2 (0.95) = 5.99 \\approx 6\\) gives a data ellipse of 95% coverage.\n\nIn not-large samples, the radius \\(c\\) of the ellipsoid is better approximated by a multiple of a \\(F_{p, n-p}\\) distribution, becoming \\(c =\\sqrt{ 2 F_{2, n-2}^{1-\\alpha} }\\) in the bivariate case (\\(p=2\\)) for coverage \\(1-\\alpha\\).\n\n3.2.1 Ellipse properties\nThe essential ideas of correlation and regression and their relation to ellipses go back to Galton (1886). Galton’s goal was to predict (or explain) how a heritable trait, \\(Y\\), (e.g., height) of children was related to that of their parents, \\(X\\). He made a semi-graphic table of the frequencies of 928 observations of the average height of father and mother versus the height of their child, shown in Figure 3.9. He then drew smoothed contour lines of equal frequencies and had the wonderful visual insight that these formed concentric shapes that were tolerably close to ellipses.\nHe then calculated summaries, \\(\\text{Ave}(Y | X)\\), and, for symmetry, \\(\\text{Ave}(X | Y)\\), and plotted these as lines of means on his diagram. Lo and behold, he had a second visual insight: the lines of means of (\\(Y | X\\)) and (\\(X | Y\\)) corresponded approximately to the loci of horizontal and vertical tangents to the concentric ellipses. To complete the picture, he added lines showing the major and minor axes of the family of ellipses (which turned out to be the principal components) with the result shown in Figure 3.9.\n\n\n\n\n\n\n\nFigure 3.9: Galton’s 1886 diagram, showing the relationship of height of children to the average of their parents’ height. The diagram is essentially an overlay of a geometrical interpretation on a bivariate grouped frequency distribution, shown as numbers.\n\n\n\n\nFor two variables, \\(x\\) and \\(y\\), the remarkable properties of the data ellipse are illustrated in Figure 3.10, a modern reconstruction of Galton’s diagram.\n\n\n\n\n\n\n\nFigure 3.10: Sunflower plot of Galton’s data on heights of parents and their children (in.), with 40%, 68% and 95% data ellipses and the regression lines of \\(y\\) on \\(x\\) (black) and \\(x\\) on \\(y\\) (grey).\n\n\n\n\n\nThe ellipses have the mean vector \\((\\bar{x}, \\bar{y})\\) as their center.\nThe lengths of arms of the blue dashed central cross show the standard deviations of the variables, which correspond to the shadows of the ellipse covering 40% of the data. These are the bivariate analogs of the standard intervals \\(\\bar{x} \\pm 1 s_x\\) and \\(\\bar{y} \\pm 1 s_y\\).\n\nMore generally, shadows (projections) on the coordinate axes, or any linear combination of them, give any standard interval, \\(\\bar{x} \\pm k s_x\\) and \\(\\bar{y} \\pm k s_y\\). Those with \\(k=1, 1.5, 2.45\\), have bivariate coverage 40%, 68% and 95% respectively, corresponding to these quantiles of the \\(\\chi^2\\) distribution with 2 degrees of freedom, i.e., \\(\\chi^2_2 (.40) \\approx 1^2\\), \\(\\chi^2_2 (.68) \\approx 1.5^2\\), and \\(\\chi^2_2 (.95) \\approx 2.45\\). The shadows of the 68% ellipse are the bivariate analog of a univariate \\(\\bar{x} \\pm 1 s_x\\) interval.\n\n\nThe regression line predicting \\(y\\) from \\(x\\) goes through the points where the ellipses have vertical tangents. The other regression line, predicting \\(x\\) from \\(y\\) goes through the points of horizontal tangency.\nThe correlation \\(r(x, y)\\) is the ratio of the vertical segment from the mean of \\(y\\) to the regression line to the vertical segment going to the top of the ellipse as shown at the right of the figure. It is \\(r = 0.46\\) in this example.\nThe residual standard deviation, \\(s_e = \\sqrt{MSE} = \\sqrt{\\Sigma (y - \\bar{y})^2 / n-2}\\), is the half-length of the ellipse at the mean \\(\\bar{x}\\).\n\nBecause Galton’s values of parent and child height were recorded in class intervals of 1 in., they are shown as sunflower symbols in Figure 3.10, with multiple ‘petals’ reflecting the number of observations at each location. This plot (except for annotations) is constructed using sunflowerplot() and car::dataEllipse() for the ellipses.\n\ndata(Galton, package = \"HistData\")\n\nsunflowerplot(parent ~ child, data=Galton, \n      xlim=c(61,75), \n      ylim=c(61,75), \n      seg.col=\"black\", \n      xlab=\"Child height\", \n      ylab=\"Mid Parent height\")\n\ny.x &lt;- lm(parent ~ child, data=Galton)     # regression of y on x\nabline(y.x, lwd=2)\nx.y &lt;- lm(child ~ parent, data=Galton)     # regression of x on y\ncc &lt;- coef(x.y)\nabline(-cc[1]/cc[2], 1/cc[2], lwd=2, col=\"gray\")\n\nwith(Galton, \n     car::dataEllipse(child, parent, \n         plot.points=FALSE, \n         levels=c(0.40, 0.68, 0.95), \n         lty=1:3)\n    )\n\nFinally, as Galton noted in his diagram, the principal major and minor axes of the ellipse have important statistical properties. Pearson (1901) would later show that their directions are determined by the eigenvectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\dots\\) of the covariance matrix \\(\\mathbf{S}\\) and their radii by the square roots, \\(\\sqrt{\\mathbf{v}_1}, \\sqrt{\\mathbf{v}_1}, \\dots\\) of the corresponding eigenvalues.\n\n3.2.2 R functions for data ellipses\nA number of packages provide functions for drawing data ellipses in a scatterplot, with various features.\n\n\ncar::scatterplot(): uses base R graphics to draw 2D scatterplots, with a wide variety of plot enhancements including linear and non-parametric smoothers (loess, gam), a formula method, e.g., y ~ x | group, and marking points and lines using symbol shape, color, etc. Importantly, the car package generally allows automatic identification of “noteworthy” points by their labels in the plot using a variety of methods. For example, method = \"mahal\" labels cases with the most extreme Mahalanobis distances; method = \"r\" selects points according to their value of abs(y), which is appropriate in residual plots.\n\ncar::dataEllipse(): plots classical or robust data (using MASS::cov/trob()) ellipses for one or more groups, with the same facilities for point identification.\n\nheplots::covEllipses(): draws classical or robust data ellipses for one or more groups in a one-way design and optionally for the pooled total sample, where the focus is on homogeneity of within-group covariance matrices.\n\nggplot2::stat_ellipse(): uses the calculation methods of car::dataEllipse() to add unfilled (geom = \"path\") or filled (geom = polygon\") data ellipses in a ggplot scatterplot, using inherited aesthetics.\n\n3.2.3 Example: Canadian occupational prestige\nThese examples use the data on the prestige of 102 occupational categories and other measures from the 1971 Canadian Census, recorded in carData::Prestige.3 Our interest is in understanding how prestige (the Pineo & Porter (2008) prestige score for an occupational category, derived from a social survey) is related to census measures of the average education, income, percent women of incumbents in those occupations. Occupation type is a factor with levels \"bc\" (blue collar), \"wc\" (white collar) and \"prof\" (professional).\n\n\ndata(Prestige, package=\"carData\")\n# `type` is really an ordered factor. Make it so.\nPrestige$type &lt;- ordered(Prestige$type,\n                         levels=c(\"bc\", \"wc\", \"prof\"))\nstr(Prestige)\n#&gt; 'data.frame':  102 obs. of  6 variables:\n#&gt;  $ education: num  13.1 12.3 12.8 11.4 14.6 ...\n#&gt;  $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...\n#&gt;  $ women    : num  11.16 4.02 15.7 9.11 11.68 ...\n#&gt;  $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...\n#&gt;  $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...\n#&gt;  $ type     : Ord.factor w/ 3 levels \"bc\"&lt;\"wc\"&lt;\"prof\": 3 3 3 3 3 3 3 3 3 3 ...\n\nI first illustrate the relation between income and prestige in Figure 3.11 using car::scatterplot() with many of its bells and whistles, including marginal boxplots for the variables, the linear regression line, loess smooth and the 68% data ellipse.\n\nscatterplot(prestige ~ income, data=Prestige,\n  pch = 16, cex.lab = 1.25,\n  regLine = list(col = \"red\", lwd=3),\n  smooth = list(smoother=loessLine, \n                lty.smooth = 1, lwd.smooth=3,\n                col.smooth = \"darkgreen\", \n                col.var = \"darkgreen\"),\n  ellipse = list(levels = 0.68),\n  id = list(n=4, method = \"mahal\", col=\"black\", cex=1.2))\n#&gt; general.managers          lawyers        ministers       physicians \n#&gt;                2               17               20               24\n\n\n\n\n\n\nFigure 3.11: Scatterplot of prestige vs. income, showing the linear regression line (red), the loess smooth with a confidence envelope (darkgreen) and a 68% data ellipse. Points with the 4 largest \\(D^2\\) values are labeled.\n\n\n\n\nThere is a lot that can be seen here:\n\n\nincome is positively skewed, as is often the case.\nThe loess smooth, on the scale of income, shows prestige increasing up to $15,000 (these are 1971 incomes), and then leveling off.\nThe data ellipse, centered at the means encloses approximately 68% of the data points. It adds visual information about the correlation and precision of the linear regression; but here, the non-linear trend for higher incomes strongly suggests a different approach.\nThe four points identified by their labels are those with the largest Mahalanobis distances. scatterplot() prints their labels to the console.\n\nFigure 3.12 shows a similar plot for education, which from the boxplot appears to be reasonably symmetric. The smoothed curve is quite close to the linear regression, according to which prestige increases on average coef(lm(prestige ~ education, data=Prestige))[\"education\"] = 5.361 with each year of education.\n\nscatterplot(prestige ~ education, data=Prestige,\n  pch = 16, cex.lab = 1.25,\n  regLine = list(col = \"red\", lwd=3),\n  smooth = list(smoother=loessLine, \n                lty.smooth = 1, lwd.smooth=3,\n                col.smooth = \"darkgreen\", \n                col.var = \"darkgreen\"),\n  ellipse = list(levels = 0.68),\n  id = list(n=4, method = \"mahal\", col=\"black\", cex=1.2))\n#&gt;  physicians file.clerks    newsboys     farmers \n#&gt;          24          41          53          67\n\n\n\n\n\n\nFigure 3.12: Scatterplot of prestige vs. education, showing the linear regression line (red), the loess smooth with a confidence envelope (darkgreen) and a 68% data ellipse.\n\n\n\n\nIn this plot, farmers, newsboys, file.clerks and physicians are identified as noteworthy, for being furthest from the mean by Mahalanobis distance. In relation to their typical level of education, these are mostly understandable, but it is nice that farmers are rated of higher prestige than their level of education would predict.\nNote that the method argument for point identification can take a vector of case numbers indicating the points to be labeled. So, to label the observations with large absolute standardized residuals in the linear model m, you can use method = which(abs(rstandard(m)) &gt; 2).\n\nm &lt;- lm(prestige ~ education, data=Prestige)\nscatterplot(prestige ~ education, data=Prestige,\n            pch = 16, cex.lab = 1.25,\n            boxplots = FALSE,\n            regLine = list(col = \"red\", lwd=3),\n            smooth = list(smoother=loessLine,\n                          lty.smooth = 1, lwd.smooth=3,\n                          col.smooth = \"black\", \n                          col.var = \"darkgreen\"),\n            ellipse = list(levels = 0.68),\n            id = list(n=4, method = which(abs(rstandard(m))&gt;2), \n                      col=\"black\", cex=1.2)) |&gt; invisible()\n\n\n\n\n\n\nFigure 3.13: Scatterplot of prestige vs. education, labeling points whose absolute standardized residual is &gt; 2.\n\n\n\n\n\n3.2.3.1 Plotting on a log scale\nA typical remedy for the non-linear relationship of income to prestige is to plot income on a log scale. This usually makes sense, and expresses a belief that a multiple of or percentage increase in income has a constant impact on prestige, as opposed to the additive interpretation for income itself.\nFor example, the slope of the linear regression line in Figure 3.11 is given by coef(lm(prestige ~ income, data=Prestige))[\"income\"] = 0.003. Multiplying this by 1000 says that a $1000 increase in income is associated with with an average increase of prestige of 2.9.\nIn the plot below, scatterplot(..., log = \"x\") re-scales the x-axis to the \\(\\log_e()\\) scale. The slope, coef(lm(prestige ~ log(income), data=Prestige))[\"log(income)\"] = 21.556 says that a 1% increase in salary is associated with an average change of 21.55 / 100 in prestige.\n\n\nscatterplot(prestige ~ income, data=Prestige,\n  log = \"x\",\n  pch = 16, cex.lab = 1.25,\n  regLine = list(col = \"red\", lwd=3),\n  smooth = list(smoother=loessLine,\n                lty.smooth = 1, lwd.smooth=3,\n                col.smooth = \"darkgreen\", col.var = \"darkgreen\"),\n  ellipse = list(levels = 0.68),\n  id = list(n=4, method = \"mahal\", col=\"black\", cex=1.2))\n#&gt; general.managers        ministers         newsboys      babysitters \n#&gt;                2               20               53               63\n\n\n\n\n\n\nFigure 3.14: Scatterplot of prestige vs. log(income).\n\n\n\n\nThe smoothed curve in Figure 3.14 exhibits a slight tendency to bend upwards, but a linear relation is a reasonable approximation.\n\n3.2.3.2 Stratifying\nBefore going further, it is instructive to ask what we could see in the relationship between income and prestige if we stratified by type of occupation, fitting separate regressions and smooths for blue collar, white collar and professional incumbents in these occupations.\nThe formula prestige ~ income | type (read: income given type) is a natural way to specify grouping by type; separate linear regressions and smooths are calculated for each group, applying the color and point shapes specified by the col and pch arguments.\n\nscatterplot(prestige ~ income | type, data=Prestige,\n  col = c(\"blue\", \"red\", \"darkgreen\"),\n  pch = 15:17,\n  grid = FALSE,\n  legend = list(coords=\"bottomright\"),\n  regLine = list(lwd=3),\n  smooth=list(smoother=loessLine, \n              var=FALSE, lwd.smooth=2, lty.smooth=1))\n\n\n\n\n\n\nFigure 3.15: Scatterplot of prestige vs. income, stratified by occupational type. This implies a different interpretation, where occupation type is a moderator variable.\n\n\n\n\nThis visual analysis offers a different interpretation of the dependence of prestige on income, which appeared to be non-linear when occupation type was ignored. Instead, Figure 3.15 suggests an interaction of income by type. In a model formula this would be expressed as one of:\nlm(prestige ~ income + type + income:type, data = Prestige)\nlm(prestige ~ income * type, data = Prestige)\nThese models signify that there are different slopes (and intercepts) for the three occupational types. In this interpretation, type is a moderator variable, with a different story. The slopes of the fitted lines suggest that among blue collar workers, prestige increases sharply with their income. For white collar and professional workers, there is still an increasing relation of prestige with income, but the effect of income (slope) diminishes with higher occupational category. A different plot entails a different story.\n\n3.2.4 Example: Penguins data\n\n\n\n\n\n\n\nFigure 3.16: Penguin species observed in the Palmer Archipelago. This is a cartoon, but it illustrates some features of penguin body size measurements, and the colors typically used for species. Image: Allison Horst\n\n\n\n\nThe penguins dataset from the palmerpenguins package (Horst et al., 2022) provides further instructive examples of plots and analyses of multivariate data. The data consists of measurements of body size (flipper length, body mass, bill length and depth) of 344 penguins collected at the Palmer Research Station in Antarctica.\nThere were three different species of penguins (Adélie, Chinstrap & Gentoo) collected from 3 islands in the Palmer Archipelago between 2007–2009 (Gorman et al., 2014). The purpose was to examine differences in size or appearance of these species, particularly differences among the sexes (sexual dimorphism) in relation to foraging and habitat.\nHere, I use a slightly altered version of the dataset, peng, renaming variables to remove the units, making factors of character variables and deleting a few cases with missing data.\n\ndata(penguins, package = \"palmerpenguins\")\npeng &lt;- penguins |&gt;\n  rename(\n    bill_length = bill_length_mm, \n    bill_depth = bill_depth_mm, \n    flipper_length = flipper_length_mm, \n    body_mass = body_mass_g\n  ) |&gt;\n  mutate(species = as.factor(species),\n         island = as.factor(island),\n         sex = as.factor(substr(sex,1,1))) |&gt;\n  tidyr::drop_na()\n\nstr(peng)\n#&gt; tibble [333 × 8] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ species       : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ island        : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#&gt;  $ bill_length   : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n#&gt;  $ bill_depth    : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n#&gt;  $ flipper_length: int [1:333] 181 186 195 193 190 181 195 182 191 198 ...\n#&gt;  $ body_mass     : int [1:333] 3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ...\n#&gt;  $ sex           : Factor w/ 2 levels \"f\",\"m\": 2 1 1 1 2 1 2 1 2 2 ...\n#&gt;  $ year          : int [1:333] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\nThere are quite a few variables to choose for illustrating data ellipses in scatterplots. Here I focus on the measures of their bills, bill_length and bill_depth (indicating curvature) and show how to use ggplot2 for these plots.\nI’ll be using the penguins data quite a lot, so it is useful to set up custom colors like those used in Figure 3.16, and shown in Figure 3.17 with their color codes. These are shades of:\n\n\nAdelie: orange,\n\nChinstrap: purple, and\n\nGentoo: green.\n\n\n\n\n\n\n\n\nFigure 3.17: Color palettes used for penguin species.\n\n\n\n\nTo use these in ggplot2 I define a function peng.colors() that allows shades of light, medium and dark and then functions scale_*_penguins() for color and fill.\n\nCodepeng.colors &lt;- function(shade=c(\"medium\", \"light\", \"dark\")) {\n  shade = match.arg(shade)\n  #             light      medium     dark\n  oranges &lt;- c(\"#FDBF6F\", \"#F89D38\", \"#F37A00\")  # Adelie\n  purples &lt;- c(\"#CAB2D6\", \"#9A78B8\", \"#6A3D9A\")  # Chinstrap\n  greens &lt;-  c(\"#B2DF8A\", \"#73C05B\", \"#33a02c\")  # Gentoo\n  \n  cols.vec &lt;- c(oranges, purples, greens)\n  cols.mat &lt;- \n    matrix(cols.vec, 3, 3, \n           byrow = TRUE,\n           dimnames = list(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n                           shade = c(\"light\", \"medium\", \"dark\")))\n  # get shaded colors\n  cols.mat[, shade ]\n}\n\n# define color and fill scales\nscale_fill_penguins &lt;- function(shade=c(\"medium\", \"light\", \"dark\"), ...){\n  shade = match.arg(shade)\n  ggplot2::discrete_scale(\n    \"fill\",\"penguins\",\n     scales:::manual_pal(values = peng.colors(shade)), ...)\n}\n\nscale_colour_penguins &lt;- function(shade=c(\"medium\", \"light\", \"dark\"), ...){\n  shade = match.arg(shade)\n  ggplot2::discrete_scale(\n    \"colour\",\"penguins\",\n    scales:::manual_pal(values = peng.colors(shade)), ...)\n}\nscale_color_penguins &lt;- scale_colour_penguins\n\n\nThis is used to define a theme_penguins() function that I use to simply change the color and fill scales for plots below.\n\ntheme_penguins &lt;- function(shade=c(\"medium\", \"light\", \"dark\"), ...) {\n  shade = match.arg(shade)\n  list(scale_color_penguins(shade=shade),\n       scale_fill_penguins(shade=shade)\n      )\n}\n\nAn initial plot using ggplot2 shown in Figure 3.18 uses color and point shape to distinguish the three penguin species. I annotate the plot of points using the linear regression lines, loess smooths to check for non-linearity and 95% data ellipses to show precision of the linear relation.\n\nCodeggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, shape = species, fill=species)) +\n  geom_point(size=2) +\n  geom_smooth(method = \"lm\", formula = y ~ x,\n              se=FALSE, linewidth=2) +\n  geom_smooth(method = \"loess\",  formula = y ~ x,\n              linewidth = 1.5, se = FALSE, alpha=0.1) +\n  stat_ellipse(geom = \"polygon\", level = 0.95, alpha = 0.2) +\n  theme_penguins(\"dark\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.15))\n\n\n\n\n\n\nFigure 3.18: Penguin bill length and bill depth according to species.\n\n\n\n\nOverall, the three species occupy different regions of this 2D space and for each species the relation between bill length and depth appears reasonably linear. Given this, we can suppress plotting the data points to get a visual summary of the data using the fitted regression lines and data ellipses, as shown in Figure 3.19.\nThis idea, of visual thinning a graph to focus on what should be seen, becomes increasingly useful as the data becomes more complex. The ggplot2 framework encourages this, because we can think of various components as layers, to be included or not. Here I chose to include only the regression line and add data ellipses of 40%, 68% and 95% coverage to highlight the increasing bivariate density around the group means.\n\nCodeggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, shape = species, fill=species)) +\n  geom_smooth(method = \"lm\",  se=FALSE, linewidth=2) +\n  stat_ellipse(geom = \"polygon\", level = 0.95, alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", level = 0.68, alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", level = 0.40, alpha = 0.2) +\n  theme_penguins(\"dark\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.15))\n\n\n\n\n\n\nFigure 3.19: Visual thinning: Suppressing the data points gives a visual summary of the relation between bill length and bill depth using the regression line and data ellipses.\n\n\n\n\n\n3.2.4.1 Nonparamtric bivariate density plots\nWhile I emphasize data ellipses (because I like their beautiful geometry), other visual summaries of the bivariate density are possible and often useful.\nFor a single variable, stats::density() and ggplot2::geom_density() calculate a smoothed estimate of the density using nonparametric kernel methods (Silverman, 1986) whose smoothness is controlled by a bandwidth parameter, analogous to the span in a loess smoother. This idea extends to two (and more) variables (Scott, 1992). For bivariate data, MASS::kde2d() estimates the density on a square \\(n \\times n\\) grid over the ranges of the variables.\nggplot2 provides geom_density_2d() which uses MASS::kde2d() and displays these as contours— horizontal slices of the 3D surface at equally-spaced heights and projects these onto the 2D plane. The ggdensity package (Otto & Kahle, 2023) extends this with geom_hdr(), computing the high density regions that bound given levels of probability and maps these to the alpha transparency aesthetic. A method argument allows you to specify various nonparametric (method =\"kde\" is the default) and parametric (method =\"mvnorm\" gives normal data ellipses) ways to estimate the underlying bivariate distribution.\nFigure 3.20 shows these side-by-side for comparison. With geom_density_2d() you can specify either the number of contour bins or the width of these bins (binwidth). For geom_hdr(), the probs argument gives a result that is easier to understand.\n\nCodelibrary(ggdensity)\nlibrary(patchwork)\np1 &lt;- ggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species)) +\n  geom_smooth(method = \"lm\",  se=FALSE, linewidth=2) +\n  geom_density_2d(linewidth = 1.1, bins = 8) +\n  ggtitle(\"geom_density_2d\") +\n  theme_bw(base_size = 14) + \n  theme_penguins() +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.15))\n\np2 &lt;- ggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, fill = species)) +\n  geom_smooth(method = \"lm\",  se=FALSE, linewidth=2) +\n  geom_hdr(probs = c(0.95, 0.68, 0.4), show.legend = FALSE) +\n  ggtitle(\"ggdensity::geom_hdr\") +\n  theme_bw(base_size = 14) +\n  theme_penguins() +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\nFigure 3.20: Bivariate densities show the contours of the 3D surface representing the frequency in the joint distribution of bill length and bill depth.\n\n\n\n\n\n3.2.5 Simpson’s paradox: marginal and conditional relationships\nBecause it provides a visual representation of means, variances, and correlations, the data ellipse is ideally suited as a tool for illustrating and explicating various phenomena that occur in the analysis of linear models. One class of simple, but important, examples concerns the difference between the marginal relationship between variables, ignoring some important factor or covariate, and the conditional relationship, adjusting (controlling) for that variable.\nSimpson’s (1951) paradox occurs when the marginal and conditional relationships differ in direction. That is, the overall correlation in a model y ~ x might be negative, while the within-group correlations in separate models for each group y[g] ~ x[g] might be positive, or vice versa.\nThis may be seen in the plots of bill length against bill depth for the penguin data shown in Figure 3.21. Ignoring penguin species, the marginal, total-sample correlation is slightly negative as seen in panel (a). The individual-sample ellipses in panel (b) show that the conditional, within-species correlations are all positive, with approximately equal regression slopes. However the group means have a negative relationship, accounting for the negative marginal correlation when species is ignored.\n\n\n\n\n\n\n\nFigure 3.21: Marginal (a), conditional (b), and pooled within-sample (c) relationships of bill length and depth in the Penguins data. Each plot shows the 68% data ellipse and regression line(s) with 95% confidence bands.\n\n\n\n\nThe regression line in panel (a) is that for the linear model lm(bill_depth ~ bill_length), while the separate lines in panel (b) are those for the model lm(bill_depth ~ bill_length * species) which allows a different slope and intercept for each species.\nA correct analysis of the (conditional) relationship between these variables, controlling or adjusting for mean differences among species, is based on the pooled within-sample covariance matrix, a weighted average of the individual within-group \\(\\mathbf{S}_i\\), \\[\n\\mathbf{S}_{\\textrm{within}}  =\n\\sum_{i=1}^g\n(n_i - 1) \\mathbf{S}_i \\, / \\, (N - g)\n\\:\\: ,\n\\] where \\(N = \\sum n_i\\). The result is shown in panel (c) of Figure 3.21.\nIn this graph, the data for each species were first transformed to deviations from the species means on both variables and then translated back to the grand means. You can also see here that the shapes and sizes of the individual data ellipses are roughly comparable, but perhaps not identical. This visual idea of centering groups to a common mean will become important in Chapter 12 when we want to test the assumption of equality of error covariances in multivariate models.\nThe ggplot2 code for the panels in this figure are shown below. Note that for components that will be the same across panels, you can define elements (e.g., labels, theme_penguins(), legend_position) once, and then re-use these across several graphs.\n\n\n(a) Ignoring species\n(b) By species\n(c) Within species\n\n\n\n\nlabels &lt;- labs(\n  x = \"Bill length (mm)\",\n  y = \"Bill depth (mm)\",\n  color = \"Species\",\n  shape = \"Species\",\n  fill = \"Species\") \n\nplt1 &lt;- ggplot(data = peng,\n               aes(x = bill_length,\n                   y = bill_depth)) +\n  geom_point(size = 1.5) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              se = TRUE, color = \"gray50\") +\n  stat_ellipse(level = 0.68, linewidth = 1.1) +\n  ggtitle(\"Ignoring species\") +\n  labels\n\nplt1\n\n\n\n\nlegend_position &lt;-\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.83, 0.16))\n\nplt2 &lt;- ggplot(data = peng,\n               aes(x = bill_length,\n                   y = bill_depth,\n                   color = species,\n                   shape = species,\n                   fill = species)) +\n  geom_point(size = 1.5,\n             alpha = 0.8) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              se = TRUE, alpha = 0.3) +\n  stat_ellipse(level = 0.68, linewidth = 1.1) +\n  ggtitle(\"By species\") +\n  labels +\n  theme_penguins(\"dark\") +\n  legend_position \n\nplt2\n\n\n\n\n# center within groups, translate to grand means\nmeans &lt;- colMeans(peng[, 3:4])\npeng.centered &lt;- peng |&gt;\n  group_by(species) |&gt;\n  mutate(bill_length = means[1] + scale(bill_length, scale = FALSE),\n         bill_depth  = means[2] + scale(bill_depth, scale = FALSE))\n\nplt3 &lt;- ggplot(data = peng.centered,\n               aes(x = bill_length,\n                   y = bill_depth,\n                   color = species,\n                   shape = species,\n                   fill = species)) +\n  geom_point(size = 1.5,\n             alpha = 0.8) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              se = TRUE, alpha = 0.3) +\n  stat_ellipse(level = 0.68, linewidth = 1.1) +\n  labels +\n  ggtitle(\"Within species\") +\n  theme_penguins(\"dark\") +\n  legend_position \n\nplt3",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#sec-scatmat",
    "href": "03-multivariate_plots.html#sec-scatmat",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.3 Scatterplot matrices",
    "text": "3.3 Scatterplot matrices\nGoing beyond bivariate scatterplots, a pairs plot (or scatterplot matrix) displays all possible \\(p \\times p\\) pairs of \\(p\\) variables in a matrix-like display where variables \\((x_i, x_j)\\) are shown in a plot for row \\(i\\), column \\(j\\). This idea, due to Hartigan (1975b), uses small multiple plots, so that the eye can easily scan across a row or down a column to see how a given variable is related to all the others.\nThe most basic version is provided by pairs() in base R. When one variable is considered as an outcome or response, it is usually helpful to put this in the first row and column. For the Prestige data, in addition to income and education, we also have a measure of % women in each occupational category.\nPlotting these together gives Figure 3.22. In such plots, the diagonal cells give labels for the variables, but they are also a guide to interpreting what is shown. In each row, say row 2 for income, income is the vertical \\(y\\) variable in plots against other variables. In each column, say column 3 for education, education is the horizontal \\(x\\) variable.\n\npairs(~ prestige + income + education + women,\n      data=Prestige)\n\n\n\n\n\n\nFigure 3.22: Scatterplot matrix of the variables in the Prestige dataset produced by pairs()\n\n\n\n\nThe plots in the first row show what we have seen before for the relations between prestige and income and education, adding to those the plot of prestige vs. % women. Plots in the first column show the same data, but with \\(x\\) and \\(y\\) interchanged.\nBut this basic pairs() plot is very limited. A more feature-rich version is provided by car::scatterplotMatrix() which can add the regression lines, loess smooths and data ellipses for each pair, as shown in Figure 3.23.\nThe diagonal panels show density curves for the distribution of each variable; for example, the distribution of education appears to be multi-modal and that of women shows that most of the occupations have a low percentage of women.\nThe combination of the regression line with the loess smoothed curve, but without their confidence envelopes, provides about the right amount of detail to take in at a glance where the relations are non-linear. We’ve already seen (Figure 3.11) the non-linear relation between prestige and income (row 1, column 2) when occupational type is ignored. But all relations with income in column 2 are non-linear, reinforcing our idea (Section 3.2.3.1) that effects of income should be assessed on a log scale.\n\nscatterplotMatrix(~ prestige + income + education + women,\n  data=Prestige,\n  regLine = list(method=lm, lty=1, lwd=2, col=\"black\"),\n  smooth=list(smoother=loessLine, spread=FALSE,\n              lty.smooth=1, lwd.smooth=3, col.smooth=\"red\"),\n  ellipse=list(levels=0.68, fill.alpha=0.1))\n\n\n\n\n\n\nFigure 3.23: Scatterplot matrix of the variables in the Prestige dataset from car::scatterplotMatrix().\n\n\n\n\nscatterplotMatrix() can also label points using the id = argument (though this can get messy) and can stratify the observations by a grouping variable with different symbols and colors. For example, Figure 3.24 uses the syntax ~ prestige + education + income + women | type to provide separate regression lines, smoothed curves and data ellipses for the three types of occupations. (The default colors are somewhat garish, so I use scales::hue_pal() to mimic the discrete color scale used in ggplot2).\n\nscatterplotMatrix(~ prestige + income + education + women | type,\n  data = Prestige,\n  col = scales::hue_pal()(3),\n  pch = 15:17,\n  smooth=list(smoother=loessLine, spread=FALSE,\n              lty.smooth=1, lwd.smooth=3, col.smooth=\"black\"),\n  ellipse=list(levels=0.68, fill.alpha=0.1))\n\n\n\n\n\n\nFigure 3.24: Scatterplot matrix of the variables in the Prestige dataset from car::scatterplotMatrix(), stratified by type of occupation.\n\n\n\n\nIt is now easy to see why education is multi-modal: blue collar, white collar and professional occupations have largely non-overlapping years of education. As well, the distribution of % women is much higher in the white collar category.\nFor the penguins data, given what we’ve seen before in Figure 3.18 and Figure 3.19, we may wish to suppress details of the points (plot.points = FALSE) and loess smooths (smooth = FALSE) to focus attention on the similarity of regression lines and data ellipses for the three penguin species. In Figure 3.25, I’ve chosen to show boxplots rather than density curves in the diagonal panels in order to highlight differences in the means and interquartile ranges of the species, and to show 68% and 95% data ellipses in the off-diagonal panels.\n\nscatterplotMatrix(~ bill_length + bill_depth + flipper_length + body_mass | species,\n  data = peng, \n  col = peng.colors(\"medium\"), \n  legend=FALSE,\n  ellipse = list(levels = c(0.68, 0.95), \n                 fill.alpha = 0.1),\n  regLine = list(lwd=3),\n  diagonal = list(method = \"boxplot\"),\n  smooth = FALSE,\n  plot.points = FALSE,\n  cex.labels=1) \n\n\n\n\n\n\nFigure 3.25: Scatterplot matrix of the quantitative variables in the penguins dataset, stratified by species.\n\n\n\n\n\nIt can be seen that the species are widely separated in most of the bivariate plots. As well, the regression lines for species have similar slopes and the data ellipses have similar size and shape in most of the plots. From the boxplots, we can also see that Adelie penguins have shorter bill lengths than the others, while Gentoo penguins have smaller bill depth, but longer flippers and are heavier than Chinstrap and Adelie penguins.\n\n\n\n\n\n\nLooking ahead\n\n\n\nFigure 3.25 provides a reasonably complete visual summary of the data in relation to multivariate models that ask “do the species differ in their means on these body size measures?” This corresponds to the MANOVA model,\n\npeng.mod &lt;- lm(cbind(bill_length, bill_depth, flipper_length, body_mass) ~ species, \n               data=peng)\n\nHypothesis-error (HE) plots, described in Chapter 11 provide a better summary of the evidence for the MANOVA test of differences among means on all variables together. These give an \\(\\mathbf{H}\\) ellipse reflecting the differences among means, to be compared with an \\(\\mathbf{E}\\) ellipse reflecting within-group variation and a visual test of significance.\nA related question is “how well are the penguin species distinguished by these body size measures?” Here, the relevant model is linear discriminant analysis (LDA), where species plays the role of the response in the model,\n\npeng.lda &lt;- MASS:lda( species ~ cbind(bill_length, bill_depth, flipper_length, body_mass), \n               data=peng)\n\nBoth MANOVA and LDA depend on the assumption that the variances and correlations between the variables are the same for all groups. This assumption can be tested and visualized using the methods in Chapter 12.\n\n\n\n3.3.1 Visual thinning\nWhat can you do if there are even more variables than in these examples? If what you want is a high-level, zoomed-out display summarizing the pairwise relations more strongly, you can apply the idea of visual thinning to show only the most important features.\nThis example uses data on the rate of various crimes in the 50 U.S. states from the United States Statistical Abstracts, 1970, used by Hartigan (1975a) and Friendly (1991). These are ordered in the dataset roughly by seriousness of crime or from crimes of violence to property crimes.\n\ndata(crime, package = \"ggbiplot\")\nstr(crime)\n#&gt; 'data.frame':  50 obs. of  10 variables:\n#&gt;  $ state   : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n#&gt;  $ murder  : num  14.2 10.8 9.5 8.8 11.5 6.3 4.2 6 10.2 11.7 ...\n#&gt;  $ rape    : num  25.2 51.6 34.2 27.6 49.4 42 16.8 24.9 39.6 31.1 ...\n#&gt;  $ robbery : num  96.8 96.8 138.2 83.2 287 ...\n#&gt;  $ assault : num  278 284 312 203 358 ...\n#&gt;  $ burglary: num  1136 1332 2346 973 2139 ...\n#&gt;  $ larceny : num  1882 3370 4467 1862 3500 ...\n#&gt;  $ auto    : num  281 753 440 183 664 ...\n#&gt;  $ st      : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n#&gt;  $ region  : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n\n\nFigure 3.26 displays the scatterplot matrix for these seven variables, using only the regression line and data ellipse to show the linear relation and the loess smooth to show potential non-linearity. To make this even more schematic, the axis tick marks and labels are also removed using the par() settings xaxt = \"n\", yaxt = \"n\".\n\ncrime |&gt;\n  select(where(is.numeric)) |&gt;\n  scatterplotMatrix(\n    plot.points = FALSE,\n    ellipse = list(levels = 0.68, fill=FALSE),\n    smooth = list(spread = FALSE, \n                  lwd.smooth=2, lty.smooth = 1, col.smooth = \"red\"),\n    cex.labels = 2,\n    xaxt = \"n\", yaxt = \"n\")\n\n\n\n\n\n\nFigure 3.26: Visual thinning: Scatterplot matrix of the crime data, showing only high-level summaries of the linear and nonlinear relations betgween each pair of variables.\n\n\n\n\nWe can see that all pairwise correlations are positive, pairs closer to the main diagonal tend to be more highly correlated and in most cases the nonparametric smooth doesn’t differ much from the linear regression line. Exceptions to this appear mainly in the columns for robbery and auto (auto theft).",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#sec-corrgram",
    "href": "03-multivariate_plots.html#sec-corrgram",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.4 Corrgrams",
    "text": "3.4 Corrgrams\nWhat if you want to summarize the data even further simple visual thinning. For example with many variables you might want to show only the value of the correlation for each pair of variables, but do so in a way to help see patterns in the correlations that would be invisible in just a table.\nA corrgram (Friendly, 2002) is a visual display of a correlation matrix, where the correlation can be rendered in a variety of ways to show the direction and magnitude: circular “pac-man” (or pie) symbols, ellipses, colored vars or shaded rectangles, as shown in Figure 3.27.\nAnother aspect is that of effect ordering (Friendly & Kwan, 2003), ordering the levels of factors and variables in graphic displays to make important features most apparent. For variables, this means that we can arrange the variables in a matrix-like display in such a way as to make the pattern of relationships easiest to see. Methods to achieve this include using principal components and cluster analysis to put the most related variables together as described in Chapter 4.\n\n\n\n\n\n\n\nFigure 3.27: Corrgrams: Some renderings for the value of a correlation in a corrgram display, conveying sign and magnitude in different ways.\n\n\n\n\nIn R, these diagrams can be created using the corrgram (Wright, 2021) and corrplot (Wei & Simko, 2024) packages, with different features. corrgram::corrgram() is closest to Friendly (2002), in that it allows different rendering functions for the lower, upper and diagonal panels as illustrated in Figure 3.27. For example, a corrgram similar to Figure 3.26 can be produced as follows (not shown here):\n\ncrime |&gt;\n  select(where(is.numeric)) |&gt;\n  corrgram(lower.panel = panel.ellipse,\n           upper.panel = panel.ellipse,\n           diag.panel = panel.density)\n\nWith the corrplot package, corrplot() provides the rendering methods c(\"circle\", \"square\", \"ellipse\", \"number\", \"shade\", \"color\", \"pie\"), but only one can be used at a time. The function corrplot.mixed() allows different options to be selected for the lower and upper triangles. The iconic rendering shape is colored with a gradient in relation to the correlation value. For comparison, Figure 3.28 uses ellipses below the diagonal and filled pie charts below the diagonal using a gradient of the fill color in both cases.\n\ncrime.cor &lt;- crime |&gt;\n  dplyr::select(where(is.numeric)) |&gt; \n  cor()\n\ncorrplot.mixed(crime.cor,\n   lower = \"ellipse\",\n   upper = \"pie\",\n   tl.col = \"black\",\n   tl.srt = 0,\n   tl.cex = 1.25,\n   addCoef.col = \"black\",\n   addCoefasPercent = TRUE)\n\n\n\n\n\n\nFigure 3.28: Mixed corrplot of the crime data, showing the correlation between each pair of variables with an ellipse (lower) and a pie chart symbol (upper), all shaded in proportion to the correlation value, also shown numerically.\n\n\n\n\nThe combination of renderings shown in Figure 3.28 is instructive. Small differences among correlation values are easier to see with the pie symbols than with the ellipses; for example, compare the values for murder with larceny and auto theft in row 1, columns 6-7 with those in column 1, rows 6-7, where the former are easier to distinguish. The shading color adds another visual cue.\nThe variables in Figure 3.26 and Figure 3.28 are arranged by their order in the dataset, which is not often the most useful. A better idea is to arrange the variables so that the most highly correlated variables are adjacent.\nA general method described in Section 4.5 orders the variables according to the angles of the first two eigenvectors from a principal components analysis (PCA) around a unit circle. The function corrMatOrder() provides several methods (order = c(\"AOE\", \"FPC\", \"hclust\", \"alphabet\")) for doing this, and PCA ordering is order = \"AOE\". Murder and auto theft are still first and last, but some of the intermediate crimes have been rearranged.\n\nord &lt;- corrMatOrder(crime.cor, order = \"AOE\")\nrownames(crime.cor)[ord]\n#&gt; [1] \"murder\"   \"assault\"  \"rape\"     \"robbery\"  \"burglary\"\n#&gt; [6] \"larceny\"  \"auto\"\n\n\n\nUsing this ordering in corrplot() produces Figure 3.29.\n\ncorrplot.mixed(crime.cor,\n  order = \"AOE\", \n  lower = \"ellipse\",\n  upper = \"ellipse\",\n  tl.col = \"black\",\n  tl.srt = 0,\n  tl.cex = 1.25,\n  addCoef.col = \"black\",\n  addCoefasPercent = TRUE)\n\n\n\n\n\n\nFigure 3.29: Corrplot of the crime data with the variables reordered according to the angles of variable eigenvectors. Correlations are rendered with ellipses shaded in proportion to their magnitude.\n\n\n\n\nIn this case, where the correlations among the crime variables are all positive, the effect of variable re-ordering is subtle, but note that there is now a slightly pronounced pattern of highest correlations near the diagonal, and decreasing away from the diagonal. Figure 4.27 and Figure 4.29 in Section 4.5 provide a more dramatic example of variable ordering using this method.\nVariations of corrgrams are worthy replacements for a numeric table of correlations, which are often presented in publications only for archival value. Including the numeric value (rounded here, for presentation purposes), makes this an attractive alternative to boring tables of correlations.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#sec-ggpairs",
    "href": "03-multivariate_plots.html#sec-ggpairs",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.5 Generalized pairs plots",
    "text": "3.5 Generalized pairs plots\nWhen a dataset contains one or more discrete variables, the traditional pairs plot cannot cope, because the discrete categories would plot as many overlaid points. This cannot be represented using only color and/or point symbols in a meaningful scatterplot.\nBut the associations between categorical variables in a frequency table can be shown in mosaic displays (Friendly, 1994), using an array of tiles whose areas are depict the cell frequencies. For an \\(n\\)-way frequency, an analog of the scatterplot matrix uses mosaic plots for each pair of variables. The vcd package (Meyer et al., 2024) implements very general pairs() methods for \"table\" objects and vcdExtra (Friendly, 2023) extends this to wide classes of loglinear models (Friendly, 1999) See Friendly (1999) and my book Discrete Data Analysis with R (Friendly & Meyer, 2016) for mosaic plots and mosaic matrices.\nFor example, we can tabulate the distributions of penguin species by sex and the island where they were observed using xtabs(). ftable() prints this three-way table more compactly. (In this example, and what follows in the chapter, I’ve changed the labels for sex from (“f”, “m”) to (“Female”, “Male”)).\n\n# use better labels for sex\npeng &lt;- peng |&gt;\n  mutate(sex = factor(sex, labels = c(\"Female\", \"Male\")))\npeng.table &lt;- xtabs(~ species + sex + island, data = peng)\n\nftable(peng.table)\n#&gt;                  island Biscoe Dream Torgersen\n#&gt; species   sex                                 \n#&gt; Adelie    Female            22    27        24\n#&gt;           Male              22    28        23\n#&gt; Chinstrap Female             0    34         0\n#&gt;           Male               0    34         0\n#&gt; Gentoo    Female            58     0         0\n#&gt;           Male              61     0         0\n\nWe can see immediately that the penguin species differ by island: only Adelie were observed on all three islands; Biscoe Island had no Chinstraps and Dream Island had no Gentoos.\nvcd::pairs() produces all pairwise mosaic plots, as shown in Figure 3.30. The diagonal panels show the one-way frequencies by width of the divided bars. Each off-diagonal panel shows the bivariate counts, breaking down each column variable by splitting the bars in proportion to a second variable. Consequently, the frequency of each cell is represented by its’ area. The purpose is to show the pattern of association between each pair, and so, the tiles in the mosaic are shaded according to the signed standardized residual, \\(d_{ij} = (n_{ij} - \\hat{n}_{ij}) / \\sqrt{\\hat{n}_{ij}}\\) in a simple \\(\\chi^2 = \\Sigma_{ij} \\; d_{ij}^2\\) test for association— blue where the observed frequency \\(n_{ij}\\) is significantly greater than expected \\(\\hat{n}_{ij}\\) under independence, and red where it is less than expected. The tiles are unshaded when \\(| d_{ij} | &lt; 2\\).\n\nlibrary(vcd)\npairs(peng.table, shade = TRUE,\n      lower_panel_args = list(labeling = labeling_values()),\n      upper_panel_args = list(labeling = labeling_values()))\n\n\n\n\n\n\nFigure 3.30: Mosaic pairs plot for the combinations of species, sex and island. Diagnonal plots show the marginal frequency of each variable by the width of each rectangle. Off-diagonal mosaic plots subdivide by the conditional frequency of the second variable, shown numerically in the tiles.\n\n\n\n\nThe shading patterns in cells (1,3) and (3,1) of Figure 3.30 show what we’ve seen before in the table of frequencies: The distribution of species varies across island because on each island one or more species did not occur. Row 2 and column 2 show that sex is nearly exactly proportional among species and islands, indicating independence, \\(\\text{sex} \\perp \\{\\text{species}, \\text{island}\\}\\). More importantly, mosaic pairs plots can show, at a glance, all (bivariate) associations among multivariate categorical variables.\nThe next step, by John Emerson and others (Emerson et al., 2013) was to recognize that combinations of continuous and discrete, categorical variables could be plotted in different ways.\n\nTwo continuous variables can be shown as a standard scatterplot of points and/or bivariate density contours, or simply by numeric summaries such as a correlation value;\nA pair of one continuous and one categorical variable can be shown as side-by-side boxplots or violin plots, histograms or density plots;\nTwo categorical variables could be shown in a mosaic plot or by grouped bar plots.\n\nIn the ggplot2 framework, these displays are implemented using the ggpairs() function from the GGally package (Schloerke et al., 2024). This allows different plot types to be shown in the lower and upper triangles and in the diagonal cells of the plot matrix. As well, aesthetics such as color and shape can be used within the plots to distinguish groups directly. As illustrated below, you can define custom functions to control exactly what is plotted in any panel.\nThe basic, default plot shows scatterplots for pairs of continuous variables in the lower triangle and the values of correlations in the upper triangle. A combination of a discrete and continuous variables is plotted as histograms in the lower triangle and boxplots in the upper triangle. Figure 3.31 includes sex to illustrate the combinations.\n\n\nCodeggpairs(peng, columns=c(3:6, 7),\n        aes(color=species, alpha=0.5),\n        progress = FALSE) +\n  theme_penguins() +\n  theme(axis.text.x = element_text(angle = -45))\n\n\n\n\n\n\n\n\n\n\nFigure 3.31: Basic ggpairs() plot of penguin size variables and sex, stratified by species.\n\n\n\n\nTo my eye, printing the values of correlations in the upper triangle is often a waste of graphic space. But in this example the correlations show something peculiar and interesting if you look closely: In all pairs among the penguin size measurements, there are positive correlations within each species, as we can see in Figure 3.25. Yet, in three of these panels, the overall correlation ignoring species is negative. For example, the overall correlation between bill depth and flipper length is \\(r = -0.579\\) in row 2, column 3; the scatterplot in the diagonally opposite cell, row 3, column 2 shows the data. These cases, of differing signs for an overall correlation, ignoring a group variable and the within group correlations are examples of Simpson’s Paradox, explored later in Chapter XX. \nThe last row and column, for sex in Figure 3.31, provides an initial glance at the issue of sex differences among penguin species that motivated the collection of these data. We can go further by also examining differences among species and island, but first we need to understand how to display exactly what we want for each pairwise plot.\nggpairs() is extremely general in that for each of the lower, upper and diag sections you can assign any of a large number of built-in functions (of the form ggally_NAME), or your own custom function for what is plotted, depending on the types of variables in each plot.\n\n\ncontinuous: both X and Y are continuous variables, supply this as the NAME part of a ggally_NAME() function or the name of a custom function;\n\ncombo: one X of and Y variable is discrete while the other is continuous, using the same convention;\n\ndiscrete: both X and Y are discrete variables.\n\nThe defaults, which were used in Figure 3.31, are:\n\nupper = list(continuous = \"cor\",          # correlation values\n             combo = \"box_no_facet\",      # boxplots \n             discrete = \"count\")          # rectangles ~ count\nlower = list(continuous = \"points\",       # just data points\n             combo = \"facethist\",         # faceted histograms\n             discrete = \"facetbar\")       # faceted bar plots\ndiag  = list(continuous = \"densityDiag\",  # density plots\n             discrete = \"barDiag\")        # bar plots\n\nThus, ggpairs() uses ggally_cor() to print the correlation values for pairs of continuous variables in the upper triangle, and uses ggally_points() to plot scatterplots of points in the lower portion. The diagonal panels as shown as density plots (ggally_densityDiag()) for continuous variables but as bar plots (ggally_barDiag()) for discrete factors.\nSee the vignette, ggally_plots for an illustrated list of available high-level plots. For our purpose here, which is to illustrate enhanced displays, note that for scatterplots of continuous variables, there are two functions which plot the points and also add a smoother, _lm or _loess.\n\nls(getNamespace(\"GGally\")) |&gt; \n  stringr::str_subset(\"^ggally_smooth_\")\n#&gt; [1] \"ggally_smooth_lm\"    \"ggally_smooth_loess\"\n\nA customized display for scatterplots of continuous variables can be any function that takes data and mapping arguments and returns a \"ggplot\" object. The mapping argument supplies the aesthetics, e.g., aes(color=species, alpha=0.5), but only if you wish to override what is supplied in the ggpairs() call.\nHere is a function, my_panel() that plots the data points, regression line and loess smooth:\n\nmy_panel &lt;- function(data, mapping, ...){\n  p &lt;- ggplot(data = data, mapping = mapping) + \n    geom_point() + \n    geom_smooth(method=lm, formula = y ~ x, se = FALSE, ...) +\n    geom_smooth(method=loess, formula = y ~ x, se = FALSE, ...)\n  p\n}\n\nFor this example, I want only simple summaries of for the scatterplots, so I don’t want to plot the data points, but do want to add the regression line and a data ellipse.\n\nmy_panel1 &lt;- function(data, mapping, ...){\n  p &lt;- ggplot(data = data, mapping = mapping) + \n     geom_smooth(method=lm, formula = y ~ x, se = FALSE, ...) +\n     stat_ellipse(geom = \"polygon\", level = 0.68, ...)\n  p\n}\n\nThen, to show what can be done, Figure 3.32 uses my_panel1() for the scatterplots in the 4 x 4 block of plots in the upper left. The combination of the continuous body size measures and the discrete factors species, island and sex are shown in upper triangle by boxplots but by faceted histograms in the lower portion. The factors are shown as rectangles with area proportional to count (poor-man’s mosaic plots) above the diagonal and as faceted bar plots below.\n\n\nCodeggpairs(peng, columns=c(3:6, 1, 2, 7),\n        mapping = aes(color=species, fill = species, alpha=0.2),\n        lower = list(continuous = my_panel1),\n        upper = list(continuous = my_panel1),\n        progress = FALSE) +\n  theme_penguins() +\n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank()) + \n  theme(axis.text.x = element_text(angle = -45))\n\n\n\n\n\n\n\n\n\n\nFigure 3.32: Customized ggpairs() plot of penguin size variables, together with species, island and sex.\n\n\n\n\nThere is certainly a lot going on in Figure 3.32, but it does show a high-level overview of all the variables (except year) in the penguins dataset. It is probably easiest to “read” this figure by focusing on the four blocks for the combinations of 4 continuous and 3 categorical measures. In the upper left block, visual thinning of the scatterplots, showing only the data ellipses and regression lines gives a simple view as it did in Figure 3.25.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#sec-parcoord",
    "href": "03-multivariate_plots.html#sec-parcoord",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.6 Parallel coordinate plots",
    "text": "3.6 Parallel coordinate plots\nAs we have seen above, scatterplot matrices and generalized pairs plots extend data visualization to multivariate data, but these variables share one 2D space, so resolution decreases as the number of variable increase. You need a very large screen or sheet of paper to see more than, say 5-6 variables with any clarity.\nParallel coordinate plots are an attractive alternative, with which we can visualize an arbitrary number of variables to get a visual summary of a potentially high-dimensional dataset, and perhaps recognize outliers and clusters in the data in a different way. In these plots, each variable is shown on a separate, parallel axis. A multivariate observation is then plotted by connecting their respective values on each axis with lines across all the axes.\nThe geometry of parallel coordinates is interesting, because what is a point in \\(n\\)-dimensional (Euclidean) data space becomes a line in the projective parallel coordinate space with \\(n\\) axes, and vice-versa: lines in parallel coordinate space correspond to points in data space. Thus, a collection of points in data space map to lines that intersect in a point in projective space. What this does is to map \\(n\\)-dimensional relations into 2D patterns we can see in a parallel coordinates plot.\n\n\n\n\n\n\nHistory Corner\n\n\n\n\nThose who don’t know history are doomed to plagarize it —The author\n\nThe theory of projective geometry originated with the French mathematician Maurice d’Ocagne (1885) who sought a way to provide graphic calculation of mathematical functions with alignment diagrams or nomograms using parallel axes with different scales. A three-variable equation, for example, could be solved using three parallel axes, where known values could be marked on their scales, a line drawn between them, and an unknown read on its scale at the point where the line intersects that scale.\nHenry Gannet (1880), in work preceding the Statistical Atlas of the United States for the 1890 Census (Gannett, 1898), is widely credited with being the first to use parallel coordinates plots to show data, in his case, to show the rank ordering of US states by 10 measures including population, occupations, wealth, manufacturing, agriculture and so on.\nHowever, both d’Ocagne and Gannet were far preceded in this by Andre-Michel Guerry (1833) who used this method to show how the rank order of various crimes changed with age of the accused. See Friendly (2022), Figure 7 for his version and for an appreciation of the remarkable contributions of this amateur statistician to the history of data visualization.\n\nThe use of parallel coordinates for display of multidimensional data was rediscovered by Alfred Inselberg (1985) and extended by Edward Wegman (1990), neither of whom recognized the earlier history. Somewhat earlier, David Andrews (1972) proposed mapping multivariate observations to smooth Fourrier functions composed of alternating \\(\\sin()\\) and \\(\\cos()\\) terms. And in my book, SAS System for Statistical Graphics (Friendly, 1991), I implemented what I called profile plots without knowing their earlier history as parallel coordinate plots.\n\n\nParallel coordinate plots present a challenge for graphic developers, in that they require a different way to think about plot construction for multiple variables, which can be quantitative, as in the original idea, or categorical factors, all to be shown along parallel axes.\nHere, I use the ggpcp package (Hofmann et al., 2022), best described in VanderPlas et al. (2023), who also review the modern history.4 This takes some getting used to, because they develop pcp_*() extensions of the ggplot2 grammar of graphics framework to allow:\n\n\npcp_select(): selections of the variables to be plotted and their horizontal order on parallel axes,\n\npcp_scale(): methods for scaling of the variables to each axis,\n\npcp_arrange(): methods for breaking ties in factor variables to space them out.\n\nThen, it provides geom_pcp_*() functions to control the display of axes with appropriate aesthetics, labels for categorical factors and so forth. Figure 3.33 illustrates this type of display, using sex and species in addition to the quantitative variables for the penguin data.\n\n\nCodepeng |&gt;\n  pcp_select(bill_length:body_mass, sex, species) |&gt;\n  pcp_scale(method = \"uniminmax\") |&gt;\n  pcp_arrange() |&gt;\n  ggplot(aes_pcp()) +\n  geom_pcp_axes() +\n  geom_pcp(aes(colour = species), alpha = 0.8, overplot = \"none\") +\n  geom_pcp_labels() +\n  scale_colour_manual(values = peng.colors()) +\n  labs(x = \"\", y = \"\") +\n  theme(axis.title.y = element_blank(), axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(), legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nFigure 3.33: Parallel coordinates plot of penguin size variables, together with sex and species.\n\n\n\n\nRearranging the order of variables and the ordering of factor levels can make a difference in what we can see in such plots. For a simple example (following VanderPlas et al. (2023)), we reorder the levels of species and islands to make it clearer which species occur on each island.\n\nCodepeng1 &lt;- peng |&gt;\n  mutate(species = factor(species, levels = c(\"Chinstrap\", \"Adelie\", \"Gentoo\"))) |&gt;\n  mutate(island = factor(island, levels = c(\"Dream\", \"Torgersen\", \"Biscoe\")))\n\npeng1 |&gt;\n  pcp_select(species, island, bill_length:body_mass) |&gt;\n  pcp_scale() |&gt;\n  pcp_arrange(method = \"from-left\") |&gt;\n  ggplot(aes_pcp()) +\n  geom_pcp_axes() +\n  geom_pcp(aes(colour = species), alpha = 0.6, overplot = \"none\") +\n  geom_pcp_boxes(fill = \"white\", alpha = 0.5) +\n  geom_pcp_labels() +\n  scale_colour_manual(values = peng.colors()[c(2,1,3)]) +\n  theme_bw() +\n  labs(x = \"\", y = \"\") +\n  theme(axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(),\n        legend.position = \"none\") \n\n\n\n\n\n\n\n\n\n\n\nThe order of variables in this plot emphasizes the relation between penguin species and the island where they were observed and then shows the values of the quantitative body size measurements. More generally, quantitative variables can, and probably should, be ordered to place the most highly correlated variables adjacently to minimize the degree of crossing lines from one variable to the next (Martí & Laguna, 2003). When variables are highly negatively correlated (such as bill_depth and flipper_length here), crossings can be reduced simply by reversing the scale of one of the variables, e.g., by plotting -bill_depth.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#animated-tours",
    "href": "03-multivariate_plots.html#animated-tours",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.7 Animated tours",
    "text": "3.7 Animated tours\nIn the mid 17\\(^{th}\\) to early 19\\(^{th}\\)-century the Grand Tour became a coming-of-age custom for young Europeans (mainly British nobility and landed gentry) of sufficient rank and means to undertake a journey to the principal sites of Europe (Paris, Geneva, Rome, Athens, …) to complete their education by learning something of the cultural legacies in history, art, and music from antiquity to the Renaissance. Thereby, they could gain a wider appreciation of history and be prepared to play a role in polite society or in their chosen endeavors.\nTravels in high-dimensional data space might be less thrilling than a journey from London through Paris and Millan to Rome. Yet, in both cases it is useful to think of the path taken, and what might be seen along the way. But there are different kinds of tours. We might simply take a meandering tour, exploring all the way, or want to plan a tour to see the most interesting sites in travel or have a tour guided by an expert. Similarly in data space, we might travel randomly to see what we can find or be guided to find interesting features such as clusters, outliers or non-linear relations in data.\nFollowing the demonstration in PRIM-9 (Section 3.1) of exploring multidimensional data space by rotation Asimov (1985) developed the idea of the grand tour, a computer method for viewing multivariate statistical data via orthogonal projections onto an animated sequence of low-dimensional subspaces, like a movie. In contrast to a scatterplot matrix which shows a static view of a data cloud projected onto all pairwise variable axes, a statistical tour is like the view of an eye moving smoothly in high-dimensional space, capturing what it sees from a given location onto the 2-d plane of the computer screen.\nMore generally, statistical tours are a type of dynamic projections onto orthogonal axes (called a basis) that embed data in a \\(p\\)−dimensional space into a \\(d\\)−dimensional viewing subspace. Typically, \\(d=2\\), and the result is displayed as scatterplots, together with vectors representing the projections of the data variables in this space. But the projected data can be rendered in 1-d as densities or histograms, or in other number of dimensions as glyphs, or even as parallel coordinate plots. The essential idea is that we can define, and animate, a tour path as a smooth sequence of such projections over small changes to the projection basis, which gives the orientation of the data in the viewing space.\n\n3.7.1 Projections\nThe idea of a projection is fundamental to touring methods and other visualizations of high-D data, so it is useful to understand what a projection is. Quite simply, you can think of a projection as the shadow of an object or cloud of points. This is nicely illustrated by the cover image (Figure 3.34) used for Douglas Hofstadter’s (1979) Gödel, Bach and Escher which shows 3D solid shapes illuminated by light sources so their shadows form the letters G, B and E projected onto the planes formed by pairs of the three coordinate axes. The set of three 2D views is essentially the same that we see in a scatterplot matrix, where a 3D dataset is portrayed by the set of shadows of the points on planes formed by pairs of coordinate axes.\n\n\n\n\n\n\n\nFigure 3.34: The cover image from Hofstadter (1979) illustrates how projections are shadows of an object cast by a light from a given direction.\n\n\n\n\nIn the simplest case, a data point \\(\\mathbf{x} = (x_1, x_2)\\) in two dimensions can be represented geometrically as a vector from the origin as shown in Figure 3.35. This point can be projected on any one-dimensional axis \\(\\mathbf{p}\\) by dropping a line perpendicular to \\(\\mathbf{p}\\), which is the idea of a shadow. Mathematically, this is calculated as the product \\(\\mathbf{x}^\\mathsf{T} \\mathbf{p} = x_1 p_1 + x_2 p_2\\) and suitably normalized to give the correct length. …\n\n\n\n\n\n\n\nFigure 3.35: Projection of a point x onto a direction or axis p.\n\n\n\n\nMore generally, a projection of an \\((n \\times p)\\) data matrix \\(\\mathbf{X}\\) representing \\(n\\) observations in \\(p\\) dimensions onto a \\(d\\)-dimensional viewing space \\(\\mathbf{Y}_{n \\times d}\\) is represented by a \\(p \\times d\\) projection matrix \\(\\mathbf{P}\\) as \\(\\mathbf{Y} = \\mathbf{X} \\mathbf{P}\\), where the columns of \\(\\mathbf{P}\\) are orthogonal and of unit length,i.e., \\(\\mathbf{P}^\\mathsf{T} \\mathbf{P} = \\mathbf{I}_{(d \\times d)}\\).\nFor example, to project a data matrix \\(\\mathbf{X}\\) in three dimensions onto a 2D plane, we would multiply it by a \\((3 \\times 2)\\) orthonormal matrix \\(\\mathbf{P}\\). The matrix \\(\\mathbf{P}_1\\) below simply selects the first two columns of \\(\\mathbf{X}\\).5\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n    0 & 0 & 0 \\\\\n    0 & 0 & 10 \\\\\n    0 & 10 & 0 \\\\\n    0 & 10 & 10 \\\\\n    10 & 0 & 0 \\\\\n    10 & 0 & 10 \\\\\n    10 & 10 & 0 \\\\\n    10 & 10 & 10 \\\\\n\\end{bmatrix}_{8 \\times 3}\n;\\;\n\\mathbf{P_1} =\n\\begin{bmatrix}\n    1 & 0 \\\\\n    0 & 1 \\\\\n    0 & 0 \\\\\n\\end{bmatrix}_{3 \\times 2}\n\\;\\Rightarrow\\quad\n\\mathbf{Y} = \\mathbf{X} \\; \\mathbf{P_1} =\n\\begin{bmatrix}\n    0 & 0 \\\\\n    0 & 0 \\\\\n    0 & 10 \\\\\n    0 & 10 \\\\\n    10 & 0 \\\\\n    10 & 0 \\\\\n    10 & 10 \\\\\n    10 & 10 \\\\\n\\end{bmatrix}_{8 \\times 2}\n\\] An oblique projection using all three dimensions is given by \\(\\mathbf{P_2}\\) below. This produces a new 2D view in \\(\\mathbf{Y}\\): \\[\n\\mathbf{P_2} =\n\\begin{bmatrix}\n    0.71 & -0.42 \\\\\n    0.71 & 0.42 \\\\\n    0 & 0.84 \\\\\n\\end{bmatrix}_{3 \\times 2}\n\\quad\\Rightarrow\\quad\n\\mathbf{Y} = \\mathbf{X} \\; \\mathbf{P_2} =\n\\begin{bmatrix}\n    0 & 0 \\\\\n    0 & 8.4 \\\\\n    7.1 & 4.2 \\\\\n    7.1 & 12.6 \\\\\n    7.1 & -4.2 \\\\\n    7.1 & 4.2 \\\\\n    14.2 & 0 \\\\\n    14.2 & 8.4 \\\\\n\\end{bmatrix}\n\\]\nThe columns in \\(\\mathbf{Y}\\) are simply the linear combinations of those of \\(\\mathbf{X}\\) using the weights in each column of \\(\\mathbf{P_2}\\)\n\\[\\begin{aligned}\n\\mathbf{y}_1 & = & 0.71 \\mathbf{x}_1 + 0.71 \\mathbf{x}_2 + 0 \\mathbf{x}_3\\\\\n\\mathbf{y}_2 & = & -0.42 \\mathbf{x}_1 + 0.42 \\mathbf{x}_2 + 0.84 \\mathbf{x}_3 \\\\\n\\end{aligned}\\]\n\nCodevals &lt;- c(0, 10)\nX &lt;- expand.grid(x1 = vals, x2=vals, x3=vals) |&gt; as.matrix()\n\n# project on just x1, x2 plane\nP1 &lt;- rbind(diag(2), c(0,0))\nY1 &lt;- X %*% P1\n\n# oblique projection\nP2 &lt;- matrix(c(0.71, 0.71, 0, -0.42, .42, 0.84), ncol=2)\nY2 &lt;- X %*% P2\n\n\nIn this example, the matrix \\(\\mathbf{X}\\) consists of 8 points at the vertices of a cube of size 10, as shown in Figure 3.36 (a). The projections \\(\\mathbf{Y}_1 = \\mathbf{P}_1 \\mathbf{X}\\) and \\(\\mathbf{Y}_2 = \\mathbf{P}_2 \\mathbf{X}\\) are shown in panels (b) and (c). To make it easier to relate the points in different views, shapes and colors are assigned so that each point has a unique combination of these attributes.6\n\npch &lt;- rep(15:18, times = 2)\ncolors &lt;- c(\"red\", \"blue\", \"darkgreen\", \"brown\")\ncol &lt;- rep(colors, each = 2)\ndata.frame(X, pch, col)\n#&gt;   x1 x2 x3 pch       col\n#&gt; 1  0  0  0  15       red\n#&gt; 2 10  0  0  16       red\n#&gt; 3  0 10  0  17      blue\n#&gt; 4 10 10  0  18      blue\n#&gt; 5  0  0 10  15 darkgreen\n#&gt; 6 10  0 10  16 darkgreen\n#&gt; 7  0 10 10  17     brown\n#&gt; 8 10 10 10  18     brown\n\n\n\n\n\n\n\n\nFigure 3.36: Projection example: (a) The 8 points in X form a cube of size 10; (b) the projection by P1 is the view ignoring x3 (two points coincide at each vertex); (c) the projection by P2 is an oblique view.\n\n\n\n\nBut, if we are traveling in the projection space of \\(\\mathbf{Y}\\), we need some signposts to tell us how the new dimensions relate to those of \\(\\mathbf{X}\\). The answer is provided simply by plotting the rows of \\(\\mathbf{P}\\) as vectors, as shown in Figure 3.37. In these plots, each row of \\(\\mathbf{P}_1\\) and \\(\\mathbf{P}_2\\) appears as a vector from the origin. It’s direction shows the contribution each of \\(\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\) make to the new coordinates \\(\\mathbf{y}_1\\) and \\(\\mathbf{y}_2\\).\nIn \\(\\mathbf{P}_1\\), the projected variable \\(\\mathbf{y}_1\\) is related only to \\(\\mathbf{x}_1\\), while \\(\\mathbf{y}_2\\) is related only to \\(\\mathbf{x}_2\\) \\(\\mathbf{x}_3\\) makes no contribution, and appears at the origin. However in the projection given by \\(\\mathbf{P}_2\\), \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) make the same contribution to \\(\\mathbf{y}_1\\), while \\(\\mathbf{x}_3\\) has no contribution to that horizontal axis. The vertical axis, \\(\\mathbf{y}_2\\) here is completely aligned with \\(\\mathbf{x}_3\\); \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) have vertical components that are half of that for \\(\\mathbf{x}_3\\) in absolute value.\nCodelibrary(matlib)\nop &lt;- par(mar=c(4, 5, 1, 1)+.1)\nxlim &lt;- ylim &lt;- c(-1.1, 1.1)\naxes.x &lt;- c(-1, 1, NA, 0, 0)\naxes.y &lt;- c(0, 0, NA, -1, 1)\nlabs &lt;- c(expression(x[1]), expression(x[2]), expression(x[3]))\nplot(xlim, ylim, type = \"n\", asp=1,\n     xlab = expression(y[1]), ylab = expression(y[2]),\n     cex.lab = 1.8)\ncircle(0, 0, 1, col = adjustcolor(\"skyblue\", alpha = 0.2))\nlines(axes.x, axes.y, col = \"grey\")\nvectors(P1, labels = labs, cex.lab = 1.8, lwd = 3, pos.lab = c(4, 2, 1))\n\nplot(xlim, ylim, type = \"n\", asp=1,\n     xlab = expression(y[1]), ylab = expression(y[2]),\n     cex.lab = 1.8)\ncircle(0, 0, 1, col = adjustcolor(\"skyblue\", alpha = 0.2))\nlines(axes.x, axes.y, col = \"grey\")\nvectors(P2, labels = labs, cex.lab = 1.8, lwd = 3)\npar(op)\n\n\n\n\n\n\n\n\n\n\nFigure 3.37: Variable vectors: Data variables viewed as vectors in the space of their projections. The angles of the x vectors with respect to the y coordinate axes show their relative contributions to each. The lengths of the x vectors show the relative degree to which they are represented in the space of ys. Left: the P1 projection; right: the P2 projection.\n\n\n\n\n\n3.7.1.1 Vector lengths\nIn Figure 3.37, the lengths of the \\(\\mathbf{x}\\) vectors reflect the relative degree to which each variable is represented in the space of the projection, and this is important for interpretation. For the \\(\\mathbf{P}_1\\) projection, \\(\\mathbf{x}_3\\) is of length 0, while \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) fill the unit circle. In the projection given by \\(\\mathbf{P}_2\\), all three \\(\\mathbf{x}\\) are approximately the same length.\nIn algebra, the length of a vector \\(\\mathbf{x}\\) is \\(||\\mathbf{x}|| = (\\mathbf{x}^\\mathsf{T} \\mathbf{x})^{1/2} = \\sqrt{\\Sigma x_i^2}\\), the Euclidean distance of the tip of the vector from the origin. In R, we calculate the lengths of row vectors in a projection matrix by transposing and using matlib::len().\n\nP1 |&gt; t() |&gt; matlib::len()\n#&gt; [1] 1 1 0\nP2 |&gt; t() |&gt; matlib::len()\n#&gt; [1] 0.825 0.825 0.840\n\n\n3.7.1.2 Joint-views\nTo interpret such projections, we want to see both the projected data and the signposts that tell us where we are in relation to the original variables. To do this, we can overlay the variable vectors represented by the rows of the projection matrix \\(\\mathbf{P}\\) onto plots like Figure 3.36 (b) and Figure 3.36 (c) to see how the axes in a projection relate to those in the data. To place these together on the same plot, we can either center the columns of \\(\\mathbf{Y}\\) at their means or shift the the columns of \\(\\mathbf{P}\\) to colMeans(Y). It is only the directions of the vectors that matters, so we are free to scale their lengths by any convenient factor.\n\nCodeY2s &lt;- scale(Y2, scale=FALSE)       # center Y2\nplot(Y2s, cex = 3, \n     asp = 1,\n     pch = pch, col = col,\n     xlab = expression(y[1]), ylab = expression(y[2]),\n     xlim = c(-10, 10), ylim = c(-10, 10), cex.lab = 1.8)\nr &lt;- 7\nvecs &lt;- (r*diag(3) %*% P2)\nvectors(vecs, labels = labs, cex.lab = 1.8, lwd = 2)\nvectors(-vecs, labels = NULL, lty = 1, angle = 1, col = \"gray\")\n\n\nThe plot in Figure 3.38 illustrates this, centering \\(\\mathbf{Y}\\), and multiplying the vectors in \\(\\mathbf{P}\\) by 7. To check your understanding, try to see if you can relate what is shown in this plot to the 3D plot in Figure 3.36 (a).\n\n\n\n\n\n\n\nFigure 3.38: The P2 projection of the data showing vectors for the original variables in the space of Y.\n\n\n\n\nThe idea of viewing low-dimensional projections of data together with vectors representing the contributions of the original variables to the dimensions shown in a display is also the basis of biplot techniques (Section 4.3) we will use in relation to principal components analysis.\n\n3.7.2 Touring methods\nThe trick of statistical touring methods is to generate a smooth sequence of interpolated projections \\(\\mathbf{P}_{(t)}\\) indexed by time \\(t\\), \\(\\mathbf{P}_{(1)}, \\mathbf{P}_{(2)}, \\mathbf{P}_{(3)}, \\dots, \\mathbf{P}_{(T)}\\). This gives a path of views \\(\\mathbf{Y}_{(t)} = \\mathbf{X} \\mathbf{P}_{(t)}\\), that can be animated in successive frames, as shown schematically in Figure 3.39.\n\n\n\n\n\n\n\nFigure 3.39: Interpolations: Illustration of a grand tour of interpolations of projection planes showing 2D scatterplots of the Penguin dataset. The seqeunce of views moves smoothly from an initial frame P(1) to a final frame P(T) where the penguin species are widely separated.\n\n\n\n\nAsimov’s (1985) original idea of the grand tour was that of a random path, picking orthogonal projections \\(\\mathbf{P}_{(i)}\\) at random. Given enough time, the grand tour gives a space-filling path and would eventually show every possible projection of the data. But it does so smoothly, by interpolating from one projection to the next. In the travel analogy, the path by road from London to Paris might go smoothly through Kent to Dover, thence via Amiens and Beauvais before reaching Paris. By air, the tour would follow a smoother geodesic path, and this is what the grand tour does. The sense in watching an animation of a statistical grand tour is that of continuous motion. The grand tour algorithm is described in detail by Buja et al. (2005) and Cook et al. (2008).\n\n\n3.7.2.1 Guided tours\nThe next big idea was that rather than traveling randomly in projection space one could take a guided tour, following a path that leads to “interesting projections”, such as those that reveal clusters, gaps in data space or outliers. This idea, called projection pursuit (Cook et al., 1995), works by defining a measure of interestingness of a data projection. In a guided tour, the next projection is chosen to increase that index, so over time the projection moves toward one that is maximizes that index.\nIn the time since Asimov (1985), there have been many implementations of touring visualization methods. XGobi (Swayne et al., 1998) for X-Windows displays on Linux systems provided a test-bed for dynamic, interactive graphic methods; it’s successor, GGobi (Cook & Swayne, 2007; Swayne et al., 2003) extended the range of touring methods to include a wider variety of projection pursuit indices.\n\n3.7.2.2 tourr package\nThe current state of art is best captured in the tourr package for R (Wickham et al., 2011; Wickham & Cook, 2024). It defines a tour to consist of three components:\n\n\ndata: An \\((n \\times p)\\) numerical data matrix to be viewed.\n\npath: A tour path function that produces a smoothed sequence of projection matrices \\(\\mathbf{P}_{(p \\times d)}\\) in \\(d\\). dimensions, for example grand_tour(d = 2) or guided_tour(index = holes).\n\ndisplay: A function that renders the projected data, for example display_xy() for a scatterplot, display_depth() for a 3D plot with simulated depth, or display_pcp() for a parallel coordinates plots\n\nThis very nicely separates the aspects of a tour, and allows one to think of and define new tour path methods and display methods. The package defines two general tour functions: animate() produces a real-time animation on a display device and render() saves image frames to disk, such as a .gif file.\n\nanimate(data, tour_path, display_method)\nrender(data, tour_path, display_method)\n\nThe tourr package provides a wide range of tour path methods and display methods:\n\n# tour path methods\ngrep(\"_tour$\", lsf.str(\"package:tourr\"), value = TRUE)\n#&gt;  [1] \"dependence_tour\"     \"frozen_guided_tour\" \n#&gt;  [3] \"frozen_tour\"         \"grand_tour\"         \n#&gt;  [5] \"guided_anomaly_tour\" \"guided_section_tour\"\n#&gt;  [7] \"guided_tour\"         \"little_tour\"        \n#&gt;  [9] \"local_tour\"          \"new_tour\"           \n#&gt; [11] \"planned_tour\"        \"planned2_tour\"      \n#&gt; [13] \"radial_tour\"\n\n# display methods\ngrep(\"display_\", lsf.str(\"package:tourr\"), value = TRUE)\n#&gt;  [1] \"display_andrews\"   \"display_density2d\" \"display_depth\"    \n#&gt;  [4] \"display_dist\"      \"display_faces\"     \"display_groupxy\"  \n#&gt;  [7] \"display_idx\"       \"display_image\"     \"display_pca\"      \n#&gt; [10] \"display_pcp\"       \"display_sage\"      \"display_scatmat\"  \n#&gt; [13] \"display_slice\"     \"display_stars\"     \"display_stereo\"   \n#&gt; [16] \"display_trails\"    \"display_xy\"\n\nTour path methods take a variety of optional arguments to specify the detailed behavior of the method. For example, most allow you to specify the number of dimension (d =) of the projections. The guided_tour() is of particular interest here.\n\nargs(guided_tour)\n#&gt; function (index_f, d = 2, alpha = 0.5, cooling = 0.99, max.tries = 25, \n#&gt;     max.i = Inf, search_f = search_geodesic, n_sample = 100, \n#&gt;     ...) \n#&gt; NULL\n\nIn this, index_f specifies a function that the method tries to optimize on its path and package defines four indices:\n\nHoles (holes()): This is sensitive to projections with separated clusters of points, with few points near the origin\nCentral mass (cmass()): Sensitive to projections with lots of points in the center, but perhaps with some outliers\nLinear discriminant analysis (lda_pp()): For data with a grouping factor, optimizes a measure of separation of the group means as in MANOVA or linear discriminant analysis.\nPDA analysis (pda_pp()): A penalized version of lda_pp() for cases of large \\(p\\) relative to sample size \\(n\\) (E.-K. Lee & Cook, 2009).\n\nIn addition, there is now a guided_anomaly_tour() that looks for the best projection of observations that are outside the data ellipsoid, finding a view showing observations with large Mahalanobis distances from the centroid.\n\n3.7.2.3 Penguin tours\nPenguins are a traveling species. They make yearly travels inland to breeding sites in early spring, repeating the patterns of their ancestors. Near the beginning of summer, adult penguins and their chicks return to the sea and spend the rest of the summer feeding there (Black et al., 2018). If they were also data scientists, they might wonder about the relations among among their cousins of different species and take a tour of their measurements…\n\nFor example, using the Penguins dataset, the following calls produce grand tours in 2, 3, and 4 dimensions. The 2D tour is displayed as a scatterplot, the 3D tour using simulated depth as shown by variation in point size and transparency, and the 4D tour is shown using a parallel coordinate plot.\n\ndata(peng, package = \"heplots\")\npeng_scaled &lt;- scale(peng[,3:6])\ncolnames(peng_scaled) &lt;- c(\"BL\", \"BD\", \"FL\", \"BM\")\n\nanimate(peng_scaled, grand_tour(d = 2), display_xy())\nanimate(peng_scaled, grand_tour(d = 3), display_depth())\nanimate(peng_scaled, grand_tour(d = 4), display_pcp())\n\n\n\n\n\n\n\n\n\n\n(a) 2D, scatterplot\n\n\n\n\n\n\n\n\n\n(b) 3D, simulated depth\n\n\n\n\n\n\n\n\n\n(c) 4D, parallel coordinates plot\n\n\n\n\n\n\nFigure 3.40: Grand tours of the penguin dataset in 2, 3, and 4 dimensions using different display_*() methods.\n\n\nTo illustrate, I’ll start with a grand tour designed to explore this 4D space of penguins. I’ll abbreviate the variables to two characters, “BL” = bill_length, “BD” = bill_depth, “FL” = flipper_length, and “BM” = body_mass and identify the penguin species using point shape (pch) and color (col).\nAs you watch this pay attention to the separation of the species and any other interesting features. What do you see?\n\ndata(peng, package = \"heplots\")\npeng_scaled &lt;- scale(peng[,3:6])\ncolnames(peng_scaled) &lt;- c(\"BL\", \"BD\", \"FL\", \"BM\")\n\npch &lt;- c(15, 16, 17)[peng$species] \ncex = 1.2\n\nset.seed(1234)\nanimate(peng_scaled,\n        tour_path = grand_tour(d=2),\n        display_xy(col = peng$species,\n                   palette = peng.colors(\"dark\"),\n                   pch = pch, cex = cex,\n                   axis.col = \"black\", \n                   axis.text.col = \"black\", \n                   axis.lwd = 1.5))\n\n\n\n\n\n\n\n\nFigure 3.41: Animation of a grand tour of the Penguin data.\n\n\n\n\nFigure 3.42 shows three frames from this movie. The first (a) is the initial frame that shows the projection in the plane of bill depth and bill length. The variable vectors indicate that bill length differentiates Adelie penguins from the others. In frame (b), the three species are widely separated, with bill depth distinguishing Gentoo from the others. In frame (c) the three species are largely mixed, but two points stand out as outliers, with exceptionally long bills compared to the rest.\n\n\n\n\n\n\n\n\n\n(a) Initial frame\n\n\n\n\n\n\n\n\n\n(b) Clusters\n\n\n\n\n\n\n\n\n\n(c) Outliers\n\n\n\n\n\n\nFigure 3.42: Three frames from the grand tour of the Penguin data. (a) The initial frame is the projection showing only BD and BL, where bill length conveniently separates Adelie from the other two species. (b) A frame that shows the three species more widely separated. (c) A frame that shows two outliers with very large bills.\n\n\n\n\n\n\n\n\n\n\nLet’s take the penguins on a guided tour, trying to find views that show the greatest separations among the penguin species; that is, a guided tour, optimizing the lda_pp() index.\n\nset.seed(1234)\nanimate(peng_scaled, \n        guided_tour(lda_pp(peng$species)),\n        display_xy(col = peng$species,\n                   palette = peng.colors(\"dark\"),\n                   pch = pch,\n                   cex = cex)\n)\n\n\n\n\n\n\n\n\nFigure 3.43: Animation of a guided tour of the Penguin data, using a tour criterion designed to find an optimal separation among the penguin species. The animation shows three loops of the sequence of projections and stops when the LDA criterion cannot be improved.\n\n\n\n\nTODO: I’m trying to balance what will/can be shown in the HTML version vs. the printed PDF. Needs text here specifically for the PDF version.\n\n\n\n\n\n\n\n\n\n(a) Optimizing lda_pp()\n\n\n\n\n\n\n\n\n\n(b) Optimizing anomaly_index()\n\n\n\n\n\n\nFigure 3.44: Guided tours: These figures show the final frame in the animations of guided tours designed to find the projection that optimize an index. (a) The lda_pp() criterion optimizes the separation of the means for species relative to within-group variation. (b) The anomalies_index() optimizes the average Mahalanobis distance of points from the centroid\n\n\nThese examples are intended to highlight what is possible with dynamic graphics for exploring high-dimensional data visually. Cook & Laa (2024) extend the discussion of these methods from Cook & Swayne (2007) (which used Ggobi) to the tourr package. They illustrate dimension reduction, various cluster analysis methods, trees and random forests and some machine-learning techniques.\n\nIdeally, we should be able interact with a tour,\n\npausing when we see something interesting and saving the view for later analysis;\nselecting or highlighting unusual points,\nchanging tour methods or variables displayed on the fly, and so forth.\n\nSome packages that provide these capabilities are: detourr (Hart & Wang, 2022) liminal (S. Lee, 2021) and langevitour (Harrison, 2023, 2024) The loon package (Waddell & Oldford, 2023) is a general toolkit that enables highly interactive data visualization. It provides a loon.tour package (Xu & Oldford, 2021) for using touring methods within the loon environment.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#sec-network",
    "href": "03-multivariate_plots.html#sec-network",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.8 Network diagrams",
    "text": "3.8 Network diagrams\nA major theme throughout this chapter has been to understand how to extend data visualization from simple bivariate scatterplots to increasingly more complex situations with larger datasets. With a moderate number of variables, techniques such as smoothing, summarizing with data ellipses and fitted curves, and visual thinning can be used to tame “big \\(N\\)” datasets with thousands of cases.\nHowever “big \\(p\\)” datasets, with more than a moderate number (\\(p\\)) of variables still remain a challenge. It is hard to see how the more advanced methods (corrgrams, parallel coordinate) described earlier could cope with \\(p = 20, 50, 100, 500, \\dots\\) variables. At some point, each of these begins to break down for the purpose of visualizing associations among many variables. We are forced to thin the information presented in graphs more and more as the number of variables increases.\nIt turns out that there is a way to increase the number of variables displayed dramatically, if we are mainly interested in the pairwise correlations for reasonably normally distributed data. A graphical network diagram portrays variables by nodes (vertices), connected by (weighted) edges whose properties reflect the strength of connections between pairs, such as a correlation. Such diagrams can reveal properties not readily seen by other means.\nAs an example consider Figure 3.45, which portrays the correlations among 25 self-report items reflecting 5 factors (the “Big Five”) considered in personality psychology to represent the dominant aspects of all of personality. These factors are easily remembered by the acronum OCEAN: Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism. The dataset, psych::bfi, contains data from an online sample of \\(n=2800\\) with 5 items for each scale.\nIn this figure (taken from Rodrigues (2021)), the item nodes are labeled according to the OCEAN factor they are assumed to measure. For 25 items, there are \\(25 \\times 24 / 2 = 300\\) correlations, way too much to see. A clearer picture arises when we reduce the number of edges shown according to some criterion. Here, edges are drawn only between nodes where the correlation is considered important by a method (“glasso” = graphical LASSO) designed to make the graph optimally sparse.\n\n\n\n\n\n\n\n\nFigure 3.45: Network diagram of the correlations among 25 items from a Big-Five personality scale, 5 items for each scale. The magnitude of a correlation is shown by the thickness and transparency of the edge between two item nodes. The sign of a correlation is shown by edge color and style: solid blue for positive and dashed red for negative. Source: Rodrigues (2021)\n\n\n\n\nThe edges shown in Figure 3.45 reflect the Pearson correlation between a given pair of items by the visual attributes of color and line style: magnitude is shown by both the thickness and transparency of the edge; the sign of the correlation is shown by color and line type: solid blue for positive correlations and dashed red for negative ones.\nAccording to some theories, the five personality factors should be largely non-overlapping, so there should not be many edges connecting items of one factor with those of another. Yet, there are quite a few cross-factor connections in Figure 3.45, so perhaps the theory is wrong, or, more likely, the 25 items are not good representatives of these underlying dimensions. The network diagram shown here is a visual tool for thought and refinement. See Costantini et al. (2015) for a tutorial on network analysis of personality data in R.\nNetwork diagrams stem from mathematical graph theory (Bondy & Murty, 2008; West, 2001) of the abstract properties of nodes and edges used to represent pairwise relationships. These can be used to model many types of relations and processes in physical, biological, social and other sciences, where such properties as connectedness, centrality, cliques of connected nodes and so forth provide a vocabulary used to understand and explain complex systems.\nFor one example, Grandjean (2016) used network analysis to study the connections among 2500 Twitter users (nodes) who identified as belonging to a “digital humanities” community from the relations (edges) of who follows whom. Grandjean also used these methods to study the relationships among characters in Shakespeare’s tragedies in terms of the characters (nodes) and edges representing how often they appeared in the same scene.\nThe wide applicability of these ideas has led to what is now called network science (Barab’asi, 2016) encompassing computer networks, biological networks, cognitive and semantic networks, and social networks. Recent developments in psychology led to a framework of network psychometrics (Isvoranu et al., 2022), where, for example, symptoms of psychopathology (phobias, anxiety, substance abuse) can be conceptualized as an interconnected network of clusters and studied for possible causal relations (Robinaugh et al., 2019).\nBecause a network diagram can potentially reflect hundreds of variables, various graph layout algorithms have been developed to automatically position the nodes so as to generate aesthetically pleasing network visualizations that emphasize important structural properties, like clusters and central nodes, while minimizing visual clutter (many crossing lines) to promote understandability and usability.\nThere are quite a few R packages for constructing network diagrams, both static and dynamic / interactive, and these differ considerably in how the information required for a graph is structured as R objects, and the flexibility to produce attractive graphs. Among these, igraph (Csárdi et al., 2024) structures the data as a dataset of vertices and edges with properties\n-&gt; packages: qgraph, …\n\n3.8.1 Crime data\nFor the present purposes, let’s see what network diagrams can tell us about the crime data analyzed earlier. Here, I first reorder the variables as in Figure 3.29. In the call to qgraph(), the argument minimum = \"sig\" says to show only the edges for significant correlations (at \\(\\alpha = 0.01\\) here). In Figure 3.46, the variable nodes are positioned around a circle (layout = \"circle\"), which is the default.\n\n\nlibrary(qgraph)\nord &lt;- corrMatOrder(crime.cor, order = \"AOE\")\nrownames(crime.cor)[ord]\n#&gt; [1] \"murder\"   \"assault\"  \"rape\"     \"robbery\"  \"burglary\"\n#&gt; [6] \"larceny\"  \"auto\"\ncrime.cor &lt;- crime.cor[ord, ord]\n\n# \"association graph\": network of correlations\nqgraph(crime.cor, \n  title = \"Crime data:\\ncorrelations\", title.cex = 1.5,\n  graph = \"cor\",\n  layout = \"circle\",\n  minimum = \"sig\", sampleSize = nrow(crime), alpha = 0.01,\n  color = grey(.9), vsize = 12,\n  labels = rownames(crime.cor),\n  posCol = \"blue\")\n\n\n\n\n\n\nFigure 3.46: Network diagram depicting the correlations among the crime variables. Only edges for correlations that are significant at the \\(\\alpha = 0.01\\) level are displayed.\n\n\n\n\n\nIn this figure, you can see the group of property crimes (auto theft, larceny, burglary) at the left separated from the violent crimes against persons at the right.\n\n3.8.2 Partial correlations\nAmong the more important statistical applications of network graph theory is the idea that you can also use them to study the the partial (conditional) associations among variables with the contributions of all other variables removed in what are called Graphical Gaussian Models (GGMs) (Højsgaard et al., 2012; Lauritzen, 1996). In a network diagram of these partial associations,\n\nThe edges between nodes represent the partial correlations between those variables.\nThe absence of an edge between two nodes indicates their variables are conditionally independent, given the other variables.\n\nSo, whereas a network diagram of correlations shows marginal associations ignoring other variables, one of partial correlations allows you to visualize the direct relationship between each pair of variables, removing the indirect effects that might be mediated through all other variables.\nFor a set of variables \\(X = \\{x_1, x_2, \\dots, x_p \\}\\), the partial correlation between \\(x_i\\) and \\(x_i\\), controlling for all other variables \\(Z = X \\setminus \\{x_i, x_j\\} = x_\\text{others}\\) is equivalent to the correlation between the residuals of the linear regressions of \\(x_i\\) on all other \\(\\mathbf{Z}\\) and \\(x_j\\) on \\(\\mathbf{Z}\\). (The notation \\(X \\setminus \\{x_i, x_j\\}\\) is read as “\\(X\\) without the set \\(\\{x_i, x_j\\}\\)”).\nMathematically, let \\(\\hat{x}_i\\) and \\(\\hat{x}_j\\) be the predicted values from the linear regressions of \\(x_i\\) on \\(\\mathbf{Z}\\) and of \\(x_j\\) on \\(\\mathbf{Z}\\), respectively. The partial correlation \\(p_{ij}\\) between \\(x_i\\) and \\(x_j\\) controlling for \\(\\mathbf{Z}\\) is given by: \\[\np_{x_i,x_j|\\mathbf{Z}} = r( x_i, x_j \\mid \\text{others}) = \\text{cor}[ (x_i - \\hat{x}_i),\\; (x_j - \\hat{x}_j)]\n\\tag{3.3}\\]\nBut, rather than running all these linear regressions, they can all be computed from the inverse of the correlation matrix (Whittaker, 1990, Ch. 5), a relation first noted by Dempster (1972). Let \\(\\mathbf{R}\\) be the correlation matrix of the variables. Then, the matrix \\(\\mathbf{P}\\) of partial correlations can be obtained from the negative inverse, \\(-\\mathbf{R}^{-1}\\), standardized to a correlation matrix by dividing by the square root of product of its diagonal elements, \\[\nP_{ij} = - \\frac{R^{-1}_{ij}}{\\sqrt{(R^{-1}_{ii} \\cdot R^{-1}_{jj})}} \\:\\: .\n\\]\n\nThe practical implications of this are:\n\nIf a partial correlation is close to zero, it suggests the relationship between two variables is primarily mediated through other variables.\nNon-zero partial correlations indicate a direct relationship that persists after controlling for other variables.\n\nFigure 3.47 shows the partial correlation network for the crime data, using the qgraph() argument graph = \"pcor\" To provide a more interpretable result, the argument layout = \"spring\" positions the nodes using a force-embedded algorithm where edges act like springs, pulling connected nodes together and unconnected nodes repel each other, pushing them apart.\n\nqgraph(crime.cor, \n       title = \"Crime data:\\npartial correlations\", title.cex = 1.5,\n       graph = \"pcor\",\n       layout = \"spring\", repulsion = 1.2,\n       minimum = \"sig\", sampleSize = nrow(crime), alpha = 0.05,\n       color = grey(.9), vsize = 14,\n       labels = rownames(crime.cor),\n       edge.labels = TRUE, edge.label.cex = 1.7,\n       posCol = \"blue\")\n\n\n\n\n\n\nFigure 3.47: Network diagram of partial correlations among the crime variables, controlling for all others. Variable nodes have been positioned by a “spring” layout method …\n\n\n\n\nFigure 3.47 shows that, once all other crime variables are controlled for each pair, there remain only a few partial correlations at the \\(\\alpha = 0.05\\) level. Of these, only the largest three in absolute value are significant at \\(\\alpha = 0.01\\).\nThus, once all other variables are taken into account, what remains is mainly a strong positive association between burglary and larceny and a moderate one between auto theft and robbery. There also remains a moderate negative correlation between murder and larceny. The spring layout makes it clear that, with suppression of weak edges, auto theft and robbery form a cluster separated from the other variables.\n\n3.8.3 Visualizing partial correlations\nJust as you can visualize marginal association between variables in a scatterplot, you can also visualize conditional association. A partial variables plot is simply a scatterplot of the partial residuals \\(e_i = (x_i - \\hat{x}_i)\\) from a regression of \\(x_i\\) on the other variables \\(Z\\) against those \\(e_j = (x_j - \\hat{x}_j)\\) for another variable \\(x_j\\).\nIn this, you can use all the bells and whistles of standard scatterplots (regression lines, smooths, data ellipses, …) to listen more attentively to the story partial association has to tell. The function pvPlot() calculates the partial residuals and then calls car::dataEllipse() for display. The five most “unusual” observations by Mahalanobis \\(D^2\\) are identified with their abbreviated state labels. Figure 3.48 shows these plots for the variable pairs with the two largest partial correlations.\n\n\nsource(\"R/pvPlot.R\")\n# select numeric, make `st` into rownames\ncrime.num &lt;- crime |&gt;\n  tibble::column_to_rownames(\"st\") |&gt;\n  dplyr::select(where(is.numeric))\n\npvPlot(crime.num, vars = c(\"burglary\", \"larceny\"), \n       id = list(n=5),\n       cex.lab = 1.5)\npvPlot(crime.num, vars = c(\"robbery\", \"auto\"),\n       id = list(n=5),\n       cex.lab = 1.5)\n\n\n\n\n\n\n\n\nFigure 3.48: Partial variables plots for burglary and larceny (left) and for robbery and auto theft (right) in the network diagram for partial correlations of the crime variables.\n\n\n\n\nIn the pvPlot for burglary and larceny, you can see that the high partial correlation is largely driven by the extreme points at the left and and right sides. Once all other variables are taken into account, Arizona (AZ) and Hawaii (HI) have larger incidence of both crimes, while Arkansas (AK) are smaller on both.\nIn the pvPlot for robbery and auto theft, New York stands out as an influential, high-leverage point (see Section 6.6); Massachusetts (MA) is noteworthy because auto theft in that state is considerably higher than what would be predicted from all other variables.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#multivariate-thinking-and-visualization",
    "href": "03-multivariate_plots.html#multivariate-thinking-and-visualization",
    "title": "3  Plots of Multivariate Data",
    "section": "\n3.9 Multivariate thinking and visualization",
    "text": "3.9 Multivariate thinking and visualization\nTODO: These are just initial notes on a chapter summary, and pointing the way to dimension reduction methods in Chapter 4.\nThis chapter has covered a lot of ground. We started with simple scatterplots and how to enhance them with graphical summaries and annotations …\nThe two curses\nMultivariate data is often said to suffer from the curse of dimensionality (ref: Bellman1957), meaning that that as the dimensionality of data increases, the volume of the space increases so fast that the available data become sparse, so that the amount of data needed often grows exponentially with the dimensionality.\nBut, there is another curse here, the curse of two-dimensionality, meaning that as the dimensionality of data increases, what we can display and understand from a 2D image decreases rapidly with the number of dimensions of data. …\nPackage summary\nFor development, keep track of the packages used in each chapter.\n\n16 packages used here: car, carData, corrgram, corrplot, dplyr, GGally, ggdensity, ggpcp, ggplot2, grid, knitr, patchwork, qgraph, tidyr, tourr, vcd\n\n\n\n\n\n\nAndrews, D. F. (1972). Plots of high dimensional data. Biometrics, 28, 123–136.\n\n\nAsimov, D. (1985). Grand tour. SIAM Journal of Scientific and Statistical Computing, 6(1), 128–143.\n\n\nBarab’asi, A.-L. (2016). Network science. Cambridge University Press.\n\n\nBecker, R. A., Cleveland, W. S., & Shyu, M.-J. (1996). The visual design and control of trellis display. Journal of Computational and Graphical Statistics, 5(2), 123–155.\n\n\nBlack, C., Southwell, C., Emmerson, L., Lunn, D., & Hart, T. (2018). Time-lapse imagery of adélie penguins reveals differential winter strategies and breeding site occupation. PLOS ONE, 13(3), e0193532. https://doi.org/10.1371/journal.pone.0193532\n\n\nBlishen, B., Carroll, W., & Moore, C. (1987). The 1981 socioeconomic index for occupations in canada. Canadian Review of Sociology/Revue Canadienne de Sociologie, 24(4), 465–488. https://doi.org/10.1111/j.1755-618x.1987.tb00639.x\n\n\nBondy, J. A., & Murty, U. S. R. (2008). Graph theory. Springer.\n\n\nBuja, A., Cook, D., Asimov, D., & Hurley, C. (2005). Computational methods for high-dimensional rotations in data visualization. In J. S. CR Rao EJ Wegman (Ed.), Handbook of statistics (pp. 391–413). Elsevier. https://doi.org/10.1016/s0169-7161(04)24014-7\n\n\ncagne, M. (1885). Coordonnées parallèles et axiales: Méthode de transformation géométrique et procédé nouveau de calcul graphique déduits de la considération des coordonnées parallèlles. Gauthier-Villars. http://historical.library.cornell.edu/cgi-bin/cul.math/docviewer?did=00620001&seq=3\n\n\nChambers, J. M., & Hastie, T. J. (1991). Statistical models in s (p. 624). Chapman & Hall/CRC.\n\n\nCleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association, 74, 829–836.\n\n\nCleveland, W. S. (1985). The elements of graphing data. Wadsworth Advanced Books.\n\n\nCleveland, W. S., & Devlin, S. J. (1988). Locally weighted regression: An approach to regression analysis by local fitting. Journal of the American Statistical Association, 83, 596–610.\n\n\nCleveland, W. S., & McGill, R. (1984). Graphical perception: Theory, experimentation and application to the development of graphical methods. Journal of the American Statistical Association, 79, 531–554.\n\n\nCleveland, W. S., & McGill, R. (1985). Graphical perception and graphical methods for analyzing scientific data. Science, 229, 828–833.\n\n\nCook, D., Buja, A., Cabrera, J., & Hurley, C. (1995). Grand tour and projection pursuit. Journal of Computational and Graphical Statistics, 4(3), 155. https://doi.org/10.2307/1390844\n\n\nCook, D., Buja, A., Lee, E.-K., & Wickham, H. (2008). Grand tours, projection pursuit guided tours, and manual controls. In Handbook of data visualization (pp. 295–314). Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-33037-0_13\n\n\nCook, D., & Laa, U. (2024). Interactively exploring high-dimensional data and models in R. Online. https://dicook.github.io/mulgar_book/\n\n\nCook, D., & Swayne, D. F. (2007). Interactive and dynamic graphics for data analysis : With R and GGobi. Springer. http://www.ggobi.org/book/\n\n\nCostantini, G., Epskamp, S., Borsboom, D., Perugini, M., Mõttus, R., Waldorp, L. J., & Cramer, A. O. J. (2015). State of the aRt personality research: A tutorial on network analysis of personality data in R. Journal of Research in Personality, 54, 13–29. https://doi.org/10.1016/j.jrp.2014.07.003\n\n\nCsárdi, G., Nepusz, T., Traag, V., Horvát, S., Zanini, F., Noom, D., & Müller, K. (2024). igraph: Network analysis and visualization in r. https://doi.org/10.5281/zenodo.7682609\n\n\nDempster, A. P. (1969). Elements of continuous multivariate analysis. Addison-Wesley.\n\n\nDempster, A. P. (1972). Covariance selection. Biometrics, 28(1), 157–175.\n\n\nEmerson, J. W., Green, W. A., Schloerke, B., Crowley, J., Cook, D., Hofmann, H., & Wickham, H. (2013). The generalized pairs plot. Journal of Computational and Graphical Statistics, 22(1), 79–91. http://www.tandfonline.com/doi/ref/10.1080/10618600.2012.694762\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (Third edition.). SAGE.\n\n\nFox, J., Weisberg, S., & Price, B. (2023). Car: Companion to applied regression. https://CRAN.R-project.org/package=car\n\n\nFriendly, M. (1991). SAS System for statistical graphics (1st ed.). SAS Institute. http://www.sas. com/service/doc/pubcat/uspubcat/ind_files/56143.html\n\n\nFriendly, M. (1994). Mosaic displays for multi-way contingency tables. Journal of the American Statistical Association, 89, 190–200. http://www.jstor.org/stable/2291215\n\n\nFriendly, M. (1999). Extending mosaic displays: Marginal, conditional, and partial views of categorical data. Journal of Computational and Graphical Statistics, 8(3), 373–395. http://datavis.ca/papers/drew/drew.pdf\n\n\nFriendly, M. (2002). Corrgrams: Exploratory displays for correlation matrices. The American Statistician, 56(4), 316–324. https://doi.org/10.1198/000313002533\n\n\nFriendly, M. (2022). The life and works of andré-michel guerry, revisited. Sociological Spectrum, 42(4-6), 233–259. https://doi.org/10.1080/02732173.2022.2078450\n\n\nFriendly, M. (2023). vcdExtra: Vcd extensions and additions. https://friendly.github.io/vcdExtra/\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays. Computational Statistics and Data Analysis, 43(4), 509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGannett, H. (1898). Statistical atlas of the united states, eleventh (1890) census. U.S. Government Printing Office.\n\n\nGorman, K. B., Williams, T. D., & Fraser, W. R. (2014). Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis). PLoS ONE, 9(3), e90081. https://doi.org/10.1371/journal.pone.0090081\n\n\nGrandjean, M. (2016). A social network analysis of Twitter: Mapping the digital humanities community. Cogent Arts &Amp; Humanities, 3(1), 1171458. https://doi.org/10.1080/23311983.2016.1171458\n\n\nGuerry, A.-M. (1833). Essai sur la statistique morale de la France. Crochard.\n\n\nHarrison, P. (2023). Langevitour: Smooth interactive touring of high dimensions, demonstrated with scRNA-seq data. The R Journal, 15(2), 206–219. https://doi.org/10.32614/RJ-2023-046\n\n\nHarrison, P. (2024). Langevitour: Langevin tour. https://logarithmic.net/langevitour/\n\n\nHart, C., & Wang, E. (2022). Detourr: Portable and performant tour animations. https://CRAN.R-project.org/package=detourr\n\n\nHartigan, J. A. (1975a). Clustering algorithms. John Wiley; Sons.\n\n\nHartigan, J. A. (1975b). Printer graphics for clustering. Journal of Statistical Computing and Simulation, 4, 187–213.\n\n\nHofmann, H., VanderPlas, S., & Ge, Y. (2022). Ggpcp: Parallel coordinate plots in the ggplot2 framework. https://github.com/heike/ggpcp\n\n\nHofstadter, D. R. (1979). Gödel, escher, bach: An eternal golden braid. Basic Books.\n\n\nHøjsgaard, S., Edwards, D., & Lauritzen, S. (2012). Graphical models with R. Springer Science & Business Media.\n\n\nHorst, A., Hill, A., & Gorman, K. (2022). Palmerpenguins: Palmer archipelago (antarctica) penguin data. https://allisonhorst.github.io/palmerpenguins/\n\n\nInselberg, A. (1985). The plane with parallel coordinates. The Visual Computer, 1, 69–91.\n\n\nIsvoranu, A.-M., Epskamp, S., Waldorp, L. J., & Borsboom, D. (2022). Network psychometrics with r: A guide for behavioral and social scientists. Routledge. https://doi.org/10.4324/9781003111238\n\n\nLauritzen, S. L. (1996). Graphical models. Oxford University Press.\n\n\nLee, E.-K., & Cook, D. (2009). A projection pursuit index for large p small n data. Statistics and Computing, 20(3), 381–392. https://doi.org/10.1007/s11222-009-9131-1\n\n\nLee, S. (2021). Liminal: Multivariate data visualization with tours and embeddings. https://CRAN.R-project.org/package=liminal\n\n\nMartí, R., & Laguna, M. (2003). Heuristics and meta-heuristics for 2-layer straight line crossing minimization. Discrete Applied Mathematics, 127(3), 665–678.\n\n\nMeyer, D., Zeileis, A., Hornik, K., & Friendly, M. (2024). Vcd: Visualizing categorical data. https://CRAN.R-project.org/package=vcd\n\n\nMonette, G. (1990). Geometry of multiple regression and interactive 3-D graphics. In J. Fox & S. Long (Eds.), Modern methods of data analysis (pp. 209–256). SAGE Publications.\n\n\nOtto, J., & Kahle, D. (2023). Ggdensity: Interpretable bivariate density visualization with ggplot2. https://jamesotto852.github.io/ggdensity/\n\n\nPearson, K. (1901). On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 6(2), 559–572.\n\n\nPineo, P. O., & Porter, J. (2008). Occupational prestige in canada. Canadian Review of Sociology, 4(1), 24–40. https://doi.org/10.1111/j.1755-618x.1967.tb00472.x\n\n\nRobinaugh, D. J., Hoekstra, R. H. A., Toner, E. R., & Borsboom, D. (2019). The network approach to psychopathology: A review of the literature 2008–2018 and an agenda for future research. Psychological Medicine, 50(3), 353–366. https://doi.org/10.1017/s0033291719003404\n\n\nSarkar, D. (2024). Lattice: Trellis graphics for r. https://lattice.r-forge.r-project.org/\n\n\nSchloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M., Thoen, E., Elberg, A., & Crowley, J. (2024). GGally: Extension to ggplot2. https://ggobi.github.io/ggally/\n\n\nScott, D. W. (1992). Multivariate density estimation: Theory, practice, and visualization. Wiley.\n\n\nSilverman, B. W. (1986). Density estimation for statistics and data analysis. Chapman & Hall.\n\n\nSimpson, E. H. (1951). The interpretation of interaction in contingency tables. Journal of the Royal Statistical Society, Series B, 30, 238–241.\n\n\nSwayne, D. F., Cook, D., & Buja, A. (1998). XGobi: Interactive dynamic data visualization in the x window system. Journal of Computational and Graphical Statistics, 7(1), 113–130. https://doi.org/10.1080/10618600.1998.10474764\n\n\nSwayne, D. F., Lang, D. T., Buja, A., & Cook, D. (2003). GGobi: Evolving from XGobi into an extensible framework for interactive data visualization. Computational Statistics &Amp; Data Analysis, 43(4), 423–444. https://doi.org/10.1016/s0167-9473(02)00286-4\n\n\nVanderPlas, S., Ge, Y., Unwin, A., & Hofmann, H. (2023). Penguins go parallel: A grammar of graphics framework for generalized parallel coordinate plots. Journal of Computational and Graphical Statistics, 1–16. https://doi.org/10.1080/10618600.2023.2195462\n\n\nWaddell, A., & Oldford, R. W. (2023). Loon: Interactive statistical data visualization. https://CRAN.R-project.org/package=loon\n\n\nWegman, E. J. (1990). Hyperdimensional data analysis using parallel coordinates. Journal of the American Statistical Association, 85(411), 664–675.\n\n\nWei, T., & Simko, V. (2024). Corrplot: Visualization of a correlation matrix. https://github.com/taiyun/corrplot\n\n\nWest, D. B. (2001). Introduction to graph theory. Prentice hall.\n\n\nWhittaker, J. (1990). Graphical models in applied multivariate statistics. John Wiley; Sons.\n\n\nWickham, H., & Cook, D. (2024). Tourr: Tour methods for multivariate data visualisation. https://github.com/ggobi/tourr\n\n\nWickham, H., Cook, D., Hofmann, H., & Buja, A. (2011). Tourr: An R package for exploring multivariate data with projections. Journal of Statistical Software, 40(2). https://doi.org/10.18637/jss.v040.i02\n\n\nWood, S. N. (2006). Generalized additive models: An introduction with r. Chapman; Hall/CRC Press.\n\n\nWright, K. (2021). Corrgram: Plot a correlogram. https://kwstat.github.io/corrgram/\n\n\nXu, Z., & Oldford, R. W. (2021). Loon.tour: Tour in ’loon’. https://cran.r-project.org/package=loon.tourr",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "03-multivariate_plots.html#footnotes",
    "href": "03-multivariate_plots.html#footnotes",
    "title": "3  Plots of Multivariate Data",
    "section": "",
    "text": "Confidence bands allow us to visualize the uncertainty around a fitted regression curve, which can be of two types: pointwise intervals or simultaneous intervals. The default setting in `ggplot2::geom_smooth() calculates pointwise intervals (using stats::predict.lm(..., interval=\"confidence\") at a confidence level \\(1-\\alpha\\) for the predicted response at each value \\(x_i\\) of a predictor, and have the frequentist interpretation that over repeated sampling only \\(100\\;\\alpha\\) of the predictions at \\(x_i\\) will be outside that interval. In contrast, simultaneous intervals are calculated so that \\(1 - \\alpha\\) is the probability that all of them cover their corresponding true values simultaneously. These are necessarily wider than pointwise intervals. Commonly used methods for constructing simultaneous confidence bands in regression are the Bonferroni and Scheffé methods, which control the family-wise error rate over all values of \\(x_i\\). See  for precise definitions of these terms. These are different from a prediction band, which is used to represent the uncertainty about the value of a new data-point on the curve, but subject to the additional variance reflected in one observation.↩︎\nThe classic study by Cleveland & McGill (1984);Cleveland & McGill (1985) shows that judgements of magnitude along a common scale are more accurate than those along separate, aligned scales.↩︎\nThe dataset was collected by Bernard Blishen, William Carroll and Catherine Moore, but apparently unpublished. A version updated to the 1981 census is described in Blishen et al. (1987).↩︎\nOther implementations of parallel coordinate plots in R include: MASS::parcoord(), GGally::ggparcoord() andPairViz::pcp()`. The ggpcp version used here is the most general.↩︎\nThis example was modified from one used by Cook et al. (2008).↩︎\nPlot shapes given by pch = 15:18 correspond to: filled square (15), filled circle (16), filled triangle point-up (17), filled diamond (18).↩︎",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Plots of Multivariate Data</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html",
    "href": "04-pca-biplot.html",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "4.1 Flatland and Spaceland\nThere was a cloud in the sky above Flatland one day. But it was a huge, multidimensional cloud of sparkly points that might contain some important message, perhaps like the hidden EUREKA (Figure 5), or perhaps forecasting the upcoming harvest, if only Flatlanders could appreciate it.\nA leading citizen, A SQUARE, who had traveled once to Spaceland and therefore had an inkling of its majesty beyond the simple world of his life in the plane looked at that cloud and had a brilliant thought, an OMG moment:\nAs it happened, our Square friend, although he could never really see in three dimensions, he could now at least think of a world described by height as well as breadth and width, and think of the shadow cast by a cloud as something mutable, changing size and shape depending on its’ orientation over Flatland.\nAnd what a world it was, inhabited by Pyramids, Cubes and wondrous creatures called Polyhedrons with many \\(C\\)orners, \\(F\\)aces and \\(E\\)dges. Not only that, but all those Polyhedra were forced in Spaceland to obey a magic formula: \\(C + F - E = 2\\).1 How cool was that!\nIndeed, there were even exalted Spheres, having so many faces that its surface became as smooth as a baby’s bottom with no need for pointed corners or edges, just as Circles were the smoothest occupants of his world with far too many sides to count. It was his dream of a Sphere passing through Flatland (Figure 1) that first awakened him to a third dimension.\nHe also marveled at Ellipsoids, as smooth as Spheres, but in Spaceland having three natural axes of different extent and capable of being appearing fatter or slimmer when rotated from different views. An Ellipsoid had magical properties: it could appear as so thin in one or more dimensions that it became a simple 2D ellipse, or a 1D line, or even a 0D point (Friendly et al., 2013).\nAll of these now arose in Square’s richer 3D imagination. And, all of this came from just one more dimension than his life in Flatland.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-spaceland",
    "href": "04-pca-biplot.html#sec-spaceland",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "It is high time that I should pass from these brief and discursive notes about Flatland to the central event of this book, my initiation into the mysteries of Space. THAT is my subject; all that has gone before is merely preface — Edwin Abbott, Flatland, p. 57.\n\n\n\n\n“Oh, can I, in my imagination, rotate that cloud and squeeze its juice so that it rains down on Flatland with greatest joy?”\n\n\n\n\n\n\n\n4.1.1 Multivariate juicers\nUp to now, we have also been living in Flatland. We have been trying to understand data in data space of possibly many dimensions, but confined to the 2D plane of a graph window. Scatterplot matrices and parallel coordinate plots provided some relief. The former did so by projecting the data into sets of 2D views in the coordinates of data space; the latter did so by providing multiple axes in a 2D space along which we could trace the paths of individual observations.\nThis chapter is about seeing data in a different projection, a low-dimensional (usually 2D) space that squeezes out the most juice from multidimensional data for a particular purpose (Figure 4.1), where what we want to understand can be more easily seen.\n\n\n\n\n\n\n\nFigure 4.1: A multivariate juicer takes data from possibly high-dimensional data space and transforms it to a lower-dimenional space in which important effects can be more easily seen.\n\n\n\n\nHere, I concentrate on principal components analysis (PCA), whose goal reflects A Square’s desire to see that sparkly cloud of points in \\(nD\\) space in the plane showing the greatest variation (squeezing the most juice) among all other possible views. This appealed to his sense of geometry, but left him wondering how the variables in that high-D cloud were related to the dimensions he could see in a best-fitting plane.\nThe idea of a biplot, showing the data points in the plane, together with thick pointed arrows—variable vectors— in one view is the other topic explained in this chapter (Section 4.3). The biplot is the simplest example of a multivariate juicer. The essential idea is to project the cloud of data points in \\(n\\) dimensions into the 2D space of principal components and simultaneously show how the original variables relate to this space. For exploratory analysis to get an initial, incisive view of a multivariate dataset, a biplot is often my first choice.\n\n\n\n\n\n\nLooking ahead\n\n\n\nI’m using the term multivariate juicer here to refer the wider class of dimension reduction techniques, used for various purposes in data analysis and visualization. PCA is the simplest example and illustrates the general ideas.\nThe key point is that these methods are designed to transform the data into a low-dimensional space for a particular goal or purpose. In PCA, the goal is to extract the greatest amount of total variability in the data. In the context of univariate multiple regression, the goal is often to reduce the number of predictors necessary to account for an outcome variable, called feature extraction in the machine learning literature.\nWhen the goal is to best distinguish among groups discriminant analysis finds uncorrelated weighted sums of predictors on which the means of groups are most widely separated in a reduced space of hopefully fewer dimensions.\nThe methods I cover in this book are all linear methods, but there is also a wide variety of non-linear dimension reduction techniques.\n\n\nPackages\nIn this chapter I use the following packages. Load them now:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\nlibrary(ggbiplot)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(car)\nlibrary(ggpubr)\nlibrary(matlib)",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-pca",
    "href": "04-pca-biplot.html#sec-pca",
    "title": "4  Dimension Reduction",
    "section": "\n4.2 Principal components analysis (PCA)",
    "text": "4.2 Principal components analysis (PCA)\nWhen Francis Galton (1886) first discovered the idea of regression toward the mean and presented his famous diagram (Figure 3.9), he had little thought that he had provided a window to a higher-dimensional world, beyond what even A Square could imagine. His friend, Karl Pearson (1896) took that idea and developed it into a theory of regression and a measure of correlation that would bear his name, Pearson’s \\(r\\).\nBut then Pearson (1901) had a further inspiration, akin to that of A Square. If he also had a cloud of sparkly points in \\(2, 3, 4, ..., p\\) dimensions, could he find a point (\\(0D\\)), or line (\\(1D\\)), or plane (\\(2D\\)), or even a hyperplane (\\(nD\\)) that best summarized — squeezed out the most juice—from multivariate data? This was the first truly multivariate problem in the history of statistics (Friendly & Wainer, 2021, p. 186).\nThe best \\(0D\\) point was easy— it was simply the centroid, the means of each of the variables in the data, \\((\\bar{x}_1, \\bar{x}_2, ..., \\bar{x}_p)\\), because that was “closest” to the data in the sense of minimizing the sum of squared differences, \\(\\Sigma_i\\Sigma_j (x_{ij} - \\bar{x}_j)^2\\). In higher dimensions, his solution was also an application of the method of least squares, but he argued it geometrically and visually as shown in Figure 4.2.\n\n\n\n\n\n\n\nFigure 4.2: Karl Pearson’s (1901) geometric, visual argument for finding the line or plane of closest fit to a collection of points, P1, P2, P3, …\n\n\n\n\nFor a \\(1D\\) summary, the line of best fit to the points \\(P_1, P_2, \\dots P_n\\) is the line that goes through the centroid and made the average squared length of the perpendicular segments from those points to a line as small as possible. This was different from the case in linear regression, for fitting \\(y\\) from \\(x\\), where the average squared length of the vertical segments, \\(\\Sigma_i (y_i - \\hat{y}_i)^2\\) was minimized by least squares.\nHe went on to prove the visual insights from simple smoothing of Galton (1886) (shown in Figure 3.9) regarding the regression lines of y ~ x and x ~ y. More importantly, he proved that the cloud of points is captured, for the purpose of finding a best line, plane or hyperplane, by the ellipsoid that encloses it, as seen in his diagram, Figure 4.3. The major axis of the 2D ellipse is the line of best fit, along which the data points have the smallest average squared distance from the line. The axis at right angles to that—the minor axis— is labeled “line of worst fit” with the largest average squared distance.\n\n\n\n\n\n\n\nFigure 4.3: Karl Pearson’s diagram showing the elliptical geometry of regression and principal components analysis … Source: Pearson (1901), p. 566.\n\n\n\n\nEven more importantly— and this is the basis for PCA — he recognized that the two orthogonal axes of the ellipse gave new coordinates for the data which were uncorrelated, whatever the correlation of \\(x\\) and \\(y\\).\n\nPhysically, the axes of the correlation type-ellipse are the directions of independent and uncorrelated variation. — Pearson (1901), p. 566.\n\nIt was but a small step to recognize that for two variables, \\(x\\) and \\(y\\):\n\nThe line of best fit, the major axis (PC1) had the greatest variance of points projected onto it.\nThe line of worst fit, the minor axis (PC2), had the least variance.\nThese could be seen as a rotation of the data space of \\((x, y)\\) to a new space (PC1, PC2) with uncorrelated variables.\nThe total variation of the points in data space, \\(\\text{Var}(x) + \\text{Var}(y)\\), being unchanged by rotation, was equally well expressed as the total variation \\(\\text{Var}(PC1) + \\text{Var}(PC2)\\) of the scores on what are now called the principal component axes.\n\nIt would have appealed to Pearson (and also to A Square) to see these observations demonstrated in a 3D video. Figure 4.4 shows a 3D plot of the variables Sepal.Length, Sepal.Width and Petal.Length in Edgar Anderson’s iris data, with points colored by species and the 95% data ellipsoid. This is rotated smoothly by interpolation until the first two principal axes, PC1 and PC2 are aligned with the horizontal and vertical dimensions. Because this is a rigid rotation of the cloud of points, the total variability is obviously unchanged.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Animation of PCA as a rotation in 3D space. The plot shows three variables for the iris data, initially in data space and its’ data ellipsoid, with points colored according to species of the iris flowers. This is rotated smoothly until the first two principal axes are aligned with the horizontal and vertical directions in the final frame.\n\n\n\n\n\n4.2.1 PCA by springs\nBefore delving into the mathematics of PCA, it is useful to see how Pearson’s problem, and fitting by least squares generally, could be solved in a physical realization.\nFrom elementary statistics, you may be familiar with a physical demonstration that the mean, \\(\\bar{x}\\), of a sample is the value for which the sum of deviations, \\(\\Sigma_i (x_i - \\bar{x})\\) is zero, so the mean can be visualized as the point of balance on a line where those differences \\((x_i - \\bar{x})\\) are placed. Equally well, there is a physical realization of the mean as the point along an axis where weights connected by springs will minimize the sum of squared differences, because springs with a constant stiffness, \\(k\\), exert forces proportional to \\(k (x_i - \\bar{x}) ^2\\). That’s the reason it is useful as a measure of central tendency: it minimizes the average squared error.\nIn two dimensions, imagine that we have points, \\((x_i, y_i)\\) and these are attached by springs of equal stiffness \\(k\\), to a line anchored at the centroid, \\((\\bar{x}, \\bar{y})\\) as shown in Figure 4.5. If we rotate the line to some initial position and release it, the springs will pull the line clockwise or counterclockwise and the line will bounce around until the forces, proportional to the squares of the lengths of the springs, will eventually balance out at the position (shown by the red fixed line segments at the ends). This is the position that minimizes the the sum of squared lengths of the connecting springs, and also minimizes the kinetic energy in the system.\nIf you look closely at Figure 4.5 you will see something else: When the line is at its final position of minimum squared length and energy, the positions of the red points on this line are spread out furthest, i.e., have maximum variance. Conversely, when the line is at right angles to its final position (shown by the black line at 90\\(^o\\)) the projected points have the smallest possible variance.\n\n\n\n\n\n\n\n\nFigure 4.5: Animation of PCA fitted by springs. The blue data points are connected to their projections on the red line by springs perpendicular to that line. From an initial position, the springs pull that line in proportion to their squared distances, until the line finally settles down to the position where the forces are balanced and the minimum is achieved. Source: Amoeba, https://bit.ly/46tAicu.\n\n\n\n\n4.2.2 Mathematics and geometry of PCA\nAs the ideas of principal components developed, there was a happy marriage of Galton’s geometrical intuition and Pearson’s mathematical analysis. The best men at the wedding were ellipses and higher-dimensional ellipsoids. The bridesmaids were eigenvectors, pointing in as many different directions as space would allow, each sized according to their associated eigenvalues. Attending the wedding were the ghosts of uncles, Leonhard Euler, Jean-Louis Lagrange, Augustin-Louis Cauchy and others who had earlier discovered the mathematical properties of ellipses and quadratic forms in relation to problems in physics.\nThe key idea in the statistical application was that, for a set of variables \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_p\\), the \\(p \\times p\\) covariance matrix \\(\\mathbf{S}\\) could be expressed exactly as a matrix product involving a matrix \\(\\mathbf{V}\\), whose columns are eigenvectors (\\(\\mathbf{v}_i\\)) and a diagonal matrix \\(\\mathbf{\\Lambda}\\), whose diagonal elements (\\(\\lambda_i\\)) are the corresponding eigenvalues.\nTo explain this, it is helpful to use a bit of matrix math:\n\\[\\begin{aligned}\n\\mathbf{S}_{p \\times p} & = \\mathbf{V}_{p \\times p} \\phantom{0000000000}\n                            \\mathbf{\\Lambda}_{p \\times p} \\phantom{00000000000000}\n                            \\mathbf{V}_{p \\times p}^\\mathsf{T} \\\\\n           & = \\left( \\mathbf{v}_1, \\, \\mathbf{v}_2, \\,\\dots, \\, \\mathbf{v}_p \\right)\n           \\begin{pmatrix}\n             \\lambda_1 &  &  & \\\\\n             & \\lambda_2  &   & \\\\\n             &  & \\ddots & \\\\\n             &  &  & \\lambda_p\n            \\end{pmatrix}\n            \\;\n            \\begin{pmatrix}\n            \\mathbf{v}_1^\\mathsf{T}\\\\\n            \\mathbf{v}_2^\\mathsf{T}\\\\\n            \\vdots\\\\\n            \\mathbf{v}_p^\\mathsf{T}\\\\\n            \\end{pmatrix}\n           \\\\\n           & = \\lambda_1 \\mathbf{v}_1 \\mathbf{v}_1^\\mathsf{T} + \\lambda_2 \\mathbf{v}_2 \\mathbf{v}_2^\\mathsf{T} + \\cdots + \\lambda_p \\mathbf{v}_p \\mathbf{v}_p^\\mathsf{T}\n\\end{aligned} \\tag{4.1}\\]\nIn this equation,\n\nThe last line follows because \\(\\mathbf{\\Lambda}\\) is a diagonal matrix, so \\(\\mathbf{S}\\) is expressed as a sum of outer products of each \\(\\mathbf{v}_i\\) with itself, times the eigenvalue \\(\\lambda_i\\).\nThe columns of \\(\\mathbf{V}\\) are the eigenvectors of \\(\\mathbf{S}\\). They are orthogonal and of unit length, so \\(\\mathbf{V}^\\mathsf{T} \\mathbf{V} = \\mathbf{I}\\) and thus they represent orthogonal (uncorrelated) directions in data space.\nThe columns \\(\\mathbf{v}_i\\) are the weights applied to the variables to produce the scores on the principal components. For example, the first principal component is the weighted sum:\n\n\\[\n\\text{PC}_1 = v_{11} \\mathbf{x}_1 + v_{12} \\mathbf{x}_2 + \\cdots + v_{1p} \\mathbf{x}_p \\:\\: .\n\\]\n\nThe matrix of all scores on the principal components can be calculated by multiplying the data matrix \\(\\mathbf{X}\\) by the eigenvectors, \\(\\mathbf{PC} = \\mathbf{X} \\mathbf{V}\\).\nThe eigenvalues, \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_p\\) are the variances of the the components, because \\(\\mathbf{v}_i^\\mathsf{T} \\;\\mathbf{S} \\; \\mathbf{v}_i = \\lambda_i\\).\nIt is usually the case that the variables \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_p\\) are linearly independent, which means that none of these is an exact linear combination of the others. In this case, all eigenvalues \\(\\lambda_i\\) are positive and the covariance matrix \\(\\mathbf{S}\\) is said to have rank \\(p\\). (Rank is the number of non-zero eigenvalues.)\nHere is a key fact: If, as usual, the eigenvalues are arranged in order, so that \\(\\lambda_1 &gt; \\lambda_2 &gt; \\dots &gt; \\lambda_p\\), then the first \\(d\\) components give a \\(d\\)-dimensional approximation to \\(\\mathbf{S}\\), which accounts for \\(\\Sigma_i^d \\lambda_i\\) of the \\(\\Sigma_i^p \\lambda_i\\) total variance, usually interpreted as the proportion, \\((\\Sigma_i^d \\lambda_i) / (\\Sigma_i^p \\lambda_i)\\).\n\nFor the case of two variables, \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) Figure 4.6 shows the transformation from data space to component space. The eigenvectors, \\(\\mathbf{v}_1, \\mathbf{v}_2\\) are the major and minor axes of the data ellipse, whose lengths are the square roots \\(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}\\) of the eigenvalues.\n\n\n\n\n\n\n\nFigure 4.6: Geometry of PCA as a rotation from data space to principal component space, defined by the eigenvectors v1 and v2 of a covariance matrix\n\n\n\n\n\nExample: Workers’ experience and income\nFor a small example, consider the relation between years of experience and income in a small (contrived) sample (\\(n = 10\\)) of workers in a factory. The dataset matlib::workers contains these and other variables. In a wider context, we might want to fit a regression model to predict Income, but here we focus on a PCA of just these two variables.\n\ndata(workers, package = \"matlib\") \nhead(workers)\n#&gt;         Income Experience Skill Gender\n#&gt; Abby        20          0     2 Female\n#&gt; Betty       35          5     5 Female\n#&gt; Charles     40          5     8   Male\n#&gt; Doreen      30         10     6 Female\n#&gt; Ethan       50         10    10   Male\n#&gt; Francie     50         15     7 Female\n\n\nLet’s start with a simple scatterplot of Income vs. Experience, with points labeled by Name (and colored by Gender). There’s a fairly strong correlation (\\(r\\) = 0.853). How does a PCA capture this?\n\n\nvars &lt;- c(\"Experience\", \"Income\")\nplot(workers[, vars],\n     pch = 16, cex = 1.5,\n     cex.lab = 1.5)\ntext(workers[, vars], \n     labels = rownames(workers),\n     col = ifelse(workers$Gender == \"Female\", \"red\", \"blue\"),\n     pos = 3, xpd = TRUE)\n\n\n\n\n\n\nFigure 4.7: Scatterplot of Income vs. Experience for the workers data.\n\n\n\n\nTo carry out a PCA of these variables, first calculate the vector of means (\\(\\bar{\\mathbf{x}}\\)) and covariance matrix \\(\\mathbf{S}\\).\n\nmu &lt;- colMeans(workers[, vars]) |&gt; print()\n#&gt; Experience     Income \n#&gt;       15.5       46.5\nS &lt;- cov(workers[, vars]) |&gt; print()\n#&gt;            Experience Income\n#&gt; Experience        136    152\n#&gt; Income            152    234\n\nThe eigenvalues and eigenvectors of S are calculated by eigen(). This returns a list with components values for the \\(\\lambda_i\\) and vectors for \\(\\mathbf{V}\\).\n\nS.eig &lt;- eigen(S)\nLambda &lt;- S.eig$values |&gt; print()\n#&gt; [1] 344.3  25.1\nV &lt;- S.eig$vectors |&gt; print()\n#&gt;       [,1]   [,2]\n#&gt; [1,] 0.589 -0.808\n#&gt; [2,] 0.808  0.589\n\nFrom this, you can verify the points above regarding the relations between variances of the variables and the eigenvalues:\n\n#total variances of the variables = sum of eigenvalues\nsum(diag(S))\n#&gt; [1] 369\nsum(Lambda)\n#&gt; [1] 369\n\n# percent of variance of each PC\n100 * Lambda / sum(Lambda)\n#&gt; [1] 93.2  6.8\n\nUsing these, you can express the eigenvalue decomposition of \\(\\mathbf{S}\\) in Equation 4.1 with latexMatrix() and Eqn from the matlib package (Friendly et al., 2024) as:\n\n\noptions(digits = 4)\nrownames(S) &lt;- colnames(S) &lt;- c(\"\\\\small \\\\text{Exp}\", \n                                \"\\\\small \\\\text{Inc}\")\nspacer &lt;- \"\\\\phantom{00000000000000}\"\nEqn(\"\\\\mathbf{S} & = \\\\mathbf{V}\", spacer,\n    \"\\\\mathbf{\\\\Lambda}\", spacer,  \n    \"\\\\mathbf{V}^\\\\top\", Eqn_newline(),\n    latexMatrix(S), \"& =\", \n    latexMatrix(V), \"  \", diag(Lambda), \"  \", latexMatrix(V, transpose=TRUE),\n    align = TRUE)\n\n\n\\[\\begin{aligned}\n\\mathbf{S} & = \\mathbf{V} \\phantom{00000000000000}\n     \\mathbf{\\Lambda} \\phantom{00000000000000}  \n     \\mathbf{V}^\\top \\\\\n\\begin{matrix}\n  &  \\begin{matrix} \\phantom{i} Exp & Inc\n  \\end{matrix} \\\\\n\\begin{matrix}  \n   Exp\\\\\n   Inc\\\\\n\\end{matrix}  &\n\\begin{pmatrix}  \n136 & 152 \\\\\n152 & 234 \\\\\n\\end{pmatrix}\n\\\\\n\\end{matrix}\n& =\\begin{pmatrix}\n0.589 & -0.808 \\\\\n0.808 &  0.589 \\\\\n\\end{pmatrix}\n  \\begin{pmatrix}\n344.3 &   0.0 \\\\\n  0.0 &  25.1 \\\\\n\\end{pmatrix}\n  \\begin{pmatrix}\n0.589 & -0.808 \\\\\n0.808 &  0.589 \\\\\n\\end{pmatrix}^\\top\n\\end{aligned}\\]\nThe “scores” on the principal components can be calculated (point (5) above) as \\(\\mathbf{PC} = \\mathbf{X} \\mathbf{V}\\):\n\nPC &lt;- as.matrix(workers[, vars]) %*% V\ncolnames(PC) &lt;- paste0(\"PC\", 1:2)\nhead(PC)\n#&gt;          PC1   PC2\n#&gt; Abby    16.2 11.78\n#&gt; Betty   31.2 16.57\n#&gt; Charles 35.3 19.52\n#&gt; Doreen  30.1  9.59\n#&gt; Ethan   46.3 21.37\n#&gt; Francie 49.2 17.32\n\nThen, you can visualize the geometry of PCA as in Figure 4.6 (left) by plotting the data ellipse for the points, along with the PCA axes (heplots::ellipse.axes()). Figure 4.8 also shows the bounding box of the data ellipse, which are parallel to the PC axes and scaled to have the same “radius” as the data ellipse.\n\n\n# calculate conjugate axes for PCA factorization\npca.fac &lt;- function(x) {\n  xx &lt;- svd(x)\n  ret &lt;- t(xx$v) * sqrt(pmax( xx$d,0))\n  ret\n}\n\ndataEllipse(Income ~ Experience, data=workers,\n    pch = 16, cex = 1.5, \n    center.pch = \"+\", center.cex = 2,\n    cex.lab = 1.5,\n    levels = 0.68,\n    grid = FALSE,\n    xlim = c(-10, 40),\n    ylim = c(10, 80),\n    asp = 1)\nabline(h = mu[2], v = mu[1], \n       lty = 2, col = \"grey\")\n\n# axes of the ellipse = PC1, PC2\nradius &lt;- sqrt(2 * qf(0.68, 2, nrow(workers)-1 ))\nheplots::ellipse.axes(S, mu, \n     radius = radius,\n     labels = TRUE,\n     col = \"red\", lwd = 2,\n     cex = 1.8)\n\n# bounding box of the ellipse\nlines(spida2::ellplus(mu, S, radius = radius,\n              box = TRUE, fac = pca.fac),\n      col = \"darkgreen\",\n      lwd = 2, lty=\"longdash\")\n\n\n\n\n\n\nFigure 4.8: Geometry of the PCA for the workers data, showing the data ellipse, the eigenvectors of \\(\\mathbf{S}\\), whose half-lengths are the square roots \\(\\sqrt{\\lambda_i}\\) of the eigenvalues, and the bounding box of the ellipse.\n\n\n\n\nFinally, to preview the methods of the next section, the results calculated “by hand” above can be obtained using prcomp(). The values labeled \"Standard deviations\" are the square roots \\(\\sqrt{\\lambda}_i\\) of the two eigenvalues. The eigenvectors are labeled \"Rotation\" because \\(\\mathbf{V}\\) is the matrix that rotates the data matrix to produce the component scores.\n\nworkers.pca &lt;- prcomp(workers[, vars]) |&gt; print()\n#&gt; Standard deviations (1, .., p=2):\n#&gt; [1] 18.56  5.01\n#&gt; \n#&gt; Rotation (n x k) = (2 x 2):\n#&gt;              PC1    PC2\n#&gt; Experience 0.589  0.808\n#&gt; Income     0.808 -0.589\n\n\n4.2.3 Finding principal components\nIn R, PCA is most easily carried out using stats::prcomp() or stats::princomp() or similar functions in other packages such as FactomineR::PCA(). The FactoMineR package (Husson et al., 2017, 2024) has extensive capabilities for exploratory analysis of multivariate data (PCA, correspondence analysis, cluster analysis).\nA particular strength of FactoMineR for PCA is that it allows the inclusion of supplementary variables (which can be categorical or quantitative) and supplementary points for individuals. These are not used in the analysis, but are projected into the plots to facilitate interpretation. For example, in the analysis of the crime data described below, it would be useful to have measures of other characteristics of the U.S. states, such as poverty and average level of education (Section 4.3.5).\nUnfortunately, although all of these functions perform similar calculations, the options for analysis and the details of the result they return differ.\nThe important options for analysis include:\n\nwhether or not the data variables are centered, to a mean of \\(\\bar{x}_j =0\\)\n\nwhether or not the data variables are scaled, to a variance of \\(\\text{Var}(x_j) =1\\).\n\nIt nearly always makes sense to center the variables. The choice of scaling determines whether the correlation matrix is analyzed, so that each variable contributes equally to the total variance that is to be accounted for versus analysis of the covariance matrix, where each variable contributes its own variance to the total. Analysis of the covariance matrix makes little sense when the variables are measured on different scales2, unless you want to interpret total variance on the scales of the different variables.\nYou don’t need to scale your data in advance, but be aware of the options: prcomp() has default options center = TRUE, scale. = FALSE3 so in most cases you should specify scale. = TRUE. I mostly use this. The older princomp() has only the option cor = FALSE which centers the data and uses the covariance matrix, so in most cases the default is OK.\nTo illustrate, the analysis of the workers data presented above used scale. = FALSE by default, so the eigenvalues reflected the variances of Experience and Income. The analogous result, using standardized variables (\\(z\\)-scores) can be computed in any of the forms shown below, using either scale. = FALSE or standardizing first using scale():\n\nprcomp(workers[, vars], scale. = TRUE)\n#&gt; Standard deviations (1, .., p=2):\n#&gt; [1] 1.361 0.383\n#&gt; \n#&gt; Rotation (n x k) = (2 x 2):\n#&gt;              PC1    PC2\n#&gt; Experience 0.707  0.707\n#&gt; Income     0.707 -0.707\n\n# same as (output suppressed):\nworkers[, vars] |&gt; prcomp(scale. = TRUE) |&gt; invisible()\nworkers[, vars] |&gt; scale() |&gt; prcomp() |&gt; invisible()\n\nIn this form, each of Experience and Income have variance = 1, and the \"Standard deviations\" reported are the square roots (\\(\\sqrt{\\lambda}_i\\)) of the eigenvalues \\(\\lambda_i\\) of the correlation matrix \\(\\mathbf{R}\\). The eigenvalues of a correlation matrix always sum to \\(p\\), the number of variables. This fact prompted the rough rule of thumb to extract principal componends whose eigenvalues exceed 1.0, which is their average value, \\(\\bar{\\lambda} = (\\Sigma^p \\lambda_i) / p = p / p\\).\n\nprcomp(workers[, vars], scale. = TRUE)$sdev\n#&gt; [1] 1.361 0.383\n\n# eiven values of correlation matrix\nR &lt;- cor(workers[, vars])\nR.eig &lt;- eigen(R)\nLambda &lt;- R.eig$values |&gt; print()\n#&gt; [1] 1.853 0.147\nsum(Lambda)\n#&gt; [1] 2\n\nExample: Crime data\nThe dataset crime, analysed in Section 3.4, showed all positive correlations among the rates of various crimes in the corrgram, Figure 3.28. What can we see from a PCA? Is it possible that a few dimensions can account for most of the juice in this data?\nIn this example, you can easily find the PCA solution using prcomp() in a single line in base-R. You need to specify the numeric variables to analyze by their columns in the data frame. The most important option here is scale. = TRUE.\n\ndata(crime, package = \"ggbiplot\")\ncrime.pca &lt;- prcomp(crime[, 2:8], scale. = TRUE)\n\nThe tidy equivalent is more verbose, but also more expressive about what is being done. It selects the variables to analyze by a function, is.numeric() applied to each of the columns and feeds the result to prcomp().\n\ncrime.pca &lt;- \n  crime |&gt; \n  dplyr::select(where(is.numeric)) |&gt;\n  prcomp(scale. = TRUE)\n\nAs is typical with models in R, the result, crime.pca of prcomp() is an object of class \"prcomp\", a list of components, and there are a variety of methods for \"prcomp\" objects. Among the simplest is summary(), which gives the contributions of each component to the total variance in the dataset.\n\nsummary(crime.pca) |&gt; print(digits=2)\n#&gt; Importance of components:\n#&gt;                         PC1  PC2  PC3   PC4   PC5   PC6   PC7\n#&gt; Standard deviation     2.03 1.11 0.85 0.563 0.508 0.471 0.352\n#&gt; Proportion of Variance 0.59 0.18 0.10 0.045 0.037 0.032 0.018\n#&gt; Cumulative Proportion  0.59 0.76 0.87 0.914 0.951 0.982 1.000\n\nThe object, crime.pca returned by prcomp() is a list of the following the following elements:\n\nnames(crime.pca)\n#&gt; [1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"\n\nOf these, for \\(n\\) observations and \\(p\\) variables,\n\n\nsdev is the length \\(p\\) vector of the standard deviations of the principal components (i.e., the square roots \\(\\sqrt{\\lambda_i}\\) of the eigenvalues of the covariance/correlation matrix). When the variables are standardized, the sum of squares of the eigenvalues is equal to \\(p\\).\n\nrotation is the \\(p \\times p\\) matrix of weights or loadings of the variables on the components; the columns are the eigenvectors of the covariance or correlation matrix of the data;\n\nx is the \\(n \\times p\\) matrix of scores for the observations on the components, the result of multiplying (rotating) the data matrix by the loadings. These are uncorrelated, so cov(x) is a \\(p \\times p\\) diagonal matrix whose diagonal elements are the eigenvalues \\(\\lambda_i\\) = sdev^2.\n\ncenter gives the means of the variables when the option center. = TRUE (the default)\n\n4.2.4 Visualizing variance proportions: screeplots\nFor a high-D dataset, such as the crime data in seven dimensions, a natural question is how much of the variation in the data can be captured in 1D, 2D, 3D, … summaries and views. This is answered by considering the proportions of variance accounted by each of the dimensions, or their cumulative values. The components returned by various PCA methods have (confusingly) different names, so broom::tidy() provides methods to unify extraction of these values.\n\n(crime.eig &lt;- crime.pca |&gt; \n  broom::tidy(matrix = \"eigenvalues\"))\n#&gt; # A tibble: 7 × 4\n#&gt;      PC std.dev percent cumulative\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1     1   2.03   0.588       0.588\n#&gt; 2     2   1.11   0.177       0.765\n#&gt; 3     3   0.852  0.104       0.868\n#&gt; 4     4   0.563  0.0452      0.914\n#&gt; 5     5   0.508  0.0368      0.951\n#&gt; 6     6   0.471  0.0317      0.982\n#&gt; 7     7   0.352  0.0177      1\n\nThen, a simple visualization is a plot of the proportion of variance for each component (or cumulative proportion) against the component number, usually called a screeplot. The idea, introduced by Cattell (1966), is that after the largest, dominant components, the remainder should resemble the rubble, or scree formed by rocks falling from a cliff.\nFrom this plot, imagine drawing a straight line through the plotted eigenvalues, starting with the largest one. The typical rough guidance is that the last point to fall on this line represents the last component to extract, the idea being that beyond this, the amount of additional variance explained is non-meaningful. Another rule of thumb is to choose the number of components to extract a desired proportion of total variance, usually in the range of 80 - 90%.\nstats::plot(crime.pca) would give a bar plot of the variances of the components, however ggbiplot::ggscreeplot() gives nicer and more flexible displays as shown in Figure 4.9.\n\np1 &lt;- ggscreeplot(crime.pca) +\n  stat_smooth(data = crime.eig |&gt; filter(PC&gt;=4), \n              aes(x=PC, y=percent), method = \"lm\", \n              se = FALSE,\n              fullrange = TRUE) +\n  theme_bw(base_size = 14)\n\np2 &lt;- ggscreeplot(crime.pca, type = \"cev\") +\n  geom_hline(yintercept = c(0.8, 0.9), color = \"blue\") +\n  theme_bw(base_size = 14)\n\np1 + p2\n\n\n\n\n\n\nFigure 4.9: Screeplots for the PCA of the crime data. The left panel shows the traditional version, plotting variance proportions against component number, with linear guideline for the scree rule of thumb. The right panel plots cumulative proportions, showing cutoffs of 80%, 90%.\n\n\n\n\nFrom this we might conclude that four components are necessary to satisfy the scree criterion or to account for 90% of the total variation in these crime statistics. However two components, giving 76.5%, might be enough juice to tell a reasonable story.\n\n4.2.5 Visualizing PCA scores and variable vectors\nTo see and attempt to understand PCA results, it is useful to plot both the scores for the observations on a few of the largest components and also the loadings or variable vectors that give the weights for the variables in determining the principal components.\nIn Section 4.3 I discuss the biplot technique that plots both in a single display. However, I do this directly here, using tidy processing to explain what is going on in PCA and in these graphical displays.\nScores\nThe (uncorrelated) principal component scores can be extracted as crime.pca$x or using purrr::pluck(\"x\"). As noted above, these are uncorrelated and have variances equal to the eigenvalues of the correlation matrix.\n\nscores &lt;- crime.pca |&gt; purrr::pluck(\"x\") \ncov(scores) |&gt; zapsmall()\n#&gt;      PC1  PC2  PC3  PC4  PC5  PC6  PC7\n#&gt; PC1 4.11 0.00 0.00 0.00 0.00 0.00 0.00\n#&gt; PC2 0.00 1.24 0.00 0.00 0.00 0.00 0.00\n#&gt; PC3 0.00 0.00 0.73 0.00 0.00 0.00 0.00\n#&gt; PC4 0.00 0.00 0.00 0.32 0.00 0.00 0.00\n#&gt; PC5 0.00 0.00 0.00 0.00 0.26 0.00 0.00\n#&gt; PC6 0.00 0.00 0.00 0.00 0.00 0.22 0.00\n#&gt; PC7 0.00 0.00 0.00 0.00 0.00 0.00 0.12\n\nFor plotting, it is more convenient to use broom::augment() which extracts the scores (named .fittedPC*) and appends these to the variables in the dataset.\n\ncrime.pca |&gt;\n  broom::augment(crime) |&gt; head()\n#&gt; # A tibble: 6 × 18\n#&gt;   .rownames state      murder  rape robbery assault burglary larceny\n#&gt;   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 1         Alabama      14.2  25.2    96.8    278.    1136.   1882.\n#&gt; 2 2         Alaska       10.8  51.6    96.8    284     1332.   3370.\n#&gt; 3 3         Arizona       9.5  34.2   138.     312.    2346.   4467.\n#&gt; 4 4         Arkansas      8.8  27.6    83.2    203.     973.   1862.\n#&gt; 5 5         California   11.5  49.4   287      358     2139.   3500.\n#&gt; 6 6         Colorado      6.3  42     171.     293.    1935.   3903.\n#&gt; # ℹ 10 more variables: auto &lt;dbl&gt;, st &lt;chr&gt;, region &lt;fct&gt;,\n#&gt; #   .fittedPC1 &lt;dbl&gt;, .fittedPC2 &lt;dbl&gt;, .fittedPC3 &lt;dbl&gt;,\n#&gt; #   .fittedPC4 &lt;dbl&gt;, .fittedPC5 &lt;dbl&gt;, .fittedPC6 &lt;dbl&gt;,\n#&gt; #   .fittedPC7 &lt;dbl&gt;\n\nThen, we can use ggplot() to plot any pair of components. To aid interpretation, I label the points by their state abbreviation and color them by region of the U.S.. A geometric interpretation of the plot requires an aspect ratio of 1.0 (via coord_fixed()) so that a unit distance on the horizontal axis is the same length as a unit distance on the vertical. To demonstrate that the components are uncorrelated, I also added their data ellipse.\n\ncrime.pca |&gt;\n  broom::augment(crime) |&gt; # add original dataset back in\n  ggplot(aes(.fittedPC1, .fittedPC2, color = region)) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_point(size = 1.5) +\n  geom_text(aes(label = st), nudge_x = 0.2) +\n  stat_ellipse(color = \"grey\") +\n  coord_fixed() +\n  labs(x = \"PC Dimension 1\", y = \"PC Dimnension 2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"top\") \n\n\n\n\n\n\nFigure 4.10: Plot of component scores on the first two principal components for the crime data. States are colored by region.\n\n\n\n\nTo interpret such plots, it is useful consider the observations that are a high and low on each of the axes as well as other information, such as region here, and ask how these differ on the crime statistics. The first component, PC1, contrasts Nevada and California with North Dakota, South Dakota and West Virginia. The second component has most of the southern states on the low end and Massachusetts, Rhode Island and Hawaii on the high end. However, interpretation is easier when we also consider how the various crimes contribute to these dimensions.\nWhen, as here, there are more than two components that seem important in the scree plot, we could obviously go further and plot other pairs.\nVariable vectors\nYou can extract the variable loadings using either crime.pca$rotation or purrr::pluck(\"rotation\"), similar to what I did with the scores.\n\ncrime.pca |&gt; purrr::pluck(\"rotation\")\n#&gt;             PC1     PC2     PC3     PC4     PC5     PC6     PC7\n#&gt; murder   -0.300 -0.6292 -0.1782  0.2321  0.5381  0.2591  0.2676\n#&gt; rape     -0.432 -0.1694  0.2442 -0.0622  0.1885 -0.7733 -0.2965\n#&gt; robbery  -0.397  0.0422 -0.4959  0.5580 -0.5200 -0.1144 -0.0039\n#&gt; assault  -0.397 -0.3435  0.0695 -0.6298 -0.5067  0.1724  0.1917\n#&gt; burglary -0.440  0.2033  0.2099  0.0576  0.1010  0.5360 -0.6481\n#&gt; larceny  -0.357  0.4023  0.5392  0.2349  0.0301  0.0394  0.6017\n#&gt; auto     -0.295  0.5024 -0.5684 -0.4192  0.3698 -0.0573  0.1470\n\nBut note something important in this output: All of the weights for the first component are negative. In PCA, the directions of the eigenvectors are completely arbitrary, in the sense that the vector \\(-\\mathbf{v}_i\\) gives the same linear combination as \\(\\mathbf{v}_i\\), but with its’ sign reversed. For interpretation, it is useful (and usually recommended) to reflect the loadings to a positive orientation by multiplying them by -1. In general, you are free to reflect any of the components for ease of interpretation, and not necessarily if all the signs are negative.\nTo reflect the PCA loadings (multiplying PC1 and PC2 by -1) and get them into a convenient format for plotting with ggplot(), it is necessary to do a bit of processing, including making the row.names() into an explicit variable for the purpose of labeling.\n\n\n\n\n\n\n\nrownames in R\n\n\n\nR software evolved over many years, particularly in conventions for labeling cases in printed output and graphics. In base-R, the convention was that the row.names() of a matrix or data.frame served as observation labels in all printed output and plots, with a default to use numbers 1:n if there were no rownames. In ggplot2 and the tidyverse framework, the decision was made that observation labels had to be an explicit variable in a “tidy” dataset, so it could be used as a variable in constructs like geom_text(aes(label = label)) as in this example. This change often requires extra steps in software that uses the rownames convention.\n\n\n\nvectors &lt;- crime.pca |&gt; \n  purrr::pluck(\"rotation\") |&gt;\n  as.data.frame() |&gt;\n  mutate(PC1 = -1 * PC1, PC2 = -1 * PC2) |&gt;      # reflect axes\n  tibble::rownames_to_column(var = \"label\") \n\nvectors[, 1:3]\n#&gt;      label   PC1     PC2\n#&gt; 1   murder 0.300  0.6292\n#&gt; 2     rape 0.432  0.1694\n#&gt; 3  robbery 0.397 -0.0422\n#&gt; 4  assault 0.397  0.3435\n#&gt; 5 burglary 0.440 -0.2033\n#&gt; 6  larceny 0.357 -0.4023\n#&gt; 7     auto 0.295 -0.5024\n\nThen, I plot these using geom_segment(), taking some care to use arrows from the origin with a nice shape and add geom_text() labels for the variables positioned slightly to the right. Again, coord_fixed() ensures equal scales for the axes, which is important because we want to interpret the angles between the variable vectors and the PCA coordinate axes.\n\narrow_style &lt;- arrow(\n  angle = 20, ends = \"first\", type = \"closed\", \n  length = grid::unit(8, \"pt\")\n)\n\nvectors |&gt;\n  ggplot(aes(PC1, PC2)) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(xend = 0, yend = 0, \n               linewidth = 1, \n               arrow = arrow_style,\n               color = \"brown\") +\n  geom_text(aes(label = label), \n            size = 5,\n            hjust = \"outward\",\n            nudge_x = 0.05, \n            color = \"brown\") +\n  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 0.5),  color = gray(.50)) +\n  xlim(-0.5, 0.9) + \n  ylim(-0.8, 0.8) +\n  coord_fixed() +         # fix aspect ratio to 1:1\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\nFigure 4.11: Plot of component loadings the first two principal components for the crime data. These are interpreted as the contributions of the variables to the components.\n\n\n\n\nThe variable vectors (arrows) shown in Figure 4.11 have the following interpretations:\n\nThe lengths of the variable vectors, \\(\\lVert\\mathbf{v}_i\\rVert = \\sqrt{\\Sigma_{j} \\; v_{ij}^2}\\) give the relative proportion of variance of each variable accounted for in a two-dimensional display.\nEach vector points in the direction in component space with which that variable is most highly correlated: the value, \\(v_{ij}\\), of the vector for variable \\(\\mathbf{x}_i\\) on component \\(j\\) reflects the correlation of that variable with the \\(j\\)th principal component. Thus,\n\n* A Variable that is perfectly correlated with a component is parallel to it.\n* A variable this is uncorrelated with an component is perpendicular to it.\n\nThe angle between vectors shows the strength and direction of the correlation between those variables: the cosine of the angle \\(\\theta\\) between two variable vectors, \\(\\mathbf{v}_i\\) and \\(\\mathbf{v}_j\\), which is \\(\\cos(\\theta) = \\mathbf{v}_i^\\prime \\; \\mathbf{v}_j \\;/ \\; \\| \\mathbf{v}_i \\| \\cdot \\| \\mathbf{v}_j \\|\\) gives the approximation of the correlation \\(r_{ij}\\) between \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) that is shown in this space. This means that: * two variable vectors that point in the same direction are highly correlated; \\(r = 1\\) if they are completely aligned. * Variable vectors at right angles are approximately uncorrelated, while those pointing in opposite directions are negatively correlated; \\(r = -1\\) if they are at 180\\(^o\\).\n\nTo illustrate point (1), the following indicates that almost 70% of the variance of murder is represented in the the 2D plot shown in Figure 4.10, but only 40% of the variance of robbery is captured. For point (2), the correlation of murder with the dimensions is 0.3 for PC1 and 0.63 for PC2. For point (3), the angle between murder and burglary looks to be about 90\\(^o\\), but the actual correlation is 0.39.\n\n\n\n\n\n\n\n\n\nvectors |&gt; select(label, PC1, PC2) |&gt; \n  mutate(length = sqrt(PC1^2 + PC2^2))\n#&gt;      label   PC1     PC2 length\n#&gt; 1   murder 0.300  0.6292  0.697\n#&gt; 2     rape 0.432  0.1694  0.464\n#&gt; 3  robbery 0.397 -0.0422  0.399\n#&gt; 4  assault 0.397  0.3435  0.525\n#&gt; 5 burglary 0.440 -0.2033  0.485\n#&gt; 6  larceny 0.357 -0.4023  0.538\n#&gt; 7     auto 0.295 -0.5024  0.583",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-biplot",
    "href": "04-pca-biplot.html#sec-biplot",
    "title": "4  Dimension Reduction",
    "section": "\n4.3 Biplots",
    "text": "4.3 Biplots\nThe biplot is a visual multivariate juicer. It is the simple and powerful idea that came from the recognition that you can overlay a plot of observation scores in a principal components analysis with the information of the variable loadings (weights) to give a simultaneous display that is easy to interpret. In this sense, a biplot is generalization of a scatterplot, projecting from data space to PCA space, where the observations are shown by points, as in the plots of component scores in Figure 4.10, but with the variables also shown by vectors (or scaled linear axes aligned with those vectors).\nThe idea of the biplot was introduced by Ruben Gabriel (1971, 1981) and later expanded in scope by Gower & Hand (1996). The book by Greenacre (2010) gives a practical overview of the many variety of biplots. Gower et al. (2011) Understanding biplots provides a full treatment of many topics, including how to calibrate biplot axes, 3D plots, and so forth.\nBiplot methodolgy is far more general than I cover here. Categorical variables can be incorporated in PCA using points that represent the levels of discrete categories. Two-way frequency tables of categorical variables can be analysed using correspondence analysis, which is similar to PCA, but designed to account for the maximum amount of the \\(\\chi^2\\) statistic for association; multiple correspondence analysis extends this to method to multi-way tables (Friendly & Meyer, 2016; Greenacre, 1984).\n\n4.3.1 Constructing a biplot\nThe biplot is constructed by using the singular value decomposition (SVD) to obtain a low-rank approximation to the data matrix \\(\\mathbf{X}_{n \\times p}\\) (centered, and optionally scaled to unit variances) whose \\(n\\) rows are the observations and whose \\(p\\) columns are the variables.\n\n\n\n\n\n\n\nFigure 4.12: The singular value decomposition expresses a data matrix X as the product of a matrix U of observation scores, a diagonal matrix \\(\\Lambda\\) of singular values and a matrix V of variable weights.\n\n\n\n\nUsing the SVD, the matrix \\(\\mathbf{X}\\), of rank \\(r \\le p\\) can be expressed exactly as: \\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{V}'\n                 = \\sum_i^r \\lambda_i \\mathbf{u}_i \\mathbf{v}_i' \\; ,\n\\tag{4.2}\\]\nwhere\n * $\\mathbf{U}$ is an $n \\times r$ orthonormal matrix of uncorrelated observation scores; these are also the \n       eigenvectors of $\\mathbf{X} \\mathbf{X}'$,\n * $\\mathbf{\\Lambda}$ is an $r \\times r$ diagonal matrix of singular values, \n       $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\lambda_r$, which are also the square roots\n       of the eigenvalues of $\\mathbf{X} \\mathbf{X}'$. \n * $\\mathbf{V}$ is an $r \\times p$ orthonormal matrix of variable weights and also the \n       eigenvectors of $\\mathbf{X}' \\mathbf{X}$.\nThen, a rank 2 (or 3) PCA approximation \\(\\widehat{\\mathbf{X}}\\) to the data matrix used in the biplot can be obtained from the first 2 (or 3) singular values \\(\\lambda_i\\) and the corresponding \\(\\mathbf{u}_i, \\mathbf{v}_i\\) as:\n\\[\n\\mathbf{X} \\approx \\widehat{\\mathbf{X}} = \\lambda_1 \\mathbf{u}_1 \\mathbf{v}_1' + \\lambda_2 \\mathbf{u}_2 \\mathbf{v}_2' \\; .\n\\]\nThe variance of \\(\\mathbf{X}\\) accounted for by each term is \\(\\lambda_i^2\\).\nA biplot is then obtained by overlaying two scatterplots that share a common set of axes and have a between-set scalar product interpretation. Typically, the observations (rows of \\(\\mathbf{X}\\)) are represented as points and the variables (columns of \\(\\mathbf{X}\\)) are represented as vectors from the origin.\nThe scale factor, \\(\\alpha\\) allows the variances of the components to be apportioned between the row points and column vectors, with different interpretations, by representing the approximation \\(\\widehat{\\mathbf{X}}\\) as the product of two matrices,\n\\[\n\\widehat{\\mathbf{X}} = (\\mathbf{U} \\mathbf{\\Lambda}^\\alpha) (\\mathbf{\\Lambda}^{1-\\alpha} \\mathbf{V}') = \\mathbf{A} \\mathbf{B}'\n\\] This notation uses a little math trick involving a power, \\(0 \\le \\alpha \\le 1\\): When \\(\\alpha = 1\\), \\(\\mathbf{\\Lambda}^\\alpha = \\mathbf{\\Lambda}^1  =\\mathbf{\\Lambda}\\), and \\(\\mathbf{\\Lambda}^{1-\\alpha} = \\mathbf{\\Lambda}^0  =\\mathbf{I}\\). \\(\\alpha = 1/2\\) gives the diagonal matrix \\(\\mathbf{\\Lambda}^{1/2}\\) whose elements are the square roots of the singular values.\nThe choice \\(\\alpha = 1\\) assigns the singular values totally to the left factor; then, the angle between two variable vectors, reflecting the inner product \\(\\mathbf{x}_j^\\mathsf{T}, \\mathbf{x}_{j'}\\) approximates their correlation or covariance, and the distance between the points approximates their Mahalanobis distances. \\(\\alpha = 0\\) gives a distance interpretation to the column display. \\(\\alpha = 1/2\\) gives a symmetrically scaled biplot. *TODO**: Explain this better.\nWhen the singular values are assigned totally to the left or to the right factor, the resultant coordinates are called principal coordinates and the sum of squared coordinates on each dimension equal the corresponding singular value. The other matrix, to which no part of the singular values is assigned, contains the so-called standard coordinates and have sum of squared values equal to 1.0.\n\n4.3.2 Biplots in R\nThere are a large number of R packages providing biplots. The most basic, stats::biplot(), provides methods for \"prcomp\" and \"princomp\" objects. Among other packages, factoextra package (Kassambara & Mundt, 2020), an extension of FactoMineR (Husson et al., 2024), is perhaps the most comprehensive and provides ggplot2 graphics. In addition to biplot methods for quantitative data using PCA (fviz_pca()), it offers biplots for categorical data using correspondence analysis (fviz_ca()) and multiple correspondence analysis (fviz_mca()); factor analysis with mixed quantitative and categorical variables (fviz_famd()) and cluster analysis (fviz_cluster()). TODO: Also mention adegraphics package\nHere, I use the ggbiplot [R-ggbiplot] package, which aims to provide a simple interface to biplots within the ggplot2 framework. I also use some convenient utility functions from factoextra.\n\n4.3.3 Example: Crime data\nA basic biplot of the crime data, using standardized principal components and labeling the observation by their state abbreviation is shown in Figure 4.13. The correlation circle reflects the data ellipse of the standardized components. This reminds us that these components are uncorrelated and have equal variance in the display.\n\n\ncrime.pca &lt;- reflect(crime.pca) # reflect the axes\n\nggbiplot(crime.pca,\n   obs.scale = 1, var.scale = 1,\n   labels = crime$st ,\n   circle = TRUE,\n   varname.size = 4,\n   varname.color = \"brown\") +\n  theme_minimal(base_size = 14) \n\n\n\n\n\n\nFigure 4.13: Basic biplot of the crime data. State abbreviations are shown at their standardized scores on the first two dimensions. The variable vectors reflect the correlations of the variables with the biplot dimensions.\n\n\n\n\nIn this dataset the states are grouped by region and we saw some differences among regions in the plot (Figure 4.10) of component scores. ggbiplot() provides options to include a groups = variable, used to color the observation points and also to draw their data ellipses, facilitating interpretation.\n\nggbiplot(crime.pca,\n   obs.scale = 1, var.scale = 1,\n   groups = crime$region,\n   labels = crime$st,\n   labels.size = 4,\n   var.factor = 1.4,\n   ellipse = TRUE, \n   ellipse.prob = 0.5, ellipse.alpha = 0.1,\n   circle = TRUE,\n   varname.size = 4,\n   varname.color = \"black\",\n   clip = \"off\") +\n  labs(fill = \"Region\", color = \"Region\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.direction = 'horizontal', legend.position = 'top')\n\n\n\n\n\n\nFigure 4.14: Enhanced biplot of the crime data, grouping the states by region and adding data ellipses.\n\n\n\n\nThis plot provides what is necessary to interpret the nature of the components and also the variation of the states in relation to these. In this, the data ellipses for the regions provide a visual summary that aids interpretation.\n\nFrom the variable vectors, it seems that PC1, having all positive and nearly equal loadings, reflects a total or overall index of crimes. Nevada, California, New York and Florida are highest on this, while North Dakota, South Dakota and West Virginia are lowest.\nThe second component, PC2, shows a contrast between crimes against persons (murder, assault, rape) at the top and property crimes (auto theft, larceny) at the bottom. Nearly all the Southern states are high on personal crimes; states in the North East are generally higher on property crimes.\nWestern states tend to be somewhat higher on overall crime rate, while North Central are lower on average. In these states there is not much variation in the relative proportions of personal vs. property crimes.\n\nMoreover, in this biplot you can interpret the the value for a particular state on a given crime by considering its projection on the variable vector, where the origin corresponds to the mean, positions along the vector have greater than average values on that crime, and the opposite direction have lower than average values. For example, Massachusetts has the highest value on auto theft, but a value less than the mean. Louisiana and South Carolina on the other hand are highest in the rate of murder and slightly less than average on auto theft.\nThese 2D plots account for only 76.5% of the total variance of crimes, so it is useful to also examine the third principal component, which accounts for an additional 10.4%. The choices = option controls which dimensions are plotted.\n\nggbiplot(crime.pca,\n         choices = c(1,3),\n         obs.scale = 1, var.scale = 1,\n         groups = crime$region,\n         labels = crime$st,\n         labels.size = 4,\n         var.factor = 2,\n         ellipse = TRUE, \n         ellipse.prob = 0.5, ellipse.alpha = 0.1,\n         circle = TRUE,\n         varname.size = 4,\n         varname.color = \"black\",\n         clip = \"off\") +\n  labs(fill = \"Region\", color = \"Region\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.direction = 'horizontal', legend.position = 'top')\n\n\n\n\n\n\nFigure 4.15: Biplot of dimensions 1 & 3 of the crime data, with data ellipses for the regions.\n\n\n\n\nDimension 3 in Figure 4.15 is more subtle. One interpretation is a contrast between larceny, which is a larceny (simple theft) and robbery, which involves stealing something from a person and is considered a more serious crime with an element of possible violence. In this plot, murder has a relatively short variable vector, so does not contribute very much to differences among the states.\n\n4.3.4 Biplot contributions and quality\nTo better understand how much each variable contributes to the biplot dimensions, it is helpful to see information about the variance of variables along each dimension. Graphically, this is nothing more than a measure of the lengths of projections of the variables on each of the dimensions. factoextra::get_pca_var() calculates a number of tables from a \"prcomp\" or similar object.\n\nvar_info &lt;- factoextra::get_pca_var(crime.pca)\nnames(var_info)\n#&gt; [1] \"coord\"   \"cor\"     \"cos2\"    \"contrib\"\n\nThe component cor gives correlations of the variables with the dimensions and contrib gives their variance contributions as percents, where each row and column sums to 100.\n\ncontrib &lt;- var_info$contrib\ncbind(contrib, Total = rowSums(contrib)) |&gt;\n  rbind(Total = c(colSums(contrib), NA)) |&gt; \n  round(digits=2)\n#&gt;           Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7 Total\n#&gt; murder     9.02  39.59   3.18   5.39  28.96   6.71   7.16   100\n#&gt; rape      18.64   2.87   5.96   0.39   3.55  59.79   8.79   100\n#&gt; robbery   15.75   0.18  24.59  31.14  27.04   1.31   0.00   100\n#&gt; assault   15.73  11.80   0.48  39.67  25.67   2.97   3.68   100\n#&gt; burglary  19.37   4.13   4.41   0.33   1.02  28.73  42.01   100\n#&gt; larceny   12.77  16.19  29.08   5.52   0.09   0.16  36.20   100\n#&gt; auto       8.71  25.24  32.31  17.58  13.67   0.33   2.16   100\n#&gt; Total    100.00 100.00 100.00 100.00 100.00 100.00 100.00    NA\n\nThese contributions can be visualized as sorted barcharts for a given axis using factoextra::fviz_contrib(). The dashed horizontal lines are at the average value for each dimension.\n\np1 &lt;- fviz_contrib(crime.pca, choice = \"var\", axes = 1,\n                   fill = \"lightgreen\", color = \"black\")\np2 &lt;- fviz_contrib(crime.pca, choice = \"var\", axes = 2,\n                   fill = \"lightgreen\", color = \"black\")\np1 + p2\n\n\n\n\n\n\nFigure 4.16: Contributions of the crime variables to dimensions 1 (left) & 2 (right) of the PCA solution\n\n\n\n\nA simple rubric for interpreting the dimensions in terms of the variable contributions is to mention those that are largest or above average on each dimension. So, burglary and rape contribute most to the first dimension, while murder and auto theft contribute most to the second.\nAnother useful measure is called cos2, the quality of representation, meaning how much of a variable is represented in a given component. The columns sum to the eigenvalue for each dimension. The rows each sum to 1.0, meaning each variable is completely represented on all components, but we can find the quality of a \\(k\\)-D solution by summing the values in the first \\(k\\) columns. These can be plotted in a style similar to Figure 4.16 using factoextra::fviz_cos2().\n\nquality &lt;- var_info$cos2\nrowSums(quality)\n#&gt;   murder     rape  robbery  assault burglary  larceny     auto \n#&gt;        1        1        1        1        1        1        1\n\ncolSums(quality)\n#&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 \n#&gt; 4.115 1.239 0.726 0.316 0.258 0.222 0.124\n\ncbind(quality[, 1:2], \n      Total = rowSums(quality[, 1:2])) |&gt;\n  round(digits = 2)\n#&gt;          Dim.1 Dim.2 Total\n#&gt; murder    0.37  0.49  0.86\n#&gt; rape      0.77  0.04  0.80\n#&gt; robbery   0.65  0.00  0.65\n#&gt; assault   0.65  0.15  0.79\n#&gt; burglary  0.80  0.05  0.85\n#&gt; larceny   0.53  0.20  0.73\n#&gt; auto      0.36  0.31  0.67\n\nIn two dimensions, murder and burglary are best represented; robbery and larceny are the worst, but as we saw above (Figure 4.15), these crimes are implicated in the third dimension.\n\n4.3.5 Supplementary variables\nAn important feature of biplot methodology is that once you have a reduced-rank display of the relations among a set of variables, you can use other available data to help interpret what what is shown in the biplot. In a sense, this is what I did above in Figure 4.14 and Figure 4.15 using region as a grouping variable and summarizing the variability in the scores for states with their data ellipses by region.\nWhen we have other quantitative variables on the same observations, these can be represented as supplementary variables in the same space. Geometrically, this amounts to projecting the new variables on the space of the principal components. It is carried out by regressions of these supplementary variables on the scores for the principal component dimensions.\nFor example, the left panel of Figure 4.17 depicts the vector geometry of a regression of a variable \\(\\mathbf{y}\\) on two predictors, \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). The fitted vector, \\(\\widehat{\\mathbf{y}}\\), is the perpendicular projection of \\(\\mathbf{y}\\) onto the plane of \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). In the same way, in the right panel, a supplementary variable is projected into the plane of two principal component axes shown as an ellipse. The black fitted vector shows how that additional variable relates to the biplot dimensions.\n\n\n\n\n\n\n\nFigure 4.17: Fitting supplementary variables in a biplot is analogous (right) to regression on the principal component dimensions (left). Source: Aluja et al. (2018), Figure 2.11\n\n\n\n\nFor this example, it happens that some suitable supplementary variables to aid interpretation of crime rates are available in the dataset datsets::state.x77, which was obtained from the U.S. Bureau of the Census Statistical Abstract of the United States for 1977. I select a few of these below and make the state name a column variable so it can be merged with the crime data.\n\nsupp_data &lt;- state.x77 |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(var = \"state\") |&gt;\n  rename(Life_Exp = `Life Exp`,\n         HS_Grad = `HS Grad`) |&gt;\n  select(state, Income:Life_Exp, HS_Grad) \n\nhead(supp_data)\n#&gt;        state Income Illiteracy Life_Exp HS_Grad\n#&gt; 1    Alabama   3624        2.1     69.0    41.3\n#&gt; 2     Alaska   6315        1.5     69.3    66.7\n#&gt; 3    Arizona   4530        1.8     70.5    58.1\n#&gt; 4   Arkansas   3378        1.9     70.7    39.9\n#&gt; 5 California   5114        1.1     71.7    62.6\n#&gt; 6   Colorado   4884        0.7     72.1    63.9\n\nThen, we can merge the crime data with the supp_data dataset to produce something suitable for analysis using factoMineR::PCA().\n\ncrime_joined &lt;-\n  dplyr::left_join(crime[, 1:8], supp_data, by = \"state\")\nnames(crime_joined)\n#&gt;  [1] \"state\"      \"murder\"     \"rape\"       \"robbery\"   \n#&gt;  [5] \"assault\"    \"burglary\"   \"larceny\"    \"auto\"      \n#&gt;  [9] \"Income\"     \"Illiteracy\" \"Life_Exp\"   \"HS_Grad\"\n\nPCA() can only get the labels for the observations from the row.names() of the dataset, so I assign them explicitly. The supplementary variables are specified by the argument quanti.sup as the indices of the columns in what is passed as the data argument.\n\nrow.names(crime_joined) &lt;- crime$st\ncrime.PCA_sup &lt;- PCA(crime_joined[,c(2:8, 9:12)], \n                     quanti.sup = 8:11,\n                     scale.unit=TRUE, \n                     ncp=3, \n                     graph = FALSE)\n\nThe essential difference between the result of prcomp() used earlier to get the crime.pca object and the result of PCA() with supplementary variables is that the crime.PCA_sup object now contains a quanti.sup component containing the coordinates for the supplementary variables in PCA space.\nThese can be calculated directly as a the coefficients of a multivariate regression of the standardized supplementary variables on the PCA scores for the dimensions, with no intercept—which forces the fitted vectors to go through the origin. For example, in the plot below (Figure 4.18), the vector for Income has coordinates (0.192, -0.530) on the first two PCA dimensions.\n\nreg.data &lt;- cbind(scale(supp_data[, -1]), \n                  crime.PCA_sup$ind$coord) |&gt;\n  as.data.frame()\n\nsup.mod &lt;- lm(cbind(Income, Illiteracy, Life_Exp, HS_Grad) ~ \n                    0 + Dim.1 + Dim.2 + Dim.3, \n              data = reg.data )\n\n(coefs &lt;- t(coef(sup.mod)))\n#&gt;             Dim.1  Dim.2   Dim.3\n#&gt; Income      0.192  0.530  0.0482\n#&gt; Illiteracy  0.112 -0.536  0.1689\n#&gt; Life_Exp   -0.131  0.649 -0.2158\n#&gt; HS_Grad     0.103  0.610 -0.4095\n\nNote that, because the supplementary variables are standardized, these coefficients are the same as the correlations between the supplementary variables and the scores on the principal components, up to a scaling factor for each dimension. This provides a general way to relate dimensions found in other methods to the original data variables using vectors as in biplot techniques.\n\ncor(reg.data[, 1:4], reg.data[, 5:7]) |&gt;\n  print() -&gt; R\n#&gt;             Dim.1  Dim.2   Dim.3\n#&gt; Income      0.393  0.596  0.0415\n#&gt; Illiteracy  0.230 -0.602  0.1453\n#&gt; Life_Exp   -0.268  0.730 -0.1857\n#&gt; HS_Grad     0.211  0.686 -0.3524\n\nR / coefs\n#&gt;            Dim.1 Dim.2 Dim.3\n#&gt; Income      2.05  1.12 0.861\n#&gt; Illiteracy  2.05  1.12 0.861\n#&gt; Life_Exp    2.05  1.12 0.861\n#&gt; HS_Grad     2.05  1.12 0.861\n\nThe PCA() result can then be plotted using FactoMiner::plot() or various factoextra functions like fviz_pca_var() for a plot of the variable vectors or fviz_pca_biplot() for a biplot. When a quanti.sup component is present, supplementary variables are also shown in the displays.\nFor simplicity I use FactoMiner::plot() here and only show the variable vectors. For consistency with earlier plots, I first reflect the orientation of the 2nd PCA dimension so that crimes of personal violence are at the top, as in Figure 4.11.\n\n# reverse coordinates of Dim 2\ncrime.PCA_sup &lt;- ggbiplot::reflect(crime.PCA_sup, columns = 2)\n# also reverse the orientation of coordinates for supplementary vars on Dim 2\n# crime.PCA_sup$quanti.sup$coord[, 2] &lt;- -crime.PCA_sup$quanti.sup$coord[, 2]\nplot(crime.PCA_sup, choix = \"var\")\n\n\n\n\n\n\nFigure 4.18: PCA plot of variables for the crime data, with vectors for the supplementary variables showing their association with the principal component dimensions.\n\n\n\n\nRecall that from earlier analyses, I interpreted the the dominant PC1 dimension as reflecting overall rate of crime. The contributions to this dimension, which are the projections of the variable vectors on the horizontal axis in Figure 4.11 and Figure 4.14 were shown graphically by barcharts in the left panel of Figure 4.16.\nBut now in Figure 4.18, with the addition of variable vectors for the supplementary variables, you can see how income, rate of illiteracy, life expectancy and proportion of high school graduates are related to the variation in rates of crimes for the U.S. states.\nOn dimension 1, what stands out is that life expectancy is associated with lower overall crime, while other supplementary variable have positive associations. On dimension 2, crimes against persons (murder, assault, rape) are associated with greater rates of illiteracy among the states, which as we earlier saw (Figure 4.14) were more often Southern states. Crimes against property (auto theft, larceny) at the bottom of this dimension are associated with higher levels of income and high school graduates\n\n4.3.6 Example: Diabetes data\nAs another example, consider the data from Reaven & Miller (1979) on measures of insulin and glucose shown in Figure 6 and that led to the discovery of two distinct types of development of Type 2 diabetes (Section 3.1). This dataset is available as heplots::Diabetes. The three groups are Normal, Chemical_Diabetic and Overt_Diabetic, and the (numerical) diagnostic variables are:\n\n\nrelwt: relative weight, the ratio of actual to expected weight, given the person’s height,\n\nglufast: fasting blood plasma glucose level\n\nglutest: test blood plasma glucose level, a measure of glucose intolerance\n\ninstest: plasma insulin during test, a measure of insulin response to oral glucose\n\nsspg: steady state plasma glucose, a measure of insulin resistance\n\nTODO: Should introduce 3D plots earlier, in Ch3 before Section 3.3.\nFirst, let’s try to create a 3D plot, analogous to the artist’s drawing from PRIM-9 shown in Figure 7. For this, I use car::scatter3d() which can show data ellipsoids summarizing each group. The formula notation, z ~ x + y assigns the z variable to the vertical direction in the plot, and the x and y variable form a base plane.\n\ncols &lt;- c(\"darkgreen\", \"blue\", \"red\")\nscatter3d(sspg ~ instest + glutest, data=Diabetes,\n          groups = Diabetes$group,\n          ellipsoid = TRUE,\n          surface = FALSE,\n          col = cols,\n          surface.col = cols)\n\ncar::scatter3d() uses the rgl package (Adler & Murdoch, 2023) to render 3D graphics on a display device, which means that it has facilities for perspective, lighting and other visual properties. You can interactively zoom in or out or rotate the display in any of the three dimensions and use rgl::spin3d() to animate rotations around any axes and record this a a movie3d(). Figure 4.19 shows two views of this plot, one from the front and one from the back. The data ellipsoids are not as evocative as the artist’s rendering, but they give a sense of the relative sizes and shapes of the clouds of points for the three diagnostic groups.\n\n\n\n\n\n\n\nFigure 4.19: Two views of a 3D scatterplot of three main diagnostic variables in the Diabetes dataset. The left panel shows an orientation similar to that of Figure 7; the right panel shows a view from the back.\n\n\n\n\nThe normal group is concentrated near the origin, with relatively low values on all three diagnostic measures. The chemical diabetic group forms a wing with higher values on insulin response to oral glucose (instest), while the overt diabetics form the other wing, with higher values on glucose intolerance (glutest). The relative sizes and orientations of the data ellipsoids are also informative.\nGiven this, what can we see in a biplot view based on PCA? The PCA of these data shows that 83% of the variance is captured in two dimensions and 96% in three. The result for 3D is interesting, in that the view from PRIM-9 shown in Figure 7 and Figure 4.19 nearly captured all available information.\n\ndata(Diabetes, package=\"heplots\")\n\ndiab.pca &lt;- \n  Diabetes |&gt; \n  dplyr::select(where(is.numeric)) |&gt;\n  prcomp(scale. = TRUE)\nsummary(diab.pca)\n#&gt; Importance of components:\n#&gt;                          PC1   PC2   PC3    PC4     PC5\n#&gt; Standard deviation     1.662 1.177 0.818 0.3934 0.17589\n#&gt; Proportion of Variance 0.552 0.277 0.134 0.0309 0.00619\n#&gt; Cumulative Proportion  0.552 0.829 0.963 0.9938 1.00000\n\nA 2D biplot, with data ellipses for the groups, can be produced as before, but I also want to illustrate labeling the groups directly, rather than in a legend.\n\nplt &lt;- ggbiplot(diab.pca,\n     obs.scale = 1, var.scale = 1,\n     groups = Diabetes$group,\n     var.factor = 1.4,\n     ellipse = TRUE, \n     ellipse.prob = 0.5, ellipse.alpha = 0.1,\n     circle = TRUE,\n     point.size = 2,\n     varname.size = 4) +\n  labs(fill = \"Group\", color = \"Group\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\nThen, find the centroids of the component scores and use geom_label() to plot the group labels.\n\nscores &lt;- data.frame(diab.pca$x[, 1:2], group = Diabetes$group)\ncentroids &lt;- scores |&gt;\n  group_by(group) |&gt;\n  summarize(PC1 = mean(PC1),\n            PC2 = mean(PC2))\n\nplt + geom_label(data = centroids, \n                 aes(x = PC1, y = PC2, \n                     label=group, color = group),\n                 nudge_y = 0.2)\n\n\n\n\n\n\nFigure 4.20: 2D biplot of the Diabetes data\n\n\n\n\nWhat can we see here, and how does it relate to the artist’s depiction in Figure 7? The variables instest, sspg and glutest correspond approximately to the coordinate axes in the artist’s drawing. glutest and glufast primarily separate the overt diabetics from the others. The chemical diabetics are distinguished by having larger values of insulin response (instest) and are also higher in relative weight (relwt).",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-nonlinear",
    "href": "04-pca-biplot.html#sec-nonlinear",
    "title": "4  Dimension Reduction",
    "section": "\n4.4 Nonlinear dimension reduction",
    "text": "4.4 Nonlinear dimension reduction\nThe world of dimension reduction methods reflected by PCA is a simple and attractive one in which relationships among variable are at least approximately linear, and can be made visible in a lower-dimensional view by linear transformations and projections. PCA does an optimal job of capturing global linear relationships in the data. But many phenomena defy linear description or involve local nonlinear relationships and clusters within the data. Our understanding of high-D data can sometimes be improved by nonlinear dimension reduction techniques.\nTo see why, consider the data shown in the left panel of Figure 4.21 and suppose we want to be able to separate the two classes by a line. The groups are readily seen in this simple 2D example, but there is no linear combination or projection that shows them as distinct categories. The right panel shows the same data after a nonlinear transformation to polar coordinates, where the two groups are readily distinguished by radius. Such problems arise in higher dimensions where direct visualization is far more difficult and nonlinear methods become attractive.\n\n\n\n\n\n\n\nFigure 4.21: *Nonlinear patterns**: Two representations of the same data are shown. In the plot at the left, the clusters are clear to the eye, but there is no linear relation that separates them. Transforming the data nonlinearly, to polar coordinates in the plot at the right, makes the two groups distinct.\n\n\n\n\n\n4.4.1 Multidimensional scaling\nOne way to break out of the “linear-combination, maximize-variance PCA” mold is to consider a more intrinsic property of points in Spaceland: similarity or distance. The earliest expression of this idea was in multidimensional scaling (MDS) by Torgerson (1952), which involved trying to determine a metric low-D representation of objects from their interpoint distances via an application of the SVD.\nThe break-through for nonlinear methods came from Roger Shepard and William Kruskal (Kruskal, 1964; Shepard, 1962a, 1962b) who recognized that a more general, nonmetric version (nMDS) could be achieved using only the rank order of input distances \\(d_{ij}\\) among objects. nMDS maps these into a low-D spatial representation of points, \\(\\mathbf{x}_i, \\mathbf{x}_j\\) whose fitted distances, \\(\\hat{d}_{ij} = \\lVert\\mathbf{x}_i - \\mathbf{x}_j\\rVert\\) matches the order of the \\(d_{ij}\\) as closely as possible. That is, rather than assume that the observed distances are linearly related to the fitted \\(\\hat{d}_{ij}\\), nMDS assumes only that their order is the same. Borg & Groenen (2005) and Borg et al. (2018) give a comprehensive overview of modern developments in MDS.\nThe impetus for MDS stemmed largely from psychology and the behavioral sciences, where simple experimental measures of similarity or dissimilarity of psychological objects (color names, facial expressions, words, Morse code symbols) could be obtained by direct ratings, confusions, or other tasks (Shepard et al., 1972b, 1972a). MDS was revolutionary in that it provided a coherent method to study the dimensions of perceptual and cognitive space in applications where the explanation of a cognitive process was derived directly from an MDS solution (Shoben, 1983).\nTo perform nMDS, you need to calculate the matrix of distances between all pairs of observations (dist()). The basic function is MASS::isoMDS().4 In the call, you can specify the number of dimensions (k) desired, with k=2 as default. It returns the coordinates in a dataset called points.\n\ndiab.dist &lt;- dist(Diabetes[, 1:5])\nmds &lt;- diab.dist |&gt;\n  MASS::isoMDS(k = 2, trace = FALSE) |&gt;\n  purrr::pluck(\"points\") \n\ncolnames(mds) &lt;- c(\"Dim1\", \"Dim2\")\nmds &lt;- bind_cols(mds, group = Diabetes$group)\nmds |&gt; sample_n(6)\n#&gt; # A tibble: 6 × 3\n#&gt;     Dim1   Dim2 group            \n#&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;            \n#&gt; 1 -213.  -42.1  Normal           \n#&gt; 2  191.   47.3  Overt_Diabetic   \n#&gt; 3   12.0 -63.2  Overt_Diabetic   \n#&gt; 4   25.0 -38.1  Chemical_Diabetic\n#&gt; 5  774.    9.44 Overt_Diabetic   \n#&gt; 6   79.0 136.   Overt_Diabetic\n\nThe method works by trying to minimize a measure, “Stress”, of the average difference between the fitted distances \\(\\hat{d}_{ij}\\) and an optimal monotonic (order-preserving) transformation, \\(f_{\\text{mon}}(d_{ij})\\), of the distances in the data. Values of Stress around 5-8% and smaller are generally considered adequate.\nUnlike PCA, where you can fit all possible dimensions once and choose the number of components to retain by examining the eigenvalues or variance proportions, in MDS it is necessary to fit the data for several values of k and consider the trade-off between goodness of fit and complexity.\n\nstress &lt;- vector(length = 5)\nfor(k in 1:5){\n  res &lt;- MASS::isoMDS(diab.dist, k=k, trace = FALSE)\n  stress[k] &lt;- res$stress\n}\nround(stress, 3)\n#&gt; [1] 17.755  3.525  0.256  0.000  0.000\n\nPlotting these shows that a 3D solution is nearly perfect, while a 2D solution is certainly adequate. This plot is the MDS analog of a screeplot for PCA.\n\nplot(stress, type = \"b\", pch = 16, cex = 2,\n     xlab = \"Number of dimensions\",\n     ylab = \"Stress (%)\")\n\n\n\n\n\n\nFigure 4.22: Badness of fit (Stress) of the MDS solution in relation to number of dimensions.\n\n\n\n\nTo plot the 2D solution, I’ll use ggpubr::ggscatter() here because it handles grouping, provides concentration ellipses and other graphical features.\n\nlibrary(ggpubr)\ncols &lt;- scales::hue_pal()(3) |&gt; rev()\nmplot &lt;-\nggscatter(mds, x = \"Dim1\", y = \"Dim2\", \n          color = \"group\",\n          shape = \"group\",\n          palette = cols,\n          size = 2,\n          ellipse = TRUE,\n          ellipse.level = 0.5,\n          ellipse.type = \"t\") +\n  geom_hline(yintercept = 0, color = \"gray\") +\n  geom_vline(xintercept = 0, color = \"gray\") \n\nFor this and other examples using MDS, it would be nice to also show how the dimensions of this space relate to the original variables, as in a biplot. Using the idea of correlations between variables and dimensions from Section 4.3.5, I do this as shown below. Only the relative directions and lengths of the variable vectors matter, so you can choose any convenient scale factor to make the vectors fill the plot region.\n\nvectors &lt;- cor(Diabetes[, 1:5], mds[, 1:2])\nscale_fac &lt;- 500\nmplot + \n  coord_fixed() +\n  geom_segment(data=vectors,\n               aes(x=0, xend=scale_fac*Dim1, y=0, yend=scale_fac*Dim2),\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"),\n               linewidth = 1.1) +\n  geom_text(data = vectors,\n            aes(x = 1.15*scale_fac*Dim1, y = 1.07*scale_fac*Dim2, \n                label=row.names(vectors)),\n            nudge_x = 4,\n            size = 4) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(.8, .8))\n\n\n\n\n\n\nFigure 4.23: Nonmetric MDS representation of the Diabetes data. The vectors reflect the correlations of the variables with the MDS dimensions.\n\n\n\n\nThe configuration of the groups in Figure 4.23 is similar to that of the biplot in Figure 4.20, but the groups are more widely separated along the first MDS dimension. The variable vectors are also similar, except that relwt is not well-represented in the MDS solution.\n\n4.4.2 t-SNE\nWith the rise of “machine learning” methods for “feature extraction” in “supervised” vs. “unsupervised” settings, a variety of new algorithms have been proposed for the task of finding low-D representations of high-D data. Among these, t-distributed Stochastic Neighbor Embedding (t-SNE) developed by Maaten & Hinton (2008) is touted as method for revealing local structure and clustering better in possibly complex high-D data and at different scales.\nt-SNE differs from MDS in what it tries to preserve in the mapping to low-D space: Multidimensional scaling aims to preserve the distances between pairs of data points, focusing on pairs of distant points in the original space. t-SNE, on the other hand focuses on preserving neighboring data points. Data points that are close in the original data space will be tight in the t-SNE embeddings.\n\n“The t-SNE algorithm models the probability distribution of neighbors around each point. Here, the term neighbors refers to the set of points which are closest to each point. In the original, high-dimensional space, this is modeled as a Gaussian distribution. In the 2-dimensional output space this is modeled as a \\(t\\)-distribution. The goal of the procedure is to find a mapping onto the 2-dimensional space that minimizes the differences between these two distributions over all points. The fatter tails of a \\(t\\)-distribution compared to a Gaussian help to spread the points more evenly in the 2-dimensional space.” (Jake Hoare, How t-SNE works and Dimensionality Reduction).\nt-SNE also uses the idea of mapping distance measures into a low-D space, but converts Euclidean distances into conditional probabilities. Stochastic neighbor embedding means that t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability.\nAs van der Maaten and Hinton explained: “The similarity of datapoint \\(\\mathbf{x}_{j}\\) to datapoint \\(\\mathbf{x}_{i}\\) is the conditional probability, \\(p_{j|i}\\), that \\(\\mathbf{x}_{i}\\) would pick \\(\\mathbf{x}_{j}\\) as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian distribution centered at \\(\\mathbf{x}_{i}\\).” For \\(i \\ne j\\), they define:\n\\[\np_{j\\mid i} = \\frac{\\exp(-\\lVert\\mathbf{x}_i - \\mathbf{x}_j\\rVert^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\lVert\\mathbf{x}_i - \\mathbf{x}_k\\rVert^2 / 2\\sigma_i^2)} \\;.\n\\] and set \\(p_{i\\mid i} = 0\\). \\(\\sigma^2_i\\) is the variance of the normal distribution that centered on datapoint \\(\\mathbf{x}_{i}\\) and serves as a tuning bandwidth so smaller values of \\(\\sigma _{i}\\) are used in denser parts of the data space. These conditional probabilities are made symmetric via averaging, giving \\(p_{ij} = \\frac{p_{j\\mid i} + p_{i\\mid j}}{2n}\\).\nt-SNE defines a similar probability distribution \\(q_{ij}\\) over the points \\(\\mathbf{y}_i\\) in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map,\n\\[\nD_\\mathrm{KL}\\left(P \\parallel Q\\right) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} \\; ,\n\\] a measure of how different the distribution of \\(P\\) in the data is from that of \\(Q\\) in the low-D representation. The t in t-SNE comes from the fact that the probability distribution of the points \\(\\mathbf{y}_i\\) in the embedding space is taken to be a heavy-tailed \\(t_{(1)}\\) distribution with one degree of freedom to spread the points more evenly in the 2-dimensional space, rather than the Gaussian distribution for the points in the high-D data space.\nt-SNE is implemented in the Rtsne package (Krijthe, 2023) which is capable of handling thousands of points in very high dimensions. It uses a tuning parameter, “perplexity” to choose the bandwidth \\(\\sigma^2_i\\) for each point. This value effectively controls how many nearest neighbors are taken into account when constructing the embedding in the low-dimensional space. It can be thought of as the means to balance between preserving the global and the local structure of the data.5\nRtsne::Rtsne() finds the locations of the points in the low-D space, of dimension k=2 by default. It returns the coordinates in a component named Y. The package has no print(), summary() or plot methods, so you’re on your own.\n\nlibrary(Rtsne)\nset.seed(123) \ndiab.tsne &lt;- Rtsne(Diabetes[, 1:5], scale = TRUE)\ndf2 &lt;- data.frame(diab.tsne$Y, group = Diabetes$group) \ncolnames(df2) &lt;- c(\"Dim1\", \"Dim2\", \"group\")\n\nYou can plot this as shown below:\n\n\np2 &lt;- ggplot(df2, aes(x=Dim1, y=Dim2, color = group, shape=group)) + \n  geom_point(size = 3) + \n  stat_ellipse(level = 0.68, linewidth=1.1) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  scale_color_manual(values = cols) +\n  labs(x = \"Dimension 1\",\n       y = \"Dimension 2\") + \n  ggtitle(\"tSNE\") +\n  theme_bw(base_size = 16) +\n  theme(legend.position = \"bottom\") \np2\n\n\n\n\n\n\nFigure 4.24: t-SNE representation of the Diabetes data.\n\n\n\n\n\n4.4.2.1 Comparing solutions\nFor the Diabetes data, I’ve shown the results of three different dimension reduction techniques, PCA (Figure 4.20), MDS (Figure 4.23), and t-SNE (Figure 4.24). How are these similar, and how do they differ?\nOne way is to view them side by side as shown in Figure 4.25. To an initial glance, the t-SNE solution looks like a rotated version of the PCA solution, but there are differences in the shapes of the clusters as well.\n\n\n\n\n\n\n\n\nFigure 4.25: Comparison of the PCA and t-SNE 2D representations of the Diabetes data.\n\n\n\n\nAnother way to compare these two views is to animate the transition from the PCA to the t-SNE representation by a series of smooth interpolated views. This is a more generally useful visualization technique, so it is useful to spell out the details.\nThe essential idea is calculate interpolated views as a weighted average of the two endpoints using a weight \\(\\gamma\\) that is varied from 0 to 1.\n\\[\n\\mathbf{X}_{\\text{View}} = \\gamma \\;\\mathbf{X}_{\\text{PCA}} + (1-\\gamma) \\;\\mathbf{X}_{\\text{t-SNE}}\n\\] The same idea can be applied to other graphical features: lines, paths (ellipses), and so forth. These methods are implemented in the gganimate package (Pedersen & Robinson, 2024).\nIn this case, to create an animation you can extract the coordinates for the PCA, \\(\\mathbf{X}_{\\text{PCA}}\\), as a data.frame df1, and those for the t-SNE, \\(\\mathbf{X}_{\\text{t-SNE}}\\) as df2, each with a constant method variable. These two are then stacked (using rbind()) to give a combined df3. The animation can then interpolate over method going from pure PCA to pure t-SNE.\n\ndiab.pca &lt;- prcomp(Diabetes[, 1:5], scale = TRUE, rank.=2) \ndf1 &lt;- data.frame(diab.pca$x, group = Diabetes$group) \ncolnames(df1) &lt;- c(\"Dim1\", \"Dim2\", \"group\")\ndf1 &lt;- cbind(df1, method=\"PCA\")\n\nset.seed(123) \ndiab.tsne &lt;- Rtsne(Diabetes[, 1:5], scale = TRUE)\ndf2 &lt;- data.frame(diab.tsne$Y, group = Diabetes$group) \ncolnames(df2) &lt;- c(\"Dim1\", \"Dim2\", \"group\")\ndf2 &lt;- cbind(df2, method=\"tSNE\")\n\n# stack the PCA and t-SNE solutions\ndf3 &lt;- rbind(df1, df2) \n\nThen, plot the configuration of the points and add data ellipses as before. The key thing for animating the difference between the solutions is to add transition_states(method, ...), tweening from PCA to t-SNE. The state_length argument transition_states() controls the relative length of the pause between states.\nThis animated graphic is shown only in the online version of the book.\n\nlibrary(gganimate)\nanimated_plot &lt;- \n  ggplot(df3, aes(x=Dim1, y=Dim2, color=group, shape=group)) + \n  geom_point(size = 3) + \n  stat_ellipse(level = 0.68, linewidth=1.1) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  scale_color_manual(values = cols) +\n  labs(title = \"PCA vs. tSNE Dimension Reduction: {closest_state}\",\n       subtitle = \"Frame {frame} of {nframes}\",\n       x = \"Dimension 1\",\n       y = \"Dimension 2\") + \n  transition_states( method, transition_length = 3, state_length = 2 ) + \n  view_follow() + \n  theme_bw(base_size = 16) +\n  theme(legend.position = \"bottom\") \n\nanimated_plot\n\n\n\n\n\n\n\n\nFigure 4.26: Animation of the relationship of PCA to the t-SNE embedding for the Diabetes data. The method name in the title reflects the closest state\n\n\n\n\nYou can see that the PCA configuration is morphed into the that for t-SNE largely by rotation 90\\(^o\\) clockwise, so that dimension 1 in PCA becomes dimension 2 in t-SNE. This is not unexpected, because PCA finds the dimensions in to order of maximum variance, whereas t-SNE is only trying to match the distances in the data to those in the solution. To interpret the result from t-SNE you are free to interchange the axes, or indeed to rotate the solution arbitrarily.\nIt is more interesting that the sizes and shapes of the group clusters change from one solution to the other. The normal group is most compact in the PCA solution, but becomes the least compact in t-SNE.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#sec-var-order",
    "href": "04-pca-biplot.html#sec-var-order",
    "title": "4  Dimension Reduction",
    "section": "\n4.5 Application: Variable ordering for data displays",
    "text": "4.5 Application: Variable ordering for data displays\nIn many multivariate data displays, such as scatterplot matrices, parallel coordinate plots and others reviewed in Chapter 3, the order of different variables might seem arbitrary. They might appear in alphabetic order, or more often in the order they appear in your dataset, for example when you use pairs(mydata). Yet, the principle of effect ordering (Friendly & Kwan (2003)) for variables says you should try to arrange the variables so that adjacent ones are as similar as possible.6\nFor example, the mtcars dataset contains data on 32 automobiles from the 1974 U.S. magazine Motor Trend and consists of fuel comsumption (mpg) and 10 aspects of automobile design (cyl: number of cyliners; hp: horsepower, wt: weight) and performance (qsec: time to drive a quarter-mile). What can we see from a simple corrplot() of their correlations? No coherent pattern stands out in Figure 4.27.\n\ndata(mtcars)\nlibrary(corrplot)\nR &lt;- cor(mtcars)\ncorrplot(R, \n         method = 'ellipse',\n         title = \"Dataset variable order\",\n         tl.srt = 0, tl.col = \"black\", tl.pos = 'd',\n         mar = c(0,0,1,0))\n\n\n\n\n\n\nFigure 4.27: Corrplot of mtcars data, with the variables arranged in the order they appear in the dataset.\n\n\n\n\nIn this display you can scan the rows and columns to “look up” the sign and approximate magnitude of a given correlation; for example, the correlation between mpg and cyl appears to be about -0.9, while that between mpg and gear is about 0.5. Of course, you could print the correlation matrix to find the actual values (-0.86 and 0.48 respectively):\n\nprint(floor(100*R))\n#&gt;      mpg cyl disp  hp drat  wt qsec  vs  am gear carb\n#&gt; mpg  100 -86  -85 -78   68 -87   41  66  59   48  -56\n#&gt; cyl  -86 100   90  83  -70  78  -60 -82 -53  -50   52\n#&gt; disp -85  90  100  79  -72  88  -44 -72 -60  -56   39\n#&gt; hp   -78  83   79 100  -45  65  -71 -73 -25  -13   74\n#&gt; drat  68 -70  -72 -45  100 -72    9  44  71   69  -10\n#&gt; wt   -87  78   88  65  -72 100  -18 -56 -70  -59   42\n#&gt; qsec  41 -60  -44 -71    9 -18  100  74 -23  -22  -66\n#&gt; vs    66 -82  -72 -73   44 -56   74 100  16   20  -57\n#&gt; am    59 -53  -60 -25   71 -70  -23  16 100   79    5\n#&gt; gear  48 -50  -56 -13   69 -59  -22  20  79  100   27\n#&gt; carb -56  52   39  74  -10  42  -66 -57   5   27  100\n\nBecause the angles between variable vectors in the biplot reflect their correlations, Friendly & Kwan (2003) defined principal component variable ordering as the order of angles, \\(a_i\\) of the first two eigenvectors, \\(\\mathbf{v}_1, \\mathbf{v}_2\\) around the unit circle. These values are calculated going counter-clockwise from the 12:00 position as:\n\\[\na_i =\n  \\begin{cases}\n    \\tan^{-1} (v_{i2}/v_{i1}), & \\text{if $v_{i1}&gt;0$;}\n     \\\\\n    \\tan^{-1} (v_{i2}/v_{i1}) + \\pi, & \\text{otherwise.}\n  \\end{cases}     \n\\tag{4.3}\\]\nIn Equation 4.3 \\(\\tan^{-1}(x)\\) is read as “the angle whose tangent is \\(x\\)”, and so the angles are determined by the tangent ratios “opposite” / “adjacent” = \\(v_{i2} / v_{i1}\\) in the right triangle defined by the vector and the horizontal axis.\n\nFor the mtcars data the biplot in Figure 4.28 accounts for 84% of the total variance so a 2D representation is fairly good. The plot shows the variables as widely dispersed. There is a collection at the left of positively correlated variables and another positively correlated set at the right.\n\nmtcars.pca &lt;- prcomp(mtcars, scale. = TRUE)\nggbiplot(mtcars.pca,\n         circle = TRUE,\n         point.size = 2.5,\n         varname.size = 6,\n         varname.color = \"brown\") +\n  theme_minimal(base_size = 14) \n\n\n\n\n\n\nFigure 4.28: Biplot of the mtcars data. The order of the variables around the circle, starting from “gear” (say) arranges them so that the most similar variables are adjacent in graphical displays.\n\n\n\n\nIn corrplot() principal component variable ordering is implemented using the order = \"AOE\" option. There are a variety of other methods based on hierarchical cluster analysis described in the package vignette.\nFigure 4.29 shows the result of ordering the variables by this method. A nice feature of corrplot() is the ability to manually highlight blocks of variables that have a similar pattern of signs by outlining them with rectangles. From the biplot, the two main clusters of positively correlated variables seemed clear, and are outlined in the plot using corrplot::corrRect(). What became clear in the corrplot is that qsec, the time to drive a quarter-mile from a dead start didn’t quite fit this pattern, so I highlighted it separately.\n\ncorrplot(R, \n         method = 'ellipse', \n         order = \"AOE\",\n         title = \"PCA variable order\",\n         tl.srt = 0, tl.col = \"black\", tl.pos = 'd',\n         mar = c(0,0,1,0)) |&gt;\n  corrRect(c(1, 6, 7, 11))\n\n\n\n\n\n\nFigure 4.29: Corrplot of mtcars data, with the variables ordered according to the variable vectors in the biplot.\n\n\n\n\nBut wait, there is something else to be seen in Figure 4.29. Can you see one cell that doesn’t fit with the rest?\nYes, the correlation of number of forward gears (gear) and number of carburators (carb) in the upper left and lower right corners stands out as moderately positive (0.27) while all the others in their off-diagonal blocks are negative. This is another benefit of effect ordering: when you arrange the variables so that the most highly related variable are together, features that deviate from dominant pattern become visible.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#application-eigenfaces",
    "href": "04-pca-biplot.html#application-eigenfaces",
    "title": "4  Dimension Reduction",
    "section": "\n4.6 Application: Eigenfaces",
    "text": "4.6 Application: Eigenfaces\nThere are many applications of principal components analysis beyond the use for visualization for multivariate data covered here, that rely on its’ ability as a dimension reduction technique, that is, to find a low-dimensional approximation to a high-dimensional dataset.\n\n\n\n\n\n\nMachine learning uses\n\n\n\nIn machine learning, for example, PCA is a method used to reduce model complexity and avoid overfitting by feature extraction, which amounts to fitting a response variable in a low-D space of the predictors. This is just another name for principal components regression, where, instead of regressing the dependent variable on all the explanatory variables directly, a smaller number principal components of the explanatory variables is used as predictors. This has the added benefit that it avoids problems of collinearity (section-ref) due to high correlations of the predictors, because the principal component scores are necessarily uncorrelated. When the goal is model explanation rather than pure prediction, it has the disadvantage that the components may be hard to interpret.\n\n\nAn interesting class of problems have to do with image processing, where an image of size width \\(\\times\\) height in pixels can be represented by a \\(w \\times h\\) array of greyscale values \\(x_{ij}\\) in the range of [0, 1] or \\(h \\times w \\times 3\\) array \\(x_{ijk}\\) of (red, green, blue) color values. For example a single \\(640 \\times 640\\) photo is comprised of about 400K pixels in B/W and 1200K pixels in color.\nThe uses here include\n\n\nImage compression: a process applied to a graphics file to minimize its size in bytes for storage or transmission, without degrading image quality below an acceptable threshold\n\nimage enhancement: improving the quality of an image, with applications in Computer Vision tasks, remote sensing, and satellite imagery.\n\nfacial recognition: classifying or matching a facial image against a large corpus of stored images.\n\nWhen PCA is used on facial images, you can think of the process as generating eigenfaces, a representation of the pixels in the image in terms of an eigenvalue decomposition. Dimension reduction means that a facial image can be considerably compressed by removing the components associated with small dimensions.\nAs an example, consider the black and white version of the Mona Lisa shown in Figure 4.30. The idea and code for this example is adapted from this blog post by Kieran Healy.7\nTODO: Web links like this should be footnotes for PDF\n\n\n\n\n\n\n\nFigure 4.30: 640 x 954 black and white image of the Mona Lisa. Source: Wikipedia\n\n\n\n\nIt would take too long to explain the entire method, so I’ll just sketch the essential parts here. The complete script for this example is contained in PCA-MonaLisa.R. …\nTODO: Show the necessary parts, including the screeplot.\nAn image can be imported using imager::load.image() which creates a \"cimg\" object, a 4-dimensional array with dimensions named x,y,z,c. x and y are the usual spatial dimensions, z is a depth dimension (which would correspond to time in a movie), and c is a color dimension containing R, G, B values.\n\nlibrary(imager)\nimg &lt;- imager::load.image(here::here(\"images\", \"MonaLisa-BW.jpg\"))\ndim(img)\n#&gt; [1] 640 954   1   1\n\n\nAn as.data.frame() method converts this to a data frame with x and y coordinates. Each x-y pair is a location in the 640 by 954 pixel grid, and the value is a grayscale value ranging from zero to one.\n\nimg_df_long &lt;- as.data.frame(img)\nhead(img_df_long)\n#&gt;   x y value\n#&gt; 1 1 1 0.431\n#&gt; 2 2 1 0.337\n#&gt; 3 3 1 0.467\n#&gt; 4 4 1 0.337\n#&gt; 5 5 1 0.376\n#&gt; 6 6 1 0.361\n\nHowever, to do a PCA we will need a matrix of data in wide format containing the grayscale pixel values. We can do this using tidyr::pivot_wider(), giving a result with 640 rows and 954 columns.\n\nimg_df &lt;- pivot_wider(img_df_long, \n                     names_from = y, \n                     values_from = value) |&gt;\n  select(-x)\ndim(img_df)\n#&gt; [1] 640 954\n\nMona’s PCA is produced from this img_df with prcomp():\n\nimg_pca &lt;- img_df |&gt;\n  prcomp(scale = TRUE, center = TRUE)\n\nWith 955 columns, the PCA comprises 955 eigenvalue/eigenvector pairs. However, the rank of a matrix is the smaller of the number of rows and columns, so only 640 eigenvalues can be non-zero. Printing the first 10 shows that the first three dimensions account for 46% of the variance and we only get to 63% with 10 components.\n\nimg_pca |&gt;\n  broom::tidy(matrix = \"eigenvalues\") |&gt; head(10)\n#&gt; # A tibble: 10 × 4\n#&gt;       PC std.dev percent cumulative\n#&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1     1   14.1  0.209        0.209\n#&gt;  2     2   11.6  0.141        0.350\n#&gt;  3     3   10.1  0.107        0.457\n#&gt;  4     4    7.83 0.0643       0.522\n#&gt;  5     5    6.11 0.0392       0.561\n#&gt;  6     6    4.75 0.0237       0.585\n#&gt;  7     7    3.70 0.0143       0.599\n#&gt;  8     8    3.52 0.0130       0.612\n#&gt;  9     9    3.12 0.0102       0.622\n#&gt; 10    10    2.86 0.00855      0.631\n\nFigure 4.31 shows a screeplot of proportions of variance. Because there are so many components and most of the information is concentrated in the largest dimensions, I’ve used a \\(\\log_{10}()\\) scale on the horizontal axis. Beyond 10 or so dimensions, the variance of additional components looks quite tiny.\n\nggscreeplot(img_pca) +\n  scale_x_log10()\n\n\n\n\n\n\nFigure 4.31: Screeplot of the variance proportions in the Mona Lisa PCA.\n\n\n\n\nThen, if \\(\\mathbf{M}\\) is the \\(640 \\times 955\\) matrix of pixel values, a best approximation \\(\\widehat{\\mathbf{M}}_k\\) using \\(k\\) dimensions can be obtained as \\(\\widehat{\\mathbf{M}}_k = \\mathbf{X}_k\\;\\mathbf{V}_k^\\mathsf{T}\\) where \\(\\mathbf{X}_k\\) are the principal component scores and \\(\\mathbf{V}_k\\) are the eigenvectors corresponding to the \\(k\\) largest eigenvalues. The function approx_pca() does this, and also undoes the scaling and centering carried out in PCA.\nTODO: Also, separate approximation from the pivot_longer code…\n\nCodeapprox_pca &lt;- function(n_comp = 20, pca_object = img_pca){\n  ## Multiply the matrix of rotated data (component scores) by the transpose of \n  ## the matrix of eigenvectors (i.e. the component loadings) to get back to a \n  ## matrix of original data values\n\n  recon &lt;- pca_object$x[, 1:n_comp] %*% t(pca_object$rotation[, 1:n_comp])\n  \n  ## Reverse any scaling and centering that was done by prcomp()\n  if(all(pca_object$scale != FALSE)){\n    ## Rescale by the reciprocal of the scaling factor, i.e. back to\n    ## original range.\n    recon &lt;- scale(recon, center = FALSE, scale = 1/pca_object$scale)\n  }\n  if(all(pca_object$center != FALSE)){\n    ## Remove any mean centering by adding the subtracted mean back in\n    recon &lt;- scale(recon, scale = FALSE, center = -1 * pca_object$center)\n  }\n  \n  ## Make it a data frame that we can easily pivot to long format\n  ## for drawing with ggplot\n  recon_df &lt;- data.frame(cbind(1:nrow(recon), recon))\n  colnames(recon_df) &lt;- c(\"x\", 1:(ncol(recon_df)-1))\n\n  ## Return the data to long form \n  recon_df_long &lt;- recon_df |&gt;\n    tidyr::pivot_longer(cols = -x, \n                        names_to = \"y\", \n                        values_to = \"value\") |&gt;\n    mutate(y = as.numeric(y)) |&gt;\n    arrange(y) |&gt;\n    as.data.frame()\n  \n  recon_df_long\n}\n\n\nFinally, the recovered images, using 2, 3 , 4, 5, 10, 15, 20, 50, and 100 principal components can be plotted using ggplot. In the code below, the approx_pca() function is run for each of the 9 values specified by n_pcs giving a data frame recovered_imgs containing all reconstructed images, with variables x, y and value (the greyscale pixel value).\n\nn_pcs &lt;- c(2:5, 10, 15, 20, 50, 100)\nnames(n_pcs) &lt;- paste(\"First\", n_pcs, \"Components\", sep = \"_\")\n\nrecovered_imgs &lt;- map_dfr(n_pcs, \n                          approx_pca, \n                          .id = \"pcs\") |&gt;\n  mutate(pcs = stringr::str_replace_all(pcs, \"_\", \" \"), \n         pcs = factor(pcs, levels = unique(pcs), ordered = TRUE))\n\nIn ggplot(), each is plotted using geom_raster(), using value to as the fill color. A quirk of images imported to R is that origin is taken as the upper left corner, so the Y axis scale needs to be reversed. The 9 images are then plotted together using facet_wrap().\n\np &lt;- ggplot(data = recovered_imgs, \n            mapping = aes(x = x, y = y, fill = value))\np_out &lt;- p + geom_raster() + \n  scale_y_reverse() + \n  scale_fill_gradient(low = \"black\", high = \"white\") +\n  facet_wrap(~ pcs, ncol = 3) + \n  guides(fill = \"none\") + \n  labs(title = \"Recovering Mona Lisa from PCA of her pixels\") + \n  theme(strip.text = element_text(face = \"bold\", size = rel(1.2)),\n        plot.title = element_text(size = rel(1.5)))\n\np_out\n\nThe result, in Figure 4.32 is instructive about how much visual information is contained in lower-dimensional reconstructions, or conversely, how much the image can be compressed by omitting the many small dimensions.\n\n\n\n\n\n\n\nFigure 4.32: Re-construction of the Mona Lisa using 2, 3 , 4, 5, 10, 15, 20, 50, and 100 principal components.\n\n\n\n\nIn this figure, with 4-5 components most people would recognize this as a blury image of the world’s most famous portrait. It is certainly clear that this is the Mona Lisa with 10–15 components. Details of the portrait and backgound features become recognizable with 20–50 components, and with 100 components it compares favorably with the original in Figure 4.30. In numbers, the original \\(640 \\times 955\\)) image is of size 600 Kb. The 100 component version is only 93 Kb, 15.6% of this.",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#elliptical-insights-outlier-detection",
    "href": "04-pca-biplot.html#elliptical-insights-outlier-detection",
    "title": "4  Dimension Reduction",
    "section": "\n4.7 Elliptical insights: Outlier detection",
    "text": "4.7 Elliptical insights: Outlier detection\nThe data ellipse (Section 3.2), or ellipsoid in more than 2D is fundamental in regression. But also, as Pearson showed, it is key to understanding principal components analysis, where the principal component directions are simply the axes of the ellipsoid of the data. As such, observations that are unusual in data space may not stand out in univariate views of the variables, but will stand out in principal component space, usually on the smallest dimension.\nAs an illustration, I created a dataset of \\(n = 100\\) observations with a linear relation, \\(y = x + \\mathcal{N}(0, 1)\\) and then added two discrepant points at (1.5, -1.5), (-1.5, 1.5).\n\nset.seed(123345)\nx &lt;- c(rnorm(100),             1.5, -1.5)\ny &lt;- c(x[1:100] + rnorm(100), -1.5, 1.5)\n\nWhen these are plotted with a data ellipse in Figure 4.33 (left), you can see the discrepant points labeled 101 and 102, but they do not stand out as unusual on either \\(x\\) or \\(y\\). The transformation to from data space to principal components space, shown in Figure 4.33 (right), is simply a rotation of \\((x, y)\\) to a space whose coordinate axes are the major and minor axes of the data ellipse, \\((PC_1, PC_2)\\). In this view, the additional points appear a univariate outliers on the smallest dimension, \\(PC_2\\).\n\n\n\n\n\n\n\nFigure 4.33: Outlier demonstration: The left panel shows the original data and highlights the two discrepant points, which do not appear to be unusual on either x or y. The right panel shows the data rotated to principal components, where the labeled points stand out on the smallest PCA dimension.\n\n\n\n\nTo see this more clearly, Figure 4.34 shows an animation of the rotation from data space to PCA space. This uses heplots::interpPlot() …\n\n\n\n&lt;iframe width=\"480\" height=\"480\" src=\"images/outlier-demo.gif\"&gt;&lt;/iframe&gt;\n\n\n\nFigure 4.34: Animation of rotation from data space to PCA space.\n\n\nPackage summary\n\n16 packages used here: car, carData, corrplot, dplyr, factoextra, FactoMineR, ggbiplot, ggplot2, ggpubr, imager, knitr, magrittr, matlib, patchwork, Rtsne, tidyr\n\n\n\n\n\nAdler, D., & Murdoch, D. (2023). Rgl: 3D visualization using OpenGL. https://CRAN.R-project.org/package=rgl\n\n\nAluja, T., Morineau, A., & Sanchez, G. (2018). Principal component analysis for data science. https://pca4ds.github.io/\n\n\nBorg, I., & Groenen, P. J. F. (2005). Modern Multidimensional Scaling: Theory and Applications. Springer.\n\n\nBorg, I., Groenen, P. J. F., & Mair, P. (2018). Applied multidimensional scaling and unfolding. In SpringerBriefs in Statistics. Springer International Publishing. https://doi.org/10.1007/978-3-319-73471-2\n\n\nCattell, R. B. (1966). The scree test for the number of factors. Multivariate Behavioral Research, 1(2), 245–276. https://doi.org/10.1207/s15327906mbr0102_10\n\n\nEuler, L. (1758). Elementa doctrinae solidorum. Novi Commentarii Academiae Scientiarum Petropolitanae, 4, 109–140. https://scholarlycommons.pacific.edu/euler-works/230/\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix functions for teaching and learning linear algebra and multivariate statistics. https://github.com/friendly/matlib\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays. Computational Statistics and Data Analysis, 43(4), 509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with application to principal components analysis. Biometrics, 58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGabriel, K. R. (1981). Biplot display of multivariate matrices for inspection of data and diagnosis. In V. Barnett (Ed.), Interpreting multivariate data (pp. 147–173). John Wiley; Sons.\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman & Hall.\n\n\nGower, J. C., Lubbe, S. G., & Roux, N. J. L. (2011). Understanding biplots. Wiley. http://books.google.ca/books?id=66gQCi5JOKYC\n\n\nGreenacre, M. (1984). Theory and applications of correspondence analysis. Academic Press.\n\n\nGreenacre, M. (2010). Biplots in practice. Fundación BBVA. https://books.google.ca/books?id=dv4LrFP7U\\_EC\n\n\nHahsler, M., Buchta, C., & Hornik, K. (2024). Seriation: Infrastructure for ordering objects using seriation. https://github.com/mhahsler/seriation\n\n\nHusson, F., Josse, J., Le, S., & Mazet, J. (2024). FactoMineR: Multivariate exploratory data analysis and data mining. http://factominer.free.fr\n\n\nHusson, F., Le, S., & Pagès, J. (2017). Exploratory multivariate analysis by example using r. Chapman & Hall. https://doi.org/10.1201/b21874\n\n\nKassambara, A., & Mundt, F. (2020). Factoextra: Extract and visualize the results of multivariate data analyses. http://www.sthda.com/english/rpkgs/factoextra\n\n\nKrijthe, J. (2023). Rtsne: T-distributed stochastic neighbor embedding using a barnes-hut implementation. https://github.com/jkrijthe/Rtsne\n\n\nKruskal, J. B. (1964). Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29(1), 1–27. https://doi.org/10.1007/bf02289565\n\n\nMaaten, L. van der, & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9, 2579–2605. http://www.jmlr.org/papers/v9/vandermaaten08a.html\n\n\nOksanen, J., Simpson, G. L., Blanchet, F. G., Kindt, R., Legendre, P., Minchin, P. R., O’Hara, R. B., Solymos, P., Stevens, M. H. H., Szoecs, E., Wagner, H., Barbour, M., Bedward, M., Bolker, B., Borcard, D., Carvalho, G., Chirico, M., De Caceres, M., Durand, S., … Weedon, J. (2024). Vegan: Community ecology package. https://github.com/vegandevs/vegan\n\n\nPearson, K. (1896). Contributions to the mathematical theory of evolution—III, regression, heredity and panmixia. Philosophical Transactions of the Royal Society of London, 187, 253–318.\n\n\nPearson, K. (1901). On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 6(2), 559–572.\n\n\nPedersen, T. L., & Robinson, D. (2024). Gganimate: A grammar of animated graphics. https://gganimate.com\n\n\nReaven, G. M., & Miller, R. G. (1979). An attempt to define the nature of chemical diabetes using a multidimensional analysis. Diabetologia, 16, 17–24.\n\n\nShepard, R. N. (1962a). The analysis of proximities: Multidimensional scaling with an unknown distance function. i. Psychometrika, 27(2), 125–140. https://doi.org/10.1007/bf02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities: Multidimensional scaling with an unknown distance function. II. Psychometrika, 27(3), 219–246. https://doi.org/10.1007/bf02289621\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S. (1972a). Multidimensional scaling; theory and applications in the behavioral sciences: Vols. II. Applications. Seminar Press. https://books.google.ca/books?id=PpFAAQAAIAAJ\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S. (1972b). Multidimensional scaling: Theory and applications in the behavioral sciences: Vols. I. Theory. Seminar Press. https://books.google.ca/books?id=pJRAAQAAIAAJ\n\n\nShoben, E. J. (1983). Applications of multidimensional scaling in cognitive psychology. Applied Psychological Measurement, 7(4), 473–490. https://doi.org/10.1177/014662168300700406\n\n\nTorgerson, W. S. (1952). Multidimensional scaling: I. Theory and method. Psychometrika, 17(4), 401–419. https://doi.org/10.1007/bf02288916",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "04-pca-biplot.html#footnotes",
    "href": "04-pca-biplot.html#footnotes",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "This is Euler’s (1758) formula, which states that any convex polyhedron must obey the formula \\(V + F - E = 2\\) where \\(V\\) is the number of vertexes (corners), \\(F\\) is the number of faces and \\(E\\) is the number of edges. For example, a tetrahedron or pyramid has \\((V, F, E) = (4, 4, 6)\\) and a cube has \\((V, F, E) = (8, 6, 12)\\). Stated in words, for all solid bodies confined by planes, the sum of the number of vertexes and the number of faces is two less than the number of edges.↩︎\nFor example, if two variables in the analysis are height and weight, changing the unit of height from inches to centimeters would multiply its variance by \\(2.54^2\\); changing weight from pounds to kilograms would divide its variance by \\(2.2^2\\).↩︎\nThe unfortunate default scale. = FALSE was for consistency with S, the precursor to R but in general scaling is usually advisable.↩︎\nThe vegan package (Oksanen et al., 2024) provides vegan::metaMDS() which allows a wide range of distance measures …↩︎\nThe usual default, perplexity = 30 focuses on preserving the distances to the 30 nearest neighbors and puts virtually no weight on preserving distances to the remaining points. For data sets with a small number of points e.g. \\(n=100\\), this will uncover the global structure quite well since each point will preserve distances to a third of the data set. For larger problems, e.g., \\(n = 10,000\\) points, using a higher perplexity value e.g. 500, will do a much better job for of uncovering the global structure. (This description comes from https://opentsne.readthedocs.io/en/latest/parameters.html)↩︎\nThe general topic of arranging items (variables, factor values) in an orderly sequence is called seriation, and stems from methods of dating in archaeology, used to arrange stone tools, pottery fragments, and other artifacts in time order. In R, the seriation package (Hahsler et al., 2024) provides a wide range of techniques. …↩︎\nhttps://kieranhealy.org/blog/archives/2019/10/27/reconstructing-images-using-pca/↩︎",
    "crumbs": [
      "Exploratory Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "05-linear_models.html",
    "href": "05-linear_models.html",
    "title": "5  Overview of Linear models",
    "section": "",
    "text": "5.1 Linear combinations\nAll methods of multivariate statistics involve a simple idea: Finding weighted sums—linear combinations— of observed variables to optimize some criterion—maximizing a measure of goodness-of-fit, like \\(R^2\\) or minimizing a measure of badness-of-fit like sums of squares of residuals. Methods differ according to whether:",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "05-linear_models.html#linear-combinations",
    "href": "05-linear_models.html#linear-combinations",
    "title": "5  Overview of Linear models",
    "section": "",
    "text": "All variables belong to one set (say, \\(\\mathbf{X}\\)), not distinguished as to whether they are responses or predictors, as in PCA and factor analysis, vs. two sets where one set is considered outcome, dependent variables, to be explained by predictors, independent variables (\\(\\mathbf{X}\\)), as in multiple regression, multivariate analysis of variance, discriminant analysis and canonical correlation analysis.\nThe variables in \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are discrete, categorical factors like sex and level of education or quantitative variables like salary and number of years of experience.\n\nPCA\nFor example, Figure 5.2 illustrates PCA (as we saw in Chapter 4) as finding weights to maximize the variance of linear combinations, \\(v_1, v_2, ...\\), \\[\\begin{aligned}\n\\mathbf{v}_1 & = & a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 + a_3 \\mathbf{x}_3 + a_4 \\mathbf{x}_4 \\\\\n\\mathbf{v}_2 & = & b_1 \\mathbf{x}_1 + b_2 \\mathbf{x}_2 + b_3 \\mathbf{x}_3 + b_4 \\mathbf{x}_4 \\\\\n\\vdots & = & \\vdots \\; , \\\\\n\\end{aligned}\\]\nsubject to all \\(\\mathbf{v}_i, \\mathbf{v}_j\\) being uncorrelated, \\(\\mathbf{v}_i \\;\\perp\\; \\mathbf{v}_j\\).\n\n\n\n\n\n\n\nFigure 5.2: Principal components analysis as linear combinations to maximize variance accounted for. Left: diagram of PCA showing two uncorrelated linear combinations, v1 and v2. Right: Geometry of PCA.\n\n\n\n\nMultiple regression\nAn analogous diagram for multiple regression is shown in Figure 5.3. Here, we find the weights \\(b_1, b_2, \\dots\\) to maximize the \\(R^2\\) of \\(\\mathbf{y}\\) with the predicted values \\(\\widehat{\\mathbf{y}}\\),\n\\[\n\\widehat{\\mathbf{y}} = b_1 \\mathbf{x}_1 + b_2 \\mathbf{x}_2 + b_3 \\mathbf{x}_3 \\:\\: .\n\\] In the vector diagram at the right, saying that the fitted vector \\(\\widehat{\\mathbf{y}}\\) is a linear combination of \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) means that it lies in the plane that they define. The fitted vector is the orthogonal projection of \\(\\mathbf{y}\\) on this plane, and the least squares weights \\(b_1\\) and \\(b_2\\) give the maximum possible correlation \\(r^2 (\\mathbf{y}, \\widehat{\\mathbf{y}})\\).\n\n\n\n\n\n\n\nFigure 5.3: Multiple regression as a linear combination to maximize the squared correlation with the predicted values \\(\\hat{\\mathbf{y}}\\). Right: vector geometry of multiple regression for two predictors.\n\n\n\n\nThe vector of residuals, \\(\\mathbf{e} = \\mathbf{y} -\\widehat{\\mathbf{y}}\\) is orthogonal to that plane (\\(\\mathbf{y}\\) and \\(\\mathbf{e}\\) are uncorrelated), and the least squares solution also minimizes length \\(\\parallel \\mathbf{e} \\parallel = \\sqrt(\\Sigma e_i^2)\\).\nMultivariate regression\nMultivariate multiple regression does the same thing for each response variable, \\(\\mathbf{y}_1\\) and \\(\\mathbf{y}_2\\), as shown in Figure 5.4. It finds the weights to maximize the correlation between each \\(\\mathbf{y}_j\\) and the corresponding predicted value \\(\\widehat{\\mathbf{y}}_j\\).\n\\[\\begin{aligned}\n\\widehat{\\mathbf{y}}_1 & = & a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 + a_3 \\mathbf{x}_3 \\\\\n\\widehat{\\mathbf{y}}_2 & = & b_1 \\mathbf{x}_1 + b_2 \\mathbf{x}_2 + b_3 \\mathbf{x}_3 \\\\\n\\end{aligned}\\]\n\n\n\n\n\n\n\nFigure 5.4: Multivariate multiple regression as linear combinations to maximize the R squared for each response variable separately.\n\n\n\n\nThe weights \\(a_1, a_2, a_3\\) and \\(b_1, b_2, b_3\\) are the same as would be found in separate multiple regressions for the response variables. However, the multivariate tests used here take the correlations among the \\(\\mathbf{y}\\)s, and can be more powerful than fitting separate univariate response models.\nCanonical correlation analysis\nFinally, canonical correlation analysis uses a different approach to fitting relations between a set of responses, \\(\\mathbf{y}_1, \\mathbf{y}_2, \\dots\\) and a set of predictors, \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots\\). …\n\n\n\n\n\n\n\nFigure 5.5: Canonical correlation analyis finds uncorrelated linear combinations of the responses which maximize the R squared with linear combinations of the predictors.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "05-linear_models.html#sec-GLM",
    "href": "05-linear_models.html#sec-GLM",
    "title": "5  Overview of Linear models",
    "section": "\n5.2 The General Linear Model",
    "text": "5.2 The General Linear Model\nTo establish notation and terminology, it is worthwhile to state the the general linear model formally. For convenience, I use vector and matrix notation. This expresses a response variable, \\(\\mathbf{y} = (y_1, y_2, \\dots , y_n)^\\mathsf{T}\\) for \\(n\\) observations, as a sum of terms involving \\(p\\) regressors, \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots , \\mathbf{x}_p\\), each of length \\(n\\).\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\mathbf{y} & = \\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\cdots + \\beta_p \\mathbf{x}_p + \\mathbf{\\epsilon} \\\\\n           & = \\left[ \\mathbf{1},\\; \\mathbf{x}_1,\\; \\mathbf{x}_2,\\; \\dots ,\\; \\mathbf{x}_p \\right] \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\\\\n\\end{aligned} \\tag{5.1}\\]\n\nor, expressed in matrices,\n\\[\n\\Large{\\mathord{\\mathop{\\mathbf{y}}\\limits_{n \\times 1}} = \\mathord{\\mathop{\\mathbf{X}}\\limits_{n \\times (p+1)}}\\; \\mathord{\\mathop{\\mathbf{\\boldsymbol{\\beta}}}\\limits_{(p+1) \\times 1}} + \\boldsymbol{\\epsilon}}\n\\]\nThe matrix \\(\\mathbf{X}\\) is called the model matrix and contains the numerical representations of the predictor variables called regressors. The essential thing about a linear model is that it is linear in the parameters \\(\\beta_i\\). That is, the predicted value of \\(\\mathbf{y}\\) is a linear combination of some \\(\\mathbf{x}_i\\) with weights \\(\\beta_i\\). An example of a nonlinear model is the exponential growth model, \\(y = \\beta_0 + e^{\\beta_1 x}\\), where the parameter \\(\\beta_1\\) appears as an exponent.1\n\nThese can be quantitative variables like age, salary or years of education. But they can also be transformed versions, like sqrt(age) or log(salary).\nA quantitative variable can be represented by more than one model regressor, for example if it is expressed as a polynomial like poly(age, degree=2) or a natural spline like ns(salary, df=5). The model matrix portion for such terms contains one column for each degree of freedom (df) and there are df coefficients in the corresponding portion of \\(\\boldsymbol{\\beta}\\).\nA categorical or discrete predictor– a factor variable in R– with \\(d\\) levels is expressed as \\(d - 1\\) columns in \\(\\mathbf{X}\\). Typically these are contrasts or comparisons between a baseline or reference level and each of the remaining ones, but any set of \\(d - 1\\) linearly independent contrasts can be used by assigning to contrasts(factor). For example, contrasts(factor) &lt;- contr.treatment(4) for a 4-level factor assigns 3 contrasts representing comparisons with a baseline level, typically the first (in alphabetic order). For an ordered factor, such as one for political knowledge with levels “low”, “medium”, “high”, contrasts.poly() returns the coefficients of orthogonal polynomial contrasts representing linear and quadratic trends.\nInteractions between predictors are represented as the direct products of the corresponding columns of \\(\\mathbf{X}\\). This allows the effect of one predictor on the response to depend on values of other predictors. For example, the interaction of two quantitative variables, \\(\\mathbf{x}_1, \\mathbf{x}_2\\) is represented by the product \\(\\mathbf{x}_1 \\times \\mathbf{x}_2\\). More generally, for variables or factors \\(A\\) and \\(B\\) with degrees of freedom \\(\\text{df}_A\\) and \\(\\text{df}_B\\) the regressors in \\(\\mathbf{X}\\) are the \\(\\text{df}_A \\times \\text{df}_B\\) products of each column for \\(A\\) with each column for \\(B\\).\n\n\n5.2.1 Model formulas\nStatistical models in R, such as those fit by lm(), glm() and many other modelling function in R are expressed in a simple notation that was developed by Wilkinson & Rogers (1973) for the GENSTAT software system at the Rothamsted Research Station. It solves the problem of having a compact way to specify any model consisting of any combinations of quantitative and discrete factor variables, interactions of these and arbitrary transformations of these.\nIn this, a model formula take the forms\n\nresponse ~ terms\nresponse ~ term1 + term2 + ...\n\nwhere the left-hand side, response specifies the response variable in the model and the right-hand side specifies the terms in the model specifying the columns in the \\(\\mathbf{X}\\) matrix of Equation 5.1; the coefficients \\(\\beta\\) are implied and not represented explicitly in the formula.\nThe notation y ~ x is read as “y is modeled by x”. The left-hand side is usually a variable name (such as height), but it could be an expression that evaluates to the the response, such as log(salary) or weight/height^2 which represents the body mass index.\nOn the right-hand side (RHS), the usual arithmetic operator, +, -, *, /, ^ have special meanings as described below. The most fundamental is that y ~ a + b is interpreted as “y is modeled by a and b”; that is, the sum of linear terms for a and b.\nSome examples for regression-like models using only quantitative variables, x, x1, x2, x3, ... are shown below:\n\ny ~ x                      # simple linear regression\ny ~ x - 1                  # no intercept: regression through the origin \ny ~ x + I(x^2)             # quadratic model\ny ~ poly(x, 3)             # cubic model\ny ~ x1 * x2                # crossing: x1 + x2  +  x1 : x2\ny ~ x1 + x2 + x3           # multiple regression\ny ~ (x1 + x2 + x3)^2       # response surface: all quadratics & two-way interactions\nlog(y) ~ x1 + poly(x, 2)   # arbitrary transformation of response\ny1 + y2 ~ x1 + x2 + x3     # response is sum of y1 and y2\n\nThe intercept \\(\\beta_0\\) is automatically included in the model without need to specify it explicitly. The minus sign, - on the right-hand side removes terms from the model, so a model with no intercept \\(\\beta_0 = 0\\) can be specifies as y ~ X -1 (or perhaps more naturally, y ~ 0 + X).\nFunction calls on the RHS, such as poly(x, 3) are evaluated directly, but to use a special model operator, like ^ must be “protected” by wrapping the term in I(), meaning “identity” or “inhibit”. Thus, the model y ~ x + I(x^2) means the quadratic model \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\). This differs from the model y ~ poly(x, 2) in that the former uses the raw x, x^2 values (which are necessarily positively correlated) while poly() converts these to orthogonal polynomial scores, which are uncorrelated (and therefore free from problems of collinearity).\nFactor variables are treated specially in linear models, but have simple notations in R formulas. The following examples use A, B, C to represent discrete factors with two or more levels.\n\ny ~ A                 # one-way ANOVA\ny ~ A + B             # two-way, main effects only\ny ~ A * B             # full two-way, with interaction\ny ~ A + B + A:B       # same, in long-hand\ny ~ x + A             # one-way ANCOVA\ny ~ (A + B + C)^2     # three-way ANOVA, incl. all two-way interactions\n\n\n5.2.1.1 Crossing\nThe * operator has special meaning used to specify the crossing of variables and factors and : specifies interactions (products of variables). So, the model y ~ x1 * x2 is expanded to give y ~ x1 + x2 + x1:x2 and the interaction term x1:x2 is calculated as \\(x_1 \\times x_2\\). In algebraic notation (omitting the error term) this works out to the model,\n\\[\\begin{aligned}\ny & = & \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 +  \\beta_1 x_1 *  \\beta_2 x_2 \\\\\n  & = & \\beta_0 + (\\beta_1 + \\beta_2 x_2) x_1 + \\beta_2 x_2 \\:\\: ,\\\\\n\\end{aligned}\\]\nwhich means that the coefficient for \\(x_1\\) in the model is not constant for all values of \\(x_2\\), but rather changes with the value of \\(x_2\\). If \\(\\beta_2 &gt; 0\\), the slope for \\(x_1\\) increases with \\(x_2\\) and vice-versa.\ny ~ A * B for factors is similar, expanding to y ~ A + B + A:B, but the columns in the model matrix represent contrasts among the factor levels as describe above. The main effects, A and B come from contrasts among the means of their factor levels and the interaction term A:B reflects differences among means of A across the levels of B (and vice-versa).\nThe model formula y ~ x + A specifies an ANCOVA model with different intercepts for the levels of A, but with a common slope for x. Adding an interaction of x:A in the model y ~ x * A allow separate slopes and intercepts for the groups.\n\n5.2.1.2 Powers\nThe ^ exponent operator indicates powers of a term expression to a specified degree. Thus the term (A + B)^2 is identical to (A + B) * (A + B) which expands to the main effects of A, B and their interaction, also identical to A * B. In general, the product of parenthesized terms expands as in ordinary algebra,\n\ny ~ (A + B) * (C + D) -&gt; A + B + C + D + A:C + A:D + B:C + B:D\n\nPowers get more interesting with more terms, so (A + B + C)^2 is the same as (A + B + C) * (A + B + C), which includes main effects of A, B and C as well as all two-way interactions, A:B, A:C and B:C. The model formula (A + B + C)^3 expands to include all two-way interactions and the three-way interaction A:B:C.\n\n(A + B + C)^3 -&gt; A + B + C + A:B + A:C + B:C + A:B:C\n\nIn this context - can be use to remove terms, as shown in the following examples\n\n(A + B + C)^2 &lt;-&gt; (A + B + C)^3 - A:B:C\n(A + B + C)^3 - B:C - A:B:C  &lt;-&gt; A + B + C + A:B + A:C \n\nFinally, the symbol . on the right-hand side specifies all terms in the current dataset other than the response. Thus if you have a data.frame containing y, x1, x2, ..., x6, you can specify a model with all variables except x6 as predictors as\n\ny ~ . - x6\n\nTo test what we’ve covered above,\n\nWhat do you think the model formula y ~ .^2 means in a data set containing variables x1, x2, x3, and x4?\nWhat about the formula with y ~ .^2 - A:B:C:D with factors A, B, C, D?\n\nYou can work out questions like these or explore model formulae using terms() for a \"formula\" object. The labels of these terms can then be concatenated to a string and turned back into a formula using as.formula():\n\nf &lt;- formula(y ~ (x1 + x2 + x3 + x4)^2)\nterms = attr(terms(f), \"term.labels\")\n\nterms |&gt; paste(collapse = \" + \")\n#&gt; [1] \"x1 + x2 + x3 + x4 + x1:x2 + x1:x3 + x1:x4 + x2:x3 + x2:x4 + x3:x4\"\n# convert back to a formula\nas.formula(sprintf(\"y ~ %s\", paste(terms, collapse=\" + \"))) \n#&gt; y ~ x1 + x2 + x3 + x4 + x1:x2 + x1:x3 + x1:x4 + x2:x3 + x2:x4 + \n#&gt;     x3:x4\n\n\n5.2.2 Model matrices\nAs noted above, a model formula is used to generate the \\(n \\times (p+1)\\) model matrix, \\(\\mathbf{X}\\), typically containing the column of 1s for the intercept \\(\\beta_0\\) in the model, followed by \\(p\\) columns representing the regressors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots , \\mathbf{x}_p\\). Internally, lm() uses stats::model.matrix() and you can use this to explore how factors, interactions and other model terms are represented in a model object.\nFor a small example, here are a few observations representing income (inc) and type of occupation. model.matrix() takes a one-sided formula with the terms on the right-hand side. The main effect model looks like this:\n\nset.seed(42)\ninc &lt;- round(runif(n=9, 20, 40))\ntype &lt;- rep(c(\"bc\", \"wc\", \"prof\"), each =3)\n\nmm &lt;- model.matrix(~ inc + type) \ndata.frame(type, mm)\n#&gt;   type X.Intercept. inc typeprof typewc\n#&gt; 1   bc            1  38        0      0\n#&gt; 2   bc            1  39        0      0\n#&gt; 3   bc            1  26        0      0\n#&gt; 4   wc            1  37        0      1\n#&gt; 5   wc            1  33        0      1\n#&gt; 6   wc            1  30        0      1\n#&gt; 7 prof            1  35        1      0\n#&gt; 8 prof            1  23        1      0\n#&gt; 9 prof            1  33        1      0\n\nAs you can see, type, with 2 degrees of freedom is represented by two dummy (0/1) variables, typeprof and typewc. Together, these represent treatment contrasts (comparisons) between the baseline group type==\"bc\", which is coded (0, 0) and each of the others: type==\"prof\", coded (1, 0) and type==\"wc\", codes (0, 1). By default, the baseline level is the first in alphabetic or numerical order, but you can change this using relevel().\nIn a model with the interaction inc * type, additional columns are constructed as the product of inc with each of the columns for type.\n\nmodel.matrix(~ inc * type)\n#&gt;   (Intercept) inc typeprof typewc inc:typeprof inc:typewc\n#&gt; 1           1  38        0      0            0          0\n#&gt; 2           1  39        0      0            0          0\n#&gt; 3           1  26        0      0            0          0\n#&gt; 4           1  37        0      1            0         37\n#&gt; 5           1  33        0      1            0         33\n#&gt; 6           1  30        0      1            0         30\n#&gt; 7           1  35        1      0           35          0\n#&gt; 8           1  23        1      0           23          0\n#&gt; 9           1  33        1      0           33          0\n#&gt; attr(,\"assign\")\n#&gt; [1] 0 1 2 2 3 3\n#&gt; attr(,\"contrasts\")\n#&gt; attr(,\"contrasts\")$type\n#&gt; [1] \"contr.treatment\"\n\n\n5.2.3 Contrasts",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "05-linear_models.html#regression",
    "href": "05-linear_models.html#regression",
    "title": "5  Overview of Linear models",
    "section": "\n5.3 Regression",
    "text": "5.3 Regression",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "05-linear_models.html#anova",
    "href": "05-linear_models.html#anova",
    "title": "5  Overview of Linear models",
    "section": "\n5.4 ANOVA",
    "text": "5.4 ANOVA",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "05-linear_models.html#ancova",
    "href": "05-linear_models.html#ancova",
    "title": "5  Overview of Linear models",
    "section": "\n5.5 ANCOVA",
    "text": "5.5 ANCOVA",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "05-linear_models.html#regression-trees",
    "href": "05-linear_models.html#regression-trees",
    "title": "5  Overview of Linear models",
    "section": "\n5.6 Regression trees",
    "text": "5.6 Regression trees\n\nPackage summary\n\nPackages used here:\n1 packages used here: knitr\n\n\n\n\n\nBock, R. D. (1963). Programming univariate and multivariate analysis of variance. Technometrics, 5(1), 95–117. https://doi.org/10.1080/00401706.1963.10490061\n\n\nBock, R. D. (1964). A computer program forunivariate and multivariate analysis of variance. Proceedings of Scientific Symposium on Statistics.\n\n\nClyde, D. J., Cramer, E. M., & Sherin, R. J. (1966). Multivariate statistical programs. Biometric Laboratory,University of Miami.\n\n\nDixon, W. J. (1965). BMD biomedical computer programs. Health Sciences Computing Facility, School of Medicine, University of California; Health Sciences Computing Faculty.\n\n\nFinn, J. D. (1967). MULTIVARIANCE: Fortran program for univariate and multivariate analysis of variance and covariance. School of Education, State University of New York at Buffalo.\n\n\nFisher, R. A. (1923). Studies in crop variation. II. The manurial response of different potato varieties. The Journal of Agricultural Science, 13(2), 311–320. https://hdl.handle.net/2440/15179\n\n\nFisher, R. A. (1925). Statistical methods for research workers. Oliver & Boyd.\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute, 15, 246–263. http://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGalton, F. (1889). Natural inheritance. Macmillan. http://galton.org/books/natural-inheritance/pdf/galton-nat-inh-1up-clean.pdf\n\n\nGraybill, F. A. (1961). An introduction to linear statistical models. McGraw-Hill.\n\n\nIBM. (1965). Proceedings of the IBM scientific computing symposium on statistics: Oct 21-23, 1963 (L. Robinson, Ed.). IBM. https://www.amazon.com/Proceedings-Scientific-Computing-Symposium-Statistics/dp/B000GL5RLU\n\n\nPearson, K. (1896). Contributions to the mathematical theory of evolution—III, regression, heredity and panmixia. Philosophical Transactions of the Royal Society of London, 187, 253–318.\n\n\nScheffé, H. A. (1960). The analysis of variance. Wiley.\n\n\nWilkinson, G. N., & Rogers, C. E. (1973). Symbolic description of factorial models for analysis of variance. Applied Statistics, 22(3), 392. https://doi.org/10.2307/2346786\n\n\nWiner, B. J. (1962). Statistical principles in experimental design. McGraw-Hill.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "05-linear_models.html#footnotes",
    "href": "05-linear_models.html#footnotes",
    "title": "5  Overview of Linear models",
    "section": "",
    "text": "Taking logarithms of both sides would yield the linear model, \\(log(y) = c + \\beta_1 x\\).↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of Linear models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html",
    "href": "06-linear_models-plots.html",
    "title": "6  Plots for univariate response models",
    "section": "",
    "text": "6.1 The “regression quartet”\nFor a fitted model, plotting the model object with plot(model) provides for any of six basic plots, of which four are produced by default, giving rise to the term regression quartet for this collection. These are:\nOne key feature of these plots is providing reference lines or smoothed curves for ease of judging the extent to which a plot conforms to the expected pattern; another is the labeling of observations which deviate from an assumption.\nThe base-R plot(model) plots are done much better in a variety of packages. I illustrate some versions from the car (Fox et al., 2023) and performance (Lüdecke et al., 2021) packages, part of the easystats (Lüdecke et al., 2022) suite of packages.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#the-regression-quartet",
    "href": "06-linear_models-plots.html#the-regression-quartet",
    "title": "6  Plots for univariate response models",
    "section": "",
    "text": "Residuals vs. Fitted: For well-behaved data, the points should hover around a horizontal line at residual = 0, with no obvious pattern or trend.\nNormal Q-Q plot: A plot of sorted standardized residuals \\(e_i\\) (obtained fromrstudent(model)) against the theoretical values those values would have in a standard normal \\(\\mathcal{N}(0, 1)\\) distribution.\nScale-Location: Plots the square-root of the absolute values of the standardized residuals \\(\\sqrt{| e_i |}\\) as a measure of “scale” against the fitted values \\(\\hat{y}_i\\) as a measure of “location”. This provides an assessment of homogeneity of variance. Violations appear as a tendency for scale (variability) to vary with location.\nResiduals vs. Leverage: Plots standardized residuals against leverage to help identify possibly influential observations. Leverage, or “hat” values (given by hat(model)) are proportional to the squared Mahalanobis distances of the predictor values \\(\\mathbf{x}_i\\) from the means, and measure the potential of an observation to change the fitted coefficients if that observation was deleted. Actual influence is measured by Cooks’s distance (cooks.distance(model)) and is proportional to the product of residual times leverage. Contours of constant Cook’s \\(D\\) are added to the plot.\n\n\n\n\n6.1.1 Example: Duncan’s occupational prestige\nIn a classic study in sociology, Duncan (1961) used data from the U.S. Census in 1950 to study how one could predict the prestige of occupational categories — which is hard to measure — from available information in the census for those occupations. His data is available in carData:Duncan, and contains\n\n\ntype: the category of occupation, one of prof (professional), wc (white collar) or bc (blue collar);\n\nincome: the percentage of occupational incumbents with a reported income &gt; 3500 (about 40,000 in current dollars);\n\neducation: the percentage of occupational incumbents who were high school graduates;\n\nprestige: the percentage of respondents in a social survey who rated the occupation as “good” or better in prestige.\n\nThese variables are a bit quirky in they are measured in percents, 0-100, rather dollars for income and years for education, but this common scale permitted Duncan to ask an interesting sociological question: Assuming that both income and education predict prestige, are they equally important, as might be assessed by testing the hypothesis \\(\\mathcal{H}_0: \\beta_{\\text{income}} = \\beta_{\\text{education}}\\). If so, this would provide a very simple theory for occupational prestige.\nA quick look at the data shows the variables and a selection of the occupational categories, which are the row.names() of the dataset.\n\ndata(Duncan, package = \"carData\")\nset.seed(42)\ncar::some(Duncan)\n#&gt;                  type income education prestige\n#&gt; accountant       prof     62        86       82\n#&gt; professor        prof     64        93       93\n#&gt; engineer         prof     72        86       88\n#&gt; factory.owner    prof     60        56       81\n#&gt; store.clerk        wc     29        50       16\n#&gt; carpenter          bc     21        23       33\n#&gt; machine.operator   bc     21        20       24\n#&gt; barber             bc     16        26       20\n#&gt; soda.clerk         bc     12        30        6\n#&gt; janitor            bc      7        20        8\n\nLet’s start by fitting a simple model using just income and education as predictors. The results look very good! Both income and education are highly significant and the \\(R^2 = 0.828\\) for the model indicates that prestige is very well predicted by just these variables.\n\nduncan.mod &lt;- lm(prestige ~ income + education, data=Duncan)\nsummary(duncan.mod)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = prestige ~ income + education, data = Duncan)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -29.54  -6.42   0.65   6.61  34.64 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  -6.0647     4.2719   -1.42     0.16    \n#&gt; income        0.5987     0.1197    5.00  1.1e-05 ***\n#&gt; education     0.5458     0.0983    5.56  1.7e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 13.4 on 42 degrees of freedom\n#&gt; Multiple R-squared:  0.828,  Adjusted R-squared:  0.82 \n#&gt; F-statistic:  101 on 2 and 42 DF,  p-value: &lt;2e-16\n\nBeyond this, Duncan was interested in the coefficients and whether income and education could be said to have equal impacts on predicting occupational prestige. A nice display of model coefficients with confidence intervals is provided by parameters::model_parameters().\n\nparameters::model_parameters(duncan.mod)\n#&gt; Parameter   | Coefficient |   SE |         95% CI | t(42) |      p\n#&gt; ------------------------------------------------------------------\n#&gt; (Intercept) |       -6.06 | 4.27 | [-14.69, 2.56] | -1.42 | 0.163 \n#&gt; income      |        0.60 | 0.12 | [  0.36, 0.84] |  5.00 | &lt; .001\n#&gt; education   |        0.55 | 0.10 | [  0.35, 0.74] |  5.56 | &lt; .001\n\nWe can also test Duncan’s hypothesis that income and education have equal effects on prestige with car::linearHypothesis(). This is constructed as a test of a restricted model in which the two coefficients are forced to be equal against the unrestricted model. Duncan was very happy with this result.\n\ncar::linearHypothesis(duncan.mod, \"income = education\")\n#&gt; \n#&gt; Linear hypothesis test:\n#&gt; income - education = 0\n#&gt; \n#&gt; Model 1: restricted model\n#&gt; Model 2: prestige ~ income + education\n#&gt; \n#&gt;   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n#&gt; 1     43 7519                         \n#&gt; 2     42 7507  1      12.2 0.07    0.8\n\nEquivalently, the linear hypothesis that \\(\\beta_{\\text{Inc}} = \\beta_{\\text{Educ}}\\) can be tested with a Wald test for difference between these coefficients, expressed as \\(\\mathcal{H}_0 : \\mathbf{C} \\mathbf{\\beta} = 0\\), using \\(\\mathbf{C} = (0, -1, 1)\\). The estimated value, -0.053 has a confidence interval [-0.462, 0.356], consistent with Duncan’s hypothesis.\n\nwtest &lt;- spida2::wald(duncan.mod, c(0, -1, 1))[[1]]\nwtest$estimate\n#&gt;       \n#&gt;        Estimate Std.Error DF t-value p-value Lower 0.95 Upper 0.95\n#&gt;   Larg  -0.0529     0.203 42  -0.261   0.795     -0.462      0.356\n\nWe can visualize this test and confidence intervals using a joint confidence ellipse for the coefficients for income and education in the model duncan.mod. In Figure 6.1 the size of the ellipse is set to \\(\\sqrt{F^{0.95}_{1,\\nu}} = t^{0.95}_{\\nu}\\), so that its shadows on the horizontal and vertical axes correspond to 1D 95% confidence intervals. In this plot, the line through the origin with slope \\(= 1\\) corresponds to equal coefficients for income and education and the line with slope \\(= -1\\) corresponds to their difference, \\(\\beta_{\\text{Educ}} - \\beta_{\\text{Inc}}\\). The orthogonal projection of the coefficient vector \\((\\widehat{\\beta}_{\\text{Inc}}, \\widehat{\\beta}_{\\text{Educ}})\\) (the center of the ellipse) is the point estimate of \\(\\widehat{\\beta}_{\\text{Educ}} - \\widehat{\\beta}_{\\text{Inc}}\\) and the shadow of the ellipse along this axis is the 95% confidence interval for the difference in slopes.\n\n\nSee the codeconfidenceEllipse(duncan.mod, col = \"blue\",\n  levels = 0.95, dfn = 1,\n  fill = TRUE, fill.alpha = 0.2,\n  xlim = c(-.4, 1),\n  ylim = c(-.4, 1), asp = 1,\n  cex.lab = 1.3,\n  grid = FALSE,\n  xlab = expression(paste(\"Income coefficient, \", beta[Inc])),\n  ylab = expression(paste(\"Education coefficient, \", beta[Educ])))\n\nabline(h=0, v=0, lwd = 2)\n\n# confidence intervals for each coefficient\nbeta &lt;- coef( duncan.mod )[-1]\nCI &lt;- confint(duncan.mod)       # confidence intervals\nlines( y = c(0,0), x = CI[\"income\",] , lwd = 5, col = 'blue')\nlines( x = c(0,0), y = CI[\"education\",] , lwd = 5, col = 'blue')\npoints(rbind(beta), col = 'black', pch = 16, cex=1.5)\npoints(diag(beta) , col = 'black', pch = 16, cex=1.4)\narrows(beta[1], beta[2], beta[1], 0, angle=8, len=0.2)\narrows(beta[1], beta[2], 0, beta[2], angle=8, len=0.2)\n\n# add line for equal slopes\nabline(a=0, b = 1, lwd = 2, col = \"darkgreen\")\ntext(0.8, 0.8, expression(beta[Educ] == beta[Inc]), \n     srt=45, pos=3, cex = 1.5, col = \"darkgreen\")\n\n# add line for difference in slopes\ncol &lt;- \"darkred\"\nx &lt;- c(-1.5, .5)\nlines(x=x, y=-x)\ntext(-.15, -.15, expression(~beta[\"Educ\"] - ~beta[\"Inc\"]), \n     col=col, cex=1.5, srt=-45)\n\n# confidence interval for b1 - b2\nwtest &lt;- spida2::wald(duncan.mod, c(0, -1, 1))[[1]]\nlower &lt;- wtest$estimate$Lower /2\nupper &lt;- wtest$estimate$Upper / 2\nlines(-c(lower, upper), c(lower,upper), lwd=6, col=col)\n\n# projection of (b1, b2) on b1-b2 axis\nbeta &lt;- coef( duncan.mod )[-1]\nbdiff &lt;- beta %*% c(1, -1)/2\npoints(bdiff, -bdiff, pch=16, cex=1.3)\narrows(beta[1], beta[2], bdiff, -bdiff, \n       angle=8, len=0.2, col=col, lwd = 2)\n\n# calibrate the diff axis\nticks &lt;- seq(-0.3, 0.3, by=0.2)\nticklen &lt;- 0.02\nsegments(ticks, -ticks, ticks-sqrt(2)*ticklen, -ticks-sqrt(2)*ticklen)\ntext(ticks-2.4*ticklen, -ticks-2.4*ticklen, ticks, srt=-45)\n\n\n\n\n\n\nFigure 6.1: Joint 95% confidence ellipse for \\((\\beta_{\\text{Inc}}, \\beta_{\\text{Educ}})\\), together with their 1D shadows, which give 95% confidence intervals for the separate coefficients and the linear hypothesis that the coefficients are equal. Projecting the confidence ellipse along the line with unit slope gives a confidence interval for the difference between coefficients, shown by the dark red line.\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.1.1 Diagnostic plots\nBut, should Duncan be so happy? It is unlikely that he ran any model diagnostics or plotted his model; we do so now. Here is the regression quartet (Figure 6.2) for this model. Each plot shows some trend lines, and importantly, labels some observations that stand out and might deserve attention.\n\nop &lt;- par(mfrow = c(2,2), \n          mar = c(4,4,3,1)+.1)\nplot(duncan.mod, lwd=2, pch=16)\npar(op)\n\n\n\n\n\n\nFigure 6.2: Regression quartet of diagnostic plots for the Duncan data. Several possibly unusual observations are labeled.\n\n\n\n\nSome points to note:\n\nA few observations (minister, reporter, conductor, contractor) are flagged in multiple panels. It turns out (Section 6.6.3) that the observations for minister and reporter noted in the residuals vs. leverage plot are highly influential and largely responsible for Duncan’s finding that the slopes for income and education could be considered equal.\nThe red trend line in the scale-location plot indicates that residual variance is not constant, but rather increases from both ends. This is a consequence of the fact that prestige is measured as a percentage, bounded at [0, 100], and the standard deviation of a percentage \\(p\\) is proportional to \\(\\sqrt{p \\times (1-p)}\\) which is maximal at $p = 0.5.\n\nSimilar, but nicer-looking diagnostic plots are provided by performance::check_model() which uses ggplot2 for graphics. These include helpful captions indicating what should be observed for each for a good-fitting model. However, they don’t have as good facilities for labeling unusual observations as the base R plot() or functions in the car package.\n\ncheck_model(duncan.mod, check=c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\"))\n\n\n\n\n\n\nFigure 6.3: Diagnostic plots for the Duncan data, using check_model().\n\n\n\n\n\n6.1.2 Example: Canadian occupational prestige\n\nFollowing Duncan (1961), occupational prestige was studied in a Canadian context by Bernard Blishen and others at York University, giving the dataset carData::Prestige which we looked at in Section 3.2.3. It differs from the Duncan dataset primarily in that the main variables—prestige, income and education were revamped to better reflect the underlying constructs in more meaningful units.\n\nprestige: Rather than a simple percentage of “good+” ratings, this uses a wider and more reliable scale from Pineo & Porter (1967) on a scale from 10–90.\nincome is measured as the average income of incumbents in each occupation, in 1971 dollars, rather than percent exceeding a given threshold ($3500)\neducation is measured as the average education of occupational incumbents, years.\n\nThe dataset again includes type of occupation with the same levels \"bc\" (blue collar), \"wc\" (white collar) and \"prof\" (professional)1, but in addition includes the percent of women in these occupational categories.\nOur interest again is in understanding how prestige is related to census measures of the average education, income, percent women of incumbents in those occupations, but with attention to the scales of measurement and possibly more complex relationships.\n\ndata(Prestige, package=\"carData\")\n# Reorder levels of type\nPrestige$type &lt;- factor(Prestige$type, \n                        levels=c(\"bc\", \"wc\", \"prof\")) \nstr(Prestige)\n#&gt; 'data.frame':  102 obs. of  6 variables:\n#&gt;  $ education: num  13.1 12.3 12.8 11.4 14.6 ...\n#&gt;  $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...\n#&gt;  $ women    : num  11.16 4.02 15.7 9.11 11.68 ...\n#&gt;  $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...\n#&gt;  $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...\n#&gt;  $ type     : Factor w/ 3 levels \"bc\",\"wc\",\"prof\": 3 3 3 3 3 3 3 3 3 3 ...\n\nWe fit a main-effects model using all predictors (ignoring census, the Canadian Census occupational code):\n\nprestige.mod &lt;- lm(prestige ~ education + income + women + type,\n                   data=Prestige)\n\nplot(model) produces four separate plots. For a quick look, I like to arrange them in a single 2x2 figure.\n\nop &lt;- par(mfrow = c(2,2), \n          mar=c(4,4,3,1)+.1)\nplot(prestige.mod, lwd=2, cex.lab=1.4)\npar(op)\n\n\n\n\n\n\nFigure 6.4: Regression quartet of diagnostic plots for the Prestige data. Several possibly unusual observations are labeled in each plot.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#other-model-plots",
    "href": "06-linear_models-plots.html#other-model-plots",
    "title": "6  Plots for univariate response models",
    "section": "\n6.2 Other Model plots",
    "text": "6.2 Other Model plots\nTODO: What goes here?",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#coefficient-displays",
    "href": "06-linear_models-plots.html#coefficient-displays",
    "title": "6  Plots for univariate response models",
    "section": "\n6.3 Coefficient displays",
    "text": "6.3 Coefficient displays\nThe results of linear models are most often reported in tables and typically with “significance stars” (*, **, ***) to indicate the outcome of hypothesis tests. These are useful for looking up precise values and you can use this format to compare a small number of competing models side-by-side. However, as illustrated by Kastellec & Leoni (2007), plots of coefficients can increase the clarity of presentation and make it easier to draw correct conclusions. Yet, when you need to present tables, there is a variety of tools in R that can help make them attractive in publications.\nFor illustration, I’ll consider three models for the Prestige data of increasing complexity:\n\n\nmod1 fits the main effects of the three quantitative predictors;\n\nmod2 adds the categorical variable type of occupation;\n\nmod3 allows an interaction of income with type.\n\n\nmod1 &lt;- lm(prestige ~ education + income + women,\n           data=Prestige)\nmod2 &lt;- lm(prestige ~ education + women + income + type,\n           data=Prestige)\nmod3 &lt;- lm(prestige ~ education + women + income * type,\n           data=Prestige)\n\nFrom our earlier analyses (Section 3.2.3) we saw that the marginal relationship between income and prestige was nonlinear Figure 3.11), and was better represented in a linear model using log(income) (Section 3.2.3.1) shown in Figure 3.14. However, this possibly non-linear relationship could also be explained by stratifying (Section 3.2.3.2) the data by type of occupation (Figure 3.15).\n\n6.3.1 Displaying coefficients\nsummary() gives the complete precis of a fitted model, with information about the estimated coefficients, residuals and goodness-of fit statistics like \\(R^2\\). But if you only want to see the coefficients, standard errors, etc. lmtest::coeftest() gives these results in the familiar format for console output. broom::tidy() places these in a tidy format common to many modeling functions which is useful for futher processing (e.g., comparing models).\n\nlmtest::coeftest(mod1)\n#&gt; \n#&gt; t test of coefficients:\n#&gt; \n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -6.794334   3.239089   -2.10    0.039 *  \n#&gt; education    4.186637   0.388701   10.77  &lt; 2e-16 ***\n#&gt; income       0.001314   0.000278    4.73  7.6e-06 ***\n#&gt; women       -0.008905   0.030407   -0.29    0.770    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nbroom::tidy(mod1)\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept) -6.79     3.24        -2.10  3.85e- 2\n#&gt; 2 education    4.19     0.389       10.8   2.59e-18\n#&gt; 3 income       0.00131  0.000278     4.73  7.58e- 6\n#&gt; 4 women       -0.00891  0.0304      -0.293 7.70e- 1\n\nThe modelsummary package (Arel-Bundock, 2024) is an easy to use, very general package to summarize data and statistical models in R. The main function modelsummary() can produce highly customizable tables of coefficients in a wide variety of output formats, including HTML, PDF, LaTeX, Markdown, and MS Word. You can select the statistics displayed for any model term with the estimate and statistic arguments.\n\nmodelsummary(list(\"Model1\" = mod1),\n  coef_omit = \"Intercept\",\n  shape = term ~ statistic,\n  estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n  statistic = c(\"std.error\", \"p.value\"),\n  fmt = fmt_statistic(\"estimate\" = 3, \"conf.low\" = 4, \"conf.high\" = 4),\n  gof_omit = \".\")\n\n\nTable 6.1: Table of coefficients for the main effects model.\n\n\n\n\n    \n\n      \n\n\n \nModel1\n\n\n \n                Est.\n                S.E.\n                p\n              \n\n\n\neducation\n                  4.187 [3.4153, 4.9580]  \n                  0.389\n                  0.000\n                \n\nincome   \n                  0.001 [0.0008, 0.0019]  \n                  0.000\n                  0.000\n                \n\nwomen    \n                  -0.009 [-0.0692, 0.0514]\n                  0.030\n                  0.770\n                \n\n\n\n\n\n\n\n\n\ngof_omit allows you to omit or select the goodness-of-fit statistics and other model information available from those listed by get_gof():\n\nget_gof(mod1)\n#&gt;   aic bic r.squared adj.r.squared rmse nobs   F logLik\n#&gt; 1 716 729     0.798         0.792 7.69  102 129   -353\n\n\n6.3.2 Visualizing coefficients\nmodelplot() is the companion function. It allows you to plot model estimates and confidence intervals. It makes it easy to subset, rename, reorder, and customize plots using same mechanics as in modelsummary().\n\ntheme_set(theme_minimal(base_size = 14))\n\nmodelplot(mod1, coef_omit=\"Intercept\", \n          color=\"red\", size=1, linewidth=2) +\n  labs(title=\"Raw coefficients for mod1\")\n\n\n\n\n\n\nFigure 6.5: Plot of coefficients and their standard error bar for the simple main effects model\n\n\n\n\nBut this plot is disappointing and misleading because it show the raw coefficients. From the plot, it looks like only education has a non-zero effect, but the effect of income is also highly significant. The problem is that the magnitude of the coefficient \\(\\hat{b}_{\\text{education}}\\) is more than 40,000 times that of the other coefficients, because education is measured years, while income is measured in dollars. The 95% confidence interval for \\(\\hat{b}_{\\text{income}} = [0.0008, 0.0019]\\), but this is invisible in the plot.\nBefore figuring out how to fix this issue, I show the comparable displays from modelsummary() and modelplot() for all three models. When you give modelsummary() a list of models, it displays their coefficients side-by-side as shown in Table 6.2.\n\nmodels &lt;- list(\"Model1\" = mod1, \"Model2\" = mod2, \"Model3\" = mod3)\nmodelsummary(models,\n     coef_omit = \"Intercept\",\n     fmt = 2,\n     stars = TRUE,\n     shape = term ~ statistic,\n     statistic = c(\"std.error\", \"p.value\"),\n     gof_map = c(\"rmse\", \"r.squared\")\n     )\n\n\nTable 6.2: Table of coefficients for three models.\n\n\n\n\n    \n\n      \n\n\n \nModel1\nModel2\nModel3\n\n\n \n                Est.\n                S.E.\n                p\n                Est.\n                S.E.\n                p\n                Est.\n                S.E.\n                p\n              \n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\neducation        \n                  4.19***\n                  0.39\n                  &lt;0.01\n                  3.66***\n                  0.65\n                  &lt;0.01\n                  2.80*** \n                  0.59\n                  &lt;0.01\n                \n\nincome           \n                  0.00***\n                  0.00\n                  &lt;0.01\n                  0.00***\n                  0.00\n                  &lt;0.01\n                  0.00*** \n                  0.00\n                  &lt;0.01\n                \n\nwomen            \n                  -0.01  \n                  0.03\n                  0.77 \n                  0.01   \n                  0.03\n                  0.83 \n                  0.08*   \n                  0.03\n                  0.02 \n                \n\ntypewc           \n                         \n                      \n                       \n                  -2.92  \n                  2.67\n                  0.28 \n                  3.43    \n                  5.37\n                  0.52 \n                \n\ntypeprof         \n                         \n                      \n                       \n                  5.91   \n                  3.94\n                  0.14 \n                  27.55***\n                  5.41\n                  &lt;0.01\n                \n\nincome × typewc  \n                         \n                      \n                       \n                         \n                      \n                       \n                  0.00    \n                  0.00\n                  0.21 \n                \n\nincome × typeprof\n                         \n                      \n                       \n                         \n                      \n                       \n                  0.00*** \n                  0.00\n                  &lt;0.01\n                \n\nRMSE             \n                  7.69   \n                      \n                       \n                  6.91   \n                      \n                       \n                  6.02    \n                      \n                       \n                \n\nR2               \n                  0.798  \n                      \n                       \n                  0.835  \n                      \n                       \n                  0.875   \n                      \n                       \n                \n\n\n\n\n\n\n\n\n\nNote that a factor predictor (like type here) with \\(d\\) levels is represented by \\(d-1\\) coefficients in main effects and in interactions with quantitative variables. These levels are coded with treatment contrasts by default. Also by default, the first level is set as the reference level in alphabetical order. Here the reference level is blue collar (bc), so the coefficient typeprof = 5.91 indicates that professional occupations on average are rated 5.91 greater on the Prestige scale than blue collar workers.\nNote also that unlike the table, the coefficients in Figure 6.5 are ordered from bottom to top, because the Y axis starts at the lower left corner. In Figure 6.6 I use scale_y_discrete() to reverse the order. It is also useful to add a vertical reference line at \\(\\beta = 0\\).\n\nmodelplot(models, \n          coef_omit=\"Intercept\", \n          size=1.3, linewidth=2) +\n  ggtitle(\"Raw coefficients\") +\n  geom_vline(xintercept = 0, linewidth=1.5) +\n  scale_y_discrete(limits=rev) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.2))\n\n\n\n\n\n\nFigure 6.6: Plot of raw coefficients and their confidence intervals for all three models\n\n\n\n\n\n6.3.3 More useful coefficient plots\nThe problem with plots of raw coefficients shown in Figure 6.5 and Figure 6.6 is that the coefficients for different predictors are not directly comparable because they are measured in different units.\nOne alternative is to plot the standardized coefficients. Another way is to re-scale the predictors into more comparable and meaningful units. I illustrate these ideas below.\nStandardized coefficients\nThe simplest way to do this is to transform all variables to standardized (\\(z\\)) scores. The coefficients are then interpreted as the standardized change in prestige for a one standard deviation change in the predictors. The syntax below uses scale to transform all the numeric variables. Then, we re-fit the models using the standardized data.\n\nPrestige_std &lt;- Prestige |&gt;\n  as_tibble() |&gt;\n  mutate(across(where(is.numeric), scale))\n\nmod1_std &lt;- lm(prestige ~ education + income + women, \n               data=Prestige_std)\nmod2_std &lt;- lm(prestige ~ education + women + income + type, \n               data=Prestige_std)\nmod3_std &lt;- lm(prestige ~ education + women + income * type, \n               data=Prestige_std)\n\nThe plot in Figure 6.7 now shows the significant effect of income in all three models. As well, it offers a more sensitive comparison of the coefficients of other terms across models; for example women is not significant in models 1 and 2, but becomes significant in Model 3 when the interaction of income * type is included.\n\nmodels &lt;- list(\"Model1\" = mod1_std, \"Model2\" = mod2_std, \"Model3\" = mod3_std)\nmodelplot(models, \n          coef_omit=\"Intercept\", size=1.3) +\n  ggtitle(\"Standardized coefficients\") +\n  geom_vline(xintercept = 0, linewidth = 1.5) +\n  scale_y_discrete(limits=rev) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.2))\n\n\n\n\n\n\nFigure 6.7: Plot of standardized coefficients and their confidence intervals for all three models\n\n\n\n\nIt turns out there is an easier way to get plots of standardized coefficients. modelsummary() extracts coefficients from model objects using the parameters package, and that package offers several options for standardization: See model parameters documentation. We can pass the standardize=\"refit\" (or other) argument directly to modelsummary() or modelplot(), and that argument will be forwarded to parameters. The plot produced by the code below is identical to Figure 6.7 and is not shown.\n\nmodelplot(list(\"mod1\" = mod1, \"mod2\" = mod2, \"mod3\" = mod3),\n          standardize = \"refit\",\n          coef_omit=\"Intercept\", size=1.3) +\n  ggtitle(\"Standardized coefficients\") +\n  geom_vline(xintercept = 0, linewidth=1.5) +\n  scale_y_discrete(limits=rev) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.2))\n\nThe ggstats package (Larmarange, 2024) provides even nicer versions of coefficient plots that handle factors in a more reasonable way, as levels within the factor. ggcoef_model() plots a single model and ggcoef_compare() plots a list of models using sensible defaults. A small but nice feature is that it explicitly shows the 0 value for the reference level of a factor (type = \"bc\" here) and uses better labels for factors and their interactions.\n\nmodels &lt;- list(\n  \"Base model\"      = mod1_std,\n  \"Add type\"        = mod2_std,\n  \"Add interaction\" = mod3_std)\n\nggcoef_compare(models) +\n  labs(x = \"Standarized Coefficient\")\n\n\n\n\n\n\nFigure 6.8: Model comparison plot from ggcoef_compare()\n\n\n\n\nMore meaningful units\nStandardizing the variables makes the coefficients directly comparable, but it may be harder to understand what they mean in terms of the variables. For example, the coefficient of income in mod2_std is 0.25. A literal interpretation is that occupational prestige is expected to increase 0.25 standard deviations for each standard deviation increase in income, but it may be difficult to appreciate what this means.\nA better substantive comparison of the coefficients could use understandable scales for the predictors, e.g., months of education, $100,000 of income or 10% of women’s participation. Note that the effect of this is just to multiply the coefficients and their standard errors by a factor. The statistical conclusions of significance are unchanged.\nFor simplicity, I do this just for Model 1.\n\nPrestige_scaled &lt;- Prestige |&gt;\n  mutate(education = 12 * education,\n         income = income / 100,\n         women = women / 10)\n\nmod1_scaled &lt;- lm(prestige ~ education + income + women,\n                  data=Prestige_scaled)\n\nWhen we plot this with ggcoef_model(), there are many options to control how variables are labeled and other details.\n\nggcoef_model(mod1_scaled,\n  signif_stars = FALSE,\n  variable_labels = c(education = \"education\\n(months)\",\n                      income = \"income\\n(/$100K)\",\n                      women = \"women\\n(/10%)\")) +\n  xlab(\"Coefficients for prestige with scaled predictors\")\n\n\n\n\n\n\nFigure 6.9: Plot of coefficients for prestige with scaled predictors for Model 1.\n\n\n\n\nSo, on average, each additional month of education increases the prestige rating by 0.34 units, while an additional $100,000 of income increases it by 0.13 units. While these are significant effects, they are not large in relation to the scale of prestige which ranges 14.8—87.2.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#sec-avplots",
    "href": "06-linear_models-plots.html#sec-avplots",
    "title": "6  Plots for univariate response models",
    "section": "\n6.4 Added-variable and related plots",
    "text": "6.4 Added-variable and related plots\nIn multiple regression problems, it is most often useful to construct a scatterplot matrix and examine the plot of the response vs. each of the predictors as well as those of the predictors against each other. However, the simple, marginal scatterplots of a response \\(y\\) against each of several predictors \\(x_1, x_2, \\dots\\) can be misleading because each one ignores the other predictors.\nTo see this consider a toy dataset, coffee, giving measures of coffee consumption, occupational stress and an index of heart problems in a sample of \\(n=20\\) graduate students and professors.\n\ndata(coffee, package=\"matlib\")\n\nscatterplotMatrix(~ Heart + Coffee + Stress, \n  data=coffee,\n  smooth = FALSE,\n  ellipse = list(levels=0.68, fill.alpha = 0.1),\n  pch = 19, cex.labels = 2.5)\n\n\n\n\n\n\nFigure 6.10: Scatterplot matrix showing pairwise relations among Heart (\\(y\\)), Coffee consumption (\\(x_1\\)) and Stress (\\(x_2\\)), with linear regression lines and 68% data ellipses for the bivariate relations\n\n\n\n\nThe message from these marginal plots in Figure 6.10 seems to be that coffee is bad for your heart, stress is bad for your heart, and stress is also strongly related to coffee consumption. Yet, when we fit a model with both variables together, we get the following results:\n\nfit.both   &lt;- lm(Heart ~ Coffee + Stress, data=coffee)\nlmtest::coeftest(fit.both)\n#&gt; \n#&gt; t test of coefficients:\n#&gt; \n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   -7.794      5.793   -1.35     0.20    \n#&gt; Coffee        -0.409      0.292   -1.40     0.18    \n#&gt; Stress         1.199      0.224    5.34  5.4e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe coefficients suggest that stress is indeed bad for your heart, but the negative (though non-significant) coefficient for coffee suggests that coffee is good for you.How can this be? Does that mean I should drink more coffee, while avoiding stress?\nThe reason for this apparent paradox is that the general linear model fit by lm() estimates all effects together and so the coefficients pertain to the partial effect of a given predictor, adjusting for the effects of all others. That is, the coefficient for coffee (\\(\\beta_{\\text{Coffee}} = -0.41\\)) estimates the effect of coffee for people with same level of stress. In the marginal scatterplot, the positive slope for coffee (1.10) ignores the correlation of coffee and stress.\nThis is an example of confounding in regression when an important predictor is omitted. Stress is positively associated with both coffee consumption and heart damage. When stress is omitted, the coefficient for coffee is biased because it “picks up” the relation with the omitted variable.\nA solution to this problem is the added-variable plot (“AV plot”, also called partial regression plot, MostellerTukey-1977). This is a multivariate analog of a a simple marginal scatterplot, designed to visualize directly the partial relation between \\(y\\) and the predictors \\(x_1, x_2, \\dots\\) in a multiple regression model.\nYou can think of this as a magic window that hides the relations of all other variables with each of the \\(y\\) and \\(x_i\\) shown in a given added-variable plot. This gives an unobstructed view of the net relation between \\(y\\) and \\(x_i\\) with the effect of all other variables removed. In effect, it reduces the problem of viewing the complete model in \\(p\\)-dimensional space to a sequence of \\(p\\) 2D plots, each of which tells the story of one predictor, unentangled from the others. This is essentially the same idea as the partial variables plot (Section 3.8.3) used to understand partial correlations.\nThe construction of an AV plot is conceptually very simple. For variable \\(x_i\\), imagine that we fit two supplementary regressions:\n\nRegress \\(\\mathbf{y}\\) on \\(\\mathbf{X_{(-i)}}\\), the model matrix of all of the regressors except \\(x_i\\). By definition, the residuals from this regression, \\(\\mathbf{y}^\\star \\equiv \\mathbf{y} \\,\\vert\\, \\text{others} = \\mathbf{y} - \\widehat{\\mathbf{y}} \\,\\vert\\, \\mathbf{X_{(-i)}}\\),  are the part of \\(\\mathbf{y}\\) that cannot be explained by all the other regression terms. These residuals are necessarily uncorrelated with the other predictors.\nRegress \\(x_i\\) on the other predictors, \\(\\mathbf{X_{(-i)}}\\) and again obtain the residuals. These residuals, \\(\\mathbf{x}_i^\\star \\equiv \\mathbf{x}_i \\,\\vert\\, \\text{others} = \\mathbf{x}_i - \\widehat{\\mathbf{x}}_i \\,\\vert\\, \\mathbf{X_{(-i)}}\\) give the part of \\(x_i\\) that cannot be explained by the others, and so are uncorrelated with them.\n\nThe AV plot is then just a simple scatterplot of these residuals, \\(\\mathbf{y}^\\star\\) on the vertical axis, and \\(\\mathbf{x}^\\star\\) on the horizontal. In practice, it is unnecessary to run the auxilliary regressions this way (Velleman & Welsh, 1981). Both can be calculated using stats::lsfit() roughly as follows:\n\nAVcalc &lt;- function(model, variable)\nX &lt;- model.matrix(model)\nresponse &lt;- model.response(model)\nx &lt;- X[, -variable]\ny &lt;- cbind(X[, variable], response)\nfit &lt;- lsfit(x, y, intercept = FALSE)\nresids &lt;- residuals(fit)\nreturn(resids)\n\nNote that y here contains both the current predictor, \\(\\mathbf{x}_i\\) and the response \\(\\mathbf{y}\\), so the residuals resids have two columns, one for \\(x_i \\,\\vert\\, \\text{others}\\) and one for \\(y \\,\\vert\\, \\text{others}\\).\nAdded-variable plots are produced using car::avPlot() for one predictor or avPlots() for any number of model terms. The id argument controls which points are identified in the plots; n=2 labels the two points that are furthest from the mean on the horizontal axis and the two with the largest absolute residuals. For instance, in Figure 6.11, observations 5 and 13 are flagged because their conditional \\(\\mathbf{x}_i^\\star\\) values are extreme; observation 17 has a large absolute residual, \\(\\mathbf{y}^\\star = \\text{Heart} \\,\\vert\\, \\text{others}\\).\n\navPlots(fit.both,\n  ellipse = list(levels = 0.68, fill=TRUE, fill.alpha = 0.1),\n  pch = 19,\n  id = list(n = 2),\n  cex.lab = 1.5,\n  main = \"Added-variable plots for Coffee data\")\n\n\n\n\n\n\nFigure 6.11: Added-variable plots for Coffee and Stress in the multiple regression model\n\n\n\n\nThe data ellipses for \\(\\mathbf{x}_i^\\star\\) and \\(\\mathbf{y}^\\star\\) summarize the conditional (or partial) relations of the response to each predictor controlling for all other predictors in each plot. The essential idea is that the data ellipse for \\((\\mathbf{x}_i^\\star, \\mathbf{y}^\\star)\\) has the identical relation to the estimate \\(\\hat{b}_i\\) in a multiple regression as the data ellipse of \\((\\mathbf{x}, \\mathbf{y})\\) has to the slope in a simple regression.\n\n6.4.1 Properties of AV plots\nAV plots are particularly interesting and useful for the following noteworthy properties:\n\nThe slope of the simple regression in the AV plot for variable \\(x_i\\) is identical to the slope \\(b_i\\) for that variable in the full multiple regression model.\nThe residuals in this plot are the same as the residuals using all predictors. This means you can see the degree of fit for observations directly in relation to the various predictors, which is not the case for marginal scatterplots.\nConsequentially, the standard deviation of the (vertical) residuals in the AV plot is the same as \\(s = \\sqrt(MSE)\\) in the full model and the standard error of a coefficient is \\(\\text{SE}(b_i) = s / \\sqrt{\\Sigma (\\mathbf{x}_i^\\star)^2}\\). This is shown by the size of the shadow of the data ellipses on the vertical axis in Figure 6.11.\nThe horizontal positions, \\(\\mathbf{x}_i^\\star\\), of points adjust for all other predictors, and so we can see points at the extreme left and right as unusual in relation to the others. If these points are also badly fitted (large residuals), we can see their influence on the fitted relation in the full model. AV plots thus provide visual displays of (partial) leverage and influence on each of the regression coefficients.\nThe correlation of \\(\\mathbf{x}_i^\\star\\) and \\(\\mathbf{y}^\\star\\) shown by the shape of the data ellipses is the partial correlation between \\(\\mathbf{x}_i\\) and \\(\\mathbf{y}_i\\) with other predictors partialled out.\n\n6.4.2 Marginal - conditional plots\nThe relation of the conditional data ellipses in AV plots to those in marginal plots of the same variables provides further insight into what it means to “control for” other variables. Figure 6.12 shows the same added-variable plots for Heart disease on Stress and Coffee as in Figure 6.11 (with a zoomed-out scaling), but here we also overlay the marginal data ellipses for \\((\\mathbf{x}_i, \\mathbf{y})\\) (centered at the means), and marginal regression lines for Stress and Coffee separately. Drawing arrows connecting the original data points to their positions in the AV plot shows what happens when we condition on or partial out the other variable.\nThese marginal - conditional plots are produced by car::mcPlot() (for one regressor) and car::mcPlots() (for several). The plots for the marginal and conditional relations can be compared separately using the same scales for both, or overlaid as shown here. The points labeled here are only those with large absolute residuals \\(\\mathbf{y}^\\star\\) in the vertical direction.\n\nmcPlots(fit.both, \n  ellipse = list(levels=0.68, fill=TRUE, fill.alpha=0.2),\n  id = list(n=2),\n  pch = c(16, 16),\n  col.marginal = \"red\", col.conditional = \"blue\",\n  col.arrows = \"black\",\n  cex.lab = 1.5)\n\n\n\n\n\n\nFigure 6.12: Marginal \\(+\\) conditional (added-variable) plots for Coffee and Stress in the multiple regression predicting Heart disease. Each panel shows the 68% conditional data ellipse for \\(x_i^\\star, y^\\star\\) residuals (shaded, blue) as well as the marginal 68% data ellipse for the \\((x_i, y)\\) variables, shifted to the origin. Arrows connect the mean-centered marginal points (red) to the residual points (blue).\n\n\n\n\nThe most obvious feature of Figure 6.12 is that Coffee has a negative slope in the conditional AV plot but a positive slope in the marginal plot. This is an example of Simpson’s paradox in a regression context: marginal and conditional relations can have opposite signs. \nLess obvious is the relation between the marginal and AVP ellipses. In 3D, the marginal data ellipse is the shadow of the ellipsoid for \\((\\mathbf{y}, \\mathbf{x}_1, \\mathbf{x}_2)\\) on one of the coordinate planes, while the AV plot is a slice through the ellipsoid where either \\(\\mathbf{x}_1\\) or \\(\\mathbf{x}_2\\) is held constant. Thus, the AVP ellipse must be contained in the marginal ellipse, as we can see in Figure 6.12. If there are only two \\(x\\)s, then the AVP ellipse must touch the marginal ellipse at two points.\nFinally, Figure 6.12 also shows how conditioning on other predictors works for individual observations, where each point of \\((\\mathbf{x}_i^\\star, \\mathbf{y}^\\star)\\) is the image of \\((\\mathbf{x}_i, \\mathbf{y})\\) along the path of the marginal regression. The variability in the response and in the focal predictor are both reduced, leaving only the uncontaminated relation of \\(\\mathbf{y}\\) with \\(\\mathbf{x}_i\\).\nThese plots are similar in spirit to the ARES plot (“Adding REgressors Smoothly”) proposed by Cook & Weisberg (1994), but their idea was an interactive animation, displaying a smooth transition between the fit of a marginal model and the fit of a larger model. They used linear interpolation, \\[\n(\\mathbf{x}_i, \\mathbf{y})_{\\text{interp}} = (\\mathbf{x}_i, \\mathbf{y}) + \\lambda [(\\mathbf{x}_i^\\star, \\mathbf{y}^\\star) - (\\mathbf{x}_i, \\mathbf{y})] \\:\\: ,\n\\] controlled by a slider whose value, \\(\\lambda \\in [0, 1]\\), was the weight given to the smaller marginal model. See this animation for an example using the Duncan data.\n\n6.4.3 Prestige data\nFor a substantive example, let’s return to the model for income, education and women in the Prestige data. The plot in Figure 6.13 shows the strong positive relations of income and education to prestige in the full model, and the negligible relation of percent women. But, in the plot for income, two occupations (physicians and general managers) with high income strongly pull the regression line down from what can be seen in the orientation of the conditional data ellipse.\n\nprestige.mod1 &lt;- lm(prestige ~ education + income + women,\n           data=Prestige)\n\navPlots(prestige.mod1, \n  ellipse = list(levels = 0.68),\n  id = list(n = 2, cex = 1.2),\n  pch = 19,\n  cex.lab = 1.5,\n  main = \"Added-variable plots for prestige\")\n\n\n\n\n\n\nFigure 6.13: Added-variable plot for the quantitative predictors in the Prestige data.\n\n\n\n\nThe influential points for physicians and general managers could just be unusual, or suggest that the relation of income to prestige is nonlinear. A rough test of this is to fit a smoothed curve through the points in the AV plot as shown in Figure 6.14.\n\nop &lt;- par(mar=c(4, 4, 1, 0) + 0.5)\nres &lt;- avPlot(prestige.mod1, \"income\",\n              ellipse = list(levels = 0.68),\n              pch = 19,\n              cex.lab = 1.5)\nsmooth &lt;- loess.smooth(res[,1], res[,2])\nlines(smooth, col = \"red\", lwd = 2.5)\n\n\n\n\n\n\nFigure 6.14: Added-variable plot for income, with a loess smooth.\n\n\n\n\nHowever, this use of AV plots to diagnose nonlinearity or suggest transformations can be misleading (Cook, 1996). Curvature in these plots is an indication of some model deficiency, but unless the predictors are uncorrelated, they cannot determine the form of a possible transformation of the predictors.\n\n6.4.4 Component + Residual plots\nA plot more suited to detecting the need to transform a predictor \\(\\mathbf{x}_i\\) to a form \\(f(\\mathbf{x}_i)\\) to make it’s relationship with the response \\(\\mathbf{y}\\) more nearly linear is the component + residual plot (“C+R plot”, also called partial residual plot, Larsen & McCleary (1972); Cook (1993)). This plot displays the partial residual \\(\\mathbf{e} + \\hat{b}_i \\mathbf{x}_i\\) on the vertical axis against \\(\\mathbf{x}_i\\) on the horizontal, where \\(\\mathbf{e}\\) are the residuals from the full model. A smoothed curve through the points will often suggest the form of the transformation \\(f()\\). The fact that the horizontal axis is \\(\\mathbf{x}_i\\) itself rather than \\(\\mathbf{x}^\\star_i\\) makes it easier to see the functional form.\nThe C+R plot has the same desirable properties as the AV plot: The slope \\(\\hat{b}_i\\) and residuals \\(\\mathbf{e}\\) in this plot are the same as those in the full model.\nC+R plots are produced by car::crPlots() and car::crPlot(). Figure 6.15 shows this just for income in the model prestige.mod1. (These plots for education and women show no strong evidence of curvilinearity.) The dashed blue line is the linear partial fit, \\(\\hat{b}_i \\mathbf{x}_i\\), whose slope \\(\\hat{b}_2 = 0.0013\\) is the same as that for income in prestige.mod1. The solid red curve is the loess smooth through the points. The same points are identified as noteworthy as in AV plot in Figure 6.14.\n\ncrPlot(prestige.mod1, \"income\",\n       smooth = TRUE,\n       order = 2,\n       pch = 19,\n       col.lines = c(\"blue\", \"red\"),\n       id = list(n=2, cex = 1.2),\n       cex.lab = 1.5) \n\n\n\n\n\n\nFigure 6.15: Component + residual plot for income in the model for the quantitative predictors of prestige. The dashed blue line is the partial linear fit for income. The solid red curve is the loess smooth.\n\n\n\n\nThe partial relation between prestige and income is clearly curved, so it would be appropriate to transform income or to include a polynomial (quadratic) term and refit the model. As suggested earlier (Section 3.2.3) it makes sense statistically and substantively to model the effect of income on a log scale, so then the slope for log(income) would measure the increment in prestige for a constant percentage increase in income.\nThe effect of percent women on prestige seen in Figure 6.13 appears very small and essentially linear. However, if we wished to examine this more closely, we could use the C+R plot in Figure 6.16.\n\ncrPlot(prestige.mod1, \"women\",\n       pch = 19,\n       col.lines = c(\"blue\", \"red\"),\n       id = list(n=2, cex = 1.2),\n       cex.lab = 1.5)\n\n\n\n\n\n\nFigure 6.16: Component + residual plot for women in the model for the quantitative predictors of prestige.\n\n\n\n\nThis shows a slight degree of curvature, with modestly larger values in the extremes. If we wished to test this statistically, we could fit a model with a quadratic effect of women, and compare that to the linear-only effect using anova().\n\nprestige.mod2 &lt;- lm(prestige ~ education + income + poly(women,2),\n           data=Prestige)\n\nanova(prestige.mod1, prestige.mod2)\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: prestige ~ education + income + women\n#&gt; Model 2: prestige ~ education + income + poly(women, 2)\n#&gt;   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n#&gt; 1     98 6034                         \n#&gt; 2     97 5907  1       127 2.08   0.15\n\nThis model ignores the type of occupation (“bc”, “wc”, “prof”) as well as any possible interactions of type with other predictors. We examine this next, using effect displays.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#effect-displays",
    "href": "06-linear_models-plots.html#effect-displays",
    "title": "6  Plots for univariate response models",
    "section": "\n6.5 Effect displays",
    "text": "6.5 Effect displays\nFor two predictors it is possible, even if awkward, to display the fitted response surface in a 3D plot or faceted 2D views in what I call a full model plot. For more than two predictors such displays become cumbersome if not impractical, particularly when there are interactions in the model, when some effects are curvilinear, or when the main substantive interest is focused understanding on one or more main effects or interaction terms in the presence of others. The method of effect displays, largely introduced by John Fox (Fox, 1987, 2003; Fox & Weisberg, 2018b) is a generally useful solution to this problem. 2 These plots are nearly always easier to understand than tables of coefficients.\nThe idea of effect displays is quite simple, but very general and handles models of arbitrary complexity. Imagine that in a model we have a particular subset of predictors (focal predictors) whose effects on the response variable we wish to visualize. The essence of an effect display is that we calculate the predicted values (and standard errors) of the response for the model term(s) involving the focal predictors (and all low-order relatives, e.g, main effects that are marginal to an interaction) as those predictors are allowed to vary over a grid covering their range.\nFor a given plot, the other, non-focal variables are “controlled” by being fixed at typical values. For example, a quantitative predictor could be fixed at it’s mean, median or some representative value. A factor could be fixed at equal proportions of its levels or its proportions in the data. The result, when plotted, shows the predicted effects of the focal variables, either with multiple lines or in a faceted display, but with all the other variables controlled, adjusted for or averaged over. For interaction effects all low-order relatives are typically included in the fitted values for the term being graphed.\nIn practical terms, a scoring matrix \\(\\mathbf{X}^\\bullet\\) is defined by the focal variables varied over their ranges and the other variables held fixed. The fitted values for a model term are then calculated as \\(\\widehat{\\mathbf{y}}^\\bullet = \\mathbf{X}^\\bullet \\; \\widehat{\\mathbf{b}}\\) using the equivalent of:\n\npredict(model, newdata = X, se.fit = TRUE) \n\nwhich also calculates the standard errors as the square roots of \\(\\mathrm{diag}\\, (\\mathbf{X}^\\bullet \\, \\widehat{\\mathbf{\\mathsf{Var}}} (\\mathbf{b}) \\, \\mathbf{X}^{\\bullet\\mathsf{T}} )\\) where \\(\\widehat{\\mathbf{\\mathsf{Var}}} (\\mathbf{b})\\) is the estimated covariance matrix of the coefficients. Consequently, predictor effect values can be obtained for any modelling function that has predict() and vcov() methods. To date, effect displays are available for over 100 different model types in various packages.\nTo illustrate the mechanics for the effect of education in the prestige.mod1 model, construct a data frame varying education, but fixing income and women at their means:\n\nX &lt;- expand.grid(\n      education = seq(8, 16, 2),\n      income = mean(Prestige$income),\n      women = mean(Prestige$women)) |&gt; \n  print(digits = 3)\n#&gt;   education income women\n#&gt; 1         8   6798    29\n#&gt; 2        10   6798    29\n#&gt; 3        12   6798    29\n#&gt; 4        14   6798    29\n#&gt; 5        16   6798    29\n\npredict() then gives the fitted values for a simple effect plot of prestige against education. predict.lm() returns list, so it is necessary to massage this to a data frame for graphing.\n\npred &lt;- predict(prestige.mod1, newdata=X, se.fit = TRUE)\ncbind(X, fit = pred$fit, se = pred$se.fit) |&gt; \n  print(digits=3)\n#&gt;   education income women  fit    se\n#&gt; 1         8   6798    29 35.4 1.318\n#&gt; 2        10   6798    29 43.7 0.828\n#&gt; 3        12   6798    29 52.1 0.919\n#&gt; 4        14   6798    29 60.5 1.487\n#&gt; 5        16   6798    29 68.9 2.188\n\nAs Fox & Weisberg (2018b) note, effect displays can be combined with partial residuals to visualize both fit and potential lack of fit simultaneously, by plotting residuals from a model around 2D slices of the fitted response surface. This adds the benefits of C+R plots, in that we can see the impact of unmodeled curvilinearity and interactions in addition to those of predictor effect displays.\nThere are several implementations of effect displays in R, whose details, terminology and ease of use vary. Among these,  ggeffects (Lüdecke, 2024) calculates adjusted predicted values under several methods for conditioning.  marginaleffects (Arel-Bundock et al., Forthcoming) is similar and also provides estimation of marginal slopes, contrasts, odds ratios, etc. Both have plot() methods based on ggplot2. My favorite is the  effects (Fox et al., 2022) package, which alone provides partial residuals, and is somewhat easier to use, though it uses lattice graphics. See the vignette Predictor Effects Graphics Gallery for details of the computations for effect displays.\nThe main functions for computing fitted effects are predictorEffect() (for one predictor) and predictorEffects() (for one or more). For a model mod with formula y ~ x1 + x2 + x3 + x1:x2, the call to predictorEffects(mod, ~ x1) recognizes that an interaction is present and calculates the fitted values for combinations of x1 and x2, holding x3 fixed at its average value. This returns an object of class \"eff\" which can be graphed using the plot.eff() method.\nThe effect displays for several predictors can be plotted together, as with avplots() (Figure 6.13) by including them in the plot formula, e.g., predictorEffects(mod, ~ x1 + x3). Another function, allEffects() calculates the effects for each high-order term in the model, so allEffects(mod) |&gt; plot() is handy for getting a visual overview of a fitted model.\n\n6.5.1 Prestige data\nTo illustrate effect plots, I consider a more complex model, allowing a quadratic effect of women, representing income on a \\(\\log_{10}\\) scale, and allowing this to interact with type of occupation. Anova() provides the Type II tests of each of the model terms.\n\nprestige.mod3 &lt;- lm(prestige ~ education + poly(women,2) +\n                       log10(income)*type, data=Prestige)\n\n# test model terms\nAnova(prestige.mod3)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: prestige\n#&gt;                    Sum Sq Df F value  Pr(&gt;F)    \n#&gt; education             994  1   25.87 2.0e-06 ***\n#&gt; poly(women, 2)        414  2    5.38 0.00620 ** \n#&gt; log10(income)        1523  1   39.63 1.1e-08 ***\n#&gt; type                  589  2    7.66 0.00085 ***\n#&gt; log10(income):type    221  2    2.88 0.06133 .  \n#&gt; Residuals            3420 89                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe fitted coefficients, standard errors and \\(t\\)-tests from coeftest() are shown below. The coefficient for education means that an increase of one year of education, holding other predictors fixed, gives an expected increase of 2.96 in prestige. The other coefficients are more difficult to understand. For example, the effect of women is represented by two coefficients for the linear and quadratic components of poly(women, 2). The interpretation of coefficients of terms involving type depend on the contrasts used. Here, with the default treatment contrasts, they represent comparisons with type = \"bc\" as the reference level. It is not obvious how to understand the interaction effects like log10(income):typeprof.\n\n\n\n\n\n\n\n\n\n\nlmtest::coeftest(prestige.mod3)\n#&gt; \n#&gt; t test of coefficients:\n#&gt; \n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            -137.500     23.522   -5.85  8.2e-08 ***\n#&gt; education                 2.959      0.582    5.09  2.0e-06 ***\n#&gt; poly(women, 2)1          28.339     10.190    2.78   0.0066 ** \n#&gt; poly(women, 2)2          12.566      7.095    1.77   0.0800 .  \n#&gt; log10(income)            40.326      6.714    6.01  4.1e-08 ***\n#&gt; typewc                    0.969     39.495    0.02   0.9805    \n#&gt; typeprof                 74.276     30.736    2.42   0.0177 *  \n#&gt; log10(income):typewc     -1.073     10.638   -0.10   0.9199    \n#&gt; log10(income):typeprof  -17.725      7.947   -2.23   0.0282 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt is easiest to produce effect displays for all terms in the model using allEffects(), accepting all defaults. This gives (Figure 6.17) effect plots for the main effects of education and income and the interaction of income with type, with the non-focal variables held fixed. Each plot shows the fitted regression relation and a default 95% pointwise confidence band using the standard errors. Rug plots at the bottom show the locations of observations for the horizontal focal variable, which is useful when the observations are not otherwise plotted.\n\nallEffects(prestige.mod3) |&gt;\n  plot()\n\n\n\n\n\n\nFigure 6.17: Predictor effect plot for all terms in the model with 95% confidence bands.\n\n\n\n\nThe effect for women, holding education, income and type constant looks to be quite strong and curved upwards. But note that these plots use different vertical scales for prestige in each plot and the range in the plot for women is much smaller than in the others. The interaction is graphed showing separate curves for the three levels of type.\nFor a more detailed look, it is useful to make separate plots for the predictors in the model, which allows customizing options for calculation and display. Partial residuals for the observations are computed by using residuals = TRUE in the call to predictorEffects(). The slope of the fitted line (in blue) is exactly coefficient for education in the full model. As with C+R plots, a smooth loess curve (in red) gives a visual assessment of linearity for a given predictor. A wide variety of graphing options are available in the call to plot(). Figure 6.18 shows the effect display for education with partial residuals and point identification of those points with the largest Mahalanobis distances from the centroid.\n\n\nlattice::trellis.par.set(par.xlab.text=list(cex=1.5),\n                         par.ylab.text=list(cex=1.5))\n\npredictorEffects(prestige.mod3, ~ education,\n                 residuals = TRUE) |&gt;\n  plot(partial.residuals = list(pch = 16, col=\"blue\"),\n       id=list(n=4, col=\"black\")) \n\n\n\n\n\n\nFigure 6.18: Predictor effect plot for education displaying partial residuals. The blue line shows the slice of the fitted regression surface where other variables are held fixed. The red curve shows a loess smooth of the partial residuals.\n\n\n\n\nThe effect plot for women in this model is shown in Figure 6.19. This uses the same vertical scale as in Figure 6.18, showing a more modest effect of percent women.\n\npredictorEffects(prestige.mod3, ~women,\n                 residuals = TRUE) |&gt;\n  plot(partial.residuals = list(pch = 16, col=\"blue\", cex=0.8),\n       id=list(n=4, col=\"black\"))\n\n\n\n\n\n\nFigure 6.19: Predictor effect plot for women with partial residuals\n\n\n\n\n\nBecause of the interaction with type, the fitted effects for income are calculated for the three types of occupation. It is easiest to compare these in the a single plot (using multiline = TRUE), rather than in separate panels as in Figure 6.17. Income is represented as log10(income) in the model prestige.mod3, and it is also easier to understand the interaction by plotting income on a log scale, using the axes argument to specify a transformation of the \\(x\\) axis. I use 68% confidence bands here to make the differences among type more apparent.\n\npredictorEffects(prestige.mod3, ~ income,\n                 confidence.level = 0.68) |&gt;\n  plot(lines=list(multiline=TRUE, lwd=3),\n       confint=list(style=\"bands\"),\n       axes=list(\n          x=list(income=list(transform=list(trans=log, inverse=exp)))),\n       key.args = list(x=.7, y=.35)) \n\n\n\n\n\n\nFigure 6.20: Predictor effect plot for income, plotted on a log scale.\n\n\n\n\nFigure 6.20 provides a clear interpretation of the interaction, represented by the coefficients shown above for log10(income):typewc and log10(income):typeprof in the model. Averaging over three occupation types, prestige increases linearly with log income with a coefficient of 40.33. This means that increasing income by 10% (say) gives an increase of \\(40.33 / 10 = 4.033\\) in prestige. The slope for professional workers is less steep: the coefficient for log10(income):typeprof is -17.725. For these workers compared with blue collar jobs, prestige increases 1.77 less with a 10% increase in income. The difference in slopes for blue collar and white collar jobs is negligible.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#sec-leverage",
    "href": "06-linear_models-plots.html#sec-leverage",
    "title": "6  Plots for univariate response models",
    "section": "\n6.6 Outliers, leverage and influence",
    "text": "6.6 Outliers, leverage and influence\nIn small to moderate samples, “unusual” observations can have dramatic effects on a fitted regression model, as we saw in the analysis of Davis’s data on reported and measured weight (Section 2.1.2) where one erroneous observations hugely altered the fitted line. As well, it turns out that two observations in Duncan’s data are unusual enough that removing them alters his conclusion that income and education have nearly equal effects on occupational prestige.\nAn observation can be unusual in three archetypal ways, with different consequences:\n\nUnusual in the response \\(y\\), but typical in the predictor(s), \\(\\mathbf{x}\\) — a badly fitted case with a large absolute residual, but with \\(x\\) not far from the mean, as in Figure 2.4. This case does not do much harm to the fitted model.\nUnusual in the predictor(s) \\(\\mathbf{x}\\), but typical in \\(y\\) — an otherwise well-fitted point. This case also does litle harm, and in fact can be considered to improve precision, a “good leverage” point.\nUnusual in both \\(\\mathbf{x}\\) and \\(y\\) — This is the case, a “bad leverage” point, revealed in the analysis of Davis’s data, Figure 2.3, where the one erroneous point for women was highly influential, pulling the regression line towards it and affecting the estimated coefficient as well as all the fitted values. In addition, subsets of observations can be jointly influential, in that their effects combine, or can mask each other’s influence.\n\nInfluential cases are the ones that matter most. As suggested above, to be influential an observation must be unusual in both \\(\\mathbf{x}\\) and \\(y\\), and affects the estimated coefficients, thereby also altering the predicted values for all observations. A heuristic formula capturing the relations among leverage, “outlyingness” on \\(y\\) and influence is\n\\[\n\\text{Influence}_{\\text{coefficients}} \\;=\\; X_\\text{leverage} \\;\\times\\; Y_\\text{residual}\n\\] As described below, leverage is proportional to the squared distance \\((x_i - \\bar{x})^2\\) of an observation \\(x_i\\) from its mean in simple regression and to the squared Mahalanobis distance in the general case. The \\(Y_\\text{residual}\\) is best measured by a studentized residual, obtained by omitting each case \\(i\\) in turn and calculating its residual from the coefficients obtained from the remaining cases.\n\n6.6.1 The leverage-influence quartet\nThese ideas can be illustrated in the “leverage-influence quartet” by considering a standard simple linear regression for a sample and then adding one additional point reflecting the three situations described above. Below, I generate a sample of \\(N = 15\\) points with \\(x\\) uniformly distributed between (40, 60) and \\(y \\sim 10 + 0.75 x + \\mathcal{N}(0, 1.25^2)\\), duplicated four times.\n\nlibrary(tidyverse)\nlibrary(car)\nset.seed(42)\nN &lt;- 15\ncase_labels &lt;- paste(1:4, c(\"OK\", \"Outlier\", \"Leverage\", \"Influence\"))\nlevdemo &lt;- tibble(\n  case = rep(case_labels, \n             each = N),\n  x = rep(round(40 + 20 * runif(N), 1), 4),\n  y = rep(round(10 + .75 * x + rnorm(N, 0, 1.25), 4)),\n  id = \" \"\n)\n\nmod &lt;- lm(y ~ x, data=levdemo)\ncoef(mod)\n#&gt; (Intercept)           x \n#&gt;      13.332       0.697\n\nThe additional points, one for each situation are set to the values below.\n\n\nOutlier: (52, 60) a low leverage point, but an outlier (O) with a large residual\n\nLeverage: (75, 65) a “good” high leverage point (L) that fits well with the regression line\n\nInfluence: (70, 40) a “bad” high leverage point (OL) with a large residual.\n\n\nextra &lt;- tibble(\n  case = case_labels,\n  x  = c(65, 52, 75, 70),\n  y  = c(NA, 65, 65, 40),\n  id = c(\"  \", \"O\", \"L\", \"OL\")\n)\n\n#' Join these to the data\nboth &lt;- bind_rows(levdemo, extra) |&gt;\n  mutate(case = factor(case))\n\nWe can plot these four situations with ggplot2 in panels faceted by case as shown below. The standard version of this plot shows the regression line for the original data and that for the ammended data with the additional point. Note that we use the levdemo dataset in geom_smooth() for the regression line with the original data, but specify data = both for that with the additional point.\n\nggplot(levdemo, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(data = both, \n              method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"red\", linewidth = 1.3, linetype = 1) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"blue\", linewidth = 1, linetype = \"longdash\" ) +\n  stat_ellipse(data = both, level = 0.5, color=\"blue\", type=\"norm\", linewidth = 1.4) +\n  geom_point(data=extra, color = \"red\", size = 4) +\n  geom_text(data=extra, aes(label = id), nudge_x = -2, size = 5) +\n  facet_wrap(~case, labeller = label_both) +\n  theme_bw(base_size = 14)\n\n\n\n\n\n\nFigure 6.21: Leverage influence quartet with data 50% ellipses. Case (1) original data; (2) adding one low-leverage outlier, “O”; (3) adding one “good” leverage point, “L”; (4) adding one “bad” leverage point, “OL”. The dashed blue line is the fitted line for the original data, while the solid red line reflects the additional point. The data ellipses show the effect of the additional point on precision.\n\n\n\n\nThe standard version of this graph shows only the fitted regression lines in each panel. As can be seen, the fitted line doesn’t change very much in panels (2) and (3); only the bad leverage point, “OL” in panel (4) is harmful. Adding data ellipses to each panel immediately makes it clear that there is another part to this story— the effect of the unusual point on precision (standard errors) of our estimates of the coefficients.\nNow, we see directly that there is a big difference in impact between the low-leverage outlier [panel (2)] and the high-leverage, small-residual case [panel (3)], even though their effect on coefficient estimates is negligible. In panel (2), the single outlier inflates the estimate of residual variance (the size of the vertical slice of the data ellipse at \\(\\bar{x}\\)), while in panel (3) this is decreased.\nTo allow direct comparison and make the added value of the data ellipse more apparent, we overlay the data ellipses from Figure 6.21 in a single graph, shown in Figure 6.22. Here, we can also see why the high-leverage point “L” (added in panel (c) of Figure 6.21) is called a “good leverage” point. By increasing the standard deviation of \\(x\\), it makes the data ellipse somewhat more elongated, giving increased precision of our estimates of \\(\\mathbf{\\beta}\\).\n\nCodecolors &lt;- c(\"black\", \"blue\", \"darkgreen\", \"red\")\nwith(both,\n     {dataEllipse(x, y, groups = case, \n          levels = 0.68,\n          plot.points = FALSE, add = FALSE,\n          center.pch = \"+\",\n          col = colors,\n          fill = TRUE, fill.alpha = 0.1)\n     })\n\ncase1 &lt;- both |&gt; filter(case == \"1 OK\")\npoints(case1[, c(\"x\", \"y\")], cex=1)\n\npoints(extra[, c(\"x\", \"y\")], \n       col = colors,\n       pch = 16, cex = 2)\n\ntext(extra[, c(\"x\", \"y\")],\n     labels = extra$id,\n     col = colors, pos = 2, offset = 0.5)\n\n\n\n\n\n\nFigure 6.22: Data ellipses in the Leverage-influence quartet. This graph overlays the data ellipses and additional points from the four panels of Figure 6.22. It can be seen that only the OL point affects the slope, while the O and L points affect precision of the estimates in opposite directions.\n\n\n\n\n\n6.6.1.1 Measuring leverage\nLeverage is thus an index of the potential impact of an observation on the model due to its’ atypical value in the X space of the predictor(s). It is commonly measured by the “hat” value, \\(h_i\\), so called because it puts the hat \\(\\hat{(\\bullet)}\\) on \\(\\mathbf{y}\\), i.e., the vector of fitted values can be expressed as\n\\[\\begin{aligned}\n\\hat{\\mathbf{y}} & =  \\mathbf{H} \\mathbf{y} \\\\\n                 & =  [\\mathbf{X} (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}] \\; \\mathbf{y} \\; ,\n\\end{aligned}\\]\nwhere \\(h_i \\equiv h_{ii}\\) are the diagonal elements of the Hat matrix \\(\\mathbf{H}\\). In simple regression, hat values are proportional to the squared distance of the observation \\(x_i\\) from the mean, \\(h_i \\propto (x_i - \\bar{x})^2\\), \\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\Sigma_i (x_i - \\bar{x})^2} \\; ,\n\\] and range from \\(1/n\\) to 1, with an average value \\(\\bar{h} = 2/n\\). Consequently, observations with \\(h_i\\) greater than \\(2 \\bar{h}\\) or \\(3 \\bar{h}\\) are commonly considered to be of high leverage.\nWith \\(p \\ge 2\\) predictors, it is demonstrated below that \\(h_i \\propto D^2 (\\mathbf{x} - \\bar{\\mathbf{x}})\\), the squared distance of \\(\\mathbf{x}\\) from the centroid \\(\\bar{\\mathbf{x}}\\)3.\nThe analogous formula is\n\\[\nh_i = \\frac{1}{n} + \\frac{1}{n-1} D^2 (\\mathbf{x} - \\bar{\\mathbf{x}}) \\; ,\n\\]\nwhere \\(D^2 (\\mathbf{x} - \\bar{\\mathbf{x}}) = (\\mathbf{x} - \\bar{\\mathbf{x}})^\\mathsf{T} \\mathbf{S}_X^{-1} (\\mathbf{x} - \\bar{\\mathbf{x}})\\). From Section 3.2, it follows that contours of constant leverage correspond to data ellipses or ellipsoids of the predictors in \\(\\mathbf{x}\\), whose boundaries, assuming normality, correspond to quantiles of the \\(\\chi^2_p\\) distribution\nExample:\nTo illustrate, I generate \\(N = 100\\) points from a bivariate normal distribution with means \\(\\mu = (30, 30)\\), variances = 10, and a correlation \\(\\rho = 0.7\\) and add two noteworthy points that show an apparently paradoxical result.\n\nset.seed(421)\nN &lt;- 100\nr &lt;- 0.7\nmu &lt;- c(30, 30)\ncov &lt;- matrix(c(10,   10*r,\n                10*r, 10), ncol=2)\n\nX &lt;- MASS::mvrnorm(N, mu, cov) |&gt; as.data.frame()\ncolnames(X) &lt;- c(\"x1\", \"x2\")\n\n# add 2 points\nX &lt;- rbind(X,\n           data.frame(x1 = c(28, 38),\n                      x2 = c(42, 35)))\n\nThe Mahalanobis squared distances of these points can be calculated using heplots::Mahalanobis(), and their corresponding hatvalues found using hatvalues() for any linear model using both x1 and x2.\n\nX &lt;- X |&gt;\n  mutate(Dsq = heplots::Mahalanobis(X)) |&gt;\n  mutate(y = 2*x1 + 3*x2 + rnorm(nrow(X), 0, 5),\n         hat = hatvalues(lm(y ~ x1 + x2))) \n\nPlotting x1 and x2 with data ellipses shows the relation of leverage to squared distance from the mean. The blue point looks to be farther from the mean, but the red point is actually very much further by Mahalanobis squared distance, which takes the correlation into account; it thus has much greater leverage.\n\npar(mar = c(4, 4, 1, 1) + 0.1)\ndataEllipse(X$x1, X$x2, \n            levels = c(0.40, 0.68, 0.95),\n            fill = TRUE, fill.alpha = 0.05,\n            col = \"darkgreen\",\n            xlab = \"X1\", ylab = \"X2\")\npoints(X[1:nrow(X) &gt; N, 1:2], pch = 16, col=c(\"red\", \"blue\"), cex = 2)\nX |&gt; slice_tail(n = 2) |&gt;      # last two rows\n  points(pch = 16, col=c(\"red\", \"blue\"), cex = 2)\n\n\n\n\n\n\nFigure 6.23: Data ellipses for a bivariate normal sample with correlation 0.7, and two additional noteworthy points. The blue point looks to be farther from the mean, but the red point is actually more than 5 times further by Mahalanobis squared distance, and thus has much greater leverage.\n\n\n\n\nThe fact that hatvalues are proportional to leverage can be seen by plotting one against the other. I highlight the two noteworthy points in their colors from Figure 6.23 to illustrate how much greater leverage the red point has compared to the blue point.\n\nplot(hat ~ Dsq, data = X,\n     cex = c(rep(1, N), rep(2, 2)), \n     col = c(rep(\"black\", N), \"red\", \"blue\"),\n     pch = 16,\n     ylab = \"Hatvalue\",\n     xlab = \"Mahalanobis Dsq\")\n\n\n\n\n\n\nFigure 6.24: Hat values are proportional to squared Mahalanobis distances from the mean.\n\n\n\n\nLook back at these two points in Figure 6.23. Can you guess how much further the red point is from the mean than the blue point? You might be surprised that its’ \\(D^2\\) and leverage are about five times as great!\n\nX |&gt; slice_tail(n=2)\n#&gt;   x1 x2   Dsq   y    hat\n#&gt; 1 28 42 25.65 179 0.2638\n#&gt; 2 38 35  4.95 175 0.0588\n\n\n6.6.1.2 Outliers: Measuring residuals\nFrom the discussion in Section 6.6, outliers for the response \\(y\\) are those observations for which the residual \\(e_i = y_i - \\hat{y}_i\\) are unusually large in magnitude. However, as demonstrated in Figure 6.21, a high-leverage point will pull the fitted line towards it, reducing its’ residual and thus making them look less unusual.\nThe standard approach (Cook & Weisberg, 1982; Hoaglin & Welsch, 1978) is to consider a deleted residual \\(e_{(-i)}\\), conceptually as that obtained by re-fitting the model with observation \\(i\\) omitted and obtaining the fitted value \\(\\hat{y}_{(-i)}\\) from the remaining \\(n-1\\) observations, \\[\ne_{(-i)} = y_i - \\hat{y}_{(-i)} \\; .\n\\] The (externally) studentized residual is then obtained by dividing \\(e_{(-i)}\\) by it’s estimated standard error, giving \\[\ne^\\star_{(-i)} = \\frac{e_{(-i)}}{\\text{sd}(e_{(-i)})} = \\frac{e_i}{\\sqrt{\\text{MSE}_{(-i)}\\; (1 - h_i)}} \\; .\n\\]\nThis is just the ordinary residual \\(e_i\\) divided by a factor that increases with the residual variance but decreases with leverage. It can be shown that these studentized residuals follow a \\(t\\) distribution with \\(n - p -2\\) degrees of freedom, so a value \\(|e^\\star_{(-i)}| &gt; 2\\) can be considered large enough to pay attention to.\nIn practice for classical linear models, it is unnecessary to actually re-fit the model \\(n\\) times. Velleman & Welsh (1981) show that all these leave-one-out quantities can be calculated from the model fitted to the full data set and the hat (projection) matrix \\(\\mathbf{H} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\) from which \\(\\widehat{\\mathbf{b}} = \\mathbf{H} \\mathbf{y}\\).\n\n6.6.1.3 Measuring influence\nAs described at the start of this section, the actual influence of a given case depends multiplicatively on its’ leverage and residual. But how can we measure it?\nThe essential idea introduced above, is to delete the observations one at a time, each time refitting the regression model on the remaining \\(n–1\\) observations. Then, for observation \\(i\\) compare the results using all \\(n\\) observations to those with the \\(i^{th}\\) observation deleted to see how much influence the observation has on the analysis.\nThe simplest such measure, called DFFITS, compares the predicted value for case \\(i\\) with what would be obtained when that observation is excluded.\n\\[\\begin{aligned}\n\\text{DFFITS}_i & = & \\frac{\\hat{y}_i - \\hat{y}_{(-i)}}{\\sqrt{\\text{MSE}_{(-i)}\\; h_i}} \\\\\n   & = & e^\\star_{(-i)} \\times \\sqrt{\\frac{h_i}{1-h_i}} \\;\\; .\n\\end{aligned}\\]\nThe first equation gives the signed difference in fitted values in units of the standard deviation of that difference weighted by leverage; the second version (Belsley et al., 1980) represents that as a product of residual and leverage. A rule of thumb is that an observation is deemed to be influential if \\(| \\text{DFFITS}_i | &gt; 2 \\sqrt{(p+1) / n}\\).\nInfluence can also be assessed in terms of the change in the estimated coefficients \\(\\mathbf{b} = \\widehat{\\mathbf{\\beta}}\\) versus their values \\(\\mathbf{b}_{(-i)}\\) when case \\(i\\) is removed. Cook’s distance, \\(D_i\\), summarizes the size of the difference as a weighted sum of squares of the differences \\(\\mathbf{d} =\\mathbf{b} - \\mathbf{b}_{(-i)}\\) (Cook, 1977).\n\\[\nD_i = \\mathbf{d}^\\mathsf{T}\\, (\\mathbf{X}^\\mathsf{T}\\mathbf{X}) \\,\\mathbf{d} / (p+1) \\hat{\\sigma}^2\n\\] This can be re-expressed in terms of the components of residual and leverage\n\\[\nD_i = \\frac{e^{\\star 2}_{(-i)}}{p+1} \\times \\frac{h_i}{(1- h_i)}\n\\]\nCook’s distance is in the metric of an \\(F\\) distribution with \\(p\\) and \\(n − p\\) degrees of freedom, so values \\(D_i &gt; 4/n\\) are considered large.\n\n6.6.2 Influence plots\nThe most common plot to detect influence is a bubble plot of the studentized residuals versus hat values, with the size (area) of the plotting symbol proportional to Cook’s \\(D\\). These plots are constructed using car::influencePlot() which also fills the bubble symbols with color whose opacity is proportional to Cook’s \\(D\\).\nThis is shown in Figure 6.25 for the demonstration dataset constructed in Section 6.6.1. In this plot, notable cutoffs for hatvalues at \\(2 \\bar{h}\\) and \\(3 \\bar{h}\\) are shown by dashed vertical lines and horizontal cutoffs for studentized residuals are shown at values of \\(\\pm 2\\).\nThe demonstration data of Section 6.6.1 has four copies of the same \\((x, y)\\) data, three of which have an unusual observation. The influence plot in Figure 6.25 subsets the data to give the \\(19 = 15 + 4\\) unique observations, including the three unusual cases. As can be seen, the high “Leverage” point has has less influence than the point labeled “Influence”, which has moderate leverage but a large absolute residual.\n\nSee the codeonce &lt;- both[c(1:16, 62, 63, 64),]      # unique observations\nonce.mod &lt;- lm(y ~ x, data=once)\ninf &lt;- influencePlot(once.mod, \n                     id = list(cex = 0.01),\n                     fill.alpha = 0.5,\n                     cex.lab = 1.5)\n# custom labels\nunusual &lt;- bind_cols(once[17:19,], inf) |&gt; \n  print(digits=3)\n#&gt; # A tibble: 3 × 7\n#&gt;   case            x     y id    StudRes    Hat CookD\n#&gt; * &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2 Outlier      52    65 O        3.11 0.0591 0.201\n#&gt; 2 3 Leverage     75    65 L        1.52 0.422  0.784\n#&gt; 3 4 Influence    70    40 OL      -4.93 0.262  1.82\nwith(unusual, {\n  casetype &lt;- gsub(\"\\\\d \", \"\", case)\n  text(Hat, StudRes, label = casetype,\n       pos = c(4, 2, 3), cex=1.5)\n})\n\n\n\n\n\n\nFigure 6.25: Influence plot for the demonstration data. The areas of the bubble symbols are proportional to Cook’s \\(D\\). The impact of the three unusual points on Cook’s \\(D\\) is clearly seen.\n\n\n\n\n\n6.6.3 Duncan data\nLet’s return to the Duncan data used as an example in Section 6.1.1 where a few points stood out as unusual in the basic diagnostic plots (Figure 6.2). The influence plot in Figure 6.26 helps to make sense of these noteworthy observations. The default method for identifying points in influencePlot() labels points with any of large studentized residuals, hat-values or Cook’s distances.\n\ninf &lt;- influencePlot(duncan.mod, id = list(n=3),\n                     cex.lab = 1.5)\n\n\n\n\n\n\nFigure 6.26: Influence plot for the model predicting occupational prestige in Duncan’s data. Cases with large studentized residuals, hat-values or Cook’s distances are labeled.\n\n\n\n\ninfluencePlot() returns (invisibly) the influence diagnostics for the cases identified in the plot. It is often useful to look at data values for these cases to understand why each of these was flagged.\n\nmerge(Duncan, inf, by=\"row.names\", all.x = FALSE) |&gt; \n  arrange(desc(CookD)) |&gt; \n  print(digits=3)\n#&gt;     Row.names type income education prestige StudRes    Hat  CookD\n#&gt; 1    minister prof     21        84       87   3.135 0.1731 0.5664\n#&gt; 2   conductor   wc     76        34       38  -1.704 0.1945 0.2236\n#&gt; 3    reporter   wc     67        87       52  -2.397 0.0544 0.0990\n#&gt; 4 RR.engineer   bc     81        28       67   0.809 0.2691 0.0810\n#&gt; 5  contractor prof     53        45       76   2.044 0.0433 0.0585\n\n\nminister has by far the largest influence, because it has an extremely positive residual and a large hat value. Looking at the data, we see that ministers have very low income, so their prestige is under-predicted. The large hat value reflects the fact that ministers have low income combined with very high education.\nconductor has the next largest Cook’s \\(D\\). It has a large hat value because its combination of relatively high income and low education is unusual in the data.\nAmong the others, reporter has a relatively large negative residual—its prestige is far less than the model predicts—but its low leverage make it not highly influential. railroad engineer has an extremely large hat value because its income is very high in relation to education. But this case is well-predicted and has a small residual, so its leverage is not large.\n\n6.6.4 Influence in added-variable plots\nThe properties of added-variable plots discussed in Section 6.4 make them also useful for understanding why cases are influential because they control for other predictors in each plot, and therefore show the partial contributions of each observation to hat values and residuals. As a consequence, we can see directly the how individual cases become individually or jointly influential.\nThe Duncan data provides a particularly instructive example of this. Figure 6.27 shows the AV plots for both income and education in the model duncan.mod, with some annotations added. I want to focus here on the joint influence of the occupations minister and conductor which were seen to be the most influential in Figure 6.26. The green vertical lines show their residuals in each panel and the red lines show the regressions when these two observations are deleted.\nThe basic AV plots are produced using the call to avPlots() below. To avoid clutter, I use the argument id = list(method = \"mahal\", n=3) so that only the three points with the greatest Mahalanobis distances from the centroid in each plot are labeled. These are the cases with the largest leverage seen in Figure 6.26.\n\navPlots(duncan.mod,\n  ellipse = list(levels = 0.68, fill = TRUE, fill.alpha = 0.1),\n  id = list(method = \"mahal\", n=3),\n  pch = 16, cex = 0.9,\n  cex.lab = 1.5)\n\n\n\n\n\n\n\n\nFigure 6.27: Added variable plots for the Duncan model, highlighting the impact of the observations for minister and conductor in each plot. The green lines show the residuals for these observations. The red line in each panel shows the regression line omitting these observations.\n\n\n\n\nThe two cases—minister and conductor—are the most highly influential, but as we can see in Figure 6.27 their influence combines because they are at opposite sides of the horizontal axis and their residuals are of opposite signs. They act together to decrease the slope for income and increase that for education.\n\nCode for income AV plotres &lt;- avPlot(duncan.mod, \"income\",\n              ellipse = list(levels = 0.68),\n              id = list(method = \"mahal\", n=3),\n              pch = 16,\n              cex.lab = 1.5) |&gt;\n  as.data.frame()\nfit &lt;- lm(prestige ~ income, data = res)\ninfo &lt;- cbind(res, fitted = fitted(fit), \n             resids = residuals(fit),\n             hat = hatvalues(fit),\n             cookd = cooks.distance(fit))\n\n# noteworthy points in this plot\nbig &lt;- which(info$cookd &gt; .20)\nwith(info, {\n  arrows(income[big], fitted[big], income[big], prestige[big], \n         angle = 12, length = .18, lwd = 2, col = \"darkgreen\")\n  })\n\n# line w/o the unusual points\nduncan.mod2 &lt;- update(duncan.mod, subset = -c(6, 16))\nbs &lt;- coef(duncan.mod2)[\"income\"]\nabline(a=0, b=bs, col = \"red\", lwd=2)\n\n\nDuncan’s hypothesis that the slopes for income and education were equal thus fails when these two observations are deleted. The slope for income then becomes 2.6 times that of education.\n\nduncan.mod2 &lt;- update(duncan.mod, subset = -c(6, 16))\ncoef(duncan.mod2)\n#&gt; (Intercept)      income   education \n#&gt;      -6.409       0.867       0.332\n\nPackage summary\n\n29 packages used here: bayestestR, car, carData, correlation, datawizard, dplyr, easystats, effects, effectsize, forcats, ggeffects, ggplot2, ggstats, insight, knitr, lubridate, marginaleffects, modelbased, modelsummary, parameters, performance, purrr, readr, report, see, stringr, tibble, tidyr, tidyverse\n\n\n\n\n\n\nArel-Bundock, V. (2024). Modelsummary: Summary tables and plots for statistical models and data: Beautiful, customizable, and publication-ready. https://modelsummary.com\n\n\nArel-Bundock, V., Greifer, N., & Heiss, A. (Forthcoming). How to interpret statistical models using marginaleffects in R and Python. Journal of Statistical Software. https://marginaleffects.com\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley; Sons.\n\n\nCook, R. D. (1977). Detection of influential observation in linear regression. Technometrics, 19(1), 15–18. http://links.jstor.org/sici?sici=0040-1706%28197702%2919%3A1%3C15%3ADOIOIL%3E2.0.CO%3B2-8\n\n\nCook, R. D. (1993). Exploring partial residual plots. Technometrics, 35(4), 351–362.\n\n\nCook, R. D. (1996). Added-variable plots and curvature in linear regression. Technometrics, 38(3), 275–278. https://doi.org/10.1080/00401706.1996.10484507\n\n\nCook, R. D., & Weisberg, S. (1982). Residuals and influence in regression. Chapman; Hall.\n\n\nCook, R. D., & Weisberg, S. (1994). ARES plots for generalized linear models. Computational Statistics & Data Analysis, 17(3), 303–315. https://doi.org/10.1016/0167-9473(92)00075-3\n\n\nDuncan, O. D. (1961). A socioeconomic index for all occupations. In Jr. A. J. Reiss, P. K. H. O. D. Duncan, & C. C. North (Eds.), Occupations and social status. The Free Press.\n\n\nFisher, R. A. (1925). Statistical methods for research workers (6th ed.). Oliver & Boyd.\n\n\nFox, J. (1987). Effect displays for generalized linear models. In C. C. Clogg (Ed.), Sociological methodology, 1987 (pp. 347–361). Jossey-Bass.\n\n\nFox, J. (2003). Effect displays in R for generalized linear models. Journal of Statistical Software, 8(15), 1–27.\n\n\nFox, J. (2020). Regression diagnostics (2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878651\n\n\nFox, J., & Weisberg, S. (2018a). An R companion to applied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFox, J., & Weisberg, S. (2018b). Visualizing fit and lack of fit in complex regression models with predictor effect plots and partial residuals. Journal of Statistical Software, 87(9). https://doi.org/10.18637/jss.v087.i09\n\n\nFox, J., Weisberg, S., & Price, B. (2023). Car: Companion to applied regression. https://CRAN.R-project.org/package=car\n\n\nFox, J., Weisberg, S., Price, B., Friendly, M., & Hong, J. (2022). Effects: Effect displays for linear, generalized linear, and other models. https://www.r-project.org\n\n\nHoaglin, D. C., & Welsch, R. E. (1978). The hat matrix in regression and ANOVA. The American Statistician, 32(1), 17–22. https://doi.org/10.1080/00031305.1978.10479237\n\n\nKastellec, J. P., & Leoni, E. L. (2007). Using graphs instead of tables in political science. Perspectives on Politics, 5(04), 755–771. https://doi.org/10.1017/S1537592707072209\n\n\nLarmarange, J. (2024). Ggstats: Extension to ggplot2 for plotting stats. https://larmarange.github.io/ggstats/\n\n\nLarsen, W. A., & McCleary, S. J. (1972). The use of partial residual plots in regression analysis. Technometrics, 14, 781–790.\n\n\nLüdecke, D. (2024). Ggeffects: Create tidy data frames of marginal effects for ggplot from model outputs. https://strengejacke.github.io/ggeffects/\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., & Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., & Makowski, D. (2022). Easystats: Framework for easy statistical modeling, visualization, and reporting. In CRAN. https://easystats.github.io/easystats/\n\n\nPineo, P. O., & Porter, J. (1967). Occupational prestige in canada*. Canadian Review of Sociology, 4(1), 24–40. https://doi.org/https://doi.org/10.1111/j.1755-618X.1967.tb00472.x\n\n\nSearle, S. R., Speed, F. M., & Milliken, G. A. (1980). Population marginal means in the linear model: An alternative to least squares means. The American Statistician, 34(4), 216–221.\n\n\nVelleman, P. F., & Welsh, R. E. (1981). Efficient computing of regression diagnostics. The American Statistician, 35(4), 234–242.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#footnotes",
    "href": "06-linear_models-plots.html#footnotes",
    "title": "6  Plots for univariate response models",
    "section": "",
    "text": "Note that the factor type in the dataset has its levels ordered alphabetically. For analysis and graphing it is useful to reorder the levels in the natural increasing order. An alternative is to make type an ordered factor, but this would represent it using polynomial contrasts for linear and quadratic trends, which seems unuseful in this context.↩︎\nEarlier, but less general expression of these ideas go back to the use of adjusted means in analysis of covariance (Fisher, 1925) or least squares means or population marginal means in analysis of variance (Searle et al., 1980)↩︎\nSee this Stats StackExchange discussion for a proof.↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html",
    "href": "07-lin-mod-topics.html",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "7.1 Ellipsoids in data space and \\(\\mathbf{\\beta}\\) space\nIt is most common to look at data and fitted models in “data space,” where axes correspond to variables, points represent observations, and fitted models are plotted as lines (or planes) in this space. As we’ve suggested, data ellipsoids provide informative summaries of relationships in data space. For linear models, particularly regression models with quantitative predictors, there is another space—“\\(\\mathbf{\\beta}\\) space”—that provides deeper views of models and the relationships among them. This discussion extends Friendly et al. (2013), Sec. 4.6.\nIn \\(\\mathbf{\\beta}\\) space, the axes pertain to coefficients, for example \\((\\beta_0, \\beta_1)\\) in a simple linear regression. Points in this space are models (true, hypothesized, fitted) whose coordinates represent values of these parameters. For example, one point \\(\\widehat{\\mathbf{\\beta}}_{\\text{OLS}} = (\\hat{\\beta}_0, \\hat{\\beta}_1)\\) represents the least squares estimate; other points, \\(\\widehat{\\mathbf{\\beta}}_{\\text{WLS}}\\) and \\(\\widehat{\\mathbf{\\beta}}_{\\text{ML}}\\) would give weighted least squares and maximum likelihood estimates, and the line \\(\\beta_1 = 0\\) represents the null hypothesis that the slope is zero.\nIn the sense described below, data space and \\(\\boldsymbol{\\beta}\\) space are each dual to the other. In simple linear regression, for example:\nFigure 7.1: Duality of \\((x, y)\\) lines in data space (left) and points in \\(\\beta\\)-space (right). Each line in data space corresponds to a point, whose intercept and slope are shown in \\(\\beta\\)-space.\nThis is illustrated in Figure 7.1. The left panel shows three lines in data space, which can be expressed as linear equations in \\(\\mathbf{z} = (x, y)\\) of the form \\(\\mathbf{A} \\mathbf{z} = \\mathbf{d}\\). matlib::showEqn(A, d) prints these as equations in \\(x\\) and \\(y\\).\nA &lt;- matrix(c( 1, 1, 0,\n              -1, 1, 1), 3, 2) \nd &lt;- c(2, 1/2, 1)\nshowEqn(A, d, vars = c(\"x\", \"y\"), simplify = TRUE)\n#&gt;   x - 1*y  =    2 \n#&gt;   x   + y  =  0.5 \n#&gt; 0*x   + y  =    1\nThe first equation, \\(x - y = 2\\) can be expressed as the line \\(y = x - 2\\) and corresponds to the point \\((\\beta_0, \\beta_1) = (-2, 1)\\) in \\(\\beta\\) space, and similarly for the other two equations. The second equation, \\(x + y = \\frac{1}{2}\\), or \\(y = 0.5 - x\\) intersects the first at the point \\((x, y) = (1.25, 0.75)\\); this corresponds to the line connecting \\((-2, 1)\\) and \\((0.5, -1)\\) in \\(\\beta\\) space.\nThis lovely duality is an example of an important principle in modern mathematics which translates concepts and structures from one perspective to another and back again. We get two views of the same thing, whose dual nature provides greater insight.\nWe have seen (Section 3.2) how ellipsoids in data space summarize variance (lack of precision) and correlation of our data. For the purpose of understanding linear models, ellipsoids in \\(\\beta\\) space do the same thing for the estimates of parameters. These ellipsoids are dual and inversely related to each other, a point first made clear by Dempster (1969, Ch. 6):\nIt is useful to understand the underlying geometry here connecting the ellipses for a matrix and its inverse. This can be seen in Figure 7.2, which shows an ellipse for a covariance matrix \\(\\mathbf{S}\\), whose axes, as we saw in Chapter 4 are the eigenvectors \\(\\mathbf{v}_i\\) of \\(\\mathbf{S}\\) and whose radii are the square roots \\(\\sqrt{\\lambda_i}\\) of the corresponding eigenvalues. The comparable ellipse for \\(2 \\mathbf{S}\\) has radii multiplied by \\(\\sqrt{2}\\).\nFigure 7.2: Geometric properties of an ellipse \\(\\mathbf{S}\\) and its inverse, \\(\\mathbf{S}^{-1}\\). The principal axes (dotted lines) are given by the eigenvectors, which are the same for \\(\\mathbf{S}\\) and \\(\\mathbf{S}^{-1}\\). Multiplying \\(\\mathbf{S}\\) by 2 makes it’s ellipse larger by \\(\\sqrt{2}\\), while the same factor makes the ellipse for \\((2 \\mathbf{S})^{-1}\\) smaller by the same factor.\nAs long as \\(\\mathbf{S}\\) is of full rank, the eigenvectors of \\(\\mathbf{S}^{-1}\\) are identical, while the eigenvalues are \\(1 / \\lambda_i\\), so the radii are the reciprocals \\(1 / \\sqrt{\\lambda_i}\\). The analogous ellipse for \\((2 \\mathbf{S}^{-1})\\) is smaller by a factor of \\(\\sqrt{2}\\).\nThus, in two dimensions, the ellipse for \\(\\mathbf{S}^{-1}\\) is a \\(90^o\\) rotation of that for \\(\\mathbf{S}\\). It is small in directions where the ellipse for \\(\\mathbf{S}\\) is large, and vice-versa. In our statistical applications, this translates as: parameter estimates in \\(\\beta\\) space are more precise (have less variance) in the directions where the data are more widely dispersed, having more information about the relationship.\nWe illustrate these ideas in the example below.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#sec-betaspace",
    "href": "07-lin-mod-topics.html#sec-betaspace",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "each line, like \\(\\mathbf{y} = \\beta_0 + \\beta_1 \\mathbf{x}\\) with intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) in data space corresponds to a point \\((\\beta_0,\\beta_1)\\) in \\(\\mathbf{\\beta}\\) space, and conversely;\nthe set of points on any line \\(\\beta_1 = x + y \\beta_0\\) in \\(\\mathbf{\\beta}\\) space corresponds to a set of lines through a given point \\((x, y)\\) in data space, and conversely;\nthe geometric proposition that every pair of points defines a line in one space corresponds to the proposition that every two lines intersect in a point in the other space.\n\n\n\n\n\n\n\n\nIn data space, joint confidence intervals for the mean vector or joint prediction regions for the data are given by the ellipsoids \\((\\bar{x}_1, \\bar{x}_2)^\\mathsf{T} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}}\\), where the covariance matrix \\(\\mathbf{S}_{\\mathbf{X}}\\) depends on \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) (\\(\\oplus\\) here shifts the ellipsoid to one centered at \\((\\bar{x}_1, \\bar{x}_2)\\) here, as in Equation 3.2).\nIn the dual \\(\\mathbf{\\beta}\\) space, joint confidence regions for the coefficients of a response variable \\(y\\) on \\((x_1, x_2)\\) are given by ellipsoids of the form \\(\\widehat{\\mathbf{\\beta}} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}^{-1}}\\), and depend on \\(\\mathbf{(\\mathbf{X}^\\mathsf{T}\\mathbf{X})}^{-1}\\).\n\n\n\n\n\n\n\n7.1.1 Coffee, stress and heart disease\nConsider the dataset coffee, giving measures of Heart (\\(y\\)), an index of cardiac damage, Coffee (\\(x_1\\)), a measure of daily coffee consumption, and Stress (\\(x_2\\)), a measure of occupational stress, in a contrived sample of \\(n=20\\) university people.1 For the sake of the example we assume that the main goal is to determine whether or not coffee is good or bad for your heart, and stress represents one potential confounding variable among others (age, smoking, etc.) that might be useful to control statistically.\n\nset.seed(1234)\ndata(coffee, package=\"matlib\")\ncoffee |&gt; dplyr::sample_n(6)\n#&gt;          Group Coffee Stress Heart\n#&gt; 1 Grad_Student    104    117    92\n#&gt; 2      Student     52     86    63\n#&gt; 3 Grad_Student     76     92    58\n#&gt; 4      Student    100    123    92\n#&gt; 5      Student     64     74    63\n#&gt; 6    Professor    141    175   145\n\nFigure 7.3 shows the scatterplot matrix, giving the marginal relations between all pairs of variables. The marginal message seems to be that coffee is bad for your heart, stress is bad for your heart and coffee consumption is also related to occupational stress.\n\nShow the codescatterplotMatrix(~ Heart + Coffee + Stress, data=coffee,\n    smooth = FALSE,\n    pch = 16, col = \"brown\",\n    cex.axis = 1.3, cex.labels = 3,\n    ellipse = list(levels = 0.68, fill.alpha = 0.1))\n\n\n\n\n\n\nFigure 7.3: Scatterplot matrix for the coffee data showing the pairwise relationships among Heart damage (\\(y\\)), Coffee consumption (\\(x_1\\)), and Stress (\\(x_2\\)), with linear regression lines and 68% data ellipses.\n\n\n\n\nYet, when we fit both variables together, we obtain the following results, suggesting that coffee is good for you—the coefficient for coffee is now negative, though non-significant. How can this be?\n\ncoffee.mod &lt;- lm(Heart ~ Coffee + Stress, data=coffee)\nbroom::tidy(coffee.mod)\n#&gt; # A tibble: 3 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)   -7.79      5.79      -1.35 0.196    \n#&gt; 2 Coffee        -0.409     0.292     -1.40 0.179    \n#&gt; 3 Stress         1.20      0.224      5.34 0.0000536\n\nThe answer is that the marginal plots of Heart vs. Coffee and Stress in the first row of Figure 7.3 each ignore the the other predictor. In contrast, the coefficients for coffee and stress in the multiple regression model coffee.mod are partial coefficients, giving the estimated change in heart damage for a unit change in each predictor, but adjusting for (controlling for, or holding constant) the other predictor.\nWe can see these effects directly in added variable plots (Section 6.4), but here I consider the relationship of coffee and stress in data space and beta space and how their ellipses relate to each other and to hypothesis tests.\nThe left panel in Figure 7.4 is the same as that in the (3,2) cell of Figure 7.3 for the relation Stress ~ Coffee but with data ellipses of 40% and 60% coverage. The shadows of the 40% ellipse on any axis give univariate intervals of the mean \\(\\bar{x} \\pm 1 s_x\\) (standard deviation) shown by the thick red lines; the shadow of the 68% ellipse corresponds to an interval \\(\\bar{x} \\pm 1.5 s_x\\).\nThe right panel shows the joint 95% confidence region for the coefficients \\((\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}})\\) and individual confidence intervals in \\(\\mathbf{\\beta}\\) space. These are determined as\n\\[\n\\widehat{\\mathbf{\\beta}} \\oplus \\sqrt{d F^{.95}_{d, \\nu}} \\times s_e \\times \\mathbf{S}_X^{-1/2} \\:\\: .\n\\] where \\(d\\) is the number of dimensions for which we want coverage, \\(\\nu\\) is the residual degrees of freedom for \\(s_e\\), and \\(\\mathbf{S}_X\\) is the covariance matrix of the predictors.\n\n\n\n\n\n\n\nFigure 7.4: Data space and \\(\\mathbf{\\beta}\\) space representations of Coffee and Stress. Left: 40% and 68% data ellipses. Right: Joint 95% confidence ellipse (blue) for (\\(\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}}\\)), confidence interval generating ellipse (red) with 95% univariate shadows. \\(H_0\\) marks the joint hypothesis that both coefficients equal zero.\n\n\n\n\nThus, the blue ellipse in Figure 7.4 (right) is the ellipse of joint 95% coverage, using the factor \\(\\sqrt{2 F^{.95}_{2, \\nu}}\\), which covers the true values of (\\(\\beta_{\\mathrm{Stress}}, \\beta_{\\mathrm{Coffee}}\\)) in 95% of samples. Moreover:\n\nAny joint hypothesis (e.g., \\(\\mathcal{H}_0:\\beta_{\\mathrm{Stress}}=0, \\beta_{\\mathrm{Coffee}}=0\\)) can be tested visually, simply by observing whether the hypothesized point, \\((0, 0)\\) here, lies inside or outside the joint confidence ellipse. That hypothesis is rejected\nThe shadows of this ellipse on the horizontal and vertical axes give Scheff'e joint 95% confidence intervals for the parameters, with protection for simultaneous inference (“fishing”) in a 2-dimensional space.\nSimilarly, using the factor \\(\\sqrt{F^{1-\\alpha/d}_{1, \\nu}} = t^{1-\\alpha/2d}_\\nu\\) would give an ellipse whose 1D shadows are \\(1-\\alpha\\) Bonferroni confidence intervals for \\(d\\) posterior hypotheses.\n\nVisual hypothesis tests and \\(d=1\\) confidence intervals for the parameters separately are obtained from the red ellipse in Figure 7.4, which is scaled by \\(\\sqrt{F^{.95}_{1, \\nu}} = t^{.975}_\\nu\\). We call this the _confidence-interval generating ellipse” (or, more compactly, the “confidence-interval ellipse”). The shadows of the confidence-interval ellipse on the axes (thick red lines) give the corresponding individual 95% confidence intervals, which are equivalent to the (partial, Type III) \\(t\\)-tests for each coefficient given in the standard multiple regression output shown above.\nThus, controlling for Stress, the confidence interval for the slope for Coffee includes 0, so we cannot reject the hypothesis that \\(\\beta_{\\mathrm{Coffee}}=0\\) in the multiple regression model, as we saw above in the numerical output. On the other hand, the interval for the slope for Stress excludes the origin, so we reject the null hypothesis that \\(\\beta_{\\mathrm{Stress}}=0\\), controlling for Coffee consumption.\nFinally, consider the relationship between the data ellipse and the confidence ellipse. These have exactly the same shape, but (with equal coordinate scaling of the axes), the confidence ellipse is exactly a \\(90^o\\) rotation and rescaling of the data ellipse. In directions in data space where the slice of the data ellipse is wide—where we have more information about the relationship between Coffee and Stress—the projection of the confidence ellipse is narrow, reflecting greater precision of the estimates of coefficients. Conversely, where slice of the the data ellipse is narrow (less information), the projection of the confidence ellipse is wide (less precision).\nConfidence ellipses are drawn using car::confidenceEllipse(). Click the button to show the code.\n\nCode for confidence ellipsesconfidenceEllipse(coffee.mod, \n    grid = FALSE,\n    xlim = c(-2, 1), ylim = c(-0.5, 2.5),\n    xlab = expression(paste(\"Coffee coefficient,  \", beta[\"Coffee\"])),\n    ylab = expression(paste(\"Stress coefficient,  \", beta[\"Stress\"])),\n    cex.lab = 1.5)\nconfidenceEllipse(coffee.mod, add=TRUE, draw = TRUE,\n    col = \"red\", fill = TRUE, fill.alpha = 0.1,\n    dfn = 1)\nabline(h = 0, v = 0, lwd = 2)\n\n# confidence intervals\nbeta &lt;- coef( coffee.mod )[-1]\nCI &lt;- confint(coffee.mod)\nlines( y = c(0,0), x = CI[\"Coffee\",] , lwd = 6, col = 'red')\nlines( x = c(0,0), y = CI[\"Stress\",] , lwd = 6, col = 'red')\npoints( diag( beta ), col = 'black', pch = 16, cex=1.8)\n\nabline(v = CI[\"Coffee\",], col = \"red\", lty = 2)\nabline(h = CI[\"Stress\",], col = \"red\", lty = 2)\n\ntext(-2.1, 2.35, \"Beta space\", cex=2, pos = 4)\narrows(beta[1], beta[2], beta[1], 0, angle=8, len=0.2)\narrows(beta[1], beta[2], 0, beta[2], angle=8, len=0.2)\n\ntext( -1.5, 1.85, \"df = 2\", col = 'blue', adj = 0, cex=1.2)\ntext( 0.2, .85, \"df = 1\", col = 'red', adj = 0, cex=1.2)\n\nheplots::mark.H0(col = \"darkgreen\", pch = \"+\", lty = 0, pos = 4, cex = 3)",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#measurement-error",
    "href": "07-lin-mod-topics.html#measurement-error",
    "title": "\n7  Topics in Linear Models\n",
    "section": "\n7.2 Measurement error",
    "text": "7.2 Measurement error\n\n7.2.1 OLS is BLUE\nIn classical linear models, the predictors are often considered to be fixed variables, or, if random, to be measured without error and independent of the regression errors. Either condition, along with the assumption of linearity, guarantees that the standard OLS estimators are unbiased. That is, in a simple linear regression, \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), the estimated slope \\(\\hat{\\beta}_1\\) wiil have an average, expected value \\(\\mathcal{E} (\\hat{\\beta}_1)\\) equal to the true population value \\(\\beta_1\\) over repeated samples.\nNot only this, but the Gauss-Markov theorem guarantees that the OLS estimator is also the most efficient because it has the least variance among all linear and unbiased estimators. The classical OLS estimator is said to be BLUE: Best (lowest variance), Linear (among linear estimators), Unbiased, Estimator.\n\n7.2.2 Errors in predictors\nErrors in the response \\(y\\) are accounted for in the model and measured by the mean squared error, \\(\\text{MSE} = \\hat{\\sigma}_\\epsilon^2\\). But in practice, of course, predictor variables are often also observed indicators, subject to their own error. Indeed, in the behavioral sciences it is rare that predictors are perfectly reliable and measured exactly. This fact that is recognized in errors-in-variables regression models (Fuller, 2006) and in more general structural equation models, but often ignored otherwise. Ellipsoids in data space and \\(\\beta\\) space are well suited to showing the effect of measurement error in predictors on OLS estimates.\nThe statistical facts are well known, though perhaps counter-intuitive in certain details: measurement error in a predictor biases regression coefficients (towards 0), while error in the measurement in \\(y\\) increases the MSE and thus standard errors of the regression coefficients but does not introduce bias in the coefficients.\n\n7.2.2.1 Example\nAn illuminating example can be constructed by starting with the simple linear regression \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\; ,\n\\] where \\(x_i\\) is the true, fully reliable predictor and \\(y\\) is the response, with error variance \\(\\sigma_\\epsilon^2\\). Now consider that we don’t measure \\(x_i\\) exactly, but instead observe \\(x^\\star_i\\). \\[\nx^\\star_i = x_i + \\eta_i \\; ,\n\\] where the measurement error \\(\\eta_i\\) is independent of the true \\(x_i\\) with variance \\(\\sigma^2_\\eta\\). We can extend this example to also consider the effect of adding additional, independent error variance to \\(y\\), so that instead of \\(y_i\\) we observe\n\\[\ny^\\star_i = y_i + \\nu_i\n\\] with variance \\(\\sigma^2_\\nu\\).\nLet’s simulate an example where the true relation is \\(y = 0.2 + 0.3 x\\) with error standard deviation \\(\\sigma = 0.5\\). I’ll take \\(x\\) to be uniformly distributed in [0, 10] and calculate \\(y\\) as normally distributed around that linear relation.\n\n\nset.seed(123)\nn &lt;- 300\n\na &lt;- 0.2    # true intercept\nb &lt;- 0.3    # true slope\nsigma &lt;- 0.5 # baseline error standard deviation\n\nx &lt;- runif(n, 0, 10)\ny &lt;- rnorm(n, a + b*x, sigma)\ndemo &lt;- data.frame(x,y)\n\nThen, generate alternative values \\(x^\\star\\) and \\(y^\\star\\) with additional error standard deviations around \\(x\\) given by \\(\\sigma_\\eta = 4\\) and around \\(y\\) given by \\(\\sigma_\\nu = 1\\).\n\nerr_y &lt;- 1   # additional error stdev for y\nerr_x &lt;- 4   # additional error stdev for x\ndemo  &lt;- demo |&gt;\n  mutate(y_star = rnorm(n, y, err_y),\n         x_star = rnorm(n, x, err_x))\n\nThere are four possible models we could fit and compare, using the combinations of \\((x, x^\\star)\\) and \\((y, y^\\star)\\)\n\nfit_1 &lt;- lm(y ~ x,           data = demo)   # no additional error\nfit_2 &lt;- lm(y_star ~ x,      data = demo)   # error in y\nfit_3 &lt;- lm(y ~ x_star,      data = demo)   # error in x\nfit_4 &lt;- lm(y_star ~ x_star, data = demo)   # error in x and y\n\nHowever, to show the differences visually, we can simply plot the data for each pair and show the regression lines (with confidence bands) and the data ellipses. To do this efficiently with ggplot2, it is necessary to transform the demo data to long format with columns x and y, distinguished by name for the four combinations.\n\n# make the demo dataset long, with names for the four conditions\ndf &lt;- bind_rows(\n  data.frame(x=demo$x,      y=demo$y,      name=\"No measurement error\"),\n  data.frame(x=demo$x,      y=demo$y_star, name=\"Measurement error on y\"),\n  data.frame(x=demo$x_star, y=demo$y,      name=\"Measurement error on x\"),\n  data.frame(x=demo$x_star, y=demo$y_star, name=\"Measurement error on x and y\")) |&gt;\n  mutate(name = fct_inorder(name)) \n\nThen, we can plot the data in df with points, regression lines and a data ellipse, faceting by name to give the measurement error quartet.\n\n\nggplot(df, aes(x, y)) +\n  geom_point(alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", \n               color = \"blue\",fill= \"blue\", \n               alpha=0.05, linewidth = 1.1) +\n  geom_smooth(method=\"lm\", formula = y~x, fullrange=TRUE, level=0.995,\n              color = \"red\", fill = \"red\", alpha = 0.2) +\n  facet_wrap(~name) \n\n\n\n\n\n\nFigure 7.5: The measurement error quartet: Each plot shows the linear regression of y on x, but where additional error variance has been added to y or x or both. The widths of the confidence bands and the vertical extent of the data ellipses show the effect on precision.\n\n\n\n\nComparing the plots in the first row, you can see that when additional error is added to \\(y\\), the regression slope remains essentially unchanged, illustrating that the estimate is unbiased. However, the confidence bounds on the regression line become wider, and the data ellipse becomes fatter in the \\(y\\) direction, illustrating the loss of precision.\nThe effect of error in \\(x\\) is less kind. Comparing the first row of plots with the second row, you can see that the estimated slope decreases when errors are added to \\(x\\). This is called attenuation bias, and it can be shown that \\[\n\\widehat{\\beta}_{x^\\star} \\longrightarrow \\frac{\\beta}{1+\\sigma^2_\\eta /\\sigma^2_x} \\; ,\n\\] where \\(\\beta\\) here refers to the regression slope and \\(\\longrightarrow\\) means “converges to”, as the sample size gets large. Thus, as \\(\\sigma^2_\\eta\\) increases, \\(\\widehat{\\beta}_{x^\\star}\\) becomes less than \\(\\beta\\).\nBeyond plots like Figure 7.5, we can see the effects of error in \\(x\\) or \\(y\\) on the model summary statistics such as the correlation \\(r_{xy}\\) or MSE by extracting these from the fitted models. This is easily done using dplyr::nest_by(name) and fitting the regression model to each subset, from which we can obtain the model statistics using sigma(), coef() and so forth. A bit of dplyr::mutate() magic is used to construct indicators errX and errY giving whether or not error was added to \\(x\\) and/or \\(y\\).\n\nmodel_stats &lt;- df |&gt;\n  dplyr::nest_by(name) |&gt;\n  mutate(model = list(lm(y ~ x, data = data)),\n         sigma = sigma(model),\n         intercept = coef(model)[1],\n         slope = coef(model)[2],\n         r = sqrt(summary(model)$r.squared)) |&gt;\n  mutate(errX = stringr::str_detect(name, \" x\"),\n         errY = stringr::str_detect(name, \" y\")) |&gt;\n  mutate(errX = factor(errX, levels = c(\"TRUE\", \"FALSE\")),\n         errY = factor(errY, levels = c(\"TRUE\", \"FALSE\"))) |&gt;\n  relocate(errX, errY, r, .after = name) |&gt;\n  select(-data) |&gt;\n  print()\n#&gt; # A tibble: 4 × 8\n#&gt; # Rowwise:  name\n#&gt;   name                errX  errY      r model sigma intercept  slope\n#&gt;   &lt;fct&gt;               &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lis&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 No measurement err… FALSE FALSE 0.858 &lt;lm&gt;  0.495    0.244  0.294 \n#&gt; 2 Measurement error … FALSE TRUE  0.648 &lt;lm&gt;  1.09     0.0838 0.329 \n#&gt; 3 Measurement error … TRUE  FALSE 0.481 &lt;lm&gt;  0.844    1.22   0.0946\n#&gt; 4 Measurement error … TRUE  TRUE  0.401 &lt;lm&gt;  1.31     1.12   0.117\n\nWe plot the model \\(R = r_{xy}\\) and the estimated residual standard error in Figure 7.6 below. The lines connecting the points are approximately parallel, indicating that errors of measurement in \\(x\\) and \\(y\\) have nearly additive effects on model summaries.\n\n\np1 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = r, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model R \",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") +\n  legend_inside(c(0.25, 0.8))\n\np2 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = sigma, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model residual standard error\",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") \n\np1 + p2\n\n\n\n\n\n\nFigure 7.6: Model statistics for the combinations of additional error variance in x or y or both. Left: model R; right: Residual standard error.\n\n\n\n\n\n7.2.3 Coffee data: \\(\\beta\\) space\nIn multiple regression the effects of measurement error in a predictor become more complex, because error variance in one predictor, \\(x_1\\), say, can affect the coefficients of other terms in the model.\nConsider the marginal relation between Heart disease and Stress in the coffee data. Figure 7.7 shows this with data ellipses in data space and the corresponding confidence ellipses in \\(\\beta\\) space. Each panel starts with the observed data (the darkest ellipse, marked \\(0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0.75, 1.0, 1.5\\}\\), to the value of Stress, while keeping the mean of Stress the same. All of the data ellipses have the same vertical shadows (\\(\\text{SD}_{\\textrm{Heart}}\\)), while the horizontal shadows increase with \\(\\delta\\), driving the slope for Stress toward 0.\nIn \\(\\beta\\) space, it can be seen that the estimated coefficients, \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) vary along a line and approach \\(\\beta_{\\textrm{Stress}}=0\\) as \\(\\delta\\) gets sufficiently large. The shadows of ellipses for \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) along the \\(\\beta_{\\textrm{Stress}}\\) axis also demonstrate the effects of measurement error on the standard error of \\(\\beta_{\\textrm{Stress}}\\).\n\n\n\n\n\n\n\nFigure 7.7: Effects of measurement error in Stress on the marginal relationship between Heart disease and Stress. Each panel starts with the observed data (\\(\\delta = 0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\text{SD}_\\text{Stress})\\) with standard deviations multiplied by \\(\\delta\\) = 0.75, 1.0, 1.5, to the value of Stress. Increasing measurement error biases the slope for Stress toward 0. Left: 50% data ellipses; right: 50% confidence ellipses.\n\n\n\n\nPerhaps less well-known, but both more surprising and interesting, is the effect that measurement error in one variable, \\(x_1\\), has on the estimate of the coefficient for an other variable, \\(x_2\\), in a multiple regression model. Figure 7.8 shows the confidence ellipses for \\((\\beta_{\\textrm{Coffee}}, \\beta_{\\textrm{Stress}})\\) in the multiple regression predicting Heart disease, adding random normal error \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0, 0.2, 0.4, 0.8\\}\\), to the value of Stress alone.\nAs can be plainly seen, while this measurement error in Stress attenuates its coefficient, it also has the effect of biasing the coefficient for Coffee toward that in the marginal regression of Heart disease on Coffee alone.\n\n\n\n\n\n\n\nFigure 7.8: Biasing effect of measurement error in one variable (Stress) on on the coefficient of another variable (Coffee) in a multiple regression. The coefficient for Coffee is driven towards its value in the marginal model using Coffee alone, as measurement error in Stress makes it less informative in the joint model.\n\n\n\n\n\nPackages used here:\n10 packages used here: car, carData, dplyr, forcats, gganimate, ggplot2, knitr, matlib, patchwork, tidyr\n\n\n\n\n\nDempster, A. P. (1969). Elements of continuous multivariate analysis. Addison-Wesley.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFuller, W. (2006). Measurement error models (2nd ed.). John Wiley & Sons.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#footnotes",
    "href": "07-lin-mod-topics.html#footnotes",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "This example was developed by Georges Monette.↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html",
    "href": "08-collinearity-ridge.html",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "8.1 What is collinearity?\nResearchers who have studies standard treatments of linear models (e.g, Graybill (1961); Hocking (2013)) are often confused about what collinearity is, how to find its sources and how to take steps to resolve them. There are a number of important diagnostic measures that can help, but these are usually presented in a tabular display like Figure 8.1, which prompted this querry on an online forum:\nFigure 8.1: Collinearity diagnostics for a multiple regression model from SPSS. Source: Arndt Regorz, How to interpret a Collinearity Diagnostics table in SPSS, https://bit.ly/3YRB82b\nThe trouble with displays like Figure 8.1 is that the important information is hidden in a sea of numbers, some of which are bad when large, others bad when they are small and a large bunch which are irrelevant. In Friendly & Kwan (2009), we liken this problem to that of the reader of Martin Hansford’s successful series of books, Where’s Waldo. These consist of a series of full-page illustrations of hundreds of people and things and a few Waldos— a character wearing a red and white striped shirt and hat, glasses, and carrying a walking stick or other paraphernalia. Waldo was never disguised, yet the complex arrangement of misleading visual cues in the pictures made him very hard to find. Collinearity diagnostics often provide a similar puzzle: where should you look in traditional tabular displays?\nFigure 8.2: A scene from one of the Where’s Waldo books. Waldo wears a red-striped shirt, but far too many of the other figures in the scene have horizontal red stripes, making it very difficult to find him among all the distractors. This is often the problem with collinearity diagnostics. Source: Modified from https://bit.ly/48KPcOo\nRecall the standard classical linear model for a response variable \\(y\\) with a collection of predictors in \\(\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_p)\\)\n\\[\\begin{aligned}\n\\mathbf{y}  & =  \\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\cdots + \\beta_p \\mathbf{x}_p + \\boldsymbol{\\epsilon} \\\\\n            & =  \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\; ,\n\\end{aligned}\\]\nfor which the ordinary least squares solution is:\n\\[\n\\widehat{\\mathbf{b}} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\; \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\; .\n\\] The sampling variances and covariances of the estimated coefficients is \\(\\text{Var} (\\widehat{\\mathbf{b}}) = \\sigma_\\epsilon^2 \\times (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1}\\) and \\(\\sigma_\\epsilon^2\\) is the variance of the residuals \\(\\mathbf{\\epsilon}\\), estimated by the mean squared error (MSE).\nIn the limiting case, when one \\(x_i\\) is perfectly predictable from the other \\(x\\)s, i.e., \\(R^2 (x_i | \\text{other }x) = 1\\),\nThis extreme case reflects a situation when one or more predictors are effectively redundant, for example when you include two variables \\(x\\) and \\(y\\) and their sum \\(z = x + y\\) in a model. For instance, a dataset may include variables for income, expenses, and savings. But income is the sum of expenses and savings, so not all three should be used as predictors. A more subtly case is the use ipsatized, defined as scores that sum to a constant, such as proportions of a total. You might have scores on tests of reading, math, spelling and geography. With ipsatized scores, any one of these is necessarily 1 \\(-\\) sum of the others.\nMore generally, collinearity refers to the case when there are very high multiple correlations among the predictors, such as \\(R^2 (x_i | \\text{other }x) \\ge 0.9\\). Note that you can’t tell simply by looking at the simple correlations. A large correlation \\(r_{ij}\\) is sufficient for collinearity, but not necessary—you can have variables \\(x_1, x_2, x_3\\) for which the pairwise correlation are low, but the multiple correlation is high.\nThe consequences are:",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#what-is-collinearity",
    "href": "08-collinearity-ridge.html#what-is-collinearity",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "Some of my collinearity diagnostics have large values, or small values, or whatever they are not supposed to be\n\nWhat is bad?\nIf bad, what can I do about it?\n\n\n\n\n\n\n\n\n\n\n\n\nthere is no unique solution for the regression coefficients \\(\\mathbf{b} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X} \\mathbf{y}\\);\nthe standard errors \\(s (b_i)\\) of the estimated coefficients are infinite and t statistics \\(t_i = b_i / s (b_i)\\) are 0.\n\n\n\n\n\nThe estimated coefficients have large standard errors, \\(s(\\hat{b}_j)\\). They are multiplied by the square root of the variance inflation factor, \\(\\sqrt{\\text{VIF}}\\), discussed below.\nThe large standard errors deflate the \\(t\\)-statistics, \\(t = \\hat{b}_j / s(\\hat{b}_j)\\), by the same factor, so a coefficient that would significant if the predictors were uncorrelated becomes insignificant when collinearity is present.\nThus you may find a situation where an overall model is highly significant (large \\(F\\)-statistic), while no (or few) of the individual predictors are. This is a puzzlement!\nBeyond this, the least squares solution may have poor numerical accurracy (Longley, 1967), because the solution depends on the determinant \\(|\\,\\mathbf{X}^\\mathsf{T} \\mathbf{X}\\,|\\), which approaches 0 as multiple correlations increase.\nThere is an interpretive problem as well. Recall that the coefficients \\(\\hat{b}\\) are partial coefficients, meaning that they estimate change \\(\\Delta y\\) in \\(y\\) when \\(x\\) changes by one unit \\(\\Delta x\\), but holding all other variables constant. Then, the model may be trying to estimate something that does not occur in the data. (For example: predicting strength from the highly correlated height and weight)\n\n\n8.1.1 Visualizing collinearity\nCollinearity can be illustrated in data space for two predictors in terms of the stability of the regression plane for a linear model Y = X1 + X2. Figure 8.3 (adapted from Fox (2016), Fig. 13.2) shows three cases as 3D plots of \\((X_1, X_2, Y)\\), where the correlation of predictors can be observed in the \\((X_1, X_2)\\) plane.\n\nshows a case where \\(X_1\\) and \\(X_2\\) are uncorrelated as can be seen in their scatter in the horizontal plane (+ symbols). The gray regression plane is well-supported; a small change in Y for one observation won’t make much difference.\nIn panel (b), \\(X_1\\) and \\(X_2\\) have a perfect correlation, \\(r (x_1, x_2) = 1.0\\). The regression plane is not unique; in fact there are an infinite number of planes that fit the data equally well. Note that, if all we care about is prediction (not the coefficients), we could use \\(X_1\\) or \\(X_2\\), or both, or any weighted sum of them in a model and get the same predicted values.\nShows a typical case where there is a strong correlation between \\(X_1\\) and \\(X_2\\). The regression plane here is unique, but is not well determined. A small change in Y can make quite a difference in the fitted value or coefficients, depending on the values of \\(X_1\\) and \\(X_2\\). Where \\(X_1\\) and \\(X_2\\) are far from their near linear relation in the botom plane, you can imagine that it is easy to tilt the plane substantially by a small change in \\(Y\\).\n\n\n\n\n\n\n\n\nFigure 8.3: Effect of collinearity on the least squares regression plane. (a) Small correlation between predictors; (b) Perfect correlation ; (c) Very strong correlation. The black points show the data Y values, white points are the fitted values in the regression plane, and + signs represent the values of X1 and X2. Source: Adapted from Fox (2016), Fig. 13.2\n\n\n\n\n\n8.1.2 Data space and \\(\\beta\\) space\nIt is also useful to visualize collinearity by comparing the representation in data space with the analogous view of the confidence ellipses for coefficients in beta space. To do so in this example, I generate data from a known model \\(y = 3 x_1 + 3 x_2 + \\epsilon\\) with \\(\\epsilon \\sim \\mathcal{N} (0, 100)\\) and various true correlations between \\(x_1\\) and \\(x_2\\), \\(\\rho_{12} = (0, 0.8, 0.97)\\) 1.\n\n\nR file: R/collin-data-beta.R\nFirst, I use MASS:mvrnorm() to construct a list of three data frames XY with the same means and standard deviations, but with different correlations. In each case, the variable \\(y\\) is generated with true coefficients beta \\(=(3, 3)\\), and the fitted model for that value of rho is added to a corresponding list of models, mods.\n\nCodelibrary(MASS)\nlibrary(car)\n\nset.seed(421)            # reproducibility\nN &lt;- 200                 # sample size\nmu &lt;- c(0, 0)            # means\ns &lt;- c(1, 1)             # standard deviations\nrho &lt;- c(0, 0.8, 0.97)   # correlations\nbeta &lt;- c(3, 3)          # true coefficients\n\n# Specify a covariance matrix, with standard deviations\n#   s[1], s[2] and correlation r\nCov &lt;- function(s, r){\n  matrix(c(s[1],    r * prod(s),\n         r * prod(s), s[2]), nrow = 2, ncol = 2)\n}\n\n# Generate a dataframe of X, y for each rho\n# Fit the model for each\nXY &lt;- vector(mode =\"list\", length = length(rho))\nmods &lt;- vector(mode =\"list\", length = length(rho))\nfor (i in seq_along(rho)) {\n  r &lt;- rho[i]\n  X &lt;- mvrnorm(N, mu, Sigma = Cov(s, r))\n  colnames(X) &lt;- c(\"x1\", \"x2\")\n  y &lt;- beta[1] * X[,1] + beta[2] * X[,2] + rnorm(N, 0, 10)\n\n  XY[[i]] &lt;- data.frame(X, y=y)\n  mods[[i]] &lt;- lm(y ~ x1 + x2, data=XY[[i]])\n}\n\n\nThe estimated coefficients can then be extracted using coef() applied to each model:\n\ncoefs &lt;- sapply(mods, coef)\ncolnames(coefs) &lt;- paste0(\"mod\", 1:3, \" (rho=\", rho, \")\")\ncoefs\n#&gt;             mod1 (rho=0) mod2 (rho=0.8) mod3 (rho=0.97)\n#&gt; (Intercept)         1.01        -0.0535           0.141\n#&gt; x1                  3.18         3.4719           3.053\n#&gt; x2                  1.68         2.9734           2.059\n\nThen, I define a function to plot the data ellipse (car::dataEllipse()) for each data frame and confidence ellipse (car::confidenceEllipse()) for the coefficients in the corresponding fitted model. In the plots in Figure 8.4, I specify the x, y limits for each plot so that the relative sizes of these ellipses are comparable, so that variance inflation can be assessed visually.\n\nCodedo_plots &lt;- function(XY, mod, r) {\n  X &lt;- as.matrix(XY[, 1:2])\n  dataEllipse(X,\n              levels= 0.95,\n              col = \"darkgreen\",\n              fill = TRUE, fill.alpha = 0.05,\n              xlim = c(-3, 3),\n              ylim = c(-3, 3), asp = 1)\n  text(0, 3, bquote(rho == .(r)), cex = 2, pos = NULL)\n\n  confidenceEllipse(mod,\n                    col = \"red\",\n                    fill = TRUE, fill.alpha = 0.1,\n                    xlab = \"x1 coefficient\",\n                    ylab = \"x2 coefficient\",\n                    xlim = c(-5, 10),\n                    ylim = c(-5, 10),\n                    asp = 1)\n  points(beta[1], beta[2], pch = \"+\", cex=2)\n  abline(v=0, h=0, lwd=2)\n}\n\nop &lt;- par(mar = c(4,4,1,1)+0.1,\n          mfcol = c(2, 3),\n          cex.lab = 1.5)\nfor (i in seq_along(rho)) {\n  do_plots(XY[[i]], mods[[i]], rho[i])\n}\npar(op)\n\n\n\n\n\n\nFigure 8.4: 95% Data ellipses for x1, x2 and the corresponding 95% confidence ellipses for their coefficients in the model predicting y. In the confidence ellipse plots, reference lines show the value (0,0) for the null hypothesis and “+” marks the true values for the coefficients. This figure adapts an example by John Fox (2022).\n\n\n\n\nRecall (Section 7.1) that the confidence ellipse for \\((\\beta_1, \\beta_2)\\) is just a 90 degree rotation (and rescaling) of the data ellipse for \\((x_1, x_2)\\): it is wide (more variance) in any direction where the data ellipse is narrow.\nThe shadows of the confidence ellipses on the coordinate axes in Figure 8.4 represent the standard errors of the coefficients, and get larger with increasing \\(\\rho\\). This is the effect of variance inflation, described in the following section.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-measure-collin",
    "href": "08-collinearity-ridge.html#sec-measure-collin",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.2 Measuring collinearity",
    "text": "8.2 Measuring collinearity\nThis section first describes the variance inflation factor (VIF) used to measure the effect of possible collinearity on each predictor and a collection of diagnostic measures designed to help interpret these. Then I describe some novel graphical methods to make these effects more readily understandable, to answer the “Where’s Waldo” question posed at the outset.\n\n8.2.1 Variance inflation factors\nHow can we measure the effect of collinearity? The essential idea is to compare, for each predictor the variance \\(s^2 (\\widehat{b_j})\\) that the coefficient that \\(x_j\\) would have if it was totally unrelated to the other predictors to the actual variance it has in the given model.\nFor two predictors such as shown in Figure 8.4 the sampling variance of \\(x_1\\) can be expressed as\n\\[\ns^2 (\\widehat{b_1}) = \\frac{MSE}{(n-1) \\; s^2(x_1)} \\; \\times \\; \\left[ \\frac{1}{1-r^2_{12}} \\right]\n\\] The first term here is the variance of \\(b_1\\) when the two predictors are uncorrelated. The term in brackets represents the variance inflation factor (Marquardt, 1970), the amount by which the variance of the coefficient is multiplied as a consequence of the correlation \\(r_{12}\\) of the predictors. As \\(r_{12} \\rightarrow 1\\), the variances approaches infinity.\nMore generally, with any number of predictors, this relation has a similar form, replacing the simple correlation \\(r_{12}\\) with the multiple correlation predicting \\(x_j\\) from all others,\n\\[\ns^2 (\\widehat{b_j}) = \\frac{MSE}{(n-1) \\; s^2(x_j)} \\; \\times \\; \\left[ \\frac{1}{1-R^2_{j | \\text{others}}} \\right]\n\\] So, we have that the variance inflation factors are:\n\\[\n\\text{VIF}_j = \\frac{1}{1-R^2_{j \\,|\\, \\text{others}}}\n\\] In practice, it is often easier to think in terms of the square root, \\(\\sqrt{\\text{VIF}_j}\\) as the multiplier of the standard errors. The denominator, \\(1-R^2_{j | \\text{others}}\\) is sometimes called tolerance, a term I don’t find particularly useful, but it is just the proportion of the variance of \\(x_j\\) that is not explainable from the others.\nFor the cases shown in Figure 8.4 the VIFs and their square roots are:\n\nvifs &lt;- sapply(mods, car::vif)\ncolnames(vifs) &lt;- paste(\"rho:\", rho)\nvifs\n#&gt;    rho: 0 rho: 0.8 rho: 0.97\n#&gt; x1      1     3.09      18.6\n#&gt; x2      1     3.09      18.6\n\nsqrt(vifs)\n#&gt;    rho: 0 rho: 0.8 rho: 0.97\n#&gt; x1      1     1.76      4.31\n#&gt; x2      1     1.76      4.31\n\nNote that when there are terms in the model with more than one degree of freedom, such as education with four levels (and hence 3 df) or a polynomial term specified as poly(age, 3), that variable, education or age is represented by three separate \\(x\\)s in the model matrix, and the standard VIF calculation gives results that vary with how those terms are coded in the model.\nTo allow for these cases, Fox & Monette (1992) define generalized, GVIFs as the inflation in the squared area of the confidence ellipse for the coefficients of such terms, relative to what would be obtained with uncorrelated data. Visually, this can be seen by comparing the areas of the ellipses in the bottom row of Figure 8.4. Because the magnitude of the GVIF increases with the number of degrees of freedom for the set of parameters, Fox & Monette suggest the analog \\(\\sqrt{\\text{GVIF}^{1/2 \\text{df}}}\\) as the measure of impact on standard errors.\nExample: This example uses the cars dataset in the VisCollin package containing various measures of size and performance on 406 models of automobiles from 1982. Interest is focused on predicting gas mileage, mpg.\n\ndata(cars, package = \"VisCollin\")\nstr(cars)\n#&gt; 'data.frame':  406 obs. of  10 variables:\n#&gt;  $ make    : Factor w/ 30 levels \"amc\",\"audi\",\"bmw\",..: 6 4 22 1 12 12 6 22 23 1 ...\n#&gt;  $ model   : chr  \"chevelle\" \"skylark\" \"satellite\" \"rebel\" ...\n#&gt;  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...\n#&gt;  $ cylinder: int  8 8 8 8 8 8 8 8 8 8 ...\n#&gt;  $ engine  : num  307 350 318 304 302 429 454 440 455 390 ...\n#&gt;  $ horse   : int  130 165 150 150 140 198 220 215 225 190 ...\n#&gt;  $ weight  : int  3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ...\n#&gt;  $ accel   : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n#&gt;  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...\n#&gt;  $ origin  : Factor w/ 3 levels \"Amer\",\"Eur\",\"Japan\": 1 1 1 1 1 1 1 1 1 1 ...\n\nWe fit a model predicting gas mileage (mpg) from the number of cylinders, engine displacement, horsepower, weight, time to accelerate from 0 – 60 mph and model year (1970–1982). Perhaps surprisingly, only weight and year appear to significantly predict gas mileage. What’s going on here?\n\ncars.mod &lt;- lm (mpg ~ cylinder + engine + horse + weight + accel + year, \n                data=cars)\nAnova(cars.mod)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: mpg\n#&gt;           Sum Sq  Df F value Pr(&gt;F)    \n#&gt; cylinder      12   1    0.99   0.32    \n#&gt; engine        13   1    1.09   0.30    \n#&gt; horse          0   1    0.00   0.98    \n#&gt; weight      1214   1  102.84 &lt;2e-16 ***\n#&gt; accel          8   1    0.70   0.40    \n#&gt; year        2419   1  204.99 &lt;2e-16 ***\n#&gt; Residuals   4543 385                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe check the variance inflation factors, using car::vif(). We see that most predictors have very high VIFs, indicating moderately severe multicollinearity.\n\nvif(cars.mod)\n#&gt; cylinder   engine    horse   weight    accel     year \n#&gt;    10.63    19.64     9.40    10.73     2.63     1.24\n\nsqrt(vif(cars.mod))\n#&gt; cylinder   engine    horse   weight    accel     year \n#&gt;     3.26     4.43     3.07     3.28     1.62     1.12\n\nAccording to \\(\\sqrt{\\text{VIF}}\\), the standard error of cylinder has been multiplied by 3.26 and it’s \\(t\\)-value divided by this number, compared with the case when all predictors are uncorrelated. engine, horse and weight suffer a similar fate.\n\n\n\n\n\n\nConnection with inverse of correlation matrix\n\n\n\nIn the linear regression model with standardized predictors, the covariance matrix of the estimated intercept-excluding parameter vector \\(\\mathbf{b}^\\star\\) has the simpler form, \\[\n\\mathcal{V} (\\mathbf{b}^\\star) = \\frac{\\sigma^2}{n-1} \\mathbf{R}^{-1}_{X} \\; .\n\\] where \\(\\mathbf{R}_{X}\\) is the correlation matrix among the predictors. It can then be seen that the VIF\\(_j\\) are just the diagonal entries of \\(\\mathbf{R}^{-1}_{X}\\).\nMore generally, the matrix \\(\\mathbf{R}^{-1}_{X} = (r^{ij})\\), when standardized to a correlation matrix as \\(-r^{ij} / \\sqrt{r^{ii} \\; r^{jj}}\\) gives the matrix of all partial correlations, \\(r_{ij} \\,|\\, \\text{others}\\). }\n\n\n\n8.2.2 Collinearity diagnostics\nOK, we now know that large VIF\\(_j\\) indicate predictor coefficients whose estimation is degraded due to large \\(R^2_{j \\,|\\, \\text{others}}\\). But for this to be useful, we need to determine:\n\nhow many dimensions in the space of the predictors are associated with nearly collinear relations?\nwhich predictors are most strongly implicated in each of these?\n\nAnswers to these questions are provided using measures developed by Belsley and colleagues (Belsley et al., 1980; Belsley, 1991). These measures are based on the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots \\lambda_p\\) of the correlation matrix \\(R_{X}\\) of the predictors (preferably centered and scaled, and not including the constant term for the intercept), and the corresponding eigenvectors in the columns of \\(\\mathbf{V}_{p \\times p}\\), given by the the eigen decomposition \\[\n\\mathbf{R}_{X} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\mathsf{T}\n\\] By elementary matrix algebra, the eigen decomposition of \\(\\mathbf{R}_{XX}^{-1}\\) is then \\[\n\\mathbf{R}_{X}^{-1} = \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\mathsf{T} \\; ,\n\\tag{8.1}\\] so, \\(\\mathbf{R}_{X}\\) and \\(\\mathbf{R}_{XX}^{-1}\\) have the same eigenvectors, and the eigenvalues of \\(\\mathbf{R}_{X}^{-1}\\) are just \\(\\lambda_i^{-1}\\). Using Equation 8.1, the variance inflation factors may be expressed as \\[\n\\text{VIF}_j = \\sum_{k=1}^p \\frac{V^2_{jk}}{\\lambda_k} \\; ,\n\\] which shows that only the small eigenvalues contribute to variance inflation, but only for those predictors that have large eigenvector coefficients on those small components. These facts lead to the following diagnostic statistics for collinearity:\n\n\nCondition indices: The smallest of the eigenvalues, those for which \\(\\lambda_j \\approx 0\\), indicate collinearity and the number of small values indicates the number of near collinear relations. Because the sum of the eigenvalues, \\(\\Sigma \\lambda_i = p\\) increases with the number of predictors \\(p\\), it is useful to scale them all in relation to the largest. This leads to condition indices, defined as \\(\\kappa_j = \\sqrt{ \\lambda_1 / \\lambda_j}\\). These have the property that the resulting numbers have common interpretations regardless of the number of predictors.\n\nFor completely uncorrelated predictors, all \\(\\kappa_j = 1\\).\n\n\\(\\kappa_j \\rightarrow \\infty\\) as any \\(\\lambda_k \\rightarrow 0\\).\n\n\nVariance decomposition proportions: Large VIFs indicate variables that are involved in some nearly collinear relations, but they don’t indicate which other variable(s) each is involved with. For this purpose, Belsley et al. (1980) and Belsley (1991) proposed calculation of the proportions of variance of each variable associated with each principal component as a decomposition of the coefficient variance for each dimension.\n\nThese measures can be calculated using VisCollin::colldiag(). For the current model, the usual display contains both the condition indices and variance proportions. However, even for a small example, it is often difficult to know what numbers to pay attention to.\n\n(cd &lt;- colldiag(cars.mod, center=TRUE))\n#&gt; Condition\n#&gt; Index  Variance Decomposition Proportions\n#&gt;           cylinder engine horse weight accel year \n#&gt; 1   1.000 0.005    0.003  0.005 0.004  0.009 0.010\n#&gt; 2   2.252 0.004    0.002  0.000 0.007  0.022 0.787\n#&gt; 3   2.515 0.004    0.001  0.002 0.010  0.423 0.142\n#&gt; 4   5.660 0.309    0.014  0.306 0.087  0.063 0.005\n#&gt; 5   8.342 0.115    0.000  0.654 0.715  0.469 0.052\n#&gt; 6  10.818 0.563    0.981  0.032 0.176  0.013 0.004\n\nBelsley (1991) recommends that the sources of collinearity be diagnosed (a) only for those components with large \\(\\kappa_j\\), and (b) for those components for which the variance proportion is large (say, \\(\\ge 0.5\\)) on two or more predictors. The print method for \"colldiag\" objects has a fuzz argument controlling this.\n\nprint(cd, fuzz = 0.5)\n#&gt; Condition\n#&gt; Index  Variance Decomposition Proportions\n#&gt;           cylinder engine horse weight accel year \n#&gt; 1   1.000  .        .      .     .      .     .   \n#&gt; 2   2.252  .        .      .     .      .    0.787\n#&gt; 3   2.515  .        .      .     .      .     .   \n#&gt; 4   5.660  .        .      .     .      .     .   \n#&gt; 5   8.342  .        .     0.654 0.715   .     .   \n#&gt; 6  10.818 0.563    0.981   .     .      .     .\n\nThe mystery is solved, if you can read that table with these recommendations in mind. There are two nearly collinear relations among the predictors, corresponding to the two smallest dimensions.\n\nDimension 5 reflects the high correlation between horsepower and weight,\nDimension 6 reflects the high correlation between number of cylinders and engine displacement.\n\nNote that the high variance proportion for year (0.787) on the second component creates no problem and should be ignored because (a) the condition index is low and (b) it shares nothing with other predictors.\n\n8.2.3 Tableplots\nThe default tabular display of condition indices and variance proportions from colldiag() is what triggered the comparison to “Where’s Waldo”. It suffers from the fact that the important information — (a) how many Waldos? (b) where are they hiding — is disguised by being embedded in a sea of mostly irrelevant numbers. The simple option of using a principled fuzz factor helps considerably, but not entirely.\nThe simplified tabular display above can be improved to make the patterns of collinearity more visually apparent and to signify warnings directly to the eyes. A tableplot (Kwan et al., 2009) is a semi-graphic display that presents numerical information in a table using shapes proportional to the value in a cell and other visual attributes (shape type, color fill, and so forth) to encode other information.\nFor collinearity diagnostics, these show:\n\nthe condition indices, using squares whose background color is red for condition indices &gt; 10, brown for values &gt; 5 and green otherwise, reflecting danger, warning and OK respectively. The value of the condition index is encoded within this using a white square whose side is proportional to the value (up to some maximum value, cond.max that fills the cell).\nVariance decomposition proportions are shown by filled circles whose radius is proportional to those values and are filled (by default) with shades ranging from white through pink to red. Rounded values of those diagnostics are printed in the cells.\n\nThe tableplot below (Figure 8.5) encodes all the information from the values of colldiag() printed above. To aid perception, it uses prop.col color breaks such that variance proportions &lt; 0.3 are shaded white. The visual message is that one should attend to collinearities with large condition indices and large variance proportions implicating two or more predictors.\n\n\n\nR file: R/cars-colldiag.R\n\ntableplot(cd, title = \"Tableplot of cars data\", \n          cond.max = 30 )\n\n\n\n\n\n\nFigure 8.5: Tableplot of condition indices and variance proportions for the cars data. In column 1, the square symbols are scaled relative to a maximum condition index of 30. In the remaining columns, variance proportions (times 100) are shown as circles scaled relative to a maximum of 100.\n\n\n\n\n\n8.2.4 Collinearity biplots\nAs we have seen, the collinearity diagnostics are all functions of the eigenvalues and eigenvectors of the correlation matrix of the predictors in the regression model, or alternatively, the SVD of the \\(\\mathbf{X}\\) matrix in the linear model (excluding the constant). The standard biplot (Gabriel, 1971; Gower & Hand, 1996) (see: Section 4.3) can be regarded as a multivariate analog of a scatterplot, obtained by projecting a multivariate sample into a low-dimensional space (typically of 2 or 3 dimensions) accounting for the greatest variance in the data.\nHowever the standard biplot is less useful for visualizing the relations among the predictors that lead to nearly collinear relations. Instead, biplots of the smallest dimensions show these relations directly, and can show other features of the data as well, such as outliers and leverage points. We use prcomp(X, scale.=TRUE) to obtain the PCA of the correlation matrix of the predictors:\n\ncars.X &lt;- cars |&gt;\n  select(where(is.numeric)) |&gt;\n  select(-mpg) |&gt;\n  tidyr::drop_na()\ncars.pca &lt;- prcomp(cars.X, scale. = TRUE)\ncars.pca\n#&gt; Standard deviations (1, .., p=6):\n#&gt; [1] 2.070 0.911 0.809 0.367 0.245 0.189\n#&gt; \n#&gt; Rotation (n x k) = (6 x 6):\n#&gt;             PC1     PC2    PC3    PC4     PC5     PC6\n#&gt; cylinder -0.454 -0.1869  0.168 -0.659 -0.2711 -0.4725\n#&gt; engine   -0.467 -0.1628  0.134 -0.193 -0.0109  0.8364\n#&gt; horse    -0.462 -0.0177 -0.123  0.620 -0.6123 -0.1067\n#&gt; weight   -0.444 -0.2598  0.278  0.350  0.6860 -0.2539\n#&gt; accel     0.330 -0.2098  0.865  0.143 -0.2774  0.0337\n#&gt; year      0.237 -0.9092 -0.335  0.025 -0.0624  0.0142\n\nThe standard deviations above are the square roots \\(\\sqrt{\\lambda_j}\\) of the eigenvalues of the correlation matrix, and are returned in the sdev component of the \"prcomp\" object. The eigenvectors are returned in the rotation component, whose directions are arbitrary. Because we are interested in seeing the relative magnitude of variable vectors, we are free to multiply them by any constant to make them more visible in relation to the scores for the cars.\n\ncars.pca$rotation &lt;- -2.5 * cars.pca$rotation    # reflect & scale the variable vectors\n\nggp &lt;- fviz_pca_biplot(\n  cars.pca,\n  axes = 6:5,\n  geom = \"point\",\n  col.var = \"blue\",\n  labelsize = 5,\n  pointsize = 1.5,\n  arrowsize = 1.5,\n  addEllipses = TRUE,\n  ggtheme = ggplot2::theme_bw(base_size = 14),\n  title = \"Collinearity biplot for cars data\")\n\n# add point labels for outlying points\ndsq &lt;- heplots::Mahalanobis(cars.pca$x[, 6:5])\nscores &lt;- as.data.frame(cars.pca$x[, 6:5])\nscores$name &lt;- rownames(scores)\n\nggp + geom_text_repel(data = scores[dsq &gt; qchisq(0.95, df = 6),],\n                aes(x = PC6,\n                    y = PC5,\n                    label = name),\n                vjust = -0.5,\n                size = 5)\n\n\n\n\n\n\nFigure 8.6: Collinearity biplot of the Cars data, showing the last two dimensions. The projections of the variable vectors on the coordinate axes are proportional to their variance proportions. To reduce graphic clutter, only the most outlying observations in predictor space are identified by case labels. An extreme outlier (case 20) appears in the lower right corner.\n\n\n\n\nAs with the tabular display of variance proportions, Waldo is hiding in the dimensions associated with the smallest eigenvalues (largest condition indices). As well, it turns out that outliers in the predictor space (also high leverage observations) can often be seen as observations far from the centroid in the space of the smallest principal components.\nThe projections of the variable vectors in Figure 8.6 on the Dimension 5 and Dimension 6 axes are proportional to their variance proportions shown above. The relative lengths of these variable vectors can be considered to indicate the extent to which each variable contributes to collinearity for these two near-singular dimensions.\nThus, we see again that Dimension 6 is largely determined by engine size, with a substantial (negative) relation to cylinder. Dimension 5 has its’ strongest relations to weight and horse.\nMoreover, there is one observation, #20, that stands out as an outlier in predictor space, far from the centroid. It turns out that this vehicle, a Buick Estate wagon, is an early-year (1970) American behemoth, with an 8-cylinder, 455 cu. in, 225 horse-power engine, and able to go from 0 to 60 mph in 10 sec. (Its MPG is only slightly under-predicted from the regression model, however.)\nWith PCA and the biplot, we are used to looking at the dimensions that account for the most variation, but the answer to Where’s Waldo? is that he is hiding in the smallest data dimensions, just as he does in Figure 8.2 where the weak signals of his stripped shirt, hat and glasses are embedded in a visual field of noise. As we just saw, outliers hide there also, hoping to escape detection. These small dimensions are also implicated in ridge regression as we will see shortly (Section 8.4).",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-remedies",
    "href": "08-collinearity-ridge.html#sec-remedies",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.3 Remedies for collinearity: What can I do?",
    "text": "8.3 Remedies for collinearity: What can I do?\nCollinearity is often a data problem, for which there is no magic cure. Nevertheless there are some general guidelines and useful techniques to address this problem.\n\nPure prediction: If we are only interested in predicting / explaining an outcome, and not the model coefficients or which are “significant”, collinearity can be largely ignored. The fitted values are unaffected by collinearity, even in the case of perfect collinearity as shown in Figure 8.3 (b).\n\nStructural collinearity: Sometimes collinearity results from structural relations among the variables that relate to how they have been defined.\n\nFor example, polynomial terms, like \\(x, x^2, x^3\\) or interaction terms like \\(x_1, x_2, x_1 * x_2\\) are necessarily correlated. A simple cure is to center the predictors at their means, using \\(x - \\bar{x}, (x - \\bar{x})^2, (x - \\bar{x})^3\\) or \\((x_1 - \\bar{x}_1), (x_2 - \\bar{x}_2), (x_1 - \\bar{x}_1) * (x_2 - \\bar{x}_2)\\). Centering removes the spurious ill-conditioning, thus reducing the VIFs. Note that in polynomial models, using y ~ poly(x, 3) to specify a cubic model generates orthogonal (uncorrelated) regressors, whereas in y ~ x + I(x^2) + I(x^3) the terms have built-in correlations.\nWhen some predictors share a common cause, as in GNP or population in time-series or cross-national data, you can reduce collinearity by re-defining predictors to reflect per capita measures. In a related example with sports data, when you have cumulative totals (e.g., runs, hits, homeruns in baseball) for players over years, expressing these measures as per year will reduce the common effect of longevity on these measures.\n\n\n\nModel re-specification:\n\nDrop one or more regressors that have a high VIF, if they are not deemed to be essential to understanding the model. Care must be taken here to not omit variables which should be controlled or accounted for in interpretation.\nReplace highly correlated regressors with less correlated linear combination(s) of them. For example, two related variables, \\(x_1\\) and \\(x_2\\) can be replaced without any loss of information by replacing them with their sum and difference, \\(z_1 = x_1 + x_2\\) and \\(z_2 = x_1 - x_2\\). For instance, in a dataset on fitness, we may have correlated predictors of resting pulse rate and pulse rate while running. Transforming these to average pulse rate and their difference gives new variables which are interpretable and less correlated.\n\n\n\nStatistical remedies:\n\nTransform the predictors \\(\\mathbf{X}\\) to uncorrelated principal component scores \\(\\mathbf{Z} = \\mathbf{X} \\mathbf{V}\\), and regress \\(\\mathbf{y}\\) on \\(\\mathbf{Z}\\). These will have the identical overall model fit without loss of information. A related technique is incomplete principal components regression, where some of the smallest dimensions (those causing collinearity) are omitted from the model. The trade-off is that it may be more difficult to interpret what the model means, but this can be countered with a biplot, showing the projections of the original variables into the reduced space of the principal components.\nUse regularization methods such as ridge regression and lasso, which correct for collinearity by introducing shrinking coefficients towards 0, introducing a small amount of bias, . See the genridge package and its pkgdown documentation for visualization methods.\nuse Bayesian regression; if multicollinearity prevents a regression coefficient from being estimated precisely, then a prior on that coefficient will help to reduce its posterior variance.\n\n\n\nExample: Centering\nTo illustrate the effect of centering a predictor in a polynomial model, we generate a perfect quadratic relationship, \\(y = x^2\\) and consider the correlations of \\(y\\) with \\(x\\) and with \\((x - \\bar{x})^2\\). The correlation of \\(y\\) with \\(x\\) is 0.97, while the correlation of \\(y\\) with \\((x - \\bar{x})^2\\) is zero.\n\nx &lt;- 1:20\ny1 &lt;- x^2\ny2 &lt;- (x - mean(x))^2\nXY &lt;- data.frame(x, y1, y2)\n\n(R &lt;- cor(XY))\n#&gt;        x    y1    y2\n#&gt; x  1.000 0.971 0.000\n#&gt; y1 0.971 1.000 0.238\n#&gt; y2 0.000 0.238 1.000\n\nThe effect of centering here is remove the linear association in what is a purely quadratic relationship, as can be seen by plotting y1 and y2 against x.\n\nr1 &lt;- R[1, 2]\nr2 &lt;- R[1, 3]\n\ngg1 &lt;-\nggplot(XY, aes(x = x, y = y1)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y~x, linewidth = 2, se = FALSE) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_bw(base_size = 16) +\n  annotate(\"text\", x = 5, y = 350, size = 6,\n           label = paste(\"X Uncentered\\nr =\", round(r1, 3)))\n\ngg2 &lt;-\n  ggplot(XY, aes(x = x, y = y2)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y~x, linewidth = 2, se = FALSE) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_bw(base_size = 16) +\n  annotate(\"text\", x = 5, y = 80, size = 6,\n           label = paste(\"X Centered\\nr =\", round(r2, 3)))\n\ngg1 + gg2         # show plots side-by-side\n\n\n\n\n\n\nFigure 8.7: Centering a predictor removes the nessessary correlation in a quadratic regression\n\n\n\n\nExample: Interactions\nThe dataset genridge::Acetylene gives data from Marquardt & Snee (1975) on the yield of a chemical manufacturing process to produce acetylene in relation to reactor temperature (temp), the ratio of two components and the contact time in the reactor. A naive response surface model might suggest that yield is quadratic in time and there are potential interactions among all pairs of predictors.\n\n\ndata(Acetylene, package = \"genridge\")\nacetyl.mod0 &lt;- lm(\n  yield ~ temp + ratio + time + I(time^2) + \n          temp:time + temp:ratio + time:ratio,\n  data=Acetylene)\n\n(acetyl.vif0 &lt;- vif(acetyl.mod0))\n#&gt;       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#&gt;        383      10555      18080        564       9719       9693 \n#&gt; ratio:time \n#&gt;        225\n\nThese results are horrible! How much does centering help? I first center all three predictors and then use update() to re-fit the same model using the centered data.\n\nAcetylene.centered &lt;-\n  Acetylene |&gt;\n  mutate(temp = temp - mean(temp),\n         time = time - mean(time),\n         ratio = ratio - mean(ratio))\n\nacetyl.mod1 &lt;- update(acetyl.mod0, data=Acetylene.centered)\n(acetyl.vif1 &lt;- vif(acetyl.mod1))\n#&gt;       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#&gt;      57.09       1.09      81.57      51.49      44.67      30.69 \n#&gt; ratio:time \n#&gt;      33.33\n\nThis is far better, although still not great in terms of VIF. But, how much have we improved the situation by the simple act of centering the predictors? The square roots of the ratios of VIFs tell us the impact of centering on the standard errors.\n\nsqrt(acetyl.vif0 / acetyl.vif1)\n#&gt;       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#&gt;       2.59      98.24      14.89       3.31      14.75      17.77 \n#&gt; ratio:time \n#&gt;       2.60\n\nFinally, we use poly(time, 2) in the model for the centered data. Because there are multiple degree of freedom terms in the model, car::vif() calculates GVIFs here. The final column gives \\(\\sqrt{\\text{GVIF}^{1/2 \\text{df}}}\\), the remaining effect of collinearity on the standard errors of terms in this model.\n\nacetyl.mod2 &lt;- lm(yield ~ temp + ratio + poly(time, 2) + \n                          temp:time + temp:ratio + time:ratio,\n                  data=Acetylene.centered)\n\nvif(acetyl.mod2, type = \"term\")\n#&gt;                  GVIF Df GVIF^(1/(2*Df))\n#&gt; temp            57.09  1            7.56\n#&gt; ratio            1.09  1            1.05\n#&gt; poly(time, 2) 1733.56  2            6.45\n#&gt; temp:time       44.67  1            6.68\n#&gt; temp:ratio      30.69  1            5.54\n#&gt; ratio:time      33.33  1            5.77",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge",
    "href": "08-collinearity-ridge.html#sec-ridge",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.4 Ridge regression",
    "text": "8.4 Ridge regression\nRidge regression is an instance of a class of techniques designed to obtain more favorable predictions at the expense of some increase in bias, compared to ordinary least squares (OLS) estimation. These methods began as a way of solving collinearity problems in OLS regression with highly correlated predictors (Hoerl & Kennard, 1970). More recently ridge regression developed to a larger class of model selection methods, of which the LASSO method of Tibshirani (1996) and LAR method of Efron et al. (2004) are well-known instances. See, for example, the reviews in Vinod (1978) and McDonald (2009) for details and context omitted here. The case of ridge regression has also been extended to the case of two or more response variables (Brown & Zidek, 1980; Haitovsky, 1987).\nAn essential idea behind these methods is that the OLS estimates are constrained in some way, shrinking them, on average, toward zero, to achieve increased predictive accuracy at the expense of some increase in bias. Another common characteristic is that they involve some tuning parameter (\\(k\\)) or criterion to quantify the tradeoff between bias and variance. In many cases, analytical or computationally intensive methods have been developed to choose an optimal value of the tuning parameter, for example using generalized cross validation, bootstrap methods.\nA common means to visualize the effects of shrinkage in these problems is to make what are called univariate ridge trace plots (Section 8.5) showing how the estimated coefficients \\(\\widehat{\\boldsymbol{\\beta}}_k\\) change as the shrinkage criterion \\(k\\) increases. (An example is shown in Fig XX below.) But this only provides a view of bias. It is the wrong graphic form for a multivariate problem where we want to visualize bias in the coefficients \\(\\widehat{\\boldsymbol{\\beta}}_k\\) vs. their precision, as reflected in their estimated variances, \\(\\widehat{\\textsf{Var}} (\\widehat{\\boldsymbol{\\beta}}_k)\\). A more useful graphic plots the confidence ellipses for the coefficients, showing both bias and precision (Section 8.6). Some of the material below borrows from Friendly (2011) and Friendly (2013).\n\n8.4.1 Properties of ridge regression\nTo provide some context, I summarize the properties of ridge regression below, comparing the OLS estimates with their ridge counterparts. To avoid unnecessary details related to the intercept, assume the predictors have been centered at their means and the unit vector is omitted from \\(\\mathbf{X}\\). Further, to avoid scaling issues, we standardize the columns of \\(\\mathbf{X}\\) to unit length, so that \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is a also correlation matrix.\nThe ordinary least squares estimates of coefficients and their estimated variance covariance matrix take the (hopefully now) familiar form\n\\[\\begin{aligned}\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} = &\n    (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\mathbf{y} \\:\\: ,\\\\\n\\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}) = &\n    \\widehat{\\sigma}_{\\epsilon}^2 (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}.\n\\end{aligned} \\tag{8.2}\\]\nAs we saw ealier, one signal of the problem of collinearity is that the determinant \\(\\mathrm{det}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})\\) approaches zero as the predictors become more collinear. The inverse \\((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\) becomes numerically unstable, or does not exist if the determinant becomes zero in the case of exact dependency of one variable on the others.\nRidge regression uses a trick to avoid this. It adds a constant, \\(k\\) to the diagonal elements, replacing \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) with \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I}\\) in Equation 8.2. This drives the determinant away from zero as \\(k\\) increases. The ridge regression estimates then become,\n\\[\\begin{aligned}\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k = &\n    (\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T}\\mathbf{y}  \\\\\n                                    = & \\mathbf{G}_k \\, \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} \\:\\: ,\\\\\n\\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k) = &\n     \\widehat{\\sigma}^2  \\mathbf{G}_k (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{G}_k^\\mathsf{T}\\:\\: ,\n\\end{aligned} \\tag{8.3}\\]\nwhere \\(\\mathbf{G}_k = \\left[\\mathbf{I} + k (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\right] ^{-1}\\) is the \\((p \\times p)\\) shrinkage matrix. Thus, as \\(k\\) increases, \\(\\mathbf{G}_k\\) decreases, and drives \\(\\widehat{\\mathbf{\\beta}}^{\\mathrm{RR}}_k\\) toward \\(\\mathbf{0}\\) (Hoerl & Kennard, 1970).\nAnother insight, from the shrinkage literature, is that ridge regression can be formulated as least squares regression, minimizing a residual sum of squares, \\(\\text{RSS}(k)\\), which adds a penalty for large coefficients,\n\\[\n\\text{RSS}(k) = (\\mathbf{y}-\\mathbf{X} \\mathbf{\\beta}) ^\\mathsf{T}(\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\beta}) + k \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} \\quad\\quad (k \\ge 0)\n\\:\\: ,\n\\tag{8.4}\\] where the penalty restrict the coefficients to some squared length \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} = \\Sigma \\beta_i \\le t(k)\\).\nThe geometry of ridge regession is illustrated in Figure 8.8 for two coefficients \\(\\boldsymbol{\\beta} = (\\beta_1, \\beta_2)\\). The blue circles at the origin, having radii \\(\\sqrt{t_k}\\), show the constraint that the sum of squares of coefficients, \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} = \\beta_1^2 + \\beta_2^2\\) be less than \\(k\\). The red ellipses show contours of the covariance ellipse of \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}\\). As the shrinkage constant \\(k\\) increases, the center of these ellipses travel along the path illustrated toward \\(\\boldsymbol{\\beta} = \\mathbf{0}\\) This path is called the locus of osculation, the path along which circles or ellipses first kiss as they expand, like the pattern of ripples from rocks dropped into a pond (Friendly et al., 2013).\n\n\n\n\n\n\n\nFigure 8.8: Geometric interpretation of rigdge regression, using elliptical contours of the \\(\\text{RSS}(k)\\) function. The blue circles at the origin show the constraint that the sum of squares of coefficients, \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta}\\) be less than \\(k\\). The red ellipses show the covariance ellipse of two coefficients \\(\\boldsymbol{\\beta}\\). Ridge regression finds the point \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k\\) where the OLS contours just kiss the contraint region. _Source: @Friendly et al. (2013).\n\n\n\n\n\nEquation 8.3 is computationally expensive, potentially numerically unstable for small \\(k\\), and it is conceptually opaque, in that it sheds little light on the underlying geometry of the data in the column space of \\(\\mathbf{X}\\). An alternative formulation can be given in terms of the singular value decomposition (SVD) of \\(\\mathbf{X}\\),\n\\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\mathsf{T}\n\\]\nwhere \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are respectively \\(n\\times p\\) and \\(p\\times p\\) orthonormal matrices, so that \\(\\mathbf{U}^\\mathsf{T}\\mathbf{U} = \\mathbf{V}^\\mathsf{T}\\mathbf{V} = \\mathbf{I}\\), and \\(\\mathbf{D} = \\mathrm{diag}\\, (d_1, d_2, \\dots d_p)\\) is the diagonal matrix of ordered singular values, with entries \\(d_1 \\ge d_2 \\ge \\cdots \\ge d_p \\ge 0\\).\nBecause \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X} = \\mathbf{V} \\mathbf{D}^2 \\mathbf{V}^\\mathsf{T}\\), the eigenvalues of \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) are given by \\(\\mathbf{D}^2\\) and therefore the eigenvalues of \\(\\mathbf{G}_k\\) can be shown (Hoerl & Kennard, 1970) to be the diagonal elements of\n\\[\n\\mathbf{D}(\\mathbf{D}^2 + k \\mathbf{I} )^{-1} \\mathbf{D} = \\mathrm{diag}\\,  \\left(\\frac{d_i^2}{d_i^2 + k}\\right) \\:\\: .\n\\]\nNoting that the eigenvectors, \\(\\mathbf{V}\\) are the principal component vectors, and that \\(\\mathbf{X} \\mathbf{V} = \\mathbf{U} \\mathbf{D}\\), the ridge estimates can be calculated more simply in terms of \\(\\mathbf{U}\\) and \\(\\mathbf{D}\\) as\n\\[\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k = (\\mathbf{D}^2 + k \\mathbf{I})^{-1} \\mathbf{D} \\mathbf{U}^\\mathsf{T}\\mathbf{y} = \\left( \\frac{d_i}{d_i^2 + k}\\right) \\: \\mathbf{u}_i^\\mathsf{T}\\mathbf{y}, \\quad i=1, \\dots p \\:\\: .\n\\]\nThe terms \\(d^2_i / (d_i^2 + k) \\le 1\\) are thus the factors by which the coordinates of \\(\\mathbf{u}_i^\\mathsf{T}\\mathbf{y}\\) are shrunk with respect to the orthonormal basis for the column space of \\(\\mathbf{X}\\). The small singular values \\(d_i\\) correspond to the directions which ridge regression shrinks the most. These are the directions which contribute most to collinearity, discussed earlier.\nThis analysis also provides an alternative and more intuitive characterization of the ridge tuning constant. By analogy with OLS, where the hat matrix, \\(\\mathbf{H} = \\mathbf{X} (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\) reflects degrees of freedom \\(\\text{df} = \\mathrm{tr} (\\mathbf{H}) = p\\) corresponding to the \\(p\\) parameters, the effective degrees of freedom for ridge regression (Hastie et al., 2009) is\n\\[\\begin{aligned}\n\\text{df}_k\n    = & \\text{tr}[\\mathbf{X} (\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T}] \\\\\n    = & \\sum_i^p \\text{df}_k(i) = \\sum_i^p \\left( \\frac{d_i^2}{d_i^2 + k} \\right) \\:\\: .\n\\end{aligned} \\tag{8.5}\\]\n\\(\\text{df}_k\\) is a monotone decreasing function of \\(k\\), and hence any set of ridge constants can be specified in terms of equivalent \\(\\text{df}_k\\). Greater shrinkage corresponds to fewer coefficients being estimated.\nThere is a close connection with principal components regression mentioned in Section 8.3. Ridge regression shrinks all dimensions in proportion to \\(\\text{df}_k(i)\\), so the low variance dimensions are shrunk more. Principal components regression discards the low variance dimensions and leaves the high variance dimensions unchanged.\n\n8.4.2 The genridge package\nRidge regression and other shrinkage methods are available in several packages including MASS (the lm.ridge() function), glmnet (Friedman et al., 2023), and penalized (Goeman et al., 2022), but none of these provides insightful graphical displays. glmnet::glmnet() also implements a method for multivariate responses with a `family=“mgaussian”.\nHere, I focus in the genridge package (Friendly, 2023), where the ridge() function is the workhorse and pca.ridge() transforms these results to PCA/SVD space. vif.ridge() calculates VIFs for class \"ridge\" objects and precision() calculates precision and shrinkage measures.\nA variety of plotting functions is available for univariate, bivariate and 3D plots:\n\n\ntraceplot() Traditional univariate ridge trace plots\n\nplot.ridge() Bivariate 2D ridge trace plots, showing the covariance ellipse of the estimated coefficients\n\npairs.ridge() All pairwise bivariate ridge trace plots\n\nplot3d.ridge() 3D ridge trace plots with ellipsoids\n\nbiplot.ridge() ridge trace plots in PCA/SVD space\n\nIn addition, the pca() method for \"ridge\" objects transforms the coefficients and covariance matrices of a ridge object from predictor space to the equivalent, but more interesting space of the PCA of \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) or the SVD of \\(\\mathbf{X}\\). biplot.pcaridge() adds variable vectors to the bivariate plots of coefficients in PCA space",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge-univar",
    "href": "08-collinearity-ridge.html#sec-ridge-univar",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.5 Univariate ridge trace plots",
    "text": "8.5 Univariate ridge trace plots\nA classic example for ridge regression is Longley’s (1967) data, consisting of 7 economic variables, observed yearly from 1947 to 1962 (n=16), in the dataset datasets::longley. The goal is to predict Employed from GNP, Unemployed, Armed.Forces, Population, Year, and GNP.deflator.\n\ndata(longley, package=\"datasets\")\nstr(longley)\n#&gt; 'data.frame':  16 obs. of  7 variables:\n#&gt;  $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...\n#&gt;  $ GNP         : num  234 259 258 285 329 ...\n#&gt;  $ Unemployed  : num  236 232 368 335 210 ...\n#&gt;  $ Armed.Forces: num  159 146 162 165 310 ...\n#&gt;  $ Population  : num  108 109 110 111 112 ...\n#&gt;  $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...\n#&gt;  $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...\n\nThese data were constructed to illustrate numerical problems in least squares software at the time, and they are (purposely) perverse, in that:\n\nEach variable is a time series so that there is clearly a lack of independence among predictors.\nWorse, there is also some structural collinearity among the variables GNP, Year, GNP.deflator, and Population; for example, GNP.deflator is a multiplicative factor to account for inflation.\n\nWe fit the regression model, and sure enough, there are some extremely large VIFs. The largest, for GNP represents a multiplier of \\(\\sqrt{1788.5} = 42.3\\) on the standard errors.\n\nlongley.lm &lt;- lm(Employed ~ GNP + Unemployed + Armed.Forces + \n                            Population + Year + GNP.deflator, \n                 data=longley)\nvif(longley.lm)\n#&gt;          GNP   Unemployed Armed.Forces   Population         Year \n#&gt;      1788.51        33.62         3.59       399.15       758.98 \n#&gt; GNP.deflator \n#&gt;       135.53\n\nShrinkage values can be specified using \\(k\\) (where \\(k = 0\\) corresponds to OLS) or the equivalent degrees of freedom $ _k$ (Equation 8.5). (The function uses the notation \\(\\lambda \\equiv k\\), so the argument is lambda.) Among other quantities, ridge() returns a matrix containing the coefficients for each predictor for each shrinkage value and other quantities.\n\nlambda &lt;- c(0, 0.005, 0.01, 0.02, 0.04, 0.08)\nlridge &lt;- ridge(Employed ~ GNP + Unemployed + Armed.Forces + \n                           Population + Year + GNP.deflator, \n    data=longley, lambda=lambda)\nprint(lridge, digits = 3)\n#&gt; Ridge Coefficients:\n#&gt;        GNP      Unemployed  Armed.Forces  Population  Year   \n#&gt; 0.000  -3.4472  -1.8279     -0.6962       -0.3442      8.4320\n#&gt; 0.005  -1.0425  -1.4914     -0.6235       -0.9356      6.5665\n#&gt; 0.010  -0.1798  -1.3610     -0.5881       -1.0032      5.6563\n#&gt; 0.020   0.4995  -1.2451     -0.5476       -0.8676      4.6261\n#&gt; 0.040   0.9059  -1.1552     -0.5039       -0.5235      3.5765\n#&gt; 0.080   1.0907  -1.0864     -0.4583       -0.0860      2.6416\n#&gt;        GNP.deflator\n#&gt; 0.000   0.1574     \n#&gt; 0.005  -0.0418     \n#&gt; 0.010  -0.0261     \n#&gt; 0.020   0.0977     \n#&gt; 0.040   0.3212     \n#&gt; 0.080   0.5703\n\nThe standard univariate plot, given by traceplot(), simply plots the estimated coefficients for each predictor against the shrinkage factor \\(k\\).\n\ntraceplot(lridge, \n          X = \"lambda\",\n          xlab = \"Ridge constant (k)\",\n          xlim = c(-0.02, 0.08), cex.lab=1.25)\n\n\n\n\n\n\nFigure 8.9: Univariate ridge trace plot for the coefficients of predictors of Employment in Longley’s data via ridge regression, with ridge constants \\(k = (0, 0.005, 0.01, 0.02, 0.04, 0.08\\)). The dotted lines show optimal values for shrinkage by two criteria (HKB, LW).\n\n\n\n\nYou can see that the coefficients for Year and GNP are shrunk considerably. Differences from the \\(\\beta\\) value at \\(k =0\\) represent the bias (smaller \\(\\mid \\beta \\mid\\)) needed to achieve more stable estimates.\nThe dotted lines in Figure 8.9 show choices for the ridge constant by two commonly used criteria to balance bias against precision due to Hoerl et al. (1975) (HKB) and Lawless & Wang (1976) (LW). These values (along with a generalized cross-validation value GCV) are also stored in the “ridge” object:\n\nc(HKB=lridge$kHKB, \n  LW=lridge$kLW, \n  GCV=lridge$kGCV)\n#&gt;     HKB      LW     GCV \n#&gt; 0.00428 0.03230 0.00500\n\n\nThe shrinkage constant \\(k\\) doesn’t have much intrinsic meaning, so it is often easier to interpret the plot when coefficients are plotted against the equivalent degrees of freedom, \\(\\text{df}_k\\). OLS corresponds to \\(\\text{df}_k = 6\\) degrees of freedom in the space of six parameters, and the effect of shrinkage is to decrease the degrees of freedom, as if estimating fewer parameters. This more natural scale also makes the changes in coefficient with shrinkage more nearly linear.\n\ntraceplot(lridge, \n          X = \"df\",\n          xlim = c(4, 6.2), cex.lab=1.25)\n\n\n\n\n\n\nFigure 8.10: Univariate ridge trace plot using equivalent degrees of freedom, \\(\\text{df}_k\\) to specify shrinkage. This scale is easier to understand and makes the traces of prarameters more nearly linear.\n\n\n\n\nBut a bigger problem is that these univariate plots are the wrong kind of plot! They show the trends in increased bias (toward smaller \\(\\mid \\beta \\mid\\)) associated with larger \\(k\\), but they do not show the accompanying increase in precision (decrease in variance) achieved by allowing a bit of bias.\nFor that, we need to consider the variances and covariances of the estimated coefficients. The univariate trace plot is the wrong graphic form for what is essentially a multivariate problem, where we would like to visualize how both coefficients and their variances change with \\(k\\).",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge-bivar",
    "href": "08-collinearity-ridge.html#sec-ridge-bivar",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.6 Bivariate ridge trace plots",
    "text": "8.6 Bivariate ridge trace plots\nThe bivariate analog of the trace plot suggested by Friendly (2013) plots bivariate confidence ellipses for pairs of coefficients. Their centers, \\((\\widehat{\\beta}_i, \\widehat{\\beta}_j)\\) compared to the OLS values show the bias induced for each coefficient, and also how the change in the ridge estimate for one parameter is related to changes for other parameters.\nThe size and shapes of the covariance ellipses show directly the effect on precision of the estimates as a function of the ridge tuning constant. and their size and shape indicate sampling variance, \\(\\widehat{\\text{Var}} (\\mathbf{\\widehat{\\beta}}_{ij})\\). Here, I plot those for GNP against four of the other predictors. The plot() method for \"ridge\" objects plots these ellipses for a pair of variables.\n\nclr &lt;-  c(\"black\", \"red\", \"darkgreen\",\"blue\", \"cyan4\", \"magenta\")\npch &lt;- c(15:18, 7, 9)\nlambdaf &lt;- c(expression(~widehat(beta)^OLS), \".005\", \".01\", \".02\", \".04\", \".08\")\n\nfor (i in 2:5) {\n  plot(lridge, variables=c(1,i), \n       radius=0.5, cex.lab=1.5, col=clr, \n       labels=NULL, fill=TRUE, fill.alpha=0.2)\n  text(lridge$coef[1,1], lridge$coef[1,i], \n       expression(~widehat(beta)^OLS), cex=1.5, pos=4, offset=.1)\n  text(lridge$coef[-1,c(1,i)], lambdaf[-1], pos=3, cex=1.3)\n}\n\n\n\n\n\n\nFigure 8.11: Bivariate ridge trace plots for the coefficients of four predictors against the coefficient for GNP in Longley’s data, with λ = 0, 0.005, 0.01, 0.02, 0.04, 0.08. In most cases, the coefficients are driven toward zero, but the bivariate plot also makes clear the reduction in variance, as well as the bivariate path of shrinkage.\n\n\n\n\nAs can be seen, the coefficients for each pair of predictors trace a path generally in toward the origin (0,0), and the covariance ellipses get smaller, indicating increased precision. Sometimes, these paths are rather direct, but it takes a peculiar curvilinear route in the case of population and GNP.\nThe pairs() method for \"ridge\" objects shows all pairwise views in scatterplot matrix form. radius sets the base size of the ellipse-generating circle for the covariance ellipses.\n\npairs(lridge, radius=0.5, diag.cex = 2, \n      fill = TRUE, fill.alpha = 0.1)\n\n\n\n\n\n\nFigure 8.12: Scatterplot matrix of bivariate ridge trace plots. Each panel shows the effect of shrinkage on the covariance ellipse for a pair of predictors.\n\n\n\n\n\n8.6.1 Visualizing the bias-variance tradeoff\nThe function precision() calculates a number of measures of the effect of shrinkage of the coefficients in relation to the “size” of the covariance matrix \\(\\boldsymbol{\\mathcal{V}}_k \\equiv \\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k)\\). Larger shrinkage \\(k\\) should lead to a smaller ellipsoid for \\(\\boldsymbol{\\mathcal{V}}_k\\), indicating increased precision.\n\npdat &lt;- precision(lridge) |&gt; print()\n#&gt;       lambda   df   det  trace max.eig norm.beta\n#&gt; 0.000  0.000 6.00 -12.9 18.119  15.419     1.000\n#&gt; 0.005  0.005 5.42 -14.4  6.821   4.606     0.741\n#&gt; 0.010  0.010 5.14 -15.4  4.042   2.181     0.637\n#&gt; 0.020  0.020 4.82 -16.8  2.218   1.025     0.528\n#&gt; 0.040  0.040 4.48 -18.7  1.165   0.581     0.423\n#&gt; 0.080  0.080 4.13 -21.1  0.587   0.260     0.337\n\nHere,\n\nnorm.beta \\(= \\left \\Vert \\boldsymbol{\\beta}\\right \\Vert / \\max{\\left \\Vert \\boldsymbol{\\beta}\\right \\Vert}\\) is a summary measure of shrinkage, the normalized root mean square of the estimated coefficients. It starts at 1.0 for \\(k=0\\) and decreases with the penalty for large coefficients.\ndet \\(=\\log{| \\mathcal{V}_k |}\\) is an overall measure of variance of the coefficients. It is the (linearized) volume of the covariance ellipsoid and corresponds conceptually to Wilks’ Lambda criterion.\ntrace \\(=\\text{trace} (\\boldsymbol{\\mathcal{V}}_k)\\) is the sum of the variances and also the sum of the eigenvalues of \\(\\boldsymbol{\\mathcal{V}}_k\\), conceptually similar to Pillai’s trace criterion.\nmax.eig is the largest eigenvalue measure of size, an analog of Roy’s maximum root test.\n\nPlotting shrinkage against a measure of variance gives a direct view of the tradeoff between bias and precision. Here I plot norm.beta against det, and join the points with a curve. You can see that in this example the HKB criterion prefers a smaller degree of shrinkage, but achieves only a modest decrease in variance. But variance decreases more sharply thereafter and the LW choice achieves greater precision.\n\nShow the codelibrary(splines)\nwith(pdat, {\n  plot(norm.beta, det, type=\"b\", \n       cex.lab=1.25, pch=16, cex=1.5, col=clr, lwd=2,\n       xlab='shrinkage: ||b|| / max(||b||)',\n       ylab='variance: log |Var(b)|')\n  text(norm.beta, det, \n       labels = lambdaf, \n       cex=1.25, pos=c(rep(2,length(lambda)-1),4))\n  text(min(norm.beta), max(det), \n       labels = \"log |Variance| vs. Shrinkage\", \n       cex=1.5, pos=4)\n  })\n# find locations for optimal shrinkage criteria\nmod &lt;- lm(cbind(det, norm.beta) ~ bs(lambda, df=5), \n          data=pdat)\nx &lt;- data.frame(lambda=c(lridge$kHKB, \n                         lridge$kLW))\nfit &lt;- predict(mod, x)\npoints(fit[,2:1], pch=15, col=gray(.50), cex=1.6)\ntext(fit[,2:1], c(\"HKB\", \"LW\"), pos=3, cex=1.5, col=gray(.50))\n\n\n\n\n\n\nFigure 8.13: The tradeoff between bias and precision. Bias increases as we move away from the OLS solution, but precision increases.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#low-rank-views",
    "href": "08-collinearity-ridge.html#low-rank-views",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.7 Low-rank views",
    "text": "8.7 Low-rank views\nJust as principal components analysis gives low-dimensional views of a data set, PCA can be useful to understand ridge regression, just as it did for the problem of collinearity.\nThe pca method transforms a \"ridge\" object from parameter space, where the estimated coefficients are \\(\\beta_k\\) with covariance matrices \\(\\boldsymbol{\\mathcal{V}}_k\\), to the principal component space defined by the right singular vectors, \\(\\mathbf{V}\\), of the singular value decomposition \\(\\mathbf{U} \\mathbf{D} \\mathbf{V}^\\mathsf{T}\\) of the scaled predictor matrix, \\(\\mathbf{X}\\). In PCA space the total variance of the predictors remains the same, but it is distributed among the linear combinations that account for successively greatest variance.\n\nplridge &lt;- pca(lridge)\nplridge\n#&gt; Ridge Coefficients:\n#&gt;        dim1     dim2     dim3     dim4     dim5     dim6   \n#&gt; 0.000  1.51541  0.37939  1.80131  0.34595  5.97391  6.74225\n#&gt; 0.005  1.51531  0.37928  1.79855  0.33886  5.32221  3.68519\n#&gt; 0.010  1.51521  0.37918  1.79579  0.33205  4.79871  2.53553\n#&gt; 0.020  1.51500  0.37898  1.79031  0.31922  4.00988  1.56135\n#&gt; 0.040  1.51459  0.37858  1.77944  0.29633  3.01774  0.88291\n#&gt; 0.080  1.51377  0.37778  1.75810  0.25915  2.01876  0.47238\n\nThen, a traceplot() of the resulting \"pcaridge\" object shows how the dimensions are affected by shrinkage, shown on the scale of degrees of freedom in Figure 8.14.\n\ntraceplot(plridge, X=\"df\", \n          cex.lab = 1.2, lwd=2)\n\n\n\n\n\n\nFigure 8.14: Ridge traceplot for the longley regression viewed in PCA space. The dimensions are the linear combinations of the predictors which account for greatest variance.\n\n\n\n\nWhat may be surprising at first is that the coefficients for the first 4 components are not shrunk at all. These large dimensions are immune to ridge tuning. Rather, the effect of shrinkage is seen only on the last two dimensions. But those also are the directions that contribute most to collinearity as we saw earlier.\n\nA pairs() plot gives a dramatic representation bivariate effects of shrinkage in PCA space: the principal components of X are uncorrelated, so the ellipses are all aligned with the coordinate axes and the ellipses largely coincide for dimensions 1 to 4. You can see them shrink in one direction in the last two columns and rows.\n\npairs(plridge)\n\n\n\n\n\n\nFigure 8.15: All pairwise bivariate ridge plots shown in PCA space.\n\n\n\n\nIf we focus on the plot of dimensions 5:6, we can see where all the shrinkage action is in this representation. Generally, the predictors that are related to the smallest dimension (6) are shrunk quickly at first.\n\nplot(plridge, variables=5:6, \n     fill = TRUE, fill.alpha=0.15, cex.lab = 1.5)\ntext(plridge$coef[, 5:6], \n     label = lambdaf, \n     cex=1.5, pos=4, offset=.1)\n\n\n\n\n\n\nFigure 8.16: Bivariate ridge trace plot for the smallest two dimensions …\n\n\n\n\n\n8.7.1 Biplot view\nThe question arises how to relate this view of shrinkage in PCA space to the original predictors. The biplot is again your friend. You can project variable vectors for the predictor variables into the PCA space of the smallest dimensions, where the shrinkage action mostly occurs to see how the predictor variables relate to these dimensions.\nbiplot.pcaridge() supplements the standard display of the covariance ellipsoids for a ridge regression problem in PCA/SVD space with labeled arrows showing the contributions of the original variables to the dimensions plotted. Recall from Section 4.3 that these reflect the correlations of the variables with the PCA dimensions. The lengths of the arrows reflect the proportion of variance that each predictors shares with the components.\n\nbiplot(plridge, radius=0.5, \n       ref=FALSE, asp=1, \n       var.cex=1.15, cex.lab=1.3, col=clr,\n       fill=TRUE, fill.alpha=0.15, \n       prefix=\"Dimension \")\n#&gt; Vector scale factor set to  5.25\ntext(plridge$coef[,5:6], lambdaf, pos=2, cex=1.3)\n\n\n\n\n\n\nFigure 8.17: Biplot view of the ridge trace plot for the smallest two dimensions …\n\n\n\n\nThe biplot view in Figure 8.17 showing the two smallest dimensions is particularly useful for understanding how the predictors contribute to shrinkage in ridge regression. Here, Year and Population largely contribute to dimension 5; a contrast between (Year, Population) and GNP contributes to dimension 6.\n\n8.7.2 Summary\nThis chapter has considered the problems in regression models which stem from high correlations among the predictors. We saw that collinearity results in unstable estimates of coefficients with larger uncertainty, often dramatically so than would be the case if the predictors were uncorrelated. Collinearity can be seen as merely a “data problem” which can safely be ignored if we are only interested in prediction. When we want to understand a model, ridge regression can tame the collinearity beast by shrinking the coefficients slighty to gain greater precision in the estimates.\nBeyond these statistical considerations, the methods of this chapter highlight the roles of multivariate thinking and visualization in understanding these phenomena and the methods developed for solving them. …\n\nPackages used here:\n12 packages used here: car, carData, dplyr, factoextra, genridge, ggplot2, ggrepel, knitr, MASS, patchwork, splines, VisCollin\n\n\n\n\n\n\nBelsley, D. A. (1991). Conditioning diagnostics: Collinearity and weak data in regression. Wiley.\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley; Sons.\n\n\nBrown, P. J., & Zidek, J. V. (1980). Adaptive multivariate ridge regression. The Annals of Statistics, 8(1), 64–74. http://www.jstor.org/stable/2240743\n\n\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. The Annals of Statistics, 32(2), 407–499.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (Third edition.). SAGE.\n\n\nFox, J., & Monette, G. (1992). Generalized collinearity diagnostics. Journal of the American Statistical Association, 87(417), 178–183.\n\n\nFriedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K., Simon, N., & Yang, J. (2023). Glmnet: Lasso and elastic-net regularized generalized linear models. https://glmnet.stanford.edu\n\n\nFriendly, M. (2011). Generalized ridge trace plots: Visualizing bias and precision with the genridge R package. SCS Seminar.\n\n\nFriendly, M. (2013). The generalized ridge trace plot: Visualizing bias and precision. Journal of Computational and Graphical Statistics, 22(1), 50–68. https://doi.org/10.1080/10618600.2012.681237\n\n\nFriendly, M. (2023). Genridge: Generalized ridge trace plots for ridge regression. https://friendly.github.io/genridge/\n\n\nFriendly, M., & Kwan, E. (2009). Where’s Waldo: Visualizing collinearity diagnostics. The American Statistician, 63(1), 56–65. https://doi.org/10.1198/tast.2009.0012\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with application to principal components analysis. Biometrics, 58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGoeman, J., Meijer, R., Chaturvedi, N., & Lueder, M. (2022). Penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the cox model. https://CRAN.R-project.org/package=penalized\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman & Hall.\n\n\nGraybill, F. A. (1961). An introduction to linear statistical models. McGraw-Hill.\n\n\nHaitovsky, Y. (1987). On multivariate ridge regression. Biometrika, 74(3), 563–570. https://doi.org/10.1093/biomet/74.3.563\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference and prediction (2nd ed.). Springer. http://www-stat.stanford.edu/~tibs/ElemStatLearn/\n\n\nHocking, R. R. (2013). Methods and applications of linear models: Regression and the analysis of variance. Wiley. https://books.google.ca/books?id=iq2J-1iS6HcC\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12, 55–67.\n\n\nHoerl, A. E., Kennard, R. W., & Baldwin, K. F. (1975). Ridge regression: Some simulations. Communications in Statistics, 4(2), 105–123. https://doi.org/10.1080/03610927508827232\n\n\nKwan, E., Lu, I. R. R., & Friendly, M. (2009). Tableplot: A new tool for assessing precise predictions. Zeitschrift für Psychologie / Journal of Psychology, 217(1), 38–48. https://doi.org/10.1027/0044-3409.217.1.38\n\n\nLawless, J. F., & Wang, P. (1976). A simulation study of ridge and other regression estimators. Communications in Statistics, 5, 307–323.\n\n\nLongley, J. W. (1967). An appraisal of least squares programs for the electronic computer from the point of view of the user. Journal of the American Statistical Association, 62, 819–841. https://doi.org/https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896\n\n\nMarquardt, D. W. (1970). Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation. Technometrics, 12, 591–612.\n\n\nMarquardt, D. W., & Snee, R. D. (1975). Ridge regression in practice. The American Statistician, 29(1), 3–20. https://doi.org/10.1080/00031305.1975.10479105\n\n\nMcDonald, G. C. (2009). Ridge regression. Wiley Interdisciplinary Reviews: Computational Statistics, 1(1), 93–100. https://doi.org/10.1002/wics.14\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B: Methodological, 58, 267–288.\n\n\nVinod, H. D. (1978). A survey of ridge regression and related techniques for improvements over ordinary least squares. The Review of Economics and Statistics, 60(1), 121–131. http://www.jstor.org/stable/1924340",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#footnotes",
    "href": "08-collinearity-ridge.html#footnotes",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "This example is adapted from one by John Fox (2022), Collinearity Diagnostics↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html",
    "href": "09-hotelling.html",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "",
    "text": "9.1 \\(T^2\\) as a generalized \\(t\\)-test\nHotelling’s \\(T^2\\) (Hotelling, 1931) is an analog the square of a univariate \\(t\\) statistic, extended to the Consider the basic one-sample \\(t\\)-test, where we wish to test the hypothesis that the mean \\(\\bar{x}\\) of a set of \\(N\\) measures on a test of basic math, with standard deviation \\(s\\) does not differ from an assumed mean \\(\\mu_0 = 150\\) for a population. The \\(t\\) statistic for testing \\(\\mathcal{H}_0 : \\mu = \\mu_0\\) against the two-sided alternative, \\(\\mathcal{H}_0 : \\mu \\ne \\mu_0\\) is \\[\nt = \\frac{(\\bar{x} - \\mu_0)}{s / \\sqrt{N}} = \\frac{(\\bar{x} - \\mu_0)\\sqrt{N}}{s}\n\\]\nSquaring this gives\n\\[\nt^2 = \\frac{N (\\bar{x} - \\mu_0)^2}{s} = N (\\bar{x} - \\mu_0)(s^2)^{-1} (\\bar{x} - \\mu_0)\n\\]\nNow consider we also have measures on a test of solving word problems for the same sample. Then, a hypothesis test for the means on basic math (BM) and word problems (WP) is the test of the means of these two variables jointly equal their separate values, say, \\((150, 100)\\).\n\\[\n\\mathcal{H}_0 : \\mathbf{\\mu} = \\mathbf{\\mu_0} =\n  \\begin{pmatrix}\n    \\mu_{0,BM} \\\\ \\mu_{0,WP}\n  \\end{pmatrix}\n  =\n  \\begin{pmatrix}\n    150 \\\\ 100\n  \\end{pmatrix}\n\\]\nHotelling’s \\(T^2\\) is then the analog of \\(t^2\\), with the variance-covariance matrix \\(\\mathbf{S}\\) of the scores on (BM, WP) replacing the variance of a single score. This is nothing more than the squared Mahalanobis distance between the sample mean vector \\((\\bar{x}_{BM}, \\bar{x}_{WP})^\\mathsf{T}\\) and the hypothesized means \\(\\mathbf{\\mu}_0\\), in the metric of \\(\\mathbf{S}\\), as shown in Figure 9.1.\n\\[\\begin{aligned}\nT^2 &= N (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)^\\mathsf{T} \\; \\mathbf{S}^{-1} \\; (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0) \\\\\n    &= N D^2_M (\\bar{\\mathbf{x}}, \\mathbf{\\mu}_0)\n\\end{aligned}\\]\nFigure 9.1: Hotelling’s T^2 statistic as the squared distance between the sample means and hypothesized means relative to the variance-covariance matrix. Source: Author",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#t2-properties",
    "href": "09-hotelling.html#t2-properties",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.2 \\(T^2\\) properties",
    "text": "9.2 \\(T^2\\) properties\nAside from it’s elegant geometric interpretation Hotelling’s \\(T^2\\) has simple properties that aid in understanding the extension to more complex multivariate tests.\n\nMaximum \\(t^2\\) : Consider constructing a new variable \\(w\\) as a linear combination of the scores in a matrix \\(\\mathbf{X} = [ \\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}]\\) with weights \\(\\mathbf{a}\\), \\[\nw = a_1 \\mathbf{x_1} + a_2 \\mathbf{x_2} + \\dots + a_p \\mathbf{x_p} = \\mathbf{X} \\mathbf{a}\n\\] Hotelling’s \\(T^2\\) is then the maximum value of a univariate \\(t^2 (\\mathbf{a})\\) over all possible choices of the weights in \\(\\mathbf{a}\\). In this way, Hotellings test reduces a multivariate problem to a univariate one.\nEigenvalue : Hotelling showed that \\(T^2\\) is the one non-zero eigenvalue (latent root) \\(\\lambda\\) of the matrix \\(\\mathbf{Q}_H = N (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)^\\mathsf{T}  (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)\\) relative to \\(\\mathbf{Q}_E = \\mathbf{S}\\) that solves the equation \\[\n(\\mathbf{Q}_H - \\lambda \\mathbf{Q}_E) \\mathbf{a} = 0\n\\tag{9.1}\\] In more complex MANOVA problems, there are more than one non-zero latent roots, \\(\\lambda_1, \\lambda_2, \\dots \\lambda_s\\), and test statistics (Wilks’ \\(\\Lambda\\), Pillai and Hotelling-Lawley trace criteria, Roy’s maximum root test) are functions of these.\nEigenvector : The corresponding eigenvector is \\(\\mathbf{a} = \\mathbf{S}^{-1} (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)\\). These are the (raw) discriminant coefficients, giving the relative contribution of each variable to \\(T^2\\).\nCritical values : For a single response, the square of a \\(t\\) statistic with \\(N-1\\) degrees of freedom is an \\(F (1, N-1)\\) statistic. But we chose \\(\\mathbf{a}\\) to give the maximum \\(t^2 (\\mathbf{a})\\); this can be taken into account with a transformation of \\(T^2\\) to give an exact \\(F\\) test with the correct sampling distribution: \\[\nF^* = \\frac{N - p}{p (N-1)} T^2 \\; \\sim \\; F (p, N - p)\n\\tag{9.2}\\]\nInvariance under linear transformation : Just as a univariate \\(t\\)-test is unchanged if we apply a linear transformation to the variable, \\(x \\rightarrow a x + b\\), \\(T^2\\) is invariant under all linear (affine) transformations, \\[\n\\mathbf{x}_{p \\times 1} \\rightarrow \\mathbf{C}_{p \\times p} \\mathbf{x} + \\mathbf{b}\n\\] So, you get the same results if you convert penguins flipper lengths from millimeters to centimeters or inches. The same is true for all MANOVA tests.\nTwo-sample tests : With minor variations in notation, everything above applies to the more usual test of equality of multivariate means in a two sample test of \\(\\mathcal{H}_0 : \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2\\). \\[\nT^2 = N (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)^\\mathsf{T} \\; \\mathbf{S}_p^{-1} \\; (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)\n\\] where \\(\\mathbf{S}_p\\) is the pooled within-sample variance covariance matrix.\n\nExample\nThe data set heplots::mathscore gives (fictitious) scores on a test of basic math skills (BM) and solving word problems (WP) for two groups of \\(N=6\\) students in an algebra course, each taught by different instructors.\n\ndata(mathscore, package = \"heplots\")\nstr(mathscore)\n#&gt; 'data.frame':  12 obs. of  3 variables:\n#&gt;  $ group: Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 2 2 2 2 ...\n#&gt;  $ BM   : int  190 170 180 200 150 180 160 190 150 160 ...\n#&gt;  $ WP   : int  90 80 80 120 60 70 120 150 90 130 ...\n\nYou can carry out the test that the means for both variables are jointly equal using either Hotelling::hotelling.test() (Curran & Hersh, 2021) or car::Anova(),\n\nhotelling.test(cbind(BM, WP) ~ group, data=mathscore) |&gt; print()\n#&gt; Test stat:  64.174 \n#&gt; Numerator df:  2 \n#&gt; Denominator df:  9 \n#&gt; P-value:  0.0001213\n\nmath.mod &lt;- lm(cbind(BM, WP) ~ group, data=mathscore)\nAnova(math.mod)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;       Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; group  1     0.865     28.9      2      9 0.00012 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWhat’s wrong with just doing the two \\(t\\)-tests (or equivalent \\(F\\)-test with lm())?\n\nAnova(mod1 &lt;- lm(BM ~ group, data=mathscore))\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: BM\n#&gt;           Sum Sq Df F value Pr(&gt;F)  \n#&gt; group       1302  1    4.24  0.066 .\n#&gt; Residuals   3071 10                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnova(mod2 &lt;- lm(WP ~ group, data=mathscore))\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: WP\n#&gt;           Sum Sq Df F value Pr(&gt;F)   \n#&gt; group       4408  1    10.4  0.009 **\n#&gt; Residuals   4217 10                  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFrom this, we might conclude that the two groups do not differ significantly on Basic Math but strongly differ on Word problems. But the two univariate tests do not take the correlation among the mean differences into account.\nTo see the differences between the groups on both variables together, we draw their data (68%) ellipses, using heplots::covEllpses()\n\n\ncolors &lt;- c(\"darkgreen\", \"blue\")\ncovEllipses(mathscore[,c(\"BM\", \"WP\")], mathscore$group,\n            pooled=FALSE, \n            col = colors,\n            fill = TRUE, \n            fill.alpha = 0.05,\n            cex = 2, cex.lab = 1.5,\n            asp = 1,\n            xlab=\"Basic math\", ylab=\"Word problems\")\n# plot points\npch &lt;- ifelse(mathscore$group==1, 15, 16)\ncol &lt;- ifelse(mathscore$group==1, colors[1], colors[2])\npoints(mathscore[,2:3], pch=pch, col=col, cex=1.25)\n\n\n\n\n\n\nFigure 9.2: Data ellipses for the mathscore data, enclosing approximately 68% of the observations in each group\n\n\n\n\nWe can see that:\n\nGroup 1 &gt; Group 2 on Basic Math, but worse on Word Problems\nGroup 2 &gt; Group 1 on Word Problems, but worse on Basic Math\nWithin each group, those who do better on Basic Math also do better on Word Problems\n\nWe can also see why the univariate test, at least for Basic math is non-significant: the scores for the two groups overlap considerably on the horizontal axis. They are slightly better separated along the vertical axis for word problems. The plot also reveals why Hotelling’s \\(T^2\\) reveals such a strongly significant result: the two groups are very widely separated along an approximately 45\\(^o\\) line between them.\nA relatively simple interpretation is that the groups don’t really differ in overall math ability, but perhaps the instructor in Group 1 put more focus on basic math skills, while the instructor for Group 2 placed greater emphasis on solving word problems.\nIn Hotelling’s \\(T^2\\), the “size” of the difference between the means (labeled “1” and “2”) is assessed relative to the pooled within-group covariance matrix \\(\\mathbf{S}_p\\), which is just a size-weighted average of the two within-sample matrices, \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\),\n\\[\n\\mathbf{S}_p = [ (n_1 - 1) \\mathbf{S}_1 + (n_2 - 1) \\mathbf{S}_2 ] / (n_1 + n_2 - 2)\n\\]\nVisually, imagine sliding the the separate data ellipses to the grand mean, \\((\\bar{x}_{\\text{BM}}, \\bar{x}_{\\text{WP}})\\) and finding their combined data ellipse. This is just the data ellipse of the sample of deviations of the scores from their group means, or that of the residuals from the model lm(cbind(BM, WP) ~ group, data=mathscore)\nTo see this, we plot \\(\\mathbf{S}_1\\), \\(\\mathbf{S}_2\\) and \\(\\mathbf{S}_p\\) together,\n\ncovEllipses(mathscore[,c(\"BM\", \"WP\")], mathscore$group,\n            col = c(colors, \"red\"),\n            fill = c(FALSE, FALSE, TRUE), \n            fill.alpha = 0.3,\n            cex = 2, cex.lab = 1.5,\n            asp = 1,\n            xlab=\"Basic math\", ylab=\"Word problems\")\n\n\n\n\n\n\nFigure 9.3: Data ellipses and the pooled covariance matrix mathscore data.\n\n\n\n\nOne of the assumptions of the \\(T^2\\) test (and of MANOVA) is that the within-group variance covariance matrices, \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\), are the same. In Figure 9.3, you can see how the shapes of \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\) are very similar, differing in that the variance of word Problems is slightly greater for group 2. In Chapter XX we take of the topic of visualizing tests of this assumption, based on Box’s \\(M\\)-test.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#he-plot-and-discriminant-axis",
    "href": "09-hotelling.html#he-plot-and-discriminant-axis",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.3 HE plot and discriminant axis",
    "text": "9.3 HE plot and discriminant axis\nAs we describe in detail in Chapter XX, all the information relevant to the \\(T^2\\) test and MANOVA can be captured in the remarkably simple Hypothesis Error plot, which shows the relative size of two data ellipses,\n\n\n\\(\\mathbf{H}\\): the data ellipse of the fitted values, which are just the group means on the two variables, \\(\\bar{\\mathbf{x}}\\), corresponding to \\(\\mathbf{Q}_H\\) in Equation 9.1. In case of \\(T^2\\), the \\(\\mathbf{H}\\) matrix is of rank 1, so the “ellipse” plots as a line.\n\n\n# calculate H directly\nfit &lt;- fitted(math.mod)\nxbar &lt;- colMeans(mathscore[,2:3])\nN &lt;- nrow(mathscore)\ncrossprod(fit) - N * outer(xbar, xbar)\n#&gt;       BM    WP\n#&gt; BM  1302 -2396\n#&gt; WP -2396  4408\n\n# same as: SSP for group effect from Anova\nmath.aov &lt;- Anova(math.mod)\n(H &lt;- math.aov$SSP)\n#&gt; $group\n#&gt;       BM    WP\n#&gt; BM  1302 -2396\n#&gt; WP -2396  4408\n\n\n\n\\(\\mathbf{E}\\): the data ellipse of the residuals, the deviations of the scores from the group means, \\(\\mathbf{x} - \\bar{\\mathbf{x}}\\), corresponding to \\(\\mathbf{Q}_E\\).\n\n\n# calculate E directly\nresids &lt;- residuals(math.mod)\ncrossprod(resids)\n#&gt;      BM   WP\n#&gt; BM 3071 2808\n#&gt; WP 2808 4217\n\n# same as: SSPE from Anova\n(E &lt;- math.aov$SSPE)\n#&gt;      BM   WP\n#&gt; BM 3071 2808\n#&gt; WP 2808 4217\n\n\n9.3.1 heplot()\n\nheplots::heplot() takes the model object, extracts the \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) matrices (from summary(Anova(math.mod))) and plots them. There are many options to control the details.\n\nheplot(math.mod, \n       fill=TRUE, lwd = 3,\n       asp = 1,\n       cex=2, cex.lab=1.8,\n       xlab=\"Basic math\", ylab=\"Word problems\")\n\n\n\n\n\n\nFigure 9.4: Hypothesis error plot of the mathscore data. The line through the group means is the H ellipse, which plots as a line here. The red ellipse labeled ‘Error’ represents the pooled within-group covariance matrix.\n\n\n\n\nBut the HE plot offers more:\n\nA visual test of significance: the \\(\\mathbf{H}\\) ellipse is scaled so that it projects anywhere outside the \\(\\mathbf{E}\\) ellipse, if and only if the test is significant at a given \\(\\alpha\\) level (\\(\\alpha = 0.05\\) by default)\nThe \\(\\mathbf{H}\\) ellipse, which appears as a line, goes through the means of the two groups. This is also the discriminant axis, the direction in the space of the variables which maximally discriminates between the groups. That is, if we project the data points onto this line, we get the linear combination \\(w\\) which has the maximum possible univariate \\(t^2\\).\n\nYou can see how the HE plot relates to the plots of the separate data ellipses by overlaying them in a single figure. We also plot the scores on the discriminant axis, by using this small function to find the orthogonal projection of a point \\(\\mathbf{a}\\) on the line joining two points, \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\), which in math is \\(\\mathbf{p}_1 + \\frac{\\mathbf{d}^\\mathsf{T} (\\mathbf{a} - \\mathbf{p}_1)} {\\mathbf{d}^\\mathsf{T} \\mathbf{d}}\\), letting \\(\\mathbf{d} = \\mathbf{p}_1 - \\mathbf{p}_2\\).\n\ndot &lt;- function(x, y) sum(x*y)\nproject_on &lt;- function(a, p1, p2) {\n  a &lt;- as.numeric(a)\n  p1 &lt;- as.numeric(p1)\n  p2 &lt;- as.numeric(p2)\n  dot &lt;- function(x,y) sum( x * y)  \n  t &lt;- dot(p2-p1, a-p1) / dot(p2-p1, p2-p1)\n  C &lt;- p1 + t*(p2-p1)\n  C\n}\n\nThen, we run the same code as before to plot the data ellipses, and follow this with a call to heplot() using the option add=TRUE which adds to an existing plot. Following this, we find the group means and draw lines projecting the points on the line between them.\n\ncovEllipses(mathscore[,c(\"BM\", \"WP\")], mathscore$group,\n            pooled=FALSE, \n            col = colors,\n            cex=2, cex.lab=1.5,\n            asp=1, \n            xlab=\"Basic math\", ylab=\"Word problems\"\n            )\npch &lt;- ifelse(mathscore$group==1, 15, 16)\ncol &lt;- ifelse(mathscore$group==1, \"red\", \"blue\")\npoints(mathscore[,2:3], pch=pch, col=col, cex=1.25)\n\n# overlay with HEplot (add = TRUE)\nheplot(math.mod, \n       fill=TRUE, \n       cex=2, cex.lab=1.8, \n       fill.alpha=0.2, lwd=c(1,3),\n       add = TRUE, \n       error.ellipse=TRUE)\n\n# find group means\nmeans &lt;- mathscore |&gt;\n  group_by(group) |&gt;\n  summarize(BM = mean(BM), WP = mean(WP))\n\nfor(i in 1:nrow(mathscore)) {\n  gp &lt;- mathscore$group[i]\n  pt &lt;- project_on( mathscore[i, 2:3], means[1, 2:3], means[2, 2:3]) \n  segments(mathscore[i, \"BM\"], mathscore[i, \"WP\"], pt[1], pt[2], lwd = 1.2)\n}\n\n\n\n\n\n\nFigure 9.5: HE plot overlaid on top of the within-group data ellipses, with lines showing the projection of each point on the discriminant axis.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#discriminant-analysis",
    "href": "09-hotelling.html#discriminant-analysis",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.4 Discriminant analysis",
    "text": "9.4 Discriminant analysis\nDiscriminant analysis for two-group designs or for one-way MANOVA essentially turns the problem around: Instead of asking whether the mean vectors for two or more groups are equal, discriminant analysis tries to find the linear combination \\(w\\) of the response variables that has the greatest separation among the groups, allowing cases to be best classified. It was developed by Fisher (1936) as a solution to the biological taxonomy problem of developing a rule to classify instances of flowers—in his famous case, Iris flowers—into known species (I. setosa, I. versicolor, I. virginica) on the basis of multiple measurements (length and width of their sepals and petals).\n\n(math.lda &lt;- MASS::lda(group ~ ., data=mathscore))\n#&gt; Call:\n#&gt; lda(group ~ ., data = mathscore)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;   1   2 \n#&gt; 0.5 0.5 \n#&gt; \n#&gt; Group means:\n#&gt;    BM    WP\n#&gt; 1 178  83.3\n#&gt; 2 158 121.7\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;        LD1\n#&gt; BM -0.0835\n#&gt; WP  0.0753\n\nThe coefficients give \\(w = -0.84 \\text{BM} + 0.75 \\text{WP}\\). This is exactly the direction given by the line for the \\(\\mathbf{H}\\) ellipse in Figure 9.5.\nTo round this out, we can calculate the discriminant scores by multiplying the matrix \\(\\mathbf{X}\\) by the vector \\(\\mathbf{a} = \\mathbf{S}^{-1} (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)\\) of the discriminant weights.\n\nmath.lda$scaling\n#&gt;        LD1\n#&gt; BM -0.0835\n#&gt; WP  0.0753\n\nscores &lt;- cbind(group = mathscore$group,\n                as.matrix(mathscore[, 2:3]) %*% math.lda$scaling) |&gt;\n  as.data.frame()\nscores |&gt;\n  group_by(group) |&gt;\n  slice(1:3)\n#&gt; # A tibble: 6 × 2\n#&gt; # Groups:   group [2]\n#&gt;   group   LD1\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1 -9.09\n#&gt; 2     1 -8.17\n#&gt; 3     1 -9.01\n#&gt; 4     2 -4.33\n#&gt; 5     2 -4.58\n#&gt; 6     2 -5.75\n\nThen a \\(t\\)-test on these scores gives Hotelling’s \\(T\\), accessed via the statistic component of t.test()\n\nt &lt;- t.test(LD1 ~ group, data=scores)$statistic\nc(t, T2 =t^2)\n#&gt;     t  T2.t \n#&gt; -8.01 64.17\n\nFinally, it is instructive to compare violin plots for the three measures, BM, WP and LD1. To do this with ggplot2 requires reshaping the data from wide to long format so the plots can be faceted.\n\nscores &lt;- mathscore |&gt;\n  bind_cols(LD1 = scores[, \"LD1\"]) \n\nscores |&gt;\n  tidyr::gather(key = \"measure\", value =\"Score\", BM:LD1) |&gt;\n  mutate(measure = factor(measure, levels = c(\"BM\", \"WP\", \"LD1\"))) |&gt;\n  ggplot(aes(x = group, y = Score, color = group, fill = group)) +\n    geom_violin(alpha = 0.2) +\n    geom_jitter(width = .2, size = 2) +\n    facet_wrap( ~ measure, scales = \"free\", labeller = label_both) +\n    scale_fill_manual(values = c(\"darkgreen\", \"blue\")) +\n    scale_color_manual(values = c(\"darkgreen\", \"blue\")) +\n    theme_bw(base_size = 14) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 9.6: Violin plots comparing group 1 and 2 for the two observed measures and the linear discriminant score.\n\n\n\n\nYou can readily see how well the groups are separated on the discriminant axes, relative to the two individual variables.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#more-variables",
    "href": "09-hotelling.html#more-variables",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.5 More variables",
    "text": "9.5 More variables\nThe mathscore data gave a simple example with two outcomes to explain the essential ideas behind Hotelling’s \\(T^2\\) and multivariate tests. Multivariate methods become increasingly useful as the number of response variables increases because it is harder to show them all together and see how they relate to differences between groups.\nA classic example is the dataset mbclust::banknote, containing six size measures made on 100 genuine and 100 counterfeit old-Swiss 1000-franc bank notes (Flury & Riedwyl, 1988). The goal is to see how well the real and fake banknotes can be distinguished. The measures are the Length and Diagonal lengths of a banknote and the Left, Right, Top and Bottom edge margins in mm.\nBefore considering hypothesis tests, let’s look at some exploratory graphics. Figure 9.7 shows univariate violin and boxplots of each of the measures. To make this plot, faceted by measure, I first reshape the data from wide to long and make measure a factor with levels in the order of the variables in the data set.\n\n\ndata(banknote, package= \"mclust\")\nbanknote |&gt;\n  tidyr::gather(key = \"measure\", \n                value = \"Size\", \n                Length:Diagonal) |&gt; \n  mutate(measure = factor(measure, \n                          levels = c(names(banknote)[-1]))) |&gt; \n\n  ggplot(aes(x = Status, y = Size, color = Status)) +\n  geom_violin(aes(fill = Status), alpha = 0.2) +           # (1)\n  geom_jitter(width = .2, size = 1.2) +                    # (2)\n  geom_boxplot(width = 0.25,                               # (3)\n               linewidth = 1.1, \n               color = \"black\", \n               alpha = 0.5) +\n  labs(y = \"Size (mm)\") +\n  facet_wrap( ~ measure, scales = \"free\", labeller = label_both) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nFigure 9.7: Overlaid violin and boxplots of the banknote variables. The violin plots give a sense of the shapes of the distributions, while the boxplots highlight the center and spread.\n\n\n\n\nA quick glance at Figure 9.7 shows that the counterfeit and genuine bills differ in their means on most of the measures, with the counterfeit ones slightly larger on Left, Right, Bottom and Top margins. But univariate plots don’t give an overall sense of how these variables are related to one another.\n\n\n\n\n\n\nGraph craft: Layers and transparency\n\n\n\nFigure 9.7 is somewhat complex, so it is useful to understand the steps needed to make this figure show what I wanted. The plot in each panel contains three layers:\n\nthe violin plot based on a density estimate, showing the shape of each distribution;\nthe data points, but they are jittered horizontally using geom_jiter() because otherwise they would all overlap on the X axis;\nthe boxplot, showing the center (median) and spread (IQR) of each distribution.\n\nIn composing graphs with layers, order matters, and also does the alpha transparency, because each layer adds data ink on top of earlier ones. I plotted these in the order shown because I wanted the violin plot to provide the background, and the boxplot to show a simple univariate summary, not obscured by the other layers. The alpha values allow the data ink to be blended for each layer, and in this case, alpha = 0.5 for the boxplot let the earlier layers show through.\n\n\n\n9.5.1 Biplots\nMultivariate relations among these six variables could be explored in data space using scatterplots or other methods, but I turn to my trusty multivariate juicer, a biplot, to give a 2D summary. Two dimensions account for 70% of the total variance of all the banknotes, while three would give 85%.\n\nbanknote.pca &lt;- prcomp(banknote[, -1], scale = TRUE)\nsummary(banknote.pca)\n#&gt; Importance of components:\n#&gt;                          PC1   PC2   PC3   PC4    PC5    PC6\n#&gt; Standard deviation     1.716 1.131 0.932 0.671 0.5183 0.4346\n#&gt; Proportion of Variance 0.491 0.213 0.145 0.075 0.0448 0.0315\n#&gt; Cumulative Proportion  0.491 0.704 0.849 0.924 0.9685 1.0000\n\nThe biplot in Figure 9.8 gives a nicely coherent overview, at least in two dimensions. The first component shows the positive correlations among the measures of the margins, where the counterfeit bills are larger than the real ones and a negative correlation of the Diagonal with the other measures. The length of bills only distinguishes the types of banknotes on the second dimension.\n\nbanknote.pca &lt;- reflect(banknote.pca)\nggbiplot(banknote.pca,\n   obs.scale = 1, var.scale = 1,\n   groups = banknote$Status,\n   ellipse = TRUE, \n   ellipse.level = 0.5, \n   ellipse.alpha = 0.1, \n   ellipse.linewidth = 0,\n   varname.size = 4,\n   varname.color = \"black\") +\n  labs(fill = \"Status\", \n       color = \"Status\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = 'top')\n\n\n\n\n\n\nFigure 9.8: Biplot of the banknote variables, showing how the size measurements are related to each other. The points and data ellipses for the component scores are colored by Status, showing how the counterfeit and genuine bills are distinguished by these measures.\n\n\n\n\n\n9.5.2 Testing mean differences\nAs noted above, Hotelling’s \\(T^2\\) is equivalent to a one-way MANOVA, fitting the size measures to the Status of the banknotes. Anova() reports only the \\(F\\)-statistic based on Pillai’s trace criterion.\n\nbanknote.mlm &lt;- lm(cbind(Length, Left, Right, Bottom, Top, Diagonal) ~ Status,\n                    data = banknote)\nAnova(banknote.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;        Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Status  1     0.924      392      6    193 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nYou can see all the multivariate test statistics with the summary() method for \"Anova.mlm\" objects. With two groups, and hence a 1 df test, these all translate into identical \\(F\\)-statistics.\n\nsummary(Anova(banknote.mlm)) |&gt; print(SSP = FALSE)\n#&gt; \n#&gt; Type II MANOVA Tests:\n#&gt; \n#&gt; ------------------------------------------\n#&gt;  \n#&gt; Term: Status \n#&gt; \n#&gt; Multivariate Tests: Status\n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1      0.92      392      6    193 &lt;2e-16 ***\n#&gt; Wilks             1      0.08      392      6    193 &lt;2e-16 ***\n#&gt; Hotelling-Lawley  1     12.18      392      6    193 &lt;2e-16 ***\n#&gt; Roy               1     12.18      392      6    193 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIf you wish, you can extract the univariate \\(t\\)-tests or equivalent \\(F = t^2\\) statistics from the \"mlm\" object using broom::tidy.mlm(). What is given as the estimate is the difference in the mean for the genuine banknotes relative to the counterfeit ones.\n\nbroom::tidy(banknote.mlm) |&gt; \n  filter(term != \"(Intercept)\") |&gt;\n  dplyr::select(-term) |&gt;\n  rename(t = statistic) |&gt;\n  mutate(F = t^2) |&gt;\n  relocate(F, .after = t)\n#&gt; # A tibble: 6 × 6\n#&gt;   response estimate std.error      t      F  p.value\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 Length      0.146    0.0524   2.79   7.77 5.82e- 3\n#&gt; 2 Left       -0.357    0.0445  -8.03  64.5  8.50e-14\n#&gt; 3 Right      -0.473    0.0464 -10.2  104.   6.84e-20\n#&gt; 4 Bottom     -2.22     0.130  -17.1  292.   7.78e-41\n#&gt; 5 Top        -0.965    0.0909 -10.6  113.   3.85e-21\n#&gt; 6 Diagonal    2.07     0.0715  28.9  836.   5.35e-73\n\nThe individual \\(F_{(1, 198)}\\) statistics can be compared to the \\(F_{(6, 193)} = 392\\) value for the overall multivariate test. While all of the individual tests are highly significant, the average of the univariate \\(F\\)s is only 236. The multivariate test gains power by taking the correlations of the size measures into account.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#variance-accounted-for-eta-square-eta2",
    "href": "09-hotelling.html#variance-accounted-for-eta-square-eta2",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.6 Variance accounted for: Eta square (\\(\\eta^2\\))",
    "text": "9.6 Variance accounted for: Eta square (\\(\\eta^2\\))\nIn a univariate multiple regression model, the coefficient of determination \\(R^2 = \\text{SS}_H / \\text{SS}_\\text{Total}\\) gives the proportion of variance accounted for by hypothesized terms in \\(H\\) relative to the total variance. An analog for ANOVA-type models with categorical, group factors as predictors is \\(\\eta^2\\) (Pearson, 1903), defined as \\[\n\\eta^2 = \\frac{\\text{SS}_\\text{Between groups}}{\\text{SS}_\\text{Total}}\n\\] For multivariate response models, the generalization of \\(\\eta^2\\) uses multivariate analogs of these sums of squares, \\(\\mathbf{Q}_H\\) and \\(\\mathbf{Q}_T = \\mathbf{Q}_H + \\mathbf{Q}_E\\), and there are different different calculations for a single measure corresponding to the various test statistics (Wilks’ \\(\\Lambda\\), etc.), as described in Chapter XX.\nLet’s calculate the \\(\\eta^2\\) for the multivariate model banknote.mlm with Status as the only predictor, giving \\(\\eta^2 = 0.92\\), or 92% of the total variance.\n\nheplots::etasq(banknote.mlm)\n#&gt;        eta^2\n#&gt; Status 0.924\n\nThis can be compared (unfavorably) to the principal components analysis and the biplot in Figure 9.8, where two components accounted for 70% of total variance and it took four PCA dimensions to account for over 90%. The goals of PCA and MANOVA are different, of course, but they are both concerned with accounting for variance of multivariate data. We will meet another multivariate juicer, canonical discriminant analysis in Chapter XX.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#exercises",
    "href": "09-hotelling.html#exercises",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.7 Exercises",
    "text": "9.7 Exercises\n\nThe value of Hotelling’s \\(T^2\\) found by hotelling.test() is 64.17. The value of the equivalent \\(F\\) statistic found by Anova() is 28.9. Verify that Equation 9.2 gives this result.\n\nPackage summary\n\nPackages used here:\n11 packages used here: broom, car, carData, corpcor, dplyr, ggbiplot, ggplot2, heplots, Hotelling, knitr, tidyr\n\n\n\n\n\n\nCurran, J., & Hersh, T. (2021). Hotelling: Hotelling’s t^2 test and variants. https://CRAN.R-project.org/package=Hotelling\n\n\nFisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x\n\n\nFlury, B., & Riedwyl, H. (1988). Multivariate statistics: A practical approach. Chapman & Hall.\n\n\nHotelling, H. (1931). The generalization of Student’s ratio. The Annals of Mathematical Statistics, 2(3), 360–378. https://doi.org/10.1214/aoms/1177732979\n\n\nPearson, K. (1903). I. Mathematical contributions to the theory of evolution. —XI. On the influence of natural selection on the variability and correlation of organs. Philosophical Transactions of the Royal Society of London, 200(321–330), 1–66. https://doi.org/10.1098/rsta.1903.0001",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html",
    "href": "10-mlm-review.html",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "10.1 Structure of the MLM\nIn each of these cases, the underlying MLM is given most compactly using the matrix equation,\n\\[\n\\mathord{\\mathop{\\mathbf{Y}}\\limits_{n \\times p}} =\n\\mathord{\\mathop{\\mathbf{X}}\\limits_{n \\times (q+1)}} \\, \\mathord{\\mathop{\\mathbf{B}}\\limits_{(q+1) \\times p}} + \\mathord{\\mathop{\\mathbf{\\boldsymbol{\\Large\\varepsilon}}}\\limits_{n \\times p}} \\:\\: ,\n\\tag{10.1}\\]\nwhere\nWhen it is useful to refer symbolically to the rows \\(\\mathbf{b}_i^\\mathsf{T}\\) of coefficients for the regressors \\(x_0, x_1, x_2, \\dots, x_i, \\dots x_q\\), these are expressed as:\n\\[\n\\mathord{\\mathop{\\mathbf{B}}\\limits_{(q+1) \\times p}} =\n\\begin{pmatrix}\n  \\mathbf{b}_{0}^\\mathsf{T}  \\\\\n  \\mathbf{b}_{1}^\\mathsf{T}  \\\\\n  \\vdots                     \\\\\n  \\mathbf{b}_{i}^\\mathsf{T}  \\\\\n  \\vdots                     \\\\\n  \\mathbf{b}_{q}^\\mathsf{T}  \\\\\n\\end{pmatrix}\n\\]\nThe structure of the model matrix \\(\\mathbf{X}\\) is the same as the univariate linear model, and may contain, therefore,",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#structure-of-the-mlm",
    "href": "10-mlm-review.html#structure-of-the-mlm",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "\\(\\mathbf{Y} = (\\mathbf{y}_1 , \\mathbf{y}_2, \\dots , \\mathbf{y}_j, \\dots, \\mathbf{y}_p )\\) is the matrix of \\(n\\) observations on \\(p\\) responses;\n\n\\(\\mathbf{X}\\) is the model matrix with columns \\(\\mathbf{x}_i\\) for \\(q\\) regressors, which typically includes an initial column \\(\\mathbf{x}_0\\) of 1s for the intercept;\n\n\\(\\mathbf{B} = ( \\mathbf{b}_1 , \\mathbf{b}_2 , \\dots \\mathbf{b}_j \\dots, \\mathbf{b}_p )\\) is a matrix of regression coefficients, one column \\(\\mathbf{b}_j\\) for each response variable;\n\n\\(\\boldsymbol{\\Large\\varepsilon}\\) is a matrix of errors in predicting \\(\\mathbf{Y}\\).\n\n\n\n\n\n\n\nquantitative predictors, such as age, income, years of education\n\n\ntransformed predictors like \\(\\sqrt{\\text{age}}\\) or \\(\\log{(\\text{income})}\\)\n\n\npolynomial terms: \\(\\text{age}^2\\), \\(\\text{age}^3, \\dots\\) (using poly(age, k) in R)\n\ncategorical predictors (“factors”), such as treatment (Control, Drug A, drug B), or sex; internally a factor with k levels is transformed to k-1 dummy (0, 1) variables, representing comparisons with a reference level, typically the first.\n\ninteraction terms, involving either quantitative or categorical predictors, e.g., age * sex, treatment * sex.\n\n\n10.1.1 Assumptions\nJust as in univariate models, the assumptions of the multivariate linear model almost entirely concern the behavior of the errors (residuals). Let \\(\\mathbf{\\epsilon}_{i}^{\\prime}\\) represent the \\(i\\)th row of \\(\\boldsymbol{\\Large\\varepsilon}\\). Then it is assumed that:\n\n\nNormality: The residuals, \\(\\mathbf{\\epsilon}_{i}^{\\prime}\\) are distributed as multivariate normal, \\(\\mathcal{N}_{p}(\\mathbf{0},\\boldsymbol{\\Sigma})\\), where \\(\\mathbf{\\Sigma}\\) is a non-singular error-covariance matrix. Statistical tests of multivariate normality of the residuals include the Shapiro-Wilk (Shapiro & Wilk, 1965) and Mardia (Mardia, 1970) tests (in the MVN package);  however this is often better assessed visually using a \\(\\chi^2\\) QQ plot of Mahalanobis squared distance against their corresponding \\(\\chi^2_p\\) values using heplots::cqplot().\n\nHomoscedasticity: The error-covariance matrix \\(\\mathbf{\\Sigma}\\) is constant across all observations and grouping factors. Graphical methods to determine if this assumption is met are described in Chapter 12.\n\nIndependence: \\(\\mathbf{\\epsilon}_{i}^{\\prime}\\) and \\(\\mathbf{\\epsilon}_{j}^{\\prime}\\) are independent for \\(i\\neq j\\), so knowing the data for case \\(i\\) gives no information about case \\(j\\) (as would be true if the data consisted of pairs of husbands and wives);\nThe predictors, \\(\\mathbf{X}\\), are fixed and measured without error or at least they are independent of the errors, \\(\\boldsymbol{\\Large\\varepsilon}\\).\n\nThese statements are simply the multivariate analogs of the assumptions of normality, constant variance and independence of the errors in univariate models. Note that it is unnecessary to assume that the predictors (regressors, columns of \\(\\mathbf{X}\\)) are normally distributed.\nImplicit in the above is perhaps the most important assumption—that the model has been correctly specified. This means:\n\nLinearity: The form of the relations between each \\(\\mathbf{y}\\) and the \\(\\mathbf{x}\\)s is correct. Typically this means that the relations are linear, but if not, we have specified a correct transformation of \\(\\mathbf{y}\\) and/or \\(\\mathbf{x}\\).\nCompleteness: No relevant predictors have been omitted from the model.\nAdditive effects: The combined effect of different predictors is the sum of their individual effects.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#fitting-the-model",
    "href": "10-mlm-review.html#fitting-the-model",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.2 Fitting the model",
    "text": "10.2 Fitting the model\nThe least squares (and also maximum likelihood) solution for the coefficients \\(\\mathbf{B}\\) is given by\n\n\n\n\\[\n\\widehat{\\mathbf{B}} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{Y} \\:\\: .\n\\]\nThis is precisely the same as fitting the separate responses \\(\\mathbf{y}_1 , \\mathbf{y}_2 , \\dots , \\mathbf{y}_p\\), and placing the estimated coefficients \\(\\widehat{\\mathbf{b}}_i\\) as columns in \\(\\widehat{\\mathbf{B}}\\)\n\\[\n\\widehat{\\mathbf{B}} = [ \\widehat{\\mathbf{b}}_1, \\widehat{\\mathbf{b}}_2, \\dots , \\widehat{\\mathbf{b}}_p] \\:\\: .\n\\] In R, we fit the multivariate linear model with lm() simply by giving a collection of response variables y1, y2, ... on the left-hand side of the model formula, wrapped in cbind() which combines them to form a matrix response.\n\nlm(cbind(y1, y2, y3) ~ x1 + x2 + ..., data=)\n\nIn the presence of possible outliers, robust methods are available for univariate linear models (e.g., MASS::rlm()). So too, heplots::robmlm() provides robust estimation in the multivariate case.\n\n10.2.1 Sums of squares\nIn univariate response models, statistical tests and model summaries (like \\(R^2\\)) are based on the familiar decomposition of the total sum of squares \\(SS_T\\) into regression or hypothesis (\\(SS_H\\)) and error (\\(SS_E\\)) sums of squares. In the multivariate linear model each of these becomes a \\(p \\times p\\) matrix \\(SSP\\) containing sums of squares for the \\(p\\) responses on the diagonal and sums of cross products in the off-diagonal. elements. For the MLM this is expressed as\n\\[\\begin{aligned}\n\\underset{(p\\times p)}{\\mathbf{SSP}_{T}}\n& = && \\mathbf{Y}^{\\prime} \\mathbf{Y} - n \\overline{\\mathbf{y}}\\,\\overline{\\mathbf{y}}^{\\prime} \\\\\n& = && \\left(\\widehat {\\mathbf{Y}}^{\\prime}\\widehat{\\mathbf{Y}} - n\\overline{\\mathbf{y}}\\,\\overline{\\mathbf{y}}^{\\prime} \\right) + \\widehat{\\boldsymbol{\\Large\\varepsilon}}^{\\prime}\\widehat{\\boldsymbol{\\Large\\varepsilon}} \\\\\n& = &&  \\mathbf{SSP}_{H} + \\mathbf{SSP}_{E} \\\\\n& \\equiv && \\mathbf{H} + \\mathbf{E} \\:\\: ,\n\\end{aligned}\\]\nwhere \\(\\overline{\\mathbf{y}}\\) is the \\((p\\times 1)\\) vector of means for the response variables; \\(\\widehat{\\mathbf{Y}} = \\mathbf{X}\\widehat{\\mathbf{B}}\\) is the matrix of fitted values; and \\(\\widehat{\\boldsymbol{\\Large\\varepsilon}} = \\mathbf{Y} -\\widehat{\\mathbf{Y}}\\) is the matrix of residuals. This is the decomposition that we visualize in HE plots, where the size and direction of \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) can be represented as ellipsoids.\nThe univariate \\(F\\) test statistic, \\[F = \\frac{\\text{SS}_H/\\text{df}_h}{\\text{SS}_E/\\text{df}_e} = \\frac{\\mathsf{Var}(H)}{\\mathsf{Var}(E)} \\:\\: ,\n\\] assesses “how big” \\(\\text{SS}_H\\) is, relative to \\(\\text{SS}_E\\), the variance accounted for by a hypothesized model or model terms relative to error variance. In the multivariate analog \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) are both \\(p \\times p\\) matrices, and \\(\\mathbf{H}\\) “divided by” \\(\\mathbf{E}\\) becomes \\(\\mathbf{H}\\mathbf{E}^{-1}\\). The answer, “how big” is expressed in terms of the \\(p\\) eigenvalues \\(\\lambda_i, i = 1, 2, \\dots p\\) of \\(\\mathbf{H}\\mathbf{E}^{-1}\\). These are the values \\(\\lambda\\) for which \\[\n\\mathrm{det}(\\mathbf{H}\\mathbf{E}^{-1} - \\lambda \\mathbf{I}) = 0 \\:\\: .\n\\] The solution gives the \\(\\lambda_i\\) as the eigenvalues, with vectors \\(\\mathbf{v}_i\\) as the corresponding eigenvectors, \\[\n\\mathbf{H}\\mathbf{E}^{-1} \\; \\lambda_i = \\lambda_i \\mathbf{v}_i \\:\\: .\n\\tag{10.2}\\]\nHowever, when the hypothesized model terms have \\(\\text{df}_h\\) degrees of freedom (columns of the \\(\\mathbf{X}\\) matrix for that term), \\(\\mathbf{H}\\) is of rank \\(\\text{df}_h\\), so only \\(s=\\min(p, \\text{df}_h)\\) eigenvalues can be non-zero. For example, a test for a hypothesis about a single quantitative predictor \\(\\mathbf{x}\\), has \\(\\text{df}_h = 1\\) degree of freedom and \\(\\mathrm{rank} (\\mathbf{H}) = 1\\); for a factor with \\(g\\) groups, \\(\\text{df}_h = \\mathrm{rank} (\\mathbf{H}) = g-1\\).\n\n\n\n\n\n\n\n\nThe overall multivariate test for the model in Equation 10.1 is essentially a test of the hypothesis \\(\\mathcal{H}_0: \\mathbf{B} = 0\\) (excluding the row for the intercept). Equivalently, this is a test based on the incremental \\(\\mathbf{SSP}_{H}\\) for the hypothesized terms in the model—that is, the difference between the \\(\\mathbf{SSP}_{H}\\) for the full model and the null, intercept-only model. The same idea can be applied to test the difference between any pair of nested models—the added contribution of terms in a larger model relative to a smaller model containing a subset of terms.\nThe eigenvectors \\(\\mathbf{v}_i\\) in Equation 10.2 are also important. These are the weights for the variables in a linear combination \\(v_{i1} \\mathbf{y}_1 + v_{i2} \\mathbf{y}_2 + \\cdots + v_{ip} \\mathbf{y}_p\\) which produces the largest univariate \\(F\\) statistic for the \\(i\\)-th dimension. We exploit this in canonical discriminant analysis and the corresponding canonical HE plots (Section 11.2).\n\n10.2.2 Test statistics\nIn the univariate case, the overall \\(F\\)-test of \\(\\mathcal{H}_0: \\boldsymbol{\\beta} = \\mathbf{0}\\) is the uniformly most powerful invariant test when the assumptions are met. There is nothing better. This is not the case in the MLM.\nThe reason is that when there are \\(p &gt; 1\\) response variables, and we are testing a hypothesis comprising \\(\\text{df}_h &gt;1\\) coefficients or degrees of freedom, there are \\(s &gt; 1\\) possible dimensions in which \\(\\mathbf{H}\\) can be large relative to \\(\\mathbf{E}\\), each measured by the eigenvalue \\(\\lambda_i\\). There are several test statistics that combine these into a single measure, shown in Table 10.1.\n\n\n\n\n\n\n\n\n\n\nTable 10.1: Test statistics for multivariate tests combine the size of dimensions of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) into a single measure.\n\n\n\n\nCriterion\nFormula\nPartial \\(\\eta^2\\)\n\n\n\n\n\nWilks’s \\(\\Lambda\\)\n\n\\(\\Lambda = \\prod^s_i \\frac{1}{1+\\lambda_i}\\)\n\\(\\eta^2 = 1-\\Lambda^{1/s}\\)\n\n\n\nPillai trace\n\\(V = \\sum^s_i \\frac{\\lambda_i}{1+\\lambda_i}\\)\n\\(\\eta^2 = \\frac{V}{s}\\)\n\n\n\nHotelling-Lawley trace\n\\(H = \\sum^s_i \\lambda_i\\)\n\\(\\eta^2 = \\frac{H}{H+s}\\)\n\n\n\nRoy maximum root\n\\(R = \\lambda_1\\)\n\\(\\eta^2 = \\frac{\\lambda_1}{1+\\lambda_1}\\)\n\n\n\n\n\n\n\n\nThese correspond to different kinds of “means” of the \\(\\lambda_i\\): arithmetic, geometric, harmonic and supremum. See Friendly et al. (2013) for the geometry behind these measures.\nEach of these statistics have different sampling distributions under the null hypothesis, but they can all be converted to \\(F\\) statistics, which are exact when \\(s \\le 2\\), and approximations otherwise. As well, each has an analog of the \\(R^2\\)-like partial \\(\\eta^2\\) measure, giving the partial association accounted for by each term in the MLM.\n\n10.2.3 Testing contrasts and linear hypotheses\nEven more generally, these multivariate tests apply to every linear hypothesis concerning the coefficients in \\(\\mathbf{B}\\). Suppose we want to test the hypothesis that a subset of rows (predictors) and/or columns (responses) simultaneously have null effects. This can be expressed in the general linear test, \\[\n\\mathcal{H}_0 : \\mathbf{C}_{h \\times q} \\, \\mathbf{B}_{q \\times p} = \\mathbf{0}_{h \\times p} \\:\\: ,\n\\] where \\(\\mathbf{C}\\) is a full rank \\(h \\le q\\) hypothesis matrix of constants, that selects subsets or linear combinations (contrasts) of the coefficients in \\(\\mathbf{B}\\) to be tested in a \\(h\\) degree-of-freedom hypothesis.\nIn this case, the SSP matrix for the hypothesis has the form \\[\n\\mathbf{H}  =\n(\\mathbf{C} \\widehat{\\mathbf{B}})^\\mathsf{T}\\,\n[\\mathbf{C} (\\mathbf{X}^\\mathsf{T}\\mathbf{X} )^{-1} \\mathbf{C}^\\mathsf{T}]^{-1} \\,\n(\\mathbf{C} \\widehat{\\mathbf{B}})\n\\tag{10.3}\\]\nwhere there are \\(s = \\min(h, p)\\) non-zero eigenvalues of \\(\\mathbf{H}\\mathbf{E}^{-1}\\). In Equation 10.3, \\(\\mathbf{H}\\) measures the (Mahalanobis) squared distances (and cross products) among the linear combinations \\(\\mathbf{C} \\widehat{\\mathbf{B}}\\) from the origin under the null hypothesis.  \nFor example, with three responses \\(y_1, y_2, y_3\\) and three predictors \\(x_1, x_2, x_3\\), we can test the hypothesis that neither \\(x_2\\) nor \\(x_3\\) contribute at all to the predicting the \\(y\\)s in terms of the hypothesis that the coefficients for the corresponding rows of \\(\\mathbf{B}\\) are zero using a 1-row \\(\\mathbf{C}\\) matrix that simply selects those rows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\mathcal{H}_0 : \\mathbf{C} \\mathbf{B} & =\n\\begin{bmatrix}\n0 & 1 & 1 & 0\n\\end{bmatrix}\n\\begin{pmatrix}\n  \\beta_{0,y_1} & \\beta_{0,y_2} & \\beta_{0,y_3} \\\\\n  \\beta_{1,y_1} & \\beta_{1,y_2} & \\beta_{1,y_3} \\\\\n  \\beta_{2,y_1} & \\beta_{2,y_2} & \\beta_{2,y_3} \\\\\n  \\beta_{3,y_1} & \\beta_{3,y_2} & \\beta_{3,y_3}\n\\end{pmatrix} \\\\ \\\\\n& =\n\\begin{bmatrix}\n  \\beta_{1,y_1} & \\beta_{1,y_2} & \\beta_{1,y_3} \\\\\n  \\beta_{2,y_1} & \\beta_{2,y_2} & \\beta_{2,y_3} \\\\\n\\end{bmatrix}\n=\n\\mathbf{0}_{(2 \\times 3)}\n\\end{aligned}\\]\nIn MANOVA designs, it is often desirable to follow up a significant effect for a factor with subsequent tests to determine which groups differ. While you can simply test for all pairwise differences among groups (using Bonferonni or other corrections for multiplicity), a more substantively-driven approach uses planned comparisons or contrasts among the factor levels.\nFor a factor with \\(g\\) groups, a contrast is simply a comparison of the mean of one subset of groups against the mean of another subset. This is specified as a weighted sum, \\(L\\) of the means with weights \\(\\mathbf{c}\\) that sum to zero,\n\\[\nL = \\mathbf{c}^\\mathsf{T} \\boldsymbol{\\mu} = \\sum_i c_i \\mu_i \\quad\\text{such that}\\quad \\Sigma c_i = 0\n\\] Two contrasts, \\(\\mathbf{c}_1\\) and \\(\\mathbf{c}_2\\) are orthogonal if the sum of products of their weights is zero, i.e., \\(\\mathbf{c}_1^\\mathsf{T} \\mathbf{c}_2 = \\Sigma c_{1i} \\times c_{2i} = 0\\). When contrasts are placed as columns of a matrix \\(\\mathbf{C}\\), they are all mutually orthogonal if \\(\\mathbf{C}^\\mathsf{T} \\mathbf{C}\\) is a diagonal matrix. Orthogonal contrasts correspond to statistically independent tests.\nFor example, with \\(g=4\\) groups representing the combinations of two drugs, A and B given at low and high doses, we might want to compare (a) the average of the drug A groups vs. the average of drug B; (b) low vs. high for drug A; (c) low vs. high for drug B. The contrasts that do this are:\n\\[\\begin{aligned}\nL_1 & = (\\mu_1 + \\mu_2) - (\\mu_1 + \\mu_2) & \\rightarrow\\: & \\mathbf{c}_1 =\n    \\begin{pmatrix}\n     1 &  1 & -1 & -1\n    \\end{pmatrix} \\\\\nL_2 & = \\mu_1 - \\mu_2                     & \\rightarrow\\: & \\mathbf{c}_2 =\n    \\begin{pmatrix}\n     1 &  -1 & 0 & 0\n    \\end{pmatrix} \\\\\nL_3 & = \\mu_3 - \\mu_4                     & \\rightarrow\\: & \\mathbf{c}_3 =\n    \\begin{pmatrix}\n     0 &  0 & 1 & -1\n    \\end{pmatrix}\n\\end{aligned}\\]\nNote that these correspond to nested dichotomies among the four groups: first we compare groups (1 and 2) against groups (3 and 4), then subsequently within each of these sets. Such contrasts are always orthogonal, and therefore correspond to statistically independent tests.\nIn R, contrasts for a factor are specified as columns of matrix, each of which sums to zero. For this example, we can set this up by creating each as a vector and joining them as columns using cbind():\n\nc1 &lt;- c(1,  1, -1, -1)    # A1,A2 vs. B1,B2\nc2 &lt;- c(1, -1,  0,  0)    # A1 vs. A2\nc3 &lt;- c(0,  0,  1, -1)    # B1 vs. B2\nC &lt;- cbind(c1,c2,c3) \nrownames(C) &lt;- outer(LETTERS[1:2], 1:2, paste0) |&gt; \n  t() |&gt; \n  c()\nC\n#&gt;    c1 c2 c3\n#&gt; A1  1  1  0\n#&gt; A2  1 -1  0\n#&gt; B1 -1  0  1\n#&gt; B2 -1  0 -1\n\n# show they are mutually orthogonal\nt(C) %*% C\n#&gt;    c1 c2 c3\n#&gt; c1  4  0  0\n#&gt; c2  0  2  0\n#&gt; c3  0  0  2\n\nFor a dataset, data, with this factor as group, you can set up the analyses to use these contrasts by assigning the matrix C to contrasts():\n\ncontrasts(data$group) &lt;- C\n\nFor multivariate tests, when all contrasts are pairwise orthogonal, the overall test of a factor with \\(\\text{df}_h = g-1\\) degrees of freedom can be broken down into \\(g-1\\) separate 1 df tests. This gives rise to a set of \\(\\text{df}_h\\) rank 1 \\(\\mathbf{H}\\) matrices that additively decompose the overall hypothesis SSCP matrix,\n\\[\n\\mathbf{H} = \\mathbf{H}_1 + \\mathbf{H}_2 + \\cdots + \\mathbf{H}_{\\text{df}_h} \\:\\: ,\n\\] exactly as the univariate \\(\\text{SS}_H\\) can be decomposed using orthogonal contrasts in an ANOVA.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#anova-rightarrow-manova",
    "href": "10-mlm-review.html#anova-rightarrow-manova",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.3 ANOVA \\(\\rightarrow\\) MANOVA",
    "text": "10.3 ANOVA \\(\\rightarrow\\) MANOVA\nMultivariate analysis of variance (MANOVA) generalizes the familiar ANOVA model to situations where there are two or more response variables. Unlike ANOVA, which focuses on discerning statistical differences in one continuous dependent variable influenced by an independent variable (or grouping variable), MANOVA considers several dependent variables at once. It integrates these variables into a single, composite variable through a weighted linear combination, allowing for a comprehensive analysis of how these dependent variables collectively vary with respect to the levels of the independent variable. Essentially, MANOVA investigates whether the grouping variable explains significant variations in the combined dependent variables.\nThe situation is illustrated in Figure 10.1 where there are two response measures, \\(Y_1\\) and \\(Y_2\\) with data collected for three groups. For concreteness, \\(Y_1\\) might be a score on a math test and \\(Y_2\\) might be a reading score. Let’s also say that group 1 has been studying Shakespeare, while group 2 has concentrated on physics, but group 3 has done nothing beyond the normal curriculum.\n\n\n\n\n\n\n\nFigure 10.1: Data from simple MANOVA design involving three groups and two response measures, \\(Y_1\\) and \\(Y_2\\), summarized by their data ellipses.\n\n\n\n\nAs shown in the figure, the centroids, \\((\\mu_{1g}, \\mu_{2g})\\), clearly differ—the data ellipses barely overlap. A multivariate analysis would show a highly difference among groups. From a rough visual inspection, it seems that means differ on the math test \\(Y_1\\), with the physics group out-performing the other two. On the reading test \\(Y_2\\) however it might turn out that the three group means don’t differ significantly in an ANOVA, although the Shakespeare group comes out best by a small amount. Doing separate ANOVAs on these variables would miss what is so obvious from Figure 10.1.\nFigure 10.2 illustrates a second important advantage of performing a multivariate analysis over separate ANOVAS: that of determining the number of dimensions or aspects along which groups differ. In the panel on the left, the means of the three groups increase nearly linearly on the combination of \\(Y_1\\) and \\(Y_2\\), to their differences can be ascribed to a single dimension. For example, the groups here might be patients diagnosed as normal, mild schizophrenia and profound schizophrenia, and the measures could be tests of memory and attention. The obvious multivariate interpretation from the figure is that of increasing impairment of cognitive functioning across the groups. Note also the positive association within each group: those who perform better on the memory task also do better on attention.\n\n\n\n\n\n\n\nFigure 10.2: A simple MANOVA design involving three groups and two response measures, \\(Y_1\\) and \\(Y_2\\), but with different patterns of the differences among the group means. The red arrows suggest interpretations in terms of dimensions or aspects of the response variables.\n\n\n\n\nIn contrast, the right panel of Figure 10.2 shows a situation where the group means have a low correlation. Data like this might arise in a study of parental competency, where there are are measure of the degree of caring (\\(Y_1\\)) and time spent in play (\\(Y_2\\)) by fathers and groups consisting of fathers of children with no disability, or a physical disability or a mental ability. As can be seen in Figure 10.2 fathers of the disabled children differ from those of the not disabled group in two different directions corresponding to being higher on either \\(Y_1\\) or \\(Y_2\\). The red arrows suggest that the differences among groups could be interpreted in terms of two uncorrelated dimensions, perhaps labeled overall competency and emphasis on physical activity. (The pattern in Figure 10.2 (right) is contrived for the sake of illustration; it does not reflect the data analyzed in the example below.)\n\n10.3.1 Example: Father parenting data\nI use a simple example of a three-group multivariate design to illustrate the basic ideas of fitting MLMs in R and testing hypotheses. Visualization methods using HE plots are discussed in Chapter 11.\nThe dataset heplots::Parenting come from an exercise (10B) in Meyers et al. (2006) and are probably contrived, but are modeled on a real study in which fathers were assessed on three subscales of a Perceived Parenting Competence Scale,\n\n\ncaring, caretaking responsibilities;\n\nemotion, emotional support provided to the child; and\n\nplay, recreational time spent with the child.\n\nThe dataset Parenting comprises 60 fathers selected from three groups of \\(n = 20\\) each: (a) fathers of a child with no disabilities (\"Normal\"); (b) fathers with a physically disabled child; (c) fathers with a mentally disabled child. The design is thus a three-group MANOVA, with three response variables.\n\nThe main questions concern whether the group means differ on these scales, and the nature of these differences. That is, do the means differ significantly on all three measures? Is there a consistent order of groups across these three aspects of parenting?\nMore specific questions are: (a) Do the fathers of typical children differ from the other two groups on average? (b) Do the physical and mental groups differ? These questions can be tested using contrasts, and are specified by assigning a matrix to contrasts(Parenting$group); each column is a contrast whose values sum to zero. They are given labels \"group1\" (normal vs. other) and \"group2\" (physical vs. mental) in some output.\n\ndata(Parenting, package=\"heplots\")\nC &lt;- matrix(c(1, -.5, -.5,\n              0,  1,  -1), \n            nrow = 3, ncol = 2) |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,]  1.0    0\n#&gt; [2,] -0.5    1\n#&gt; [3,] -0.5   -1\ncontrasts(Parenting$group) &lt;- C\n\nExploratory plots\nBefore setting up a model and testing, it is well-advised to examine the data graphically. The simplest plots are side-by-side boxplots (or violin plots) for the three responses. With ggplot2, this is easily done by reshaping the data to long format and using faceting. In Figure 10.3, I’ve also plotted the group means with white dots.\n\nSee the ggplot codeparenting_long &lt;- Parenting |&gt;\n  tidyr::pivot_longer(cols=caring:play, names_to = \"variable\")\n\nggplot(parenting_long, \n       aes(x=group, y=value, fill=group)) +\n  geom_boxplot(outlier.size=2.5, \n               alpha=.5, \n               outlier.alpha = 0.9) + \n  stat_summary(fun=mean, \n               color=\"white\", \n               geom=\"point\", \n               size=2) +\n  scale_fill_hue(direction = -1) +     # reverse default colors\n  labs(y = \"Scale value\", x = \"Group\") +\n  facet_wrap(~ variable) +\n  theme_bw(base_size = 14) + \n  theme(legend.position=\"top\") +\n  theme(axis.text.x = element_text(angle = 15,\n                                   hjust = 1)) \n\n\n\n\n\n\nFigure 10.3: Faceted boxplots of scores on the three parenting scales, showing also the mean for each.\n\n\n\n\nIn this figure, differences among the groups on play are most apparent, with fathers of non-disabled children scoring highest. Differences among the groups on emotion are very small, but one high outlier for the fathers of mentally disabled children is apparent. On caring, fathers of children with a physical disability stand out as highest.\nFor exploratory purposes, you might also make a scatterplot matrix. Here, because the MLM assumes homogeneity of the variances and covariance matrices \\(\\mathbf{S}_i\\), I show only the data ellipses in scatterplot matrix format, using heplots:covEllipses() (with 50% coverage, for clarity):\n\ncolors &lt;- scales::hue_pal()(3) |&gt; rev()  # match color use in ggplot\ncovEllipses(cbind(caring, play, emotion) ~ group, data=Parenting,\n  variables = 1:3,\n  fill = TRUE, fill.alpha = 0.2,\n  pooled = FALSE,\n  level = 0.50, \n  col = colors)\n\n\n\n\n\n\nFigure 10.4: Bivariate data ellipses for pairs of the three responses, showing the means, correlations and variances for the three groups.\n\n\n\n\nIf the covariance matrices were all the same, the data ellipses would have roughly the same size and orientation, but that is not the case in Figure 10.4. The normal group shows greater variability overall and the correlations among the measures differ somewhat from group to group. We’ll assess later whether this makes a difference in the conclusions that can be drawn (Chapter 12). The group centroids also differ, but the pattern is not particularly clear. We’ll see an easier to understand view in HE plots and their canonical discriminant cousins.\n\n10.3.1.1 Testing the model\nLet’s proceed to fit the multivariate model predicting all three scales from the group factor. lm() for a multivariate response returns an object of class \"mlm\", for which there are many methods (use methods(class=\"mlm\") to find them).\n\nparenting.mlm &lt;- lm(cbind(caring, play, emotion) ~ group, \n                    data=Parenting) |&gt; print()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cbind(caring, play, emotion) ~ group, data = Parenting)\n#&gt; \n#&gt; Coefficients:\n#&gt;              caring   play     emotion\n#&gt; (Intercept)   5.8833   4.6333   5.9167\n#&gt; group1       -0.3833   2.4167  -0.0667\n#&gt; group2        1.7750  -0.2250  -0.6000\n\nThe coefficients in this model are the values of the contrasts set up above. group1 is the mean of the typical group minus the average of the other two, which is negative on caring and emotion but positive for play. group2 is the difference in means for the physical vs. mental groups.\nBefore doing multivariate tests, it is useful to see what would happen if we ran univariate ANOVAs on each of the responses. These can be extracted from an MLM using stats::summary.aov():\n\nsummary.aov(parenting.mlm)\n#&gt;  Response caring :\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; group        2    130    65.2    18.6  6e-07 ***\n#&gt; Residuals   57    200     3.5                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Response play :\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; group        2    177    88.6    27.6  4e-09 ***\n#&gt; Residuals   57    183     3.2                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Response emotion :\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; group        2     15    7.27    1.02   0.37\n#&gt; Residuals   57    408    7.16\n\nIf you like, you can also extract the univariate model fit statistics from the \"mlm\" using thebroom::glance()` method for a multivariate model object.\n\nglance(parenting.mlm) |&gt;\n  select(response, r.squared, fstatistic, p.value)\n#&gt; # A tibble: 3 × 4\n#&gt;   response r.squared fstatistic       p.value\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 caring      0.395       18.6  0.000000602  \n#&gt; 2 play        0.492       27.6  0.00000000405\n#&gt; 3 emotion     0.0344       1.02 0.369\n\nFrom this, one might conclude that there are differences only in caring and play and therefore ignore emotion, but this would be short-sighted. car::Anova() gives the overall multivariate test \\(\\mathcal{H}_0: \\mathbf{B} = 0\\) of the group effect. Note that this has a much smaller \\(p\\)-value than any of the univariate \\(F\\) tests.\n\nAnova(parenting.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;       Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; group  2     0.948     16.8      6    112  9e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova() returns an object of class \"Anova.mlm\" which has various methods. The summary() method for this gives more details, including all four test statistics. These tests have \\(s = \\min(p, \\text{df}_h) = \\min(3,2) = 2\\) dimensions and the \\(F\\) approximations are not equivalent here. All four tests are highly significant\n\nparenting.summary &lt;- Anova(parenting.mlm) |&gt;  summary() \nprint(parenting.summary, SSP=FALSE)\n#&gt; \n#&gt; Type II MANOVA Tests:\n#&gt; \n#&gt; ------------------------------------------\n#&gt;  \n#&gt; Term: group \n#&gt; \n#&gt; Multivariate Tests: group\n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            2     0.948     16.8      6    112 9.0e-14 ***\n#&gt; Wilks             2     0.274     16.7      6    110 1.3e-13 ***\n#&gt; Hotelling-Lawley  2     1.840     16.6      6    108 1.8e-13 ***\n#&gt; Roy               2     1.108     20.7      3     56 3.8e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe summary() method by default prints the SSH = \\(\\mathbf{H}\\) and SSE = \\(\\mathbf{E}\\) matrices, but I suppressed them above. They can be extracted from the object using purrr::pluck():\n\nH &lt;- parenting.summary |&gt; \n  purrr::pluck(\"multivariate.tests\", \"group\", \"SSPH\") |&gt; \n  print()\n#&gt;         caring    play emotion\n#&gt; caring   130.4 -43.767 -41.833\n#&gt; play     -43.8 177.233   0.567\n#&gt; emotion  -41.8   0.567  14.533\nE &lt;- parenting.summary |&gt; \n  purrr::pluck(\"multivariate.tests\", \"group\", \"SSPE\") |&gt; \n  print()\n#&gt;         caring  play emotion\n#&gt; caring   199.8 -45.8    35.2\n#&gt; play     -45.8 182.7    80.6\n#&gt; emotion   35.2  80.6   408.0\n\nLinear hypotheses & contrasts\nWith three or more groups or with a more complex MANOVA design, contrasts provide a way of testing questions of substantive interest regarding differences among group means.\nThe test of the contrast comparing the typical group to the average of the others is the test using the contrast \\(c_1 = (1, -\\frac12, -\\frac12)\\) which produces the coefficients labeled \"group1\". The function car::linearHypothesis() carries out the multivariate test that these are all zero. This is a 1 df test, so all four test statistics produce the same \\(F\\) and \\(p\\)-values.\n\ncoef(parenting.mlm)[\"group1\",]\n#&gt;  caring    play emotion \n#&gt; -0.3833  2.4167 -0.0667\nlinearHypothesis(parenting.mlm, \"group1\") |&gt; \n  print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            1     0.521     19.9      3     55 7.1e-09 ***\n#&gt; Wilks             1     0.479     19.9      3     55 7.1e-09 ***\n#&gt; Hotelling-Lawley  1     1.088     19.9      3     55 7.1e-09 ***\n#&gt; Roy               1     1.088     19.9      3     55 7.1e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSimilarly, the difference between the physical and mental groups uses the contrast \\(c_2 = (0, 1, -1)\\) and the test that these means are equal is given by linearHypothesis() applied to group2.\n\ncoef(parenting.mlm)[\"group2\",]\n#&gt;  caring    play emotion \n#&gt;   1.775  -0.225  -0.600\nlinearHypothesis(parenting.mlm, \"group2\") |&gt; \n  print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1     0.429     13.8      3     55  8e-07 ***\n#&gt; Wilks             1     0.571     13.8      3     55  8e-07 ***\n#&gt; Hotelling-Lawley  1     0.752     13.8      3     55  8e-07 ***\n#&gt; Roy               1     0.752     13.8      3     55  8e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlinearHypothesis() is very general. The second argument (hypothesis.matrix) corresponds to \\(\\mathbf{C}\\), and can be specified as numeric matrix giving the linear combinations of coefficients by rows to be tested, or a character vector giving the hypothesis in symbolic form; \"group1\" is equivalent to \"group1 = 0\".\nBecause the contrasts used here are orthogonal, they comprise the overall test of \\(\\mathbf{B} = \\mathbf{0}\\) which implies that the means of the three groups are all equal. The test below gives the same results as Anova(parenting.mlm).\n\nlinearHypothesis(parenting.mlm, c(\"group1\", \"group2\")) |&gt; \n  print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            2     0.948     16.8      6    112 9.0e-14 ***\n#&gt; Wilks             2     0.274     16.7      6    110 1.3e-13 ***\n#&gt; Hotelling-Lawley  2     1.840     16.6      6    108 1.8e-13 ***\n#&gt; Roy               2     1.108     20.7      3     56 3.8e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n10.3.2 Ordered factors\nWhen groups are defined by an ordered factor, such as level of physical fitness (rated 1–5) or grade in school, it is tempting to treat that as a numeric variable and use a multivariate regression model. This would assume that the effect of that factor is linear and if not, we might consider adding polynomial terms. A different strategy, often preferable, is to make the group variable an ordered factor, for which R assigns polynomial contrasts. This gives separate tests of the linear, quadratic, cubic, … trends of the response, without the need to specify them separately in the model\n\n10.3.3 Example: Adolescent mental health\nThe dataset heplots::AddHealth contains a large cross-sectional sample of participants from grades 7–12 from the National Longitudinal Study of Adolescent Health, described by Warne (2014). It contains responses to two Likert-scale (1–5) items, anxiety and depression. grade is an ordered factor, which means that the default contrasts are taken as orthogonal polynomials with linear (grade.L), quadratic (grade.Q), up to 5th degree (grade^5) trends, which decompose the total effect of grade.\n\ndata(AddHealth, package=\"heplots\")\nstr(AddHealth)\n#&gt; 'data.frame':  4344 obs. of  3 variables:\n#&gt;  $ grade     : Ord.factor w/ 6 levels \"7\"&lt;\"8\"&lt;\"9\"&lt;\"10\"&lt;..: 5 4 6 1 2 2 2 3 3 3 ...\n#&gt;  $ depression: int  0 0 0 0 0 0 0 0 1 2 ...\n#&gt;  $ anxiety   : int  0 0 0 1 1 0 0 1 1 0 ...\n\nThe research questions are:\n\nHow do the means for anxiety and depression vary separately with grade? Is there evidence for linear and nonlinear trends?\nHow do anxiety and depression vary jointly with grade?\nHow does the _association* of anxiety and depression vary with age?\n\nThe first question can be answered by fitting separate linear models for each response (e.g., lm(anxiety ~ grade))). However the second question is more interesting because it considers the two responses together and takes their correlation into account. This would be fit as the MLM:\n\\[\n\\mathbf{y} = \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1 x + \\boldsymbol{\\beta}_2 x^2 + \\cdots \\boldsymbol{\\beta}_5 x^5\n\\tag{10.4}\\]\nor, expressed in terms of the variables,\n\\[\\begin{aligned}\n\\begin{bmatrix} y_{\\text{anx}} \\\\y_{\\text{dep}} \\end{bmatrix} & = &\n\\begin{bmatrix} \\beta_{0,\\text{anx}} \\\\ \\beta_{0,\\text{dep}} \\end{bmatrix} +\n\\begin{bmatrix} \\beta_{1,\\text{anx}} \\\\ \\beta_{1,\\text{dep}} \\end{bmatrix} \\text{grade} +\n\\begin{bmatrix} \\beta_{2,\\text{anx}} \\\\ \\beta_{2,\\text{dep}} \\end{bmatrix} \\text{grade}^2 + \\cdots\n\\begin{bmatrix} \\beta_{5,\\text{anx}} \\\\ \\beta_{5,\\text{dep}} \\end{bmatrix} \\text{grade}^5\n\\end{aligned} \\tag{10.5}\\]\nWithgrade represented as an ordered factor, the values of \\(x\\) in Equation 10.4 are those of the orthogonal polynomials given by poly(grade,5).\nExploratory plots\nSome exploratory analysis is useful before fitting and visualizing models. As a first step, we find the means, standard deviations, and standard errors of the means.\n\nShow the codelibrary(ggplot2)\nlibrary(dplyr)\n\nmeans &lt;- AddHealth |&gt;\n  group_by(grade) |&gt;\n  summarise(\n    n = n(),\n    dep_sd = sd(depression, na.rm = TRUE),\n    anx_sd = sd(anxiety, na.rm = TRUE),\n    dep_se = dep_sd / sqrt(n),\n    anx_se = anx_sd / sqrt(n),\n    depression = mean(depression),\n    anxiety = mean(anxiety) ) |&gt; \n  relocate(depression, anxiety, .after = grade) |&gt;\n  print()\n#&gt; # A tibble: 6 × 8\n#&gt;   grade depression anxiety     n dep_sd anx_sd dep_se anx_se\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 7          0.881   0.751   622   1.11   1.05 0.0447 0.0420\n#&gt; 2 8          1.08    0.804   664   1.19   1.06 0.0461 0.0411\n#&gt; 3 9          1.17    0.934   778   1.19   1.08 0.0426 0.0387\n#&gt; 4 10         1.27    0.956   817   1.23   1.11 0.0431 0.0388\n#&gt; 5 11         1.37    1.12    790   1.20   1.16 0.0428 0.0411\n#&gt; 6 12         1.34    1.10    673   1.14   1.11 0.0439 0.0426\n\n\nNow, plot the means with \\(\\pm 1\\) error bars. It appears that average level of both depression and anxiety increase steadily with grade, except for grades 11 and 12 which don’t differ much. Alternatively, we could describe this as relationships that seem largely linear, with a hint of curvature at the upper end.\n\np1 &lt;-ggplot(data = means, aes(x = grade, y = anxiety)) +\n  geom_point(size = 4) +\n  geom_line(aes(group = 1), linewidth = 1.2) +\n  geom_errorbar(aes(ymin = anxiety - anx_se, \n                   ymax = anxiety + anx_se),\n                width = .2) \n\np2 &lt;-ggplot(data = means, aes(x = grade, y = depression)) +\n  geom_point(size = 4) +\n  geom_line(aes(group = 1), linewidth = 1.2) +\n  geom_errorbar(aes(ymin = depression - dep_se, \n                    ymax = depression + dep_se),\n                width = .2) \n\np1 + p2\n\n\n\n\n\n\nFigure 10.5: Means of anxiety and depression by grade, with \\(\\pm 1\\) standard error bars.\n\n\n\n\nIt is also useful to within-group correlations using covEllipses(), as shown in Figure 10.6. This also plots the bivariate means showing the form of the association , treating anxiety and depression as multivariate outcomes. (Because the variability of the scores within groups is so large compared to the range of the means, I show the data ellipses with coverage of only 10%.)\n\ncovEllipses(AddHealth[, 3:2], group = AddHealth$grade,\n            pooled = FALSE, level = 0.1,\n            center.cex = 2.5, cex = 1.5, cex.lab = 1.5,\n            fill = TRUE, fill.alpha = 0.05)\n\n\n\n\n\n\nFigure 10.6: Within-group covariance ellipses for the grade groups.\n\n\n\n\nFit the MLM\nNow, let’s fit the MLM for both responses jointly in relation to grade. The null hypothesis is that the means for anxiety and depression are the same at all six grades, \\[\n\\mathcal{H}_0: \\mathbf{\\mu}_7 = \\mathbf{\\mu}_8 = \\cdots = \\mathbf{\\mu}_{12} \\; ,\n\\] or equivalently, that all coefficients except the intercept in the model @ref(eq:AH-mod) are zero, \\[\n\\mathcal{H}_0: \\boldsymbol{\\beta}_1 =  \\boldsymbol{\\beta}_2  = \\cdots =  \\boldsymbol{\\beta}_5 = \\boldsymbol{0} \\; .\n\\] We fit the MANOVA model, and test the grade effect using car::Anova(). The effect of grade is highly significant, as we could tell from Figure 10.5.\n\nAH.mlm &lt;- lm(cbind(anxiety, depression) ~ grade, data = AddHealth)\n\n# overall test of `grade`\nAnova(AH.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;       Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; grade  5    0.0224     9.83     10   8676 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHowever, the overall test, with 5 degrees of freedom is diffuse, in that it can be rejected if any pair of means differ. Given that grade is an ordered factor, it makes sense to examine narrower hypotheses of linear and nonlinear trends, car::linearHypothesis() on the coefficients of model AH.mlm.\n\ncoef(AH.mlm) |&gt; names()\n#&gt; NULL\n\nThe joint test of the linear coefficients \\(\\boldsymbol{\\beta}_1 = (\\beta_{1,\\text{anx}},  \\beta_{1,\\text{dep}})^\\mathsf{T}\\) for anxiety and depression, \\(H_0 : \\boldsymbol{\\beta}_1 = \\boldsymbol{0}\\) is highly significant,\n\n## linear effect\nlinearHypothesis(AH.mlm, \"grade.L\") |&gt; print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1     0.019     42.5      2   4337 &lt;2e-16 ***\n#&gt; Wilks             1     0.981     42.5      2   4337 &lt;2e-16 ***\n#&gt; Hotelling-Lawley  1     0.020     42.5      2   4337 &lt;2e-16 ***\n#&gt; Roy               1     0.020     42.5      2   4337 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe test of the quadratic coefficients \\(H_0 : \\boldsymbol{\\beta}_2 = \\boldsymbol{0}\\) indicates significant curvature in trends across grade, as we saw in the plots of their means in Figure 10.5. One interpretation might be that depression and anxiety after increasing steadily up to grade eleven could level off thereafter.\n\n## quadratic effect\nlinearHypothesis(AH.mlm, \"grade.Q\") |&gt; print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; Pillai            1     0.002     4.24      2   4337  0.014 *\n#&gt; Wilks             1     0.998     4.24      2   4337  0.014 *\n#&gt; Hotelling-Lawley  1     0.002     4.24      2   4337  0.014 *\n#&gt; Roy               1     0.002     4.24      2   4337  0.014 *\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAn advantage of linear hypotheses is that we can test several terms jointly. Of interest here is the hypothesis that all higher order terms beyond the quadratic are zero, \\(H_0 : \\boldsymbol{\\beta}_3 =  \\boldsymbol{\\beta}_4 =  \\boldsymbol{\\beta}_5 = \\boldsymbol{0}\\). Using linearHypothesis you can supply a vector of coefficient names to be tested for their joint effect when dropped from the model.\n\nrownames(coef(AH.mlm))\n#&gt; [1] \"(Intercept)\" \"grade.L\"     \"grade.Q\"     \"grade.C\"    \n#&gt; [5] \"grade^4\"     \"grade^5\"\n## joint test of all higher terms\nlinearHypothesis(AH.mlm, rownames(coef(AH.mlm))[3:5]) |&gt; print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; Pillai            3     0.002     1.70      6   8676   0.12  \n#&gt; Wilks             3     0.998     1.70      6   8674   0.12  \n#&gt; Hotelling-Lawley  3     0.002     1.70      6   8672   0.12  \n#&gt; Roy               3     0.002     2.98      3   4338   0.03 *\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n10.3.4 Factorial MANOVA\nWhen there are two or more categorical factors, the general linear model provides a way to investigate the effects (differences in means) of each simultaneously. More importantly, this allows you to determine if factors interact, so the effect of one factor varies with the levels of another factor …\nExample: Penguins data\nIn Chapter 3 we examined the Palmer penguins data graphically, using a mosaic plot (Figure 3.30) of the frequencies of the three factors, species, island and sex and then ggpairs() scatterplot matrix (Figure 3.31).",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#mra-rightarrow-mmra",
    "href": "10-mlm-review.html#mra-rightarrow-mmra",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.4 MRA \\(\\rightarrow\\) MMRA",
    "text": "10.4 MRA \\(\\rightarrow\\) MMRA",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#ancova-rightarrow-mancova",
    "href": "10-mlm-review.html#ancova-rightarrow-mancova",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.5 ANCOVA \\(\\rightarrow\\) MANCOVA",
    "text": "10.5 ANCOVA \\(\\rightarrow\\) MANCOVA",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#repeated-measures-designs",
    "href": "10-mlm-review.html#repeated-measures-designs",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.6 Repeated measures designs",
    "text": "10.6 Repeated measures designs\nPackage summary\n\nPackages used here:\n9 packages used here: broom, car, carData, dplyr, ggplot2, heplots, knitr, patchwork, tidyr\n\n\n\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis with applications. Biometrika, 57(3), 519–530. https://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMeyers, L. S., Gamst, G., & Guarino, A. J. (2006). Applied multivariate research: Design and interpretation. SAGE Publications.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52(3–4), 591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nWarne, F. T. (2014). A primer on multivariate analysis of variance(MANOVA) for behavioral scientists. Practical Assessment, Research & Evaluation, 19(1). https://scholarworks.umass.edu/pare/vol19/iss1/17/",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html",
    "href": "11-mlm-viz.html",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "11.1 HE plot framework\nChapter 9 illustrated the basic ideas of the framework for visualizing multivariate linear models in the context of a simple two group design, using Hotelling’s \\(T^2\\). These are illustrated in Figure 11.1.\nFigure 11.1: The Hypothesis Error plot framework. Source: author\nFor more complex models such as MANOVA with multiple factors or multivariate multivariate regression, there is one \\(\\mathbf{H}\\) ellipse for each term in the model. …",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-he-framework",
    "href": "11-mlm-viz.html#sec-he-framework",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "In data space, each group is summarized by its data ellipse, representing the means and covariances.\nVariation against the hypothesis of equal means can be seen by the \\(\\mathbf{H}\\) ellipse in the HE plot, representing the data ellipse of the fitted values. Error variance is shown in the \\(\\mathbf{E}\\) ellipse, representing the pooled within-group covariance matrix, \\(\\mathbf{S}_p\\) and the data ellipse of the residuals from the model.\nThe MANOVA (or Hotelling’s \\(T^2\\)) is formally equivalent to a discriminant analysis, predicting group membership from the response variables which can be seen in data space.\nThis effectively projects the \\(p\\)-dimensional space of the predictors into the smaller canonical space that shows the greatest differences among the groups.\n\n\n\n\n11.1.1 HE plot details",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-candisc",
    "href": "11-mlm-viz.html#sec-candisc",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.2 Canonical discriminant analysis",
    "text": "11.2 Canonical discriminant analysis\n\n#&gt; Writing packages to  C:/R/Projects/Vis-MLM-book/bib/pkgs.txt\n#&gt; 8  packages used here:\n#&gt;  broom, car, carData, dplyr, ggplot2, heplots, knitr, tidyr",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html",
    "href": "12-eqcov.html",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "",
    "text": "12.1 Homogeneity of Variance in Univariate ANOVA\nIn classical (Gaussian) univariate ANOVA models, the main interest is typically on tests of mean differences in a response \\(y\\) according to one or more factors. The validity of the typical \\(F\\) test, however, relies on the assumption of homogeneity of variance: all groups have the same (or similar) variance, \\[\n\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_g^2 \\; .\n\\]\nIt turns out that the \\(F\\) test for differences in means is relatively robust to violation of this assumption (Harwell et al., 1992), as long as the group sizes are roughly equal.1\nA variety of classical test statistics for homogeneity of variance are available, including Hartley’s \\(F_{max}\\) (Hartley, 1950), Cochran’s C (Cochran, 1941),and Bartlett’s test (Bartlett, 1937), but these have been found to have terrible statistical properties (Rogan & Keselman, 1977), which prompted Box’s famous quote.\nLevene (1960) introduced a different form of test, based on the simple idea that when variances are equal across groups, the average absolute values of differences between the observations and group means will also be equal, i.e., substituting an \\(L_1\\) norm for the \\(L_2\\) norm of variance. In a one-way design, this is equivalent to a test of group differences in the means of the auxilliary variable \\(z_{ij} = | y_{ij} - \\bar{y}_i |\\).\nMore robust versions of this test were proposed by Brown & Forsythe (1974). These tests substitute the group mean by either the group median or a trimmed mean in the ANOVA of the absolute deviations, and should be almost always preferred to Levene’s version. See Conover et al. (1981) for an early review and Gastwirth et al. (2009) for a general discussion of these tests. In what follows, we refer to this class of tests as “Levene-type” tests and suggest a multivariate extension described below (Section 12.2).",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#sec-mlevene",
    "href": "12-eqcov.html#sec-mlevene",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.2 Homogeneity of variance in ANOVA",
    "text": "12.2 Homogeneity of variance in ANOVA",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#sec-homogeneity-MANOVA",
    "href": "12-eqcov.html#sec-homogeneity-MANOVA",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.3 Homogeneity of variance in MANOVA",
    "text": "12.3 Homogeneity of variance in MANOVA\nIn the MANOVA context, the main emphasis, of course, is on differences among mean vectors, testing \\[\n\\mathcal{H}_0 : \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2 = \\cdots = \\mathbf{\\mu}_g \\; .\n\\] However, the standard test statistics (Wilks’ Lambda, Hotelling-Lawley trace, Pillai-Bartlett trace, Roy’s maximum root) rely upon the analogous assumption that the within-group covariance matrices for all groups are equal, \\[\n\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\cdots = \\mathbf{\\Sigma}_g \\; .\n\\]\nInsert pairs covEllipses for penguins data\nTo preview the main example, Figure 12.1 shows data ellipses for the main size variables in the palmerpenguins::penguins data.\n\n\n\n\n\n\n\nFigure 12.1: Data ellipses for the penguins data.\n\n\n\n\nThey covariance ellipses look pretty similar in size, shape and orientation. But what does Box’s M test (described below) say? As you can see, it concludes strongly against the null hypothesis.\n\nboxM(cbind(bill_length, bill_depth, flipper_length, body_mass) ~ species, data=peng)\n#&gt; \n#&gt;  Box's M-test for Homogeneity of Covariance Matrices\n#&gt; \n#&gt; data:  Y\n#&gt; Chi-Sq (approx.) = 75, df = 20, p-value = 3e-08",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#sec-boxM",
    "href": "12-eqcov.html#sec-boxM",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.4 Assessing heterogeneity of covariance matrices: Box’s M test",
    "text": "12.4 Assessing heterogeneity of covariance matrices: Box’s M test\nBox (1949) proposed the following likelihood-ratio test (LRT) statistic for testing the hypothesis of equal covariance matrices, \\[\nM = (N -g) \\ln \\;|\\; \\mathbf{S}_p \\;|\\; - \\sum_{i=1}^g (n_i -1) \\ln \\;|\\; \\mathbf{S}_i \\;|\\; \\; ,\n\\] {eq-boxm}\nwhere \\(N = \\sum n_i\\) is the total sample size and \\(\\mathbf{S}_p = (N-g)^{-1} \\sum_{i=1}^g (n_i - 1) \\mathbf{S}_i\\) is the pooled covariance matrix. \\(M\\) can thus be thought of as a ratio of the determinant of the pooled \\(\\mathbf{S}_p\\) to the geometric mean of the determinants of the separate \\(\\mathbf{S}_i\\).\nIn practice, there are various transformations of the value of \\(M\\) to yield a test statistic with an approximately known distribution (Timm, 1975). Roughly speaking, when each \\(n_i &gt; 20\\), a \\(\\chi^2\\) approximation is often used; otherwise an \\(F\\) approximation is known to be more accurate.\nAsymptotically, \\(-2 \\ln (M)\\) has a \\(\\chi^2\\) distribution. The \\(\\chi^2\\) approximation due to Box (1949, 1950) is that \\[\nX^2 = -2 (1-c_1) \\ln (M) \\quad \\sim \\quad \\chi^2_{df}\n\\] with \\(df = (g-1) p (p+1)/2\\) degrees of freedom, and a bias correction constant: \\[\nc_1 = \\left(\n\\sum_i \\frac{1}{n_i -1}\n- \\frac{1}{N-g}\n\\right)\n\\frac{2p^2 +3p -1}{6 (p+1)(g-1)} \\; .\n\\]\nIn this form, Bartlett’s test for equality of variances in the univariate case is the special case of Box’s M when there is only one response variable, so Bartlett’s test is sometimes used as univariate follow-up to determine which response variables show heterogeneity of variance.\nYet, like its univariate counterpart, Box’s test is well-known to be highly sensitive to violation of (multivariate) normality and the presence of outliers. For example, Tiku & Balakrishnan (1984) concluded from simulation studies that the normal-theory LRT provides poor control of Type I error under even modest departures from normality. O’Brien (1992) proposed some robust alternatives, and showed that Box’s normal theory approximation suffered both in controlling the null size of the test and in power. Zhang & Boos (1992) also carried out simulation studies with similar conclusions and used bootstrap methods to obtain corrected critical values.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#visualizing-heterogeneity",
    "href": "12-eqcov.html#visualizing-heterogeneity",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.5 Visualizing heterogeneity",
    "text": "12.5 Visualizing heterogeneity\nThe goal of this chapter is to use the above background as a platform for discussing approaches to visualizing and testing the heterogeneity of covariance matrices in multivariate designs. While researchers often rely on a single number to determine if their data have met a particular threshold, such compression will often obscure interesting information, particularly when a test concludes that differences exist, and one is left to wonder ``why?’’. It is within this context where, again, visualizations often reign supreme. In fact, we find it somewhat surprising that this issue has not been addressed before graphically in any systematic way. TODO: cut this down\nIn what follows, we propose three visualization-based approaches to questions of heterogeneity of covariance in MANOVA designs:\n\ndirect visualization of the information in the \\(\\mathbf{S}_i\\) and \\(\\mathbf{S}_p\\) using data ellipsoids to show size and shape as minimal schematic summaries;\na simple dotplot of the components of Box’s M test: the log determinants of the \\(\\mathbf{S}_i\\) together with that of the pooled \\(\\mathbf{S}_p\\). Extensions of these simple plots raise the question of whether measures of heterogeneity other than that captured in Box’s test might also be useful; and,\nthe connection between Levene-type tests and an ANOVA (of centered absolute differences) suggests a parallel with a multivariate extension of Levene-type tests and a MANOVA. We explore this with a version of Hypothesis-Error (HE) plots we have found useful for visualizing mean differences in MANOVA designs.\n\n\n#&gt; Writing packages to  C:/R/Projects/Vis-MLM-book/bib/pkgs.txt\n#&gt; 9  packages used here:\n#&gt;  broom, candisc, car, carData, dplyr, ggplot2, heplots, knitr, tidyr\n\n\n\n\n\n\nBartlett, M. S. (1937). Properties of sufficiency and statistical tests. Proceedings of the Royal Society of London. Series A, 160(901), 268–282. https://doi.org/10.2307/96803\n\n\nBox, G. E. P. (1949). A general distribution theory for a class of likelihood criteria. Biometrika, 36(3-4), 317–346. https://doi.org/10.1093/biomet/36.3-4.317\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and wear curves. Biometrics, 6, 362–389.\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances. Biometrika, 40(3/4), 318–335. https://doi.org/10.2307/2333350\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the American Statistical Association, 69(346), 364–367. https://doi.org/10.1080/01621459.1974.10482955\n\n\nCochran, W. G. (1941). The distribution of the largest of a set of estimated variances as a fraction of their total. Annals of Eugenics, 11(1), 47–52. https://doi.org/10.1111/j.1469-1809.1941.tb02271.x\n\n\nConover, W. J., Johnson, M. E., & Johnson, M. M. (1981). A comparative study of tests for homogeneity of variances, with applications to the outer continental shelf bidding data. Technometrics, 23(4), 351–361. https://doi.org/10.1080/00401706.1981.10487680\n\n\nGastwirth, J. L., Gel, Y. R., & Miao, W. (2009). The impact of Levene’s test of equality of variances on statistical theory and practice. Statistical Science, 24(3), 343–360. https://doi.org/10.1214/09-STS301\n\n\nHartley, H. O. (1950). The use of range in analysis of variance. Biometrika, 37(3–4), 271–280. https://doi.org/10.1093/biomet/37.3-4.271\n\n\nHarwell, M. R., Rubinstein, E. N., Hayes, W. S., & Olds, C. C. (1992). Summarizing monte carlo results in methodological research: The one- and two-factor fixed effects ANOVA cases. Journal of Educational and Behavioral Statistics, 17(4), 315–339. https://doi.org/10.3102/10769986017004315\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. Olkin, S. G. Ghurye, W. Hoeffding, W. G. Madow, & H. B. Mann (Eds.), Contributions to probability and statistics: Essays in honor of Harold Hotelling (pp. 278–292). Stanford University Press.\n\n\nLix, J. M., L. M. Keselman, & Keselman, H. J. (1996). Consequences of assumption violations revisited: A quantitative review of alternatives to the one-way analysis of variance F test. Review of Educational Research, 66(4), 579–619. https://doi.org/10.3102/00346543066004579\n\n\nO’Brien, P. C. (1992). Robust procedures for testing equality of covariance matrices. Biometrics, 48(3), 819–827. http://www.jstor.org/stable/2532347\n\n\nRogan, J. C., & Keselman, H. J. (1977). Is the ANOVA f-test robust to variance heterogeneity when sample sizes are equal?: An investigation via a coefficient of variation. American Educational Research Journal, 14(4), 493–498. https://doi.org/10.3102/00028312014004493\n\n\nTiku, M. L., & Balakrishnan, N. (1984). Testing equality of population variances the robust way. Communications in Statistics - Theory and Methods, 13(17), 2143–2159. https://doi.org/10.1080/03610928408828818\n\n\nTimm, N. H. (1975). Multivariate analysis with applications in education and psychology. Wadsworth (Brooks/Cole).\n\n\nZhang, J., & Boos, D. D. (1992). Bootstrap critical values for testing homogeneity of covariance matrices. Journal of the American Statistical Association, 87(418), 425–429. http://www.jstor.org/stable/2290273",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#footnotes",
    "href": "12-eqcov.html#footnotes",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "",
    "text": "If group sizes are greatly unequal and homogeneity of variance is violated, then the \\(F\\) statistic is too liberal (\\(p\\) values too large) when large sample variances are associated with small group sizes. Conversely, the \\(F\\) statistic is too conservative if large variances are associated with large group sizes.↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-case-studies.html",
    "href": "13-case-studies.html",
    "title": "\n13  Case studies\n",
    "section": "",
    "text": "13.1 Neuro- and Social-cognitive measures in psychiatric groups\nA Ph.D. dissertation by Laura Hartman (2016) at York University was designed to evaluate whether and how clinical patients diagnosed (on the DSM-IV) as schizophrenic or with schizoaffective disorder could be distinguished from each other and from a normal, control sample on collections of standardized tests in the following domains:\nThe study is an important contribution to clinical research because the two diagnostic categories are subtly different and their symptoms often overlap. Yet, they’re very different and often require different treatments. A key difference between schizoaffective disorder and schizophrenia is the prominence of mood disorder involving bipolar, manic and depressive moods. With schizoaffective disorder, mood disorders are front and center. With schizophrenia, that is not a dominant part of the disorder, but psychotic ideation (hearing voices, seeing imaginary people) is.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "13-case-studies.html#neuro--and-social-cognitive-measures-in-psychiatric-groups",
    "href": "13-case-studies.html#neuro--and-social-cognitive-measures-in-psychiatric-groups",
    "title": "\n13  Case studies\n",
    "section": "",
    "text": "Neuro-cognitive: processing speed, attention, verbal learning, visual learning and problem solving;\nSocial-cognitive: managing emotions, theory of mind, externalizing, personalizing bias.\n\n\n\n13.1.1 Research questions\nThis example is concerned with the following substantitive questions:\n\nTo what extent can patients diagnosed as schizophrenic or with schizoaffective disorder be distinguished from each other and from a normal control sample using a well-validated, comprehensive neurocognitive battery specifically designed for individuals with psychosis (Heinrichs et al., 2015) ?\nIf the groups differ, do any of the cognitive domains show particularly larger or smaller differences among these groups?\nDo the neurocognitive measures discriminate among the in the same or different ways? If different, how many separate aspects or dimensions are distinguished?\n\nApart from the research interest, it could aid diagnosis and treatment if these similar mental disorders could be distinguished by tests in the cognitive domain.\n\n13.1.2 Data\nThe clinical sample comprised 116 male and female patients who met the following criteria: 1) a diagnosis of schizophrenia (\\(n\\) = 70) or schizoaffective disorder (\\(n\\) = 46) confirmed by the Structured Clinical Interview for DSM-IV-TR Axis I Disorders; 2) were outpatients; 3) a history free of developmental or learning disability; 4) age 18-65; 5) a history free of neurological or endocrine disorder; and 6) no concurrent diagnosis of substance use disorder. Non-psychiatric control participants (\\(n\\) = 146) were screened for medical and psychiatric illness and history of substance abuse and were recruited from three outpatient clinics.\n\ndata(NeuroCog, package=\"heplots\")\nglimpse(NeuroCog)\n#&gt; Rows: 242\n#&gt; Columns: 10\n#&gt; $ Dx        &lt;fct&gt; Schizophrenia, Schizophrenia, Schizophrenia, Sch…\n#&gt; $ Speed     &lt;int&gt; 19, 8, 14, 7, 21, 31, -1, 17, 7, 37, 30, 26, 32,…\n#&gt; $ Attention &lt;int&gt; 9, 25, 23, 18, 9, 10, 8, 20, 30, 15, 27, 20, 23,…\n#&gt; $ Memory    &lt;int&gt; 19, 15, 15, 14, 35, 26, 3, 27, 26, 17, 28, 22, 2…\n#&gt; $ Verbal    &lt;int&gt; 33, 28, 20, 34, 28, 29, 20, 30, 26, 33, 34, 33, …\n#&gt; $ Visual    &lt;int&gt; 24, 24, 13, 16, 29, 21, 12, 32, 27, 21, 19, 18, …\n#&gt; $ ProbSolv  &lt;int&gt; 39, 40, 32, 31, 45, 33, 29, 29, 30, 33, 30, 39, …\n#&gt; $ SocialCog &lt;int&gt; 28, 37, 24, 36, 28, 28, 28, 44, 39, 24, 32, 36, …\n#&gt; $ Age       &lt;int&gt; 44, 26, 55, 53, 51, 21, 53, 56, 48, 46, 48, 31, …\n#&gt; $ Sex       &lt;fct&gt; Female, Male, Female, Male, Male, Male, Male, Fe…\n\nThe diagnostic classification variable is called Dx in the dataset. To facilitate answering questions regarding group differences, the following contrasts were applied: the first column compares the control group to the average of the diagnosed groups, the second compares the schizophrenia group against the schizoaffective group.\n\ncontrasts(NeuroCog$Dx)\n#&gt;                 [,1] [,2]\n#&gt; Schizophrenia   -0.5    1\n#&gt; Schizoaffective -0.5   -1\n#&gt; Control          1.0    0\n\nIn this analysis, we ignore the SocialCog variable. The primary focus is on the variables Attention : ProbSolv.\n\n13.1.3 A first look\nAs always, plot the data first! We want an overview of the distributions of the variables to see the centers, spread, shape and possible outliers for each group on each variable.\nThe plot below combines the use of boxplots and violin plots to give an informative display. As we saw earlier (Chapter XXX), doing this with ggplot2 requires reshaping the data to long format.\n\n# Reshape from wide to long\nNC_long &lt;- NeuroCog |&gt;\n  dplyr::select(-SocialCog, -Age, -Sex) |&gt;\n  tidyr::gather(key = response, value = \"value\", Speed:ProbSolv)\n# view 3 observations per group and measure\nNC_long |&gt;\n  group_by(Dx) |&gt;\n  sample_n(3) |&gt; ungroup()\n#&gt; # A tibble: 9 × 3\n#&gt;   Dx              response  value\n#&gt;   &lt;fct&gt;           &lt;chr&gt;     &lt;int&gt;\n#&gt; 1 Schizophrenia   Speed        39\n#&gt; 2 Schizophrenia   Visual       21\n#&gt; 3 Schizophrenia   Memory       40\n#&gt; 4 Schizoaffective ProbSolv     40\n#&gt; 5 Schizoaffective Speed        25\n#&gt; 6 Schizoaffective Verbal       48\n#&gt; 7 Control         Speed        33\n#&gt; 8 Control         ProbSolv     43\n#&gt; 9 Control         Attention    37\n\nIn the plot, we take care to adjust the transparency (alpha) values for the points, violin plots and boxplots so that all can be seen. Options for geom_boxplot() are used to give these greater visual prominence.\n\nCodeggplot(NC_long, aes(x=Dx, y=value, fill=Dx)) +\n  geom_jitter(shape=16, alpha=0.8, size=1, width=0.2) +\n  geom_violin(alpha = 0.1) +\n  geom_boxplot(width=0.5, alpha=0.4, \n               outlier.alpha=1, outlier.size = 3, outlier.color = \"red\") +\n  scale_x_discrete(labels = c(\"Schizo\", \"SchizAff\", \"Control\")) +\n  facet_wrap(~response, scales = \"free_y\", as.table = FALSE) +\n  theme_bw() +\n  theme(legend.position=\"bottom\",\n        axis.title = element_text(size = rel(1.2)),\n        axis.text  = element_text(face = \"bold\"),\n        strip.text = element_text(size = rel(1.2)))\n\n\n\n\n\n\nFigure 13.1: Boxplots and violin plots of the NeuroCog data.\n\n\n\n\nWe can see that the control participants score higher on all measures, but there is no consistent pattern of medians for the two patient groups. But these univariate summaries do not inform about the relations among variables.\n\n13.1.4 Bivariate views\nCorrgram\nA corrgram (Friendly, 2002) provides a useful reconnaisance plot of the bivariate correlations in the dataset. It suppresses details, and allows focus on the overall pattern. The corrgram::corrgram() function has the ability to enhance perception by permuting the variables in the order of their variable vectors in a biplot, so more highly correlated variables are adjacent in the plot, and example of effect ordering for data displays (Friendly & Kwan, 2003).\nThe plot below includes all variables except for Dx group. There are a number of panel.* functions for choosing how the correlation for each pair is rendered.\n\nNeuroCog |&gt;\n  select(-Dx) |&gt;\n  corrgram(order = TRUE,\n           diag.panel = panel.density,\n           upper.panel = panel.pie)\n\n\n\n\n\n\nFigure 13.2: corrgram of the NeuroCog data. The upper and lower triangles use two different ways of encoding the value of the correlation for each pair of variables.\n\n\n\n\nIn this plot you can see that adjacent variables are more highly correlated than those more widely separated. The diagonal panels show that most variables are reasonably symmetric in their distributions. Age, not included in this analysis is negatively correlated with the others: older participants tend to do less well on these tests.\nScatterplot matrix\nA scatterplot matrix gives a more detailed overview of all pairwise relations. The plot below suppresses the data points and summarizes the relation using data ellipses and regression lines. The model syntax, ~ Speed + ... |Dx, treats Dx as a conditioning variable (similar to the use of the color aestheic in ggplot2) giving a separate data ellipse and regression line for each group. (The legend is suppressed here. The groups are Schizophrenic, SchizoAffective, Normal.)\n\nscatterplotMatrix(~ Speed + Attention + Memory + Verbal + Visual + ProbSolv | Dx,\n  data=NeuroCog,\n  plot.points = FALSE,\n  smooth = FALSE,\n  legend = FALSE,\n  col = scales::hue_pal()(3),\n  ellipse=list(levels=0.68))\n\n\n\n\n\n\nFigure 13.3: Scatterplot matrix of the NeuroCog data. Points are suppressed here, focusing on the data ellipses and regression lines. Colors for the groups: Schizophrenic (red), SchizoAffective (green), Normal (blue)\n\n\n\n\nIn this figure, we can see that the regression lines have similar slopes and similar data ellipses for the groups, though with a few exceptions.\nTODO: Should we add biplot here?",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "13-case-studies.html#fitting-the-mlm",
    "href": "13-case-studies.html#fitting-the-mlm",
    "title": "\n13  Case studies\n",
    "section": "\n13.2 Fitting the MLM",
    "text": "13.2 Fitting the MLM\nWe proceed to fit the one-way MANOVA model.\n\nNC.mlm &lt;- lm(cbind(Speed, Attention, Memory, Verbal, Visual, ProbSolv) ~ Dx,\n             data=NeuroCog)\nAnova(NC.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Dx  2     0.299     6.89     12    470 1.6e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe first research question is captured by the contrasts for the Dx factor shown above. We can test these with car::linearHypothesis(). The contrast Dx1 for control vs. the diagnosed groups is highly significant,\n\n# control vs. patients\nprint(linearHypothesis(NC.mlm, \"Dx1\"), SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            1     0.289     15.9      6    234 2.8e-15 ***\n#&gt; Wilks             1     0.711     15.9      6    234 2.8e-15 ***\n#&gt; Hotelling-Lawley  1     0.407     15.9      6    234 2.8e-15 ***\n#&gt; Roy               1     0.407     15.9      6    234 2.8e-15 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nbut the second contrast, Dx2, comparing the schizophrenic and schizoaffective group, is not.\n\n# Schizo vs SchizAff\nprint(linearHypothesis(NC.mlm, \"Dx2\"), SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)\n#&gt; Pillai            1     0.006    0.249      6    234   0.96\n#&gt; Wilks             1     0.994    0.249      6    234   0.96\n#&gt; Hotelling-Lawley  1     0.006    0.249      6    234   0.96\n#&gt; Roy               1     0.006    0.249      6    234   0.96\n\n\n13.2.1 HE plot\nSo the question becomes: how to understand these results.heplot() shows the visualization of the multivariate model in the space of two response variables (the first two by default). The result (Figure 13.4) tells a very simple story: The control group performs higher on higher measures than the diagnosed groups, which do not differ between themselves.\n(For technical reasons, to abbreviate the group labels in the plot, we need to update() the MLM model after the labels are reassigned.)\n\n# abbreviate levels for plots\nNeuroCog$Dx &lt;- factor(NeuroCog$Dx, \n                      labels = c(\"Schiz\", \"SchAff\", \"Contr\"))\nNC.mlm &lt;- update(NC.mlm)\n\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\nheplot(NC.mlm, \n       fill=TRUE, fill.alpha=0.1,\n       cex.lab=1.3, cex=1.25)\npar(op)\n\n\n\n\n\n\nFigure 13.4: HE plot of Speed and Attention in the MLM for the NeuroCog data. The labeled points show the means of the groups on the two variables. The blue H ellipse for groups indicates the strong positive correlation of the group means.\n\n\n\n\nThis pattern is consistent across all of the response variables, as we see from a plot of pairs(NC.mlm):\n\npairs(NC.mlm, \n      fill=TRUE, fill.alpha=0.1,\n      var.cex=2)\n\n\n\n\n\n\nFigure 13.5: HE plot matrix of the MLM for NeuroCog data.\n\n\n\n\n\n13.2.2 Canonical space\nWe can gain further insight, and a simplified plot showing all the response variables by projecting the MANOVA into the canonical space, which is entirely 2-dimensional (because \\(df_h=2\\)). However, the output from candisc() shows that 98.5% of the mean differences among groups can be accounted for in one canonical dimension. ::: {.cell layout-align=“center”}\nNC.can &lt;- candisc(NC.mlm)\nNC.can\n#&gt; \n#&gt; Canonical Discriminant Analysis for Dx:\n#&gt; \n#&gt;    CanRsq Eigenvalue Difference Percent Cumulative\n#&gt; 1 0.29295    0.41433      0.408    98.5       98.5\n#&gt; 2 0.00625    0.00629      0.408     1.5      100.0\n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;   LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1        0.703     7.53    12   468   9e-13 ***\n#&gt; 2        0.994     0.30     5   235    0.91    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n:::\nFigure 13.6 is the result of the plot() method for class \"candisc\" objects, that is, the result of calling plot(NC.can, ...). It plots the two canonical scores, \\(\\mathbf{Z}_{n \\times 2}\\) for the subjects, together with data ellipses for each of the three groups.\n\npos &lt;- c(4, 1, 4, 4, 1, 3)\ncol &lt;- c(\"red\", \"darkgreen\", \"blue\")\nop &lt;- par(mar=c(5,4,1,1)+.1)\nplot(NC.can, \n     ellipse=TRUE, \n     rev.axes=c(TRUE,FALSE), \n     pch=c(7,9,10),\n     var.cex=1.2, cex.lab=1.5, var.lwd=2,  scale=4.5, \n     col=col,\n     var.col=\"black\", var.pos=pos,\n     prefix=\"Canonical dimension \")\npar(op)\n\n\n\n\n\n\nFigure 13.6: Canonical discriminant plot for the NeuroCog data MANOVA. Scores on the two canonical dimensions are plotted, together with 68% data ellipses for each group.\n\n\n\n\nThe interpretation of Figure 13.6 is again fairly straightforward. As noted earlier (REF???), the projections of the variable vectors in this plot on the coordinate axes are proportional to the correlations of the responses with the canonical scores. From this, we see that the normal group differs from the two patient groups, having higher scores on all the neurocognitive variables, most of which are highyl correlated. The problem solving measure is slightly different, and this, compared to the cluster of memory, verbal and attention, is what distinguishes the schizophrenic group from the schizoaffectives.\nThe separation of the groups is essentially one-dimensional, with the control group higher on all measures. Moreover, the variables processing speed and visual memory are the purest measures of this dimension, but all variables contribute positively. The second canonical dimension accounts for only 1.5% of group mean differences and is non-significant (by a likelihood ratio test). Yet, if we were to interpret it, we would note that the schizophrenia group is slightly higher on this dimension, scoring better in problem solving and slightly worse on working memory, attention, and verbal learning tasks.\nSummary\nThis analysis gives a very simple description of the data, in relation to the research questions posed earlier:\n\nOn the basis of these neurocognitive tests, the schizophrenic and schizoaffective groups do not differ significantly overall, but these groups differ greatly from the normal controls.\nAll cognitive domains distinguish the groups in the same direction, with the greatest differences shown for the variables most closely aligned with the horizontal axis in Figure 13.6.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "13-case-studies.html#social-cognitive-measures",
    "href": "13-case-studies.html#social-cognitive-measures",
    "title": "\n13  Case studies\n",
    "section": "\n13.3 Social cognitive measures",
    "text": "13.3 Social cognitive measures\nThe social cognitive measures were designed to tap various aspects of the perception and cognitive processing of emotions of others. Emotion perception was assessed using a Managing Emotions score from the MCCB. A “theory of mind” (ToM) score assessed ability to read the emotions of others from photographs of the eye region of male and female faces. Two other measures, externalizing bias (ExtBias) and personalizing bias (PersBias) were calculated from a scale measuring the degree to which individuals attribute internal, personal or situational causal attributions to positive and negative social events.\nThe analysis of the SocialCog data proceeds in a similar way: first we fit the MANOVA model, then test the overall differences among groups using Anova(). We find that the overall multivariate test is again significant,\n\ndata(SocialCog, package=\"heplots\")\nSC.mlm &lt;-  lm(cbind(MgeEmotions,ToM, ExtBias, PersBias) ~ Dx,\n               data=SocialCog)\nAnova(SC.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Dx  2     0.212     3.97      8    268 0.00018 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTesting the same two contrasts using linearHypothesis() (results not shown), w e find that the overall multivariate test is again significant, but now both contrasts are significant (Dx1: \\(F(4, 133)=5.21, p &lt; 0.001\\); Dx2: \\(F(4, 133)=2.49, p = 0.0461\\)), the test for Dx2 just barely.\n\n# control vs. patients\nprint(linearHypothesis(SC.mlm, \"Dx1\"), SSP=FALSE)\n# Schizo vs. SchizAff\nprint(linearHypothesis(SC.mlm, \"Dx2\"), SSP=FALSE)\n\nThese results are important, because, if they are reliable and make sense substantively, they imply that patients with schizophrenia and schizoaffective diagnoses can be distinguished by their performance on tasks assessing social perception and cognition. This was potentially a new finding in the literature on schizophrenia.\nAs we did above, it is useful to visualize the nature of these differences among groups with HE plots for the SC.mlm model. Each contrast has a corresponding \\(\\mathbf{H}\\) ellipse, which we can show in the plot using the hypotheses argument. With a single degree of freedom, these degenerate ellipses plot as lines.\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\nheplot(SC.mlm, \n       hypotheses=list(\"Dx1\"=\"Dx1\", \"Dx2\"=\"Dx2\"),\n       fill=TRUE, fill.alpha=.1,\n       cex.lab=1.5, cex=1.2)\npar(op)\n\n\n\n\n\n\nFigure 13.7: HE plot of Speed and Attention in the MLM for the SocialCog data. The labeled points show the means of the groups on the two variables. The lines for Dx1 and Dx2 show the tests of the contrasts among groups.\n\n\n\n\nIt can be seen that the three group means are approximately equally spaced on the ToM measure, whereas for MgeEmotions, the control and schizoaffective groups are quite similar, and both are higher than the schizophrenic group. This ordering of the three groups was somewhat similar for the other responses, as we could see in a pairs(SC.mlm) plot.\n\n13.3.1 Model checking\nNormally, we would continue this analysis, and consider other HE and canonical discriminant plots to further interpret the results, in particular the relations of the cognitive measures to group differences, or perhaps an analysis of the relationships between the neuro- and social-cognitive measures. We don’t pursue this here for reasons of length, but this example actually has a more important lesson to demonstrate.\nBefore beginning the MANOVA analyses, extensive data screening was done by the client using SPSS, in which all the response and predictor variables were checked for univariate normality and multivariate normality (MVN) for both sets. This traditional approach yielded a huge amount of tabular output and no graphs, and did not indicate any major violation of assumptions.1\nA simple visual test of MVN and the possible presence of multivariate outliers is related to the theory of the data ellipse: Under MVN, the squared Mahalanobis distances \\(D^2_M (\\mathbf{y}) = (\\mathbf{y} - \\bar{\\mathbf{y}})' \\, \\mathbf{S}^{-1} \\, (\\mathbf{y} - \\bar{\\mathbf{y}})\\) should follow a \\(\\chi^2_p\\) distribution. Thus, a quantile-quantile plot of the ordered \\(D^2_M\\) values vs. corresponding quantiles of the \\(\\chi^2\\) distribution should approximate a straight line (Cox, 1968; Healy, 1968). Note that this should be applied to the residuals from the model – residuals(SC.mlm) – and not to the response variables directly.\nheplots::cqplot() implements this for \"mlm\" objects Calling this function for the model SC.mlm produces Figure 13.8. It is immediately apparent that there is one extreme multivariate outlier; three other points are identified, but the remaining observations are nearly within the 95% confidence envelope (using a robust MVE estimate of \\(\\mathbf{S}\\)).\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\ncqplot(SC.mlm, method=\"mve\", \n       id.n=4, \n       main=\"\", \n       cex.lab=1.25)\npar(op)\n\n\n\n\n\n\nFigure 13.8: Chi-square quantile-quantile plot for residuals from the model SC.mlm. The confidence band gives a point-wise 95% envelope, providing information about uncertainty. One extreme multivariate outlier is highlighted.\n\n\n\n\nFurther checking revealed that this was a data entry error where one case (15) in the schizophrenia group had a score of -33 recorded on the ExtBias measure, whose valid range was (-10, +10). In R, it is very easy to re-fit a model to a subset of observations (rather than modifying the dataset itself) using update(). The result of the overall Anova and the test of Dx1 were unchanged; however, the multivariate test for the most interesting contrast Dx2 comparing the schizophrenia and schizoaffective groups became non-significant at the \\(\\alpha=0.05\\) level (\\(F(4, 133)=2.18, p = 0.0742\\)).\n\nSC.mlm1 &lt;- update(SC.mlm, \n                  subset=rownames(SocialCog)!=\"15\")\n\nAnova(SC.mlm1)\nprint(linearHypothesis(SC.mlm1, \"Dx1\"), SSP=FALSE)\nprint(linearHypothesis(SC.mlm1, \"Dx2\"), SSP=FALSE)\n\n\n13.3.2 Canonical HE plot\nThis outcome creates a bit of a quandry for further analysis (do univariate follow-up tests? try a robust model?) and reporting (what to claim about the Dx2 contrast?) that we don’t explore here. Rather, we proceed to attempt to interpret the MLM with the aid of canonical analysis and a canonical HE plot. The canonical analysis of the model SC.mlm1 now shows that both canonical dimensions are significant, and account for 83.9% and 16.1% of between group mean differences respectively.\n\nSC.can1 &lt;- candisc(SC.mlm1)\nSC.can1\n#&gt; \n#&gt; Canonical Discriminant Analysis for Dx:\n#&gt; \n#&gt;   CanRsq Eigenvalue Difference Percent Cumulative\n#&gt; 1 0.1645     0.1969      0.159    83.9       83.9\n#&gt; 2 0.0364     0.0378      0.159    16.1      100.0\n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;   LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1        0.805     3.78     8   264 0.00032 ***\n#&gt; 2        0.964     1.68     3   133 0.17537    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\nheplot(SC.can1, \n  fill=TRUE, fill.alpha=.1,\n  hypotheses=list(\"Dx1\"=\"Dx1\", \"Dx2\"=\"Dx2\"),\n  lwd = c(1, 2, 3, 3),\n  col=c(\"red\", \"blue\", \"darkgreen\", \"darkgreen\"),\n  var.lwd=2, \n  var.col=\"black\", \n  label.pos=c(3,1), \n  var.cex=1.2, \n  cex=1.25, cex.lab=1.2, \n  scale=2.8,\n  prefix=\"Canonical dimension \")\npar(op)\n\n\n\n\n\n\nFigure 13.9: Canonical HE plot for the corrected SocialCog MANOVA. The variable vectors show the correlations of the responses with the canonical variables. The embedded green lines show the projections of the H ellipses for the contrasts Dx1 and Dx2 in canonical space.\n\n\n\n\nThe HE plot version of this canonical plot is shown in Figure 13.9. Because the heplot() method for a \"candisc\" object refits the original model to the \\(\\mathbf{Z}\\) canonical scores, it is easy to also project other linear hypotheses into this space. Note that in this view, both the Dx1 and Dx2 contrasts project outside \\(\\mathbf{E}\\) ellipse.2.\nThis canonical HE plot has a very simple description:\n\nDimension 1 orders the groups from control to schizoaffective to schizophrenia, while dimension 2 separates the schizoaffective group from the others;\nExternalizing bias and theory of mind contributes most to the first dimension, while personal bias and managing emotions are more aligned with the second; and,\nThe relations of the two contrasts to group differences and to the response variables can be easily read from this plot.\n\n\n#cat(\"Packages used here:\\n\")\nwrite_pkgs(file = .pkg_file)\n#&gt; 10  packages used here:\n#&gt;  broom, candisc, car, carData, corrgram, dplyr, ggplot2, heplots, knitr, tidyr\n\n\n\n\n\n\nCox, D. R. (1968). Notes on some aspects of regression analysis. Journal of the Royal Statistical Society Series A, 131, 265–279.\n\n\nFriendly, M. (2002). Corrgrams: Exploratory displays for correlation matrices. The American Statistician, 56(4), 316–324. https://doi.org/10.1198/000313002533\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays. Computational Statistics and Data Analysis, 43(4), 509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nHartman, L. I. (2016). Schizophrenia and schizoaffective disorder: One condition or two? [PhD dissertation]. York University.\n\n\nHealy, M. J. R. (1968). Multivariate normal plotting. Journal of the Royal Statistical Society Series C, 17(2), 157–161.\n\n\nHeinrichs, R. W., Pinnock, F., Muharib, E., Hartman, L., Goldberg, J., & McDermid Vaz, S. (2015). Neurocognitive normality in schizophrenia revisited. Schizophrenia Research: Cognition, 2(4), 227–232. https://doi.org/10.1016/j.scog.2015.09.001\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis with applications. Biometrika, 57(3), 519–530. https://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMardia, K. V. (1974). Applications of some measures of multivariate skewness and kurtosis in testing normality and robustness studies. Sankhya: The Indian Journal of Statistics, Series B, 36(2), 115–128. http://www.jstor.org/stable/25051892",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "13-case-studies.html#footnotes",
    "href": "13-case-studies.html#footnotes",
    "title": "\n13  Case studies\n",
    "section": "",
    "text": "Actually, multivariate normality of the predictors in \\(\\mathbf{X}\\) is not required in the MLM. This assumption applies only to the conditional values \\(\\mathbf{Y} \\;|\\; \\mathbf{X}\\), i.e., that the errors \\(\\mathbf{\\epsilon}_{i}' \\sim \\mathcal{N}_{p}(\\mathbf{0},\\boldsymbol{\\Sigma})\\) with constant covariance matrix. Moreover, the widely used MVN test statistics, such as Mardia’s (1970) test based on multivariate skewness and kurtosis are known to be quite sensitive to mild departures in kurtosis (Mardia, 1974) which do not threaten the validity of the multivariate tests.↩︎\nThe direct application of significance tests to canonical scores probably requires some adjustment because these are computed to have the optimal between-group discrimination.↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "91-colophon.html",
    "href": "91-colophon.html",
    "title": "Colophon",
    "section": "",
    "text": "Package versions\nThe principal R package versions used in examples and illustrations are listed below. These were captured via sessioninfo:::package_info() from all library() commands in the text, and scripts which also updated the references to packages.\nAt the time of writing, most of these were current on CRAN repositories but some development versions are indicated as “local” in the source column.\npackage\nversion\ndate\nsource\n\n\n\nbayestestR\n0.15.0\n2024-10-17\nCRAN\n\n\nbroom\n1.0.7\n2024-09-26\nCRAN\n\n\ncandisc\n0.9.0\n2024-05-06\nCRAN\n\n\ncar\n3.1-3\n2024-09-27\nCRAN\n\n\ncarData\n3.0-5\n2022-01-06\nCRAN\n\n\ncorpcor\n1.6.10\n2021-09-16\nCRAN\n\n\ncorrelation\n0.8.6\n2024-10-26\nCRAN\n\n\ncorrgram\n1.14\n2021-04-29\nCRAN\n\n\ncorrplot\n0.95\n2024-10-14\nCRAN\n\n\ndatawizard\n0.13.0\n2024-10-05\nCRAN\n\n\ndplyr\n1.1.4\n2023-11-17\nCRAN\n\n\neasystats\n0.7.3\n2024-07-22\nCRAN\n\n\neffects\n4.2-2\n2022-07-13\nCRAN\n\n\neffectsize\n0.8.9\n2024-07-03\nCRAN\n\n\nfactoextra\n1.0.7\n2020-04-01\nCRAN\n\n\nFactoMineR\n2.11\n2024-04-20\nCRAN\n\n\nforcats\n1.0.0\n2023-01-29\nCRAN\n\n\ngenridge\n0.7.0\n2023-08-08\nCRAN\n\n\nGGally\n2.2.1\n2024-02-14\nCRAN\n\n\ngganimate\n1.0.9\n2024-02-27\nCRAN\n\n\nggbiplot\n0.6.3\n2024-10-12\nlocal\n\n\nggdensity\n1.0.0\n2023-02-09\nCRAN\n\n\nggeffects\n1.7.2\n2024-10-13\nCRAN\n\n\nggpcp\n0.2.0\n2022-11-28\nCRAN\n\n\nggplot2\n3.5.1\n2024-04-23\nCRAN\n\n\nggpubr\n0.6.0\n2023-02-10\nCRAN\n\n\nggrepel\n0.9.6\n2024-09-07\nCRAN\n\n\nggstats\n0.7.0\n2024-09-22\nCRAN\n\n\nheplots\n1.7.0\n2024-05-02\nCRAN\n\n\nHotelling\n1.0-8\n2021-09-09\nCRAN\n\n\nimager\n1.0.2\n2024-05-13\nCRAN\n\n\ninsight\n0.20.5\n2024-10-02\nCRAN\n\n\nknitr\n1.48\n2024-07-07\nCRAN\n\n\nlubridate\n1.9.3\n2023-09-27\nCRAN\n\n\nmagrittr\n2.0.3\n2022-03-30\nCRAN\n\n\nmarginaleffects\n0.23.0\n2024-10-05\nCRAN\n\n\nMASS\n7.3-61\n2024-06-13\nCRAN\n\n\nmatlib\n1.0.1\n2024-10-23\nlocal\n\n\nmodelbased\n0.8.9\n2024-10-26\nCRAN\n\n\nmodelsummary\n2.2.0\n2024-09-02\nCRAN\n\n\nparameters\n0.23.0\n2024-10-18\nCRAN\n\n\npatchwork\n1.3.0\n2024-09-16\nCRAN\n\n\nperformance\n0.12.4\n2024-10-18\nCRAN\n\n\npurrr\n1.0.2\n2023-08-10\nCRAN\n\n\nqgraph\n1.9.8\n2023-11-03\nCRAN\n\n\nreadr\n2.1.5\n2024-01-10\nCRAN\n\n\nreport\n0.5.9\n2024-07-10\nCRAN\n\n\nRtsne\n0.17\n2023-12-07\nCRAN\n\n\nsee\n0.9.0\n2024-09-06\nCRAN\n\n\nstringr\n1.5.1\n2023-11-14\nCRAN\n\n\ntibble\n3.2.1\n2023-03-20\nCRAN\n\n\ntidyr\n1.3.1\n2024-01-24\nCRAN\n\n\ntidyverse\n2.0.0\n2023-02-22\nCRAN\n\n\ntourr\n1.2.0\n2024-04-20\nCRAN\n\n\nvcd\n1.4-13\n2024-09-16\nCRAN\n\n\nVisCollin\n0.1.2\n2023-09-05\nCRAN",
    "crumbs": [
      "End matter",
      "Colophon"
    ]
  },
  {
    "objectID": "95-references.html",
    "href": "95-references.html",
    "title": "References",
    "section": "",
    "text": "Abbott, E. A. (1884). Flatland: A romance of many dimensions.\nBuccaneer Books.\n\n\nAdler, D., & Murdoch, D. (2023). Rgl: 3D visualization using\nOpenGL. https://CRAN.R-project.org/package=rgl\n\n\nAluja, T., Morineau, A., & Sanchez, G. (2018). Principal\ncomponent analysis for data science. https://pca4ds.github.io/\n\n\nAndrews, D. F. (1972). Plots of high dimensional data.\nBiometrics, 28, 123–136.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. The American\nStatistician, 27, 17–21.\n\n\nArel-Bundock, V. (2024). Modelsummary: Summary tables and plots for\nstatistical models and data: Beautiful, customizable, and\npublication-ready. https://modelsummary.com\n\n\nArel-Bundock, V., Greifer, N., & Heiss, A. (Forthcoming). How to\ninterpret statistical models using marginaleffects in\nR and Python. Journal of Statistical\nSoftware. https://marginaleffects.com\n\n\nAsimov, D. (1985). Grand tour. SIAM Journal of Scientific and\nStatistical Computing, 6(1), 128–143.\n\n\nBarab’asi, A.-L. (2016). Network science. Cambridge University\nPress.\n\n\nBartlett, M. S. (1937). Properties of sufficiency and statistical tests.\nProceedings of the Royal Society of London. Series A,\n160(901), 268–282. https://doi.org/10.2307/96803\n\n\nBecker, R. A., Cleveland, W. S., & Shyu, M.-J. (1996). The visual\ndesign and control of trellis display. Journal of Computational and\nGraphical Statistics, 5(2), 123–155.\n\n\nBelsley, D. A. (1991). Conditioning diagnostics: Collinearity and\nweak data in regression. Wiley.\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression\ndiagnostics: Identifying influential data and sources of\ncollinearity. John Wiley; Sons.\n\n\nBiecek, P., Baniecki, H., Krzyzinski, M., & Cook, D. (2023).\nPerformance is not enough: A story of the rashomon’s quartet.\nhttps://arxiv.org/abs/2302.13356\n\n\nBlack, C., Southwell, C., Emmerson, L., Lunn, D., & Hart, T. (2018).\nTime-lapse imagery of adélie penguins reveals differential winter\nstrategies and breeding site occupation. PLOS ONE,\n13(3), e0193532. https://doi.org/10.1371/journal.pone.0193532\n\n\nBlishen, B., Carroll, W., & Moore, C. (1987). The 1981 socioeconomic\nindex for occupations in canada. Canadian Review of Sociology/Revue\nCanadienne de Sociologie, 24(4), 465–488. https://doi.org/10.1111/j.1755-618x.1987.tb00639.x\n\n\nBock, R. D. (1963). Programming univariate and multivariate analysis of\nvariance. Technometrics, 5(1), 95–117. https://doi.org/10.1080/00401706.1963.10490061\n\n\nBock, R. D. (1964). A computer program forunivariate and multivariate\nanalysis of variance. Proceedings of Scientific Symposium on\nStatistics.\n\n\nBondy, J. A., & Murty, U. S. R. (2008). Graph theory.\nSpringer.\n\n\nBorg, I., & Groenen, P. J. F. (2005). Modern Multidimensional Scaling: Theory and\nApplications. Springer.\n\n\nBorg, I., Groenen, P. J. F., & Mair, P. (2018). Applied\nmultidimensional scaling and unfolding. In SpringerBriefs in\nStatistics. Springer International Publishing. https://doi.org/10.1007/978-3-319-73471-2\n\n\nBox, G. E. P. (1949). A general distribution theory for a class of\nlikelihood criteria. Biometrika, 36(3-4), 317–346. https://doi.org/10.1093/biomet/36.3-4.317\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and\nwear curves. Biometrics, 6, 362–389.\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances.\nBiometrika, 40(3/4), 318–335. https://doi.org/10.2307/2333350\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of\nvariances. Journal of the American Statistical Association,\n69(346), 364–367. https://doi.org/10.1080/01621459.1974.10482955\n\n\nBrown, P. J., & Zidek, J. V. (1980). Adaptive multivariate ridge\nregression. The Annals of Statistics, 8(1), 64–74. http://www.jstor.org/stable/2240743\n\n\nBuja, A., Cook, D., Asimov, D., & Hurley, C. (2005). Computational\nmethods for high-dimensional rotations in data visualization. In J. S.\nCR Rao EJ Wegman (Ed.), Handbook of statistics (pp. 391–413).\nElsevier. https://doi.org/10.1016/s0169-7161(04)24014-7\n\n\ncagne, M. (1885). Coordonnées parallèles\net axiales: Méthode de transformation\ngéométrique et\nprocédé nouveau de calcul graphique\ndéduits de la considération des\ncoordonnées parallèlles.\nGauthier-Villars. http://historical.library.cornell.edu/cgi-bin/cul.math/docviewer?did=00620001&seq=3\n\n\nCajori, F. (1926). Origins of fourth dimension concepts. The\nAmerican Mathematical Monthly, 33(8), 397–406. https://doi.org/10.1080/00029890.1926.11986607\n\n\nCattell, R. B. (1966). The scree test for the number of factors.\nMultivariate Behavioral Research, 1(2), 245–276. https://doi.org/10.1207/s15327906mbr0102_10\n\n\nChambers, J. M., & Hastie, T. J. (1991). Statistical models in\ns (p. 624). Chapman & Hall/CRC.\n\n\nCleveland, W. S. (1979). Robust locally weighted regression and\nsmoothing scatterplots. Journal of the American Statistical\nAssociation, 74, 829–836.\n\n\nCleveland, W. S. (1985). The elements of graphing data.\nWadsworth Advanced Books.\n\n\nCleveland, W. S., & Devlin, S. J. (1988). Locally weighted\nregression: An approach to regression analysis by local fitting.\nJournal of the American Statistical Association, 83,\n596–610.\n\n\nCleveland, W. S., & McGill, R. (1984). Graphical perception: Theory,\nexperimentation and application to the development of graphical methods.\nJournal of the American Statistical Association, 79,\n531–554.\n\n\nCleveland, W. S., & McGill, R. (1985). Graphical perception and\ngraphical methods for analyzing scientific data. Science,\n229, 828–833.\n\n\nClyde, D. J., Cramer, E. M., & Sherin, R. J. (1966).\nMultivariate statistical programs. Biometric\nLaboratory,University of Miami.\n\n\nCochran, W. G. (1941). The distribution of the largest of a set of\nestimated variances as a fraction of their total. Annals of\nEugenics, 11(1), 47–52. https://doi.org/10.1111/j.1469-1809.1941.tb02271.x\n\n\nConover, W. J., Johnson, M. E., & Johnson, M. M. (1981). A\ncomparative study of tests for homogeneity of variances, with\napplications to the outer continental shelf bidding data.\nTechnometrics, 23(4), 351–361. https://doi.org/10.1080/00401706.1981.10487680\n\n\nCook, D., Buja, A., Cabrera, J., & Hurley, C. (1995). Grand tour and\nprojection pursuit. Journal of Computational and Graphical\nStatistics, 4(3), 155. https://doi.org/10.2307/1390844\n\n\nCook, D., Buja, A., Lee, E.-K., & Wickham, H. (2008). Grand tours,\nprojection pursuit guided tours, and manual controls. In Handbook of\ndata visualization (pp. 295–314). Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-33037-0_13\n\n\nCook, D., & Laa, U. (2024). Interactively exploring\nhigh-dimensional data and models in R. Online. https://dicook.github.io/mulgar_book/\n\n\nCook, D., & Swayne, D. F. (2007). Interactive and dynamic\ngraphics for data analysis : With R and\nGGobi. Springer. http://www.ggobi.org/book/\n\n\nCook, R. D. (1977). Detection of influential observation in linear\nregression. Technometrics, 19(1), 15–18. http://links.jstor.org/sici?sici=0040-1706%28197702%2919%3A1%3C15%3ADOIOIL%3E2.0.CO%3B2-8\n\n\nCook, R. D. (1993). Exploring partial residual plots.\nTechnometrics, 35(4), 351–362.\n\n\nCook, R. D. (1996). Added-variable plots and curvature in linear\nregression. Technometrics, 38(3), 275–278. https://doi.org/10.1080/00401706.1996.10484507\n\n\nCook, R. D., & Weisberg, S. (1982). Residuals and influence in\nregression. Chapman; Hall.\n\n\nCook, R. D., & Weisberg, S. (1994). ARES plots for generalized\nlinear models. Computational Statistics & Data Analysis,\n17(3), 303–315. https://doi.org/10.1016/0167-9473(92)00075-3\n\n\nCostantini, G., Epskamp, S., Borsboom, D., Perugini, M., Mõttus, R.,\nWaldorp, L. J., & Cramer, A. O. J. (2015). State of the aRt personality research: A tutorial on network\nanalysis of personality data in R. Journal of Research\nin Personality, 54, 13–29. https://doi.org/10.1016/j.jrp.2014.07.003\n\n\nCotton, R. (2013). Learning R. O’Reilly Media.\n\n\nCox, D. R. (1968). Notes on some aspects of regression analysis.\nJournal of the Royal Statistical Society Series A,\n131, 265–279.\n\n\nCsárdi, G., Nepusz, T., Traag, V., Horvát, S., Zanini, F., Noom, D.,\n& Müller, K. (2024). igraph: Network\nanalysis and visualization in r. https://doi.org/10.5281/zenodo.7682609\n\n\nCurran, J., & Hersh, T. (2021). Hotelling: Hotelling’s t^2 test\nand variants. https://CRAN.R-project.org/package=Hotelling\n\n\nDavies, R., Locke, S., & D’Agostino McGowan, L. (2022).\ndatasauRus: Datasets from the datasaurus dozen. https://CRAN.R-project.org/package=datasauRus\n\n\nDavis, C. (1990). Body image and weight preoccupation: A comparison\nbetween exercising and non-exercising women. Appetite,\n16(1), 84. https://doi.org/10.1016/0195-6663(91)90115-9\n\n\nDempster, A. P. (1969). Elements of continuous multivariate\nanalysis. Addison-Wesley.\n\n\nDempster, A. P. (1972). Covariance selection. Biometrics,\n28(1), 157–175.\n\n\nDixon, W. J. (1965). BMD biomedical computer programs. Health\nSciences Computing Facility, School of Medicine, University of\nCalifornia; Health Sciences Computing Faculty.\n\n\nDuncan, O. D. (1961). A socioeconomic index for all occupations. In Jr.\nA. J. Reiss, P. K. H. O. D. Duncan, & C. C. North (Eds.),\nOccupations and social status. The Free Press.\n\n\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least\nangle regression. The Annals of Statistics, 32(2),\n407–499.\n\n\nEmerson, J. W., Green, W. A., Schloerke, B., Crowley, J., Cook, D.,\nHofmann, H., & Wickham, H. (2013). The generalized pairs plot.\nJournal of Computational and Graphical Statistics,\n22(1), 79–91. http://www.tandfonline.com/doi/ref/10.1080/10618600.2012.694762\n\n\nEuler, L. (1758). Elementa doctrinae solidorum. Novi Commentarii\nAcademiae Scientiarum Petropolitanae, 4, 109–140. https://scholarlycommons.pacific.edu/euler-works/230/\n\n\nFarquhar, A. B., & Farquhar, H. (1891). Economic and industrial\ndelusions: A discourse of the case for protection. Putnam.\n\n\nFienberg, S. E. (1971). Randomization and social affairs: The 1970 draft\nlottery. Science, 171, 255–261.\n\n\nFinn, J. D. (1967). MULTIVARIANCE: Fortran program for\nunivariate and multivariate analysis of variance and covariance.\nSchool of Education, State University of New York at Buffalo.\n\n\nFisher, R. A. (1923). Studies in crop variation. II. The manurial\nresponse of different potato varieties. The Journal of Agricultural\nScience, 13(2), 311–320. https://hdl.handle.net/2440/15179\n\n\nFisher, R. A. (1925b). Statistical methods for research\nworkers. Oliver & Boyd.\n\n\nFisher, R. A. (1925a). Statistical methods for research workers\n(6th ed.). Oliver & Boyd.\n\n\nFisher, R. A. (1936). The use of multiple measurements in taxonomic\nproblems. Annals of Eugenics, 7(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x\n\n\nFishkeller, M. A., Friedman, J. H., & Tukey, J. W. (1974).\nPRIM-9, an interactive multidimensional data display and\nanalysis system. Proceedings of the Pacific ACM Regional\nConference.\n\n\nFlury, B., & Riedwyl, H. (1988). Multivariate statistics: A\npractical approach. Chapman & Hall.\n\n\nFox, J. (1987). Effect displays for generalized linear models. In C. C.\nClogg (Ed.), Sociological methodology, 1987 (pp. 347–361).\nJossey-Bass.\n\n\nFox, J. (2003). Effect displays in R for generalized linear\nmodels. Journal of Statistical Software, 8(15), 1–27.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear\nmodels (Third edition.). SAGE.\n\n\nFox, J. (2020). Regression diagnostics (2nd ed.).\nSAGE Publications, Inc. https://doi.org/10.4135/9781071878651\n\n\nFox, J. (2021). A mathematical primer for social statistics\n(2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878835\n\n\nFox, J., & Monette, G. (1992). Generalized collinearity diagnostics.\nJournal of the American Statistical Association,\n87(417), 178–183.\n\n\nFox, J., & Weisberg, S. (2018a). An R companion to\napplied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFox, J., & Weisberg, S. (2018b). Visualizing fit and lack of fit in\ncomplex regression models with predictor effect plots and partial\nresiduals. Journal of Statistical Software, 87(9). https://doi.org/10.18637/jss.v087.i09\n\n\nFox, J., Weisberg, S., & Price, B. (2023). Car: Companion to\napplied regression. https://CRAN.R-project.org/package=car\n\n\nFox, J., Weisberg, S., Price, B., Friendly, M., & Hong, J. (2022).\nEffects: Effect displays for linear, generalized linear, and other\nmodels. https://www.r-project.org\n\n\nFriedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K.,\nSimon, N., & Yang, J. (2023). Glmnet: Lasso and elastic-net\nregularized generalized linear models. https://glmnet.stanford.edu\n\n\nFriendly, M. (1991). SAS System for statistical\ngraphics (1st ed.). SAS Institute. http://www.sas.\ncom/service/doc/pubcat/uspubcat/ind_files/56143.html\n\n\nFriendly, M. (1994). Mosaic displays for multi-way contingency tables.\nJournal of the American Statistical Association, 89,\n190–200. http://www.jstor.org/stable/2291215\n\n\nFriendly, M. (1999). Extending mosaic displays: Marginal, conditional,\nand partial views of categorical data. Journal of Computational and\nGraphical Statistics, 8(3), 373–395. http://datavis.ca/papers/drew/drew.pdf\n\n\nFriendly, M. (2002). Corrgrams: Exploratory displays for correlation\nmatrices. The American Statistician, 56(4), 316–324.\nhttps://doi.org/10.1198/000313002533\n\n\nFriendly, M. (2007). HE plots for multivariate general\nlinear models. Journal of Computational and Graphical\nStatistics, 16(2), 421–444. https://doi.org/10.1198/106186007X208407\n\n\nFriendly, M. (2008). The Golden Age of statistical\ngraphics. Statistical Science, 23(4), 502–535. https://doi.org/10.1214/08-STS268\n\n\nFriendly, M. (2011). Generalized ridge trace plots: Visualizing bias\nand precision with the genridge R package. SCS\nSeminar.\n\n\nFriendly, M. (2013). The generalized ridge trace plot: Visualizing bias\nand precision. Journal of Computational and Graphical\nStatistics, 22(1), 50–68. https://doi.org/10.1080/10618600.2012.681237\n\n\nFriendly, M. (2022). The life and works of andré-michel\nguerry, revisited. Sociological Spectrum, 42(4-6),\n233–259. https://doi.org/10.1080/02732173.2022.2078450\n\n\nFriendly, M. (2023a). Genridge: Generalized ridge trace plots for\nridge regression. https://friendly.github.io/genridge/\n\n\nFriendly, M. (2023b). vcdExtra: Vcd extensions and additions.\nhttps://friendly.github.io/vcdExtra/\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix\nfunctions for teaching and learning linear algebra and multivariate\nstatistics. https://github.com/friendly/matlib\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays.\nComputational Statistics and Data Analysis, 43(4),\n509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nFriendly, M., & Kwan, E. (2009). Where’s Waldo:\nVisualizing collinearity diagnostics. The American\nStatistician, 63(1), 56–65. https://doi.org/10.1198/tast.2009.0012\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with\nR: Visualization and modeling techniques for categorical\nand count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights:\nUnderstanding statistical methods through elliptical geometry.\nStatistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFriendly, M., & Wainer, H. (2021). A history of data\nvisualization and graphic communication. Harvard University Press.\nhttps://doi.org/10.4159/9780674259034\n\n\nFuller, W. (2006). Measurement error models (2nd ed.). John\nWiley & Sons.\n\n\nFunkhouser, H. G. (1937). Historical development of the graphical\nrepresentation of statistical data. Osiris, 3(1),\n269–405. http://tinyurl.com/32ema9\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with\napplication to principal components analysis. Biometrics,\n58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGabriel, K. R. (1981). Biplot display of multivariate matrices for\ninspection of data and diagnosis. In V. Barnett (Ed.), Interpreting\nmultivariate data (pp. 147–173). John Wiley; Sons.\n\n\nGalton, F. (1863). Meteorographica, or methods of mapping the\nweather. Macmillan. http://www.mugu.com/galton/books/meteorographica/index.htm\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature.\nJournal of the Anthropological Institute, 15, 246–263.\nhttp://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGalton, F. (1889). Natural inheritance. Macmillan. http://galton.org/books/natural-inheritance/pdf/galton-nat-inh-1up-clean.pdf\n\n\nGannett, H. (1898). Statistical atlas of the united states, eleventh\n(1890) census. U.S. Government Printing Office.\n\n\nGastwirth, J. L., Gel, Y. R., & Miao, W. (2009). The impact of Levene’s test of equality of variances on\nstatistical theory and practice. Statistical Science,\n24(3), 343–360. https://doi.org/10.1214/09-STS301\n\n\nGelman, A., Hullman, J., & Kennedy, L. (2023). Causal quartets:\nDifferent ways to attain the same average treatment effect. http://www.stat.columbia.edu/~gelman/research/unpublished/causal_quartets.pdf\n\n\nGorman, K. B., Williams, T. D., & Fraser, W. R. (2014). Ecological\nsexual dimorphism and environmental variability within a community of\nantarctic penguins (genus pygoscelis). PLoS\nONE, 9(3), e90081. https://doi.org/10.1371/journal.pone.0090081\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman &\nHall.\n\n\nGower, J. C., Lubbe, S. G., & Roux, N. J. L. (2011).\nUnderstanding biplots. Wiley. http://books.google.ca/books?id=66gQCi5JOKYC\n\n\nGrandjean, M. (2016). A social network analysis of Twitter:\nMapping the digital humanities community. Cogent Arts\n&Amp; Humanities, 3(1), 1171458. https://doi.org/10.1080/23311983.2016.1171458\n\n\nGraybill, F. A. (1961). An introduction to linear statistical\nmodels. McGraw-Hill.\n\n\nGreenacre, M. (1984). Theory and applications of correspondence\nanalysis. Academic Press.\n\n\nGreenacre, M. (2010). Biplots in practice.\nFundación BBVA. https://books.google.ca/books?id=dv4LrFP7U\\_EC\n\n\nGuerry, A.-M. (1833). Essai sur la statistique morale de la\nFrance. Crochard.\n\n\nHahsler, M., Buchta, C., & Hornik, K. (2024). Seriation:\nInfrastructure for ordering objects using seriation. https://github.com/mhahsler/seriation\n\n\nHaitovsky, Y. (1987). On multivariate ridge regression.\nBiometrika, 74(3), 563–570. https://doi.org/10.1093/biomet/74.3.563\n\n\nHarrison, P. (2023). Langevitour: Smooth interactive touring of high\ndimensions, demonstrated with scRNA-seq data. The R Journal,\n15(2), 206–219. https://doi.org/10.32614/RJ-2023-046\n\n\nHarrison, P. (2024). Langevitour: Langevin tour. https://logarithmic.net/langevitour/\n\n\nHart, C., & Wang, E. (2022). Detourr: Portable and performant\ntour animations. https://CRAN.R-project.org/package=detourr\n\n\nHartigan, J. A. (1975a). Clustering algorithms. John Wiley;\nSons.\n\n\nHartigan, J. A. (1975b). Printer graphics for clustering. Journal of\nStatistical Computing and Simulation, 4, 187–213.\n\n\nHartley, H. O. (1950). The use of range in analysis of variance.\nBiometrika, 37(3–4), 271–280. https://doi.org/10.1093/biomet/37.3-4.271\n\n\nHartman, L. I. (2016). Schizophrenia and schizoaffective disorder:\nOne condition or two? [PhD dissertation]. York University.\n\n\nHarwell, M. R., Rubinstein, E. N., Hayes, W. S., & Olds, C. C.\n(1992). Summarizing monte carlo results in methodological research: The\none- and two-factor fixed effects ANOVA cases. Journal\nof Educational and Behavioral Statistics, 17(4), 315–339.\nhttps://doi.org/10.3102/10769986017004315\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning: Data mining, inference and prediction (2nd\ned.). Springer. http://www-stat.stanford.edu/~tibs/ElemStatLearn/\n\n\nHealy, M. J. R. (1968). Multivariate normal plotting. Journal of the\nRoyal Statistical Society Series C, 17(2), 157–161.\n\n\nHeinrichs, R. W., Pinnock, F., Muharib, E., Hartman, L., Goldberg, J.,\n& McDermid Vaz, S. (2015). Neurocognitive normality in schizophrenia\nrevisited. Schizophrenia Research: Cognition, 2(4),\n227–232. https://doi.org/10.1016/j.scog.2015.09.001\n\n\nHerschel, J. F. W. (1833). On the investigation of the orbits of\nrevolving double stars: Being a supplement to a paper entitled\n\"micrometrical measures of 364 double stars\". Memoirs of the Royal\nAstronomical Society, 5, 171–222.\n\n\nHoaglin, D. C., & Welsch, R. E. (1978). The hat matrix in regression\nand ANOVA. The American Statistician,\n32(1), 17–22. https://doi.org/10.1080/00031305.1978.10479237\n\n\nHocking, R. R. (2013). Methods and applications of linear models:\nRegression and the analysis of variance. Wiley. https://books.google.ca/books?id=iq2J-1iS6HcC\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression:\nBiased estimation for nonorthogonal problems.\nTechnometrics, 12, 55–67.\n\n\nHoerl, A. E., Kennard, R. W., & Baldwin, K. F. (1975). Ridge\nregression: Some simulations. Communications in Statistics,\n4(2), 105–123. https://doi.org/10.1080/03610927508827232\n\n\nHofmann, H., VanderPlas, S., & Ge, Y. (2022). Ggpcp: Parallel\ncoordinate plots in the ggplot2 framework. https://github.com/heike/ggpcp\n\n\nHofstadter, D. R. (1979). Gödel, escher, bach: An eternal golden\nbraid. Basic Books.\n\n\nHøjsgaard, S., Edwards, D., & Lauritzen, S. (2012). Graphical\nmodels with R. Springer Science & Business Media.\n\n\nHorst, A., Hill, A., & Gorman, K. (2022). Palmerpenguins: Palmer\narchipelago (antarctica) penguin data. https://allisonhorst.github.io/palmerpenguins/\n\n\nHotelling, H. (1931). The generalization of Student’s ratio. The Annals of Mathematical\nStatistics, 2(3), 360–378. https://doi.org/10.1214/aoms/1177732979\n\n\nHusson, F., Josse, J., Le, S., & Mazet, J. (2024). FactoMineR:\nMultivariate exploratory data analysis and data mining. http://factominer.free.fr\n\n\nHusson, F., Le, S., & Pagès, J. (2017). Exploratory multivariate\nanalysis by example using r. Chapman & Hall. https://doi.org/10.1201/b21874\n\n\nIBM. (1965). Proceedings of the IBM scientific computing symposium\non statistics: Oct 21-23, 1963 (L. Robinson, Ed.). IBM. https://www.amazon.com/Proceedings-Scientific-Computing-Symposium-Statistics/dp/B000GL5RLU\n\n\nInselberg, A. (1985). The plane with parallel coordinates. The\nVisual Computer, 1, 69–91.\n\n\nIsvoranu, A.-M., Epskamp, S., Waldorp, L. J., & Borsboom, D. (2022).\nNetwork psychometrics with r: A guide for behavioral and social\nscientists. Routledge. https://doi.org/10.4324/9781003111238\n\n\nKassambara, A., & Mundt, F. (2020). Factoextra: Extract and\nvisualize the results of multivariate data analyses. http://www.sthda.com/english/rpkgs/factoextra\n\n\nKastellec, J. P., & Leoni, E. L. (2007). Using graphs instead of\ntables in political science. Perspectives on Politics,\n5(04), 755–771. https://doi.org/10.1017/S1537592707072209\n\n\nKrijthe, J. (2023). Rtsne: T-distributed stochastic neighbor\nembedding using a barnes-hut implementation. https://github.com/jkrijthe/Rtsne\n\n\nKruskal, J. B. (1964). Multidimensional scaling by optimizing goodness\nof fit to a nonmetric hypothesis. Psychometrika,\n29(1), 1–27. https://doi.org/10.1007/bf02289565\n\n\nKwan, E., Lu, I. R. R., & Friendly, M. (2009). Tableplot: A new tool\nfor assessing precise predictions. Zeitschrift für\nPsychologie / Journal of Psychology, 217(1), 38–48. https://doi.org/10.1027/0044-3409.217.1.38\n\n\nLarmarange, J. (2024). Ggstats: Extension to ggplot2 for plotting\nstats. https://larmarange.github.io/ggstats/\n\n\nLarsen, W. A., & McCleary, S. J. (1972). The use of partial residual\nplots in regression analysis. Technometrics, 14,\n781–790.\n\n\nLauritzen, S. L. (1996). Graphical models. Oxford University\nPress.\n\n\nLawless, J. F., & Wang, P. (1976). A simulation study of ridge and\nother regression estimators. Communications in Statistics,\n5, 307–323.\n\n\nLee, E.-K., & Cook, D. (2009). A projection pursuit index for large\np small n data. Statistics and Computing, 20(3),\n381–392. https://doi.org/10.1007/s11222-009-9131-1\n\n\nLee, S. (2021). Liminal: Multivariate data visualization with tours\nand embeddings. https://github.com/sa-lee/liminal/\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. Olkin,\nS. G. Ghurye, W. Hoeffding, W. G. Madow, & H. B. Mann (Eds.),\nContributions to probability and statistics: Essays in honor of\nHarold Hotelling (pp. 278–292). Stanford University\nPress.\n\n\nLix, J. M., L. M. Keselman, & Keselman, H. J. (1996). Consequences\nof assumption violations revisited: A quantitative review of\nalternatives to the one-way analysis of variance F test.\nReview of Educational Research, 66(4), 579–619. https://doi.org/10.3102/00346543066004579\n\n\nLongley, J. W. (1967). An appraisal of least squares programs for the\nelectronic computer from the point of view of the user. Journal of\nthe American Statistical Association, 62, 819–841.\nhttps://doi.org/https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896\n\n\nLüdecke, D. (2024). Ggeffects: Create tidy data frames of marginal\neffects for ggplot from model outputs. https://strengejacke.github.io/ggeffects/\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., &\nMakowski, D. (2021). performance: An\nR package for assessment, comparison and testing of\nstatistical models. Journal of Open Source Software,\n6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., &\nMakowski, D. (2022). Easystats: Framework for easy statistical modeling,\nvisualization, and reporting. In CRAN. https://easystats.github.io/easystats/\n\n\nMaaten, L. van der, & Hinton, G. (2008). Visualizing data using\nt-SNE. Journal of Machine Learning\nResearch, 9, 2579–2605. http://www.jmlr.org/papers/v9/vandermaaten08a.html\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis\nwith applications. Biometrika, 57(3), 519–530.\nhttps://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMardia, K. V. (1974). Applications of some measures of multivariate\nskewness and kurtosis in testing normality and robustness studies.\nSankhya: The Indian Journal of Statistics, Series B,\n36(2), 115–128. http://www.jstor.org/stable/25051892\n\n\nMarquardt, D. W. (1970). Generalized inverses, ridge regression, biased\nlinear estimation, and nonlinear estimation. Technometrics,\n12, 591–612.\n\n\nMarquardt, D. W., & Snee, R. D. (1975). Ridge regression in\npractice. The American Statistician, 29(1), 3–20. https://doi.org/10.1080/00031305.1975.10479105\n\n\nMartí, R., & Laguna, M. (2003). Heuristics and meta-heuristics for\n2-layer straight line crossing minimization. Discrete Applied\nMathematics, 127(3), 665–678.\n\n\nMatejka, J., & Fitzmaurice, G. (2017, May). Same stats, different\ngraphs. Proceedings of the 2017 CHI Conference on Human\nFactors in Computing Systems. https://doi.org/10.1145/3025453.3025912\n\n\nMatloff, N. (2011). The art of R programming:\nA tour of statistical software design. No Starch\nPress.\n\n\nMcDonald, G. C. (2009). Ridge regression. Wiley Interdisciplinary\nReviews: Computational Statistics, 1(1), 93–100. https://doi.org/10.1002/wics.14\n\n\nMcGowan, L. D., Gerke, T., & Barrett, M. (2023). Causal inference is\nnot just a statistics problem. Journal of Statistics and Data\nScience Education, 1–9. https://doi.org/10.1080/26939169.2023.2276446\n\n\nMeyer, D., Zeileis, A., Hornik, K., & Friendly, M. (2024). Vcd:\nVisualizing categorical data. https://CRAN.R-project.org/package=vcd\n\n\nMeyers, L. S., Gamst, G., & Guarino, A. J. (2006). Applied\nmultivariate research: Design and interpretation. SAGE\nPublications.\n\n\nMonette, G. (1990). Geometry of multiple regression and interactive\n3-D graphics. In J. Fox & S. Long (Eds.), Modern\nmethods of data analysis (pp. 209–256). SAGE Publications.\n\n\nO’Brien, P. C. (1992). Robust procedures for testing equality of\ncovariance matrices. Biometrics, 48(3), 819–827. http://www.jstor.org/stable/2532347\n\n\nOksanen, J., Simpson, G. L., Blanchet, F. G., Kindt, R., Legendre, P.,\nMinchin, P. R., O’Hara, R. B., Solymos, P., Stevens, M. H. H., Szoecs,\nE., Wagner, H., Barbour, M., Bedward, M., Bolker, B., Borcard, D.,\nCarvalho, G., Chirico, M., De Caceres, M., Durand, S., … Weedon, J.\n(2024). Vegan: Community ecology package. https://github.com/vegandevs/vegan\n\n\nOtto, J., & Kahle, D. (2023). Ggdensity: Interpretable bivariate\ndensity visualization with ggplot2. https://jamesotto852.github.io/ggdensity/\n\n\nPearson, K. (1896). Contributions to the mathematical theory of\nevolution—III, regression, heredity and panmixia.\nPhilosophical Transactions of the Royal Society of London,\n187, 253–318.\n\n\nPearson, K. (1901). On lines and planes of closest fit to systems of\npoints in space. Philosophical Magazine, 6(2),\n559–572.\n\n\nPearson, K. (1903). I. Mathematical contributions to the theory of\nevolution. —XI. On the influence of natural selection on the variability\nand correlation of organs. Philosophical Transactions of the Royal\nSociety of London, 200(321–330), 1–66. https://doi.org/10.1098/rsta.1903.0001\n\n\nPedersen, T. L., & Robinson, D. (2024). Gganimate: A grammar of\nanimated graphics. https://gganimate.com\n\n\nPineo, P. O., & Porter, J. (1967). Occupational prestige in canada*.\nCanadian Review of Sociology, 4(1), 24–40.\nhttps://doi.org/https://doi.org/10.1111/j.1755-618X.1967.tb00472.x\n\n\nPineo, P. O., & Porter, J. (2008). Occupational prestige in canada.\nCanadian Review of Sociology, 4(1), 24–40. https://doi.org/10.1111/j.1755-618x.1967.tb00472.x\n\n\nPlayfair, W. (1786). Commercial and political atlas: Representing,\nby copper-plate charts, the progress of the commerce, revenues,\nexpenditure, and debts of england, during the whole of the eighteenth\ncentury. Debrett; Robinson;; Sewell. http://ucpj.uchicago.edu/Isis/journal/demo/v000n000/000000/000000.fg4.html\n\n\nPlayfair, W. (1801). Statistical breviary; shewing, on a principle\nentirely new, the resources of every state and kingdom in\nEurope. Wallis.\n\n\nReaven, G. M., & Miller, R. G. (1968). Study of the relationship\nbetween glucose and insulin responses to an oral glucose load in man.\nDiabetes, 17(9), 560–569. https://doi.org/10.2337/diab.17.9.560\n\n\nReaven, G. M., & Miller, R. G. (1979). An attempt to define the\nnature of chemical diabetes using a multidimensional analysis.\nDiabetologia, 16, 17–24.\n\n\nRobinaugh, D. J., Hoekstra, R. H. A., Toner, E. R., & Borsboom, D.\n(2019). The network approach to psychopathology: A review of the\nliterature 2008–2018 and an agenda for future research.\nPsychological Medicine, 50(3), 353–366. https://doi.org/10.1017/s0033291719003404\n\n\nRogan, J. C., & Keselman, H. J. (1977). Is the ANOVA\nf-test robust to variance heterogeneity when sample sizes are equal?: An\ninvestigation via a coefficient of variation. American Educational\nResearch Journal, 14(4), 493–498. https://doi.org/10.3102/00028312014004493\n\n\nSarkar, D. (2024). Lattice: Trellis graphics for r. https://lattice.r-forge.r-project.org/\n\n\nScheffé, H. A. (1960). The analysis of variance. Wiley.\n\n\nSchloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M.,\nThoen, E., Elberg, A., & Crowley, J. (2024). GGally: Extension\nto ggplot2. https://ggobi.github.io/ggally/\n\n\nScott, D. W. (1992). Multivariate density estimation: Theory,\npractice, and visualization. Wiley.\n\n\nSearle, S. R., Speed, F. M., & Milliken, G. A. (1980). Population\nmarginal means in the linear model: An alternative to least squares\nmeans. The American Statistician, 34(4), 216–221.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\nfor normality (complete samples). Biometrika, 52(3–4),\n591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nShepard, R. N. (1962a). The analysis of proximities: Multidimensional\nscaling with an unknown distance function. i. Psychometrika,\n27(2), 125–140. https://doi.org/10.1007/bf02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities: Multidimensional\nscaling with an unknown distance function. II. Psychometrika,\n27(3), 219–246. https://doi.org/10.1007/bf02289621\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S.\n(1972a). Multidimensional scaling; theory and applications in the\nbehavioral sciences: Vols. II. Applications. Seminar Press. https://books.google.ca/books?id=PpFAAQAAIAAJ\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S.\n(1972b). Multidimensional scaling: Theory and applications in the\nbehavioral sciences: Vols. I. Theory. Seminar Press. https://books.google.ca/books?id=pJRAAQAAIAAJ\n\n\nShoben, E. J. (1983). Applications of multidimensional scaling in\ncognitive psychology. Applied Psychological Measurement,\n7(4), 473–490. https://doi.org/10.1177/014662168300700406\n\n\nSilverman, B. W. (1986). Density estimation for statistics and data\nanalysis. Chapman & Hall.\n\n\nSimpson, E. H. (1951). The interpretation of interaction in contingency\ntables. Journal of the Royal Statistical Society, Series B,\n30, 238–241.\n\n\nSwayne, D. F., Cook, D., & Buja, A. (1998). XGobi: Interactive\ndynamic data visualization in the x window system. Journal of\nComputational and Graphical Statistics, 7(1), 113–130. https://doi.org/10.1080/10618600.1998.10474764\n\n\nSwayne, D. F., Lang, D. T., Buja, A., & Cook, D. (2003).\nGGobi: Evolving from XGobi into an extensible\nframework for interactive data visualization. Computational\nStatistics &Amp; Data Analysis, 43(4), 423–444. https://doi.org/10.1016/s0167-9473(02)00286-4\n\n\nTeetor, P. (2011). R cookbook.\nO’Reilly Media.\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso.\nJournal of the Royal Statistical Society, Series B:\nMethodological, 58, 267–288.\n\n\nTiku, M. L., & Balakrishnan, N. (1984). Testing equality of\npopulation variances the robust way. Communications in Statistics -\nTheory and Methods, 13(17), 2143–2159. https://doi.org/10.1080/03610928408828818\n\n\nTimm, N. H. (1975). Multivariate analysis with applications in\neducation and psychology. Wadsworth (Brooks/Cole).\n\n\nTorgerson, W. S. (1952). Multidimensional scaling: I. Theory and method.\nPsychometrika, 17(4), 401–419. https://doi.org/10.1007/bf02288916\n\n\nVanderPlas, S., Ge, Y., Unwin, A., & Hofmann, H. (2023). Penguins go\nparallel: A grammar of graphics framework for generalized parallel\ncoordinate plots. Journal of Computational and Graphical\nStatistics, 1–16. https://doi.org/10.1080/10618600.2023.2195462\n\n\nVelleman, P. F., & Welsh, R. E. (1981). Efficient computing of\nregression diagnostics. The American Statistician,\n35(4), 234–242.\n\n\nVinod, H. D. (1978). A survey of ridge regression and related techniques\nfor improvements over ordinary least squares. The Review of\nEconomics and Statistics, 60(1), 121–131. http://www.jstor.org/stable/1924340\n\n\nWaddell, A., & Oldford, R. W. (2023). Loon: Interactive\nstatistical data visualization. https://CRAN.R-project.org/package=loon\n\n\nWarne, F. T. (2014). A primer on multivariate analysis of\nvariance(MANOVA) for behavioral scientists. Practical Assessment,\nResearch & Evaluation, 19(1). https://scholarworks.umass.edu/pare/vol19/iss1/17/\n\n\nWegman, E. J. (1990). Hyperdimensional data analysis using parallel\ncoordinates. Journal of the American Statistical Association,\n85(411), 664–675.\n\n\nWei, T., & Simko, V. (2024). Corrplot: Visualization of a\ncorrelation matrix. https://github.com/taiyun/corrplot\n\n\nWest, D. B. (2001). Introduction to graph theory. Prentice\nhall.\n\n\nWhittaker, J. (1990). Graphical models in applied multivariate\nstatistics. John Wiley; Sons.\n\n\nWickham, H. (2014). Advanced R. Chapman and\nHall/CRC.\n\n\nWickham, H., & Cook, D. (2024). Tourr: Tour methods for\nmultivariate data visualisation. https://github.com/ggobi/tourr\n\n\nWickham, H., Cook, D., Hofmann, H., & Buja, A. (2011). Tourr: An\nR package for exploring multivariate data with projections.\nJournal of Statistical Software, 40(2). https://doi.org/10.18637/jss.v040.i02\n\n\nWilkinson, G. N., & Rogers, C. E. (1973). Symbolic description of\nfactorial models for analysis of variance. Applied Statistics,\n22(3), 392. https://doi.org/10.2307/2346786\n\n\nWiner, B. J. (1962). Statistical principles in experimental\ndesign. McGraw-Hill.\n\n\nWood, S. N. (2006). Generalized additive models: An introduction\nwith r. Chapman; Hall/CRC Press.\n\n\nWright, K. (2021). Corrgram: Plot a correlogram. https://kwstat.github.io/corrgram/\n\n\nXie, Y. (2021). Animation: A gallery of animations in statistics and\nutilities to create animations. https://yihui.org/animation/\n\n\nXu, Z., & Oldford, R. W. (2021). Loon.tour: Tour in ’loon’.\nhttps://cran.r-project.org/package=loon.tourr\n\n\nZhang, J., & Boos, D. D. (1992). Bootstrap critical values for\ntesting homogeneity of covariance matrices. Journal of the American\nStatistical Association, 87(418), 425–429. http://www.jstor.org/stable/2290273\n\n\nPackage used",
    "crumbs": [
      "End matter",
      "References"
    ]
  }
]