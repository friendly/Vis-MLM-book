[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "",
    "text": "Preface\nThis book is about graphical methods developed recently for multivariate data, and their uses in understanding relationships when there are several aspects to be considered together. Data visualization methods for statistical analysis are well-developed for simple linear models with a single outcome variable. However, with applied research in the social and behavioral sciences, it is often the case that the phenomena of interest (e.g., depression, job satisfaction, academic achievement, childhood ADHD disorders, etc.) can be measured in several different ways or related aspects.\nFor example, if academic achievement can be measured for adolescents by reading, mathematics, science and history scores, how do predictors such as parental encouragement, school environment and socioeconomic status affect all these outcomes? In a similar way? In different ways? In such cases, much more can be understood from a multivariate approach that considers the correlations among the outcomes. Yet, sadly, researchers typically examine the outcomes one by one which often only tells part of the data story.\nHowever, to do this it is useful to set the stage for multivariate thinking, with a grand scheme for statistics and data visualization, a parable, and an example of multivariate discovery.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#one-two-many",
    "href": "index.html#one-two-many",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "ONE, TWO, MANY",
    "text": "ONE, TWO, MANY\nThere is an old and helpful idea I learned from John Hartigan in my graduate days at Princeton:\n\nIn statistics and data visualization all methods can be classified by the number of dimensions contemplated, on a scale of ONE, TWO, MANY.\n\nBy this, he meant that, at a global level, all data, statistical summaries, and graphical displays could be classified as:\n\n\nunivariate: a single variable, considered in isolation (age, COVID cases, pizzas ordered). Univariate numerical summaries are means, medians, measures of variablilty, and so forth. Univariate displays include dot plots, boxplots, histograms and density estimates.\n\nbivariate: two variables, considered jointly. Numerical summaries include correlations, covariances and two-way tables of frequencies or measures of association for categorical variables. Bivariate displays include scatterplots and mosaic plots.\n\nmultivariate: three or more variables, considered jointly. Numerical summaries include correlation and covariance matrices, consisting of all pairwise values, but also derived measures from the analysis of these matrices (eigenvalues, eigenvectors). Graphical displays of multivariate data can sometimes be shown in 3D, but often involve multiple views of the data projected into 2D plots.\n\nAs a quasi-numerical scale, I refer to these as 1D, 2D and nD. This admits the possibility of half-integer cases, such as 1.5D, where the main focus is on a single variable, but that is classified by a simple factor (e.g., gender), or 2.5D where a 2D scatterplot can show other variables using color, shape or other visual attributes His point in this classification was that once you’ve reached three variables, all higher dimensions involve similar summaries and data displays.\nUnivariate and bivariate methods and displays are well-known. This book is about how these ideas can be extended to an \\(n\\)-dimensional world. Three-dimensional data displays are now fairly easy to produce, even if they are sometimes difficult to understand. But how can we even think about four or more dimensions? The difficulty can be appreciated by considering the tale of Flatland.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#flatland",
    "href": "index.html#flatland",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Flatland",
    "text": "Flatland\n\nTo comport oneself with perfect propriety in Polygonal society, one ought to be a Polygon oneself. — Edwin A. Abbott, Flatland\n\nIn 1884, an English schoolmaster, Edwin Abbott Abbott, shook the world of Victorian culture with a slim volume, Flatland: A Romance of Many Dimensions (Abbott, 1884). He described a two-dimensional world, Flatland, inhabited entirely by geometric figures in the plane. His purpose was satirical, to poke fun at the social and gender class system at the time: Women were mere line segments, while men were represented as polygons with varying numbers of sides— a triangle was a working man, but acute isosceles were soldiers or criminals of very small angle; gentlemen and professionals had more sides. Abbot published this under the pseudonym, “A Square”, suggesting his place in the hierarchy.\n\nTrue, said the Sphere; it appears to you a Plane, because you are not accustomed to light and shade and perspective; just as in Flatland a Hexagon would appear a Straight Line to one who has not the Art of Sight Recognition. But in reality it is a Solid, as you shall learn by the sense of Feeling. — Edwin A. Abbott, Flatland\n\nBut how did it feel to be a member of a flatland society? How could a point (a newborn child?) understand a line (a woman)? How does a Triangle “see” a Hexagon or even a infinitely-sided Circle? Abbott introduces the very idea of different dimensions of existence through dreams and visions:\n\nA Square dreams of visiting a one-dimensional Lineland where men appear as lines, and women are merely “illustrious points”, but the inhabitants can only see the Square as lines.\nIn a vision, the Square is visited by a Sphere, to illustrate what a 2D Flatlander could understand from a 3D sphere (Figure 1) that passes through the plane he inhabits. It is a large circle when seen at the moment of its’ greatest extent. As the Spehere rises, it becomes progressively smaller, until it becomes a point, and then vanishes.\n\n\n\n\n\n\n\n\nFigure 1: A 2D Flatlander seeing a sphere as it passes through Flatland. The line, labeled ‘My Eye’ indicates what the Flatlander would see. Source: Abbott (1884)\n\n\n\n\nAbbott goes on to state what could be considered as a demonstration (or proof) by induction of the difficulties of seeing in 1, 2, 3 dimensions, and how the idea motion over time (one more dimension) could allow citizens of any 1D, 2D, 3D world to contemplate one more dimension.\n\nIn One Dimensions, did not a moving Point produce a Line with two terminal points? In two Dimensions, did not a moving Line produce a Square with four terminal points? In Three Dimensions, did not a moving Square produce - did not the eyes of mine behold it - that blessed being, a Cube, with eight terminal points? And in Four Dimensions, shall not a moving Cube - alas, for Analogy, and alas for the Progress of Truth if it be not so - shall not, I say the motion of a divine Cube result in a still more divine organization with sixteen terminal points? — Edwin A. Abbott\n\nFor Abbot, the way for a citizen of any world to imagine one more dimension was to consider how a higher-dimensional object would change over time.1 A line moved over time could produce a rectangle as shown in Figure 2; that rectangle moving in another direction over time would produce a 3D figure, and so forth.\n\n\n\n\n\n\n\nFigure 2: Geometrical objects in 1 to 4 dimensions. One more dimension can be thought of as the trace of movement over time.\n\n\n\n\nBut wait! Where does that 4D thing (a tesseract) come from? To really see a tesseract it helps to view it in an animation over time (Figure 3). But like the Square, contemplating 3D from a 2D world, it takes some imagination.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Animation of a tesseract, a cube changing over time.\n\n\nYet the deep mathematics of more than three dimensions only emerged in the 19th century. In Newtonian mechanics, space and time were always considered independent of each other. Our familiar three-dimensional space, of length, width, and height had formed the backbone of Euclidean geometry for millenea. However, the idea that space and time are indeed interwoven was first proposed by German mathematician Hermann Minkowski (1864–1909) in 1908. This was a powerful idea. It bore fruit when Albert Einstein revolutionized the Newtonian conceptions of gravity in 1915 when he presented a theory of general relativity which was based primarily on the fact that mass and energy warp the fabric of four-dimensional spacetime.\nThe parable of Flatland can provide inspiration for statistical thinking and data visualization. Once we go beyond bivariate statistics and 2D plots, we are in a multivariate world of possibly MANY dimensions. It takes only some imagination and suitable methods to get there.\nLike Abbott’s Flatland, this book is a romance, in many dimensions, of what we can learn from modern methods of data visualization.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#eureka",
    "href": "index.html#eureka",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "EUREKA!",
    "text": "EUREKA!\nEven modest sized multivariate data can have secrets that can be revealed in the right view. As an example, David Coleman at RCA Laboratories in Princeton, N.J. generated a dataset of five (fictitious) measurements of grains of pollen for the 1986 Data Exposition at the Joint statistical Meetings. The first three variables are the lengths of geometric features 3848 observed sampled pollen grains – in the x, y, and z dimensions: a ridge along x, a nub in the y direction, and a crack in along the z dimension. The fourth variable is pollen grain weight, and the fifth is density. The challenge was to “find something interesting” in this dataset, now available as animation::pollen. \nThose who solved the puzzle were able to find an orientation of this 5-dimensional dataset, such that zooming in revealed a magic word, “EUREKA” spelled in points, as in the following figure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Four views of the pollen data, zooming in, clockwise from the upper left to discover the word “EUREKA”.\n\n\nThis can be seen better in a 3D animation. The rgl package (Adler & Murdoch, 2023) is used to create a 3D scatterplot of the first three variables. Then the animation package (Xie, 2021) is use to record a sequence of images, adjusting the rgl::par3d(zoom) value.\n\nCodelibrary(animation)\nlibrary(rgl)\ndata(pollen, package = \"animation\")\noopt = ani.options(interval = 0.05)\n## adjust the viewpoint\nuM =\n  matrix(c(-0.3709192276, -0.5133571028, -0.7738776206, 0, \n           -0.7305060625,  0.6758151054, -0.0981751680, 0, \n            0.57339602708, 0.5289064049, -0.6256819367, 0, \n           0, 0, 0, 1), 4, 4)\nopen3d(userMatrix = uM, \n       windowRect = c(10, 10, 510, 510))\n\nplot3d(pollen[, 1:3])\n\n# zoom in\nzm = seq(1, 0.045, length = 200)\npar3d(zoom = 1)\nfor (i in 1:length(zm)) {\n  par3d(zoom = zm[i])\n  ani.pause()\n}\nani.options(oopt)\n\n\n\n\n\n\nAnimation of zooming in on the pollen data. This figure only appears in the online version.\n\n\n\nFigure 5\n\n\nMultivariate scientific discoveries\nLest this example seem contrived (which it admittedly is), multivariate visualization has played an important role in quite a few scientific discoveries. Among these, Francis Galton’s (1863) discovery of the anti-cyclonic pattern of wind direction in relation to barometric pressure from many weather measures recorded systematically across all weather stations, lighthouses and observatories in Europe in December 1861 stands out as the best example of a scientific discovery achieved almost entirely through graphical means–– something that was totally unexpected, and purely the product of his use of remarkably novel high-dimensional graphs (Friendly & Wainer, 2021, pp. 170–173).\nA more recent example is the discovery of two general classes in the development of Type 2 diabetes by Reaven & Miller (1979), using PRIM-9 (Fishkeller et al., 1974), the first computer system for high-dimensional visualization2. In an earlier study Reaven & Miller (1968) examined the relation between blood glucose levels and the production of insulin in normal subjects and in patients with varying degrees of hyperglicemia (elevated blood sugar level). They found a peculiar ‘’horse shoe’’ shape in this relation (shown in Figure 6), about which they could only speculate: perhaps individuals with the best glucose tolerance also had the lowest levels of insulin as a response to an oral dose of glucose; perhaps those with low glucose response could secrete higher levels of insulin; perhaps those who were low on both glucose and insulin responses followed some other mechanism. In 2D plots, this was a mystery.\n\ndata(Diabetes, package=\"heplots\")\nplot(instest ~ glutest, data=Diabetes, \n     pch=16,\n     cex.lab=1.25,\n     xlab=\"Glucose response\",\n     ylab=\"Insulin response\")\n\n\n\n\n\n\nFigure 6: Reproduction of a graph similar to that from Reaven & Miller (1968) on the relationship between glucose and insulin response to being given an oral dose of glucose.\n\n\n\n\n\nAn answer to their questions came ten years later, when they were able to visualize similar but new data in 3D using the PRIM-9 system. In a carefully controlled study, they also measured ‘’steady state plasma glucose’’ (SSPG), a measure of the efficiency of use of insulin in the body, where large values mean insulin resistance, as well as other variables. PRIM-9 allowed them to explore various sets of three variables, and, more importantly, to rotate a given plot in three dimensions to search for interesting features. One plot that stood out concerned the relation between plasma glucose response, plasma insulin response and SSPG response, shown in Figure 7.\n\n\n\n\n\n\n\nFigure 7: Artist’s rendition of data from Reaven & Miller (1979) as seen in three dimensions using the PRIM-9 system. Labels for the clusters have been added, identifying the three groups of patients. Source: Reaven & Miller (1979).\n\n\n\n\nFrom this graphical insight, they were able to classify the participants into three groups, based on clinical levels of glucose and insulin. The people in the wing on the left in Figure 7 were considered to have overt diabetes, the most advanced form, characterized by elevated fasting blood glucose concentration and classical diabetic symptoms. Those in the right wing were classified as latent or chemical diabetics, with no symptoms of diabetes but demonstrable abnormality of oral or intravenous glucose tolerance. Those in the central blob were classified as normal.\nPrevious thinking was that Type 2 diabetes (when the body cannot make enough insulin, as opposed to Type I, an autoimmune condition where the pancreatic cells have been destroyed) progressed from the chemical stage to an overt one in a smooth transition. However, it was clear from Figure 7 that the only “path” from one to the other lead through the cluster of normal patients near the origin, so that explanation must be wrong. Instead, this suggested that the chemical and overt diabetics were distinct classes. Indeed, longitudinal studies showed that patients classified as chemical diabetics rarely developed the overt form. The understanding of the etiology of Type 2 diabetes was altered dramatically by the power of high-D interactive graphics.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-i-assume",
    "href": "index.html#what-i-assume",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "What I assume",
    "text": "What I assume\nIt is assumed that the reader has a background in applied intermediate statistics including material on univariate linear models including analysis of variance (ANOVA) and multiple regression. This means you should be familiar with … TODO: Complete this required background\nThere will also be some mathematics in the book where words and diagrams are not enough. The mathematical level will be intermediate, mostly consisting of simple algebra. No derivations, proofs, theorems here! For multivariate methods, it will be useful to express ideas using matrix notation to simplify presentation. The single symbol I’m using math to express ideas, and all you will need is a reading-level of understanding. For this, the first chapter of Fox (2021), A mathematical primer for social statistics, is excellent. If you want to learn something of using matrix algebra for data analysis and statistics, I recommend our package matlib (Friendly et al., 2024).\nI also assume the reader to have at least a basic familiarity with R. While R fundamentals are outside the scope of the book, I believe that this language provides a rich set of resources, far beyond that offered by other statistical software packages, and is well worth learning.\nFor those not familiar with R, I recommend Matloff (2011), Wickham (2014), and Cotton (2013) for introductions to programming in the language. Fox & Weisberg (2018) and Teetor (2011) are great for learning about how to conduct basic statistical analyses in R. TODO: Revise this list.\nTODO: Add stuff on general books about graphics",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#conventions-used-in-this-book",
    "href": "index.html#conventions-used-in-this-book",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "Conventions used in this book",
    "text": "Conventions used in this book\nTODO: Some stuff below is just for testing… Revise.\nThe following typographic conventions are used in this book:\n\nitalic : indicates terms to be emphasized or defined in the text, …\n\nbold : is used for names of R packages. Or, better yet: bold monospace, but I’d rather this be in a different color. Perhaps I can use “r colorize(”lattice”, “green”)” inline -&gt; lattice will do this? This does bold & color, but can’t use monospace.\nI can now use inline ‘pkg(“lattice”)’ generating lattice, or also with a citation, pkg(\"lattice\", cite=TRUE) -&gt; lattice (Sarkar, 2025). Can also refer to the matlib package (Friendly et al., 2024), including “package” between the name and citation.\n\nfixed-width : is used in program listings as well as in text to refer to variable and function names, R statement elements and keywords.\nR code in program listings and output is presented in monospaced (typewriter) font, fira mono\nfixed-width italic : isn’t used yet, but probably should be.\n\nFor R functions in packages, we use the notation package::function(), for example: car::Anova() to identify where those functions are defined\n\n\n\n\n\nAbbott, E. A. (1884). Flatland: A romance of many dimensions. Buccaneer Books.\n\n\nAdler, D., & Murdoch, D. (2023). Rgl: 3D visualization using OpenGL. https://CRAN.R-project.org/package=rgl\n\n\nCajori, F. (1926). Origins of fourth dimension concepts. The American Mathematical Monthly, 33(8), 397–406. https://doi.org/10.1080/00029890.1926.11986607\n\n\nCotton, R. (2013). Learning R. O’Reilly Media.\n\n\nFishkeller, M. A., Friedman, J. H., & Tukey, J. W. (1974). PRIM-9, an interactive multidimensional data display and analysis system. Proceedings of the Pacific ACM Regional Conference.\n\n\nFox, J. (2021). A mathematical primer for social statistics (2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878835\n\n\nFox, J., & Weisberg, S. (2018). An R companion to applied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix functions for teaching and learning linear algebra and multivariate statistics. https://github.com/friendly/matlib\n\n\nFriendly, M., & Wainer, H. (2021). A history of data visualization and graphic communication. Harvard University Press. https://doi.org/10.4159/9780674259034\n\n\nGalton, F. (1863). Meteorographica, or methods of mapping the weather. Macmillan. http://www.mugu.com/galton/books/meteorographica/index.htm\n\n\nMatloff, N. (2011). The art of R programming: A tour of statistical software design. No Starch Press.\n\n\nReaven, G. M., & Miller, R. G. (1968). Study of the relationship between glucose and insulin responses to an oral glucose load in man. Diabetes, 17(9), 560–569. https://doi.org/10.2337/diab.17.9.560\n\n\nReaven, G. M., & Miller, R. G. (1979). An attempt to define the nature of chemical diabetes using a multidimensional analysis. Diabetologia, 16, 17–24.\n\n\nSarkar, D. (2025). Lattice: Trellis graphics for r. https://lattice.r-forge.r-project.org/\n\n\nTeetor, P. (2011). R cookbook. O’Reilly Media.\n\n\nWickham, H. (2014). Advanced R. Chapman and Hall/CRC.\n\n\nXie, Y. (2021). Animation: A gallery of animations in statistics and utilities to create animations. https://yihui.org/animation/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Visualizing Multivariate Data and Models in R",
    "section": "",
    "text": "In his famous TV series, Cosmos, Carl Sagan provides an intriguing video presentation Flatland and the 4th dimension. However, as far back as 1754 (Cajori, 1926), the idea of adding a fourth dimension appears in Jean le Rond d’Alembert’s “Dimensions”, and one realization of a four-dimensional object is a tesseract, shown in Figure 2.↩︎\nPRIM-9 is an acronym for Picturing, Rotation, Isolation and Masking in up to 9 dimensions. These operations are fundamental to interactive and dynamic data visualization.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html",
    "href": "06-linear_models-plots.html",
    "title": "6  Plots for univariate response models",
    "section": "",
    "text": "6.1 The “regression quartet”\nFor a univariate linear model fit using lm(), glm() and similar functions, the standard plot() method gives basic versions of diagnostic plots of residuals and other calculated quantities for assessing possible violations of the model assumptions. Some of these can be considerably enhanced using other packages.\nBeyond this,\nThe classic reference on regression diagnostics is Belsley et al. (1980). My favorite modern texts are the brief Fox (2020) and the more complete Fox & Weisberg (2018a), both of which are supported by the car package (Fox et al., 2023). Some of the examples in this chapter are inspired by Fox & Weisberg (2018a).\nPackages\nIn this chapter I use the following packages. Load them now:\nFor a fitted model, plotting the model object with plot(model) provides for any of six basic plots, of which four are produced by default, giving rise to the term regression quartet for this collection. These are:\nOne key feature of these plots is providing reference lines or smoothed curves for ease of judging the extent to which a plot conforms to the expected pattern; another is the labeling of observations which deviate from an assumption.\nThe base-R plot(model) plots are done much better in a variety of packages. I illustrate some versions from the car (Fox et al., 2023) and performance (Lüdecke et al., 2021) packages, part of the easystats (Lüdecke et al., 2022) suite of packages.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#the-regression-quartet",
    "href": "06-linear_models-plots.html#the-regression-quartet",
    "title": "6  Plots for univariate response models",
    "section": "",
    "text": "Residuals vs. Fitted: For well-behaved data, the points should hover around a horizontal line at residual = 0, with no obvious pattern or trend.\nNormal Q-Q plot: A plot of sorted standardized residuals \\(e_i\\) (obtained fromrstudent(model)) against the theoretical values those values would have in a standard normal \\(\\mathcal{N}(0, 1)\\) distribution.\nScale-Location: Plots the square-root of the absolute values of the standardized residuals \\(\\sqrt{| e_i |}\\) as a measure of “scale” against the fitted values \\(\\hat{y}_i\\) as a measure of “location”. This provides an assessment of homogeneity of variance. Violations appear as a tendency for scale (variability) to vary with location.\nResiduals vs. Leverage: Plots standardized residuals against leverage to help identify possibly influential observations. Leverage, or “hat” values (given by hat(model)) are proportional to the squared Mahalanobis distances of the predictor values \\(\\mathbf{x}_i\\) from the means, and measure the potential of an observation to change the fitted coefficients if that observation was deleted. Actual influence is measured by Cooks’s distance (cooks.distance(model)) and is proportional to the product of residual times leverage. Contours of constant Cook’s \\(D\\) are added to the plot.\n\n\n\n\n6.1.1 Example: Duncan’s occupational prestige\nIn a classic study in sociology, Duncan (1961) used data from the U.S. Census in 1950 to study how one could predict the prestige of occupational categories — which is hard to measure — from available information in the census for those occupations. His data is available in carData:Duncan, and contains\n\n\ntype: the category of occupation, one of prof (professional), wc (white collar) or bc (blue collar);\n\nincome: the percentage of occupational incumbents with a reported income &gt; 3500 (about 40,000 in current dollars);\n\neducation: the percentage of occupational incumbents who were high school graduates;\n\nprestige: the percentage of respondents in a social survey who rated the occupation as “good” or better in prestige.\n\nThese variables are a bit quirky in they are measured in percents, 0-100, rather dollars for income and years for education, but this common scale permitted Duncan to ask an interesting sociological question: Assuming that both income and education predict prestige, are they equally important, as might be assessed by testing the hypothesis \\(\\mathcal{H}_0: \\beta_{\\text{income}} = \\beta_{\\text{education}}\\). If so, this would provide a very simple theory for occupational prestige.\nA quick look at the data shows the variables and a selection of the occupational categories, which are the row.names() of the dataset.\n\ndata(Duncan, package = \"carData\")\nset.seed(42)\ncar::some(Duncan)\n#&gt;                  type income education prestige\n#&gt; accountant       prof     62        86       82\n#&gt; professor        prof     64        93       93\n#&gt; engineer         prof     72        86       88\n#&gt; factory.owner    prof     60        56       81\n#&gt; store.clerk        wc     29        50       16\n#&gt; carpenter          bc     21        23       33\n#&gt; machine.operator   bc     21        20       24\n#&gt; barber             bc     16        26       20\n#&gt; soda.clerk         bc     12        30        6\n#&gt; janitor            bc      7        20        8\n\nLet’s start by fitting a simple model using just income and education as predictors. The results look very good! Both income and education are highly significant and the \\(R^2 = 0.828\\) for the model indicates that prestige is very well predicted by just these variables.\n\nduncan.mod &lt;- lm(prestige ~ income + education, data=Duncan)\nsummary(duncan.mod)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = prestige ~ income + education, data = Duncan)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -29.54  -6.42   0.65   6.61  34.64 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  -6.0647     4.2719   -1.42     0.16    \n#&gt; income        0.5987     0.1197    5.00  1.1e-05 ***\n#&gt; education     0.5458     0.0983    5.56  1.7e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 13.4 on 42 degrees of freedom\n#&gt; Multiple R-squared:  0.828,  Adjusted R-squared:  0.82 \n#&gt; F-statistic:  101 on 2 and 42 DF,  p-value: &lt;2e-16\n\nBeyond this, Duncan was interested in the coefficients and whether income and education could be said to have equal impacts on predicting occupational prestige. A nice display of model coefficients with confidence intervals is provided by parameters::model_parameters().\n\nparameters::model_parameters(duncan.mod)\n#&gt; Parameter   | Coefficient |   SE |         95% CI | t(42) |      p\n#&gt; ------------------------------------------------------------------\n#&gt; (Intercept) |       -6.06 | 4.27 | [-14.69, 2.56] | -1.42 | 0.163 \n#&gt; income      |        0.60 | 0.12 | [  0.36, 0.84] |  5.00 | &lt; .001\n#&gt; education   |        0.55 | 0.10 | [  0.35, 0.74] |  5.56 | &lt; .001\n\nWe can also test Duncan’s hypothesis that income and education have equal effects on prestige with car::linearHypothesis(). This is constructed as a test of a restricted model in which the two coefficients are forced to be equal against the unrestricted model. Duncan was very happy with this result.\n\ncar::linearHypothesis(duncan.mod, \"income = education\")\n#&gt; \n#&gt; Linear hypothesis test:\n#&gt; income - education = 0\n#&gt; \n#&gt; Model 1: restricted model\n#&gt; Model 2: prestige ~ income + education\n#&gt; \n#&gt;   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n#&gt; 1     43 7519                         \n#&gt; 2     42 7507  1      12.2 0.07    0.8\n\nEquivalently, the linear hypothesis that \\(\\beta_{\\text{Inc}} = \\beta_{\\text{Educ}}\\) can be tested with a Wald test for difference between these coefficients, expressed as \\(\\mathcal{H}_0 : \\mathbf{C} \\mathbf{\\beta} = 0\\), using \\(\\mathbf{C} = (0, -1, 1)\\). The estimated value, -0.053 has a confidence interval [-0.462, 0.356], consistent with Duncan’s hypothesis.\n\nwtest &lt;- spida2::wald(duncan.mod, c(0, -1, 1))[[1]]\nwtest$estimate\n#&gt;       \n#&gt;        Estimate Std.Error DF t-value p-value Lower 0.95 Upper 0.95\n#&gt;   Larg  -0.0529     0.203 42  -0.261   0.795     -0.462      0.356\n\nWe can visualize this test and confidence intervals using a joint confidence ellipse for the coefficients for income and education in the model duncan.mod. In Figure 6.1 the size of the ellipse is set to \\(\\sqrt{F^{0.95}_{1,\\nu}} = t^{0.95}_{\\nu}\\), so that its shadows on the horizontal and vertical axes correspond to 1D 95% confidence intervals. In this plot, the line through the origin with slope \\(= 1\\) corresponds to equal coefficients for income and education and the line with slope \\(= -1\\) corresponds to their difference, \\(\\beta_{\\text{Educ}} - \\beta_{\\text{Inc}}\\). The orthogonal projection of the coefficient vector \\((\\widehat{\\beta}_{\\text{Inc}}, \\widehat{\\beta}_{\\text{Educ}})\\) (the center of the ellipse) is the point estimate of \\(\\widehat{\\beta}_{\\text{Educ}} - \\widehat{\\beta}_{\\text{Inc}}\\) and the shadow of the ellipse along this axis is the 95% confidence interval for the difference in slopes.\n\n\nSee the codeconfidenceEllipse(duncan.mod, col = \"blue\",\n  levels = 0.95, dfn = 1,\n  fill = TRUE, fill.alpha = 0.2,\n  xlim = c(-.4, 1),\n  ylim = c(-.4, 1), asp = 1,\n  cex.lab = 1.3,\n  grid = FALSE,\n  xlab = expression(paste(\"Income coefficient, \", beta[Inc])),\n  ylab = expression(paste(\"Education coefficient, \", beta[Educ])))\n\nabline(h=0, v=0, lwd = 2)\n\n# confidence intervals for each coefficient\nbeta &lt;- coef( duncan.mod )[-1]\nCI &lt;- confint(duncan.mod)       # confidence intervals\nlines( y = c(0,0), x = CI[\"income\",] , lwd = 5, col = 'blue')\nlines( x = c(0,0), y = CI[\"education\",] , lwd = 5, col = 'blue')\npoints(rbind(beta), col = 'black', pch = 16, cex=1.5)\npoints(diag(beta) , col = 'black', pch = 16, cex=1.4)\narrows(beta[1], beta[2], beta[1], 0, angle=8, len=0.2)\narrows(beta[1], beta[2], 0, beta[2], angle=8, len=0.2)\n\n# add line for equal slopes\nabline(a=0, b = 1, lwd = 2, col = \"darkgreen\")\ntext(0.8, 0.8, expression(beta[Educ] == beta[Inc]), \n     srt=45, pos=3, cex = 1.5, col = \"darkgreen\")\n\n# add line for difference in slopes\ncol &lt;- \"darkred\"\nx &lt;- c(-1.5, .5)\nlines(x=x, y=-x)\ntext(-.15, -.15, expression(~beta[\"Educ\"] - ~beta[\"Inc\"]), \n     col=col, cex=1.5, srt=-45)\n\n# confidence interval for b1 - b2\nwtest &lt;- spida2::wald(duncan.mod, c(0, -1, 1))[[1]]\nlower &lt;- wtest$estimate$Lower /2\nupper &lt;- wtest$estimate$Upper / 2\nlines(-c(lower, upper), c(lower,upper), lwd=6, col=col)\n\n# projection of (b1, b2) on b1-b2 axis\nbeta &lt;- coef( duncan.mod )[-1]\nbdiff &lt;- beta %*% c(1, -1)/2\npoints(bdiff, -bdiff, pch=16, cex=1.3)\narrows(beta[1], beta[2], bdiff, -bdiff, \n       angle=8, len=0.2, col=col, lwd = 2)\n\n# calibrate the diff axis\nticks &lt;- seq(-0.3, 0.3, by=0.2)\nticklen &lt;- 0.02\nsegments(ticks, -ticks, ticks-sqrt(2)*ticklen, -ticks-sqrt(2)*ticklen)\ntext(ticks-2.4*ticklen, -ticks-2.4*ticklen, ticks, srt=-45)\n\n\n\n\n\n\nFigure 6.1: Joint 95% confidence ellipse for \\((\\beta_{\\text{Inc}}, \\beta_{\\text{Educ}})\\), together with their 1D shadows, which give 95% confidence intervals for the separate coefficients and the linear hypothesis that the coefficients are equal. Projecting the confidence ellipse along the line with unit slope gives a confidence interval for the difference between coefficients, shown by the dark red line.\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.1.1 Diagnostic plots\nBut, should Duncan be so happy? It is unlikely that he ran any model diagnostics or plotted his model; we do so now. Here is the regression quartet (Figure 6.2) for this model. Each plot shows some trend lines, and importantly, labels some observations that stand out and might deserve attention.\n\nop &lt;- par(mfrow = c(2,2), \n          mar = c(4,4,3,1)+.1)\nplot(duncan.mod, lwd=2, pch=16)\npar(op)\n\n\n\n\n\n\nFigure 6.2: Regression quartet of diagnostic plots for the Duncan data. Several possibly unusual observations are labeled.\n\n\n\n\nSome points to note:\n\nA few observations (minister, reporter, conductor, contractor) are flagged in multiple panels. It turns out (Section 6.6.3) that the observations for minister and reporter noted in the residuals vs. leverage plot are highly influential and largely responsible for Duncan’s finding that the slopes for income and education could be considered equal.\nThe red trend line in the scale-location plot indicates that residual variance is not constant, but rather increases from both ends. This is a consequence of the fact that prestige is measured as a percentage, bounded at [0, 100], and the standard deviation of a percentage \\(p\\) is proportional to \\(\\sqrt{p \\times (1-p)}\\) which is maximal at $p = 0.5.\n\nSimilar, but nicer-looking diagnostic plots are provided by performance::check_model() which uses ggplot2 for graphics. These include helpful captions indicating what should be observed for each for a good-fitting model. However, they don’t have as good facilities for labeling unusual observations as the base R plot() or functions in the car package.\n\ncheck_model(duncan.mod, \n            check=c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\"))\n\n\n\n\n\n\nFigure 6.3: Diagnostic plots for the Duncan data, using check_model().\n\n\n\n\n\n6.1.2 Example: Canadian occupational prestige\n\nFollowing Duncan (1961), occupational prestige was studied in a Canadian context by Bernard Blishen and others at York University, giving the dataset carData::Prestige which we looked at in Section 3.2.3. It differs from the Duncan dataset primarily in that the main variables—prestige, income and education were revamped to better reflect the underlying constructs in more meaningful units.\n\nprestige: Rather than a simple percentage of “good+” ratings, this uses a wider and more reliable scale from Pineo & Porter (1967) on a scale from 10–90.\nincome is measured as the average income of incumbents in each occupation, in 1971 dollars, rather than percent exceeding a given threshold ($3500)\neducation is measured as the average education of occupational incumbents, years.\n\nThe dataset again includes type of occupation with the same levels \"bc\" (blue collar), \"wc\" (white collar) and \"prof\" (professional)1, but in addition includes the percent of women in these occupational categories.\nOur interest again is in understanding how prestige is related to census measures of the average education, income, percent women of incumbents in those occupations, but with attention to the scales of measurement and possibly more complex relationships.\n\ndata(Prestige, package=\"carData\")\n# Reorder levels of type\nPrestige$type &lt;- factor(Prestige$type, \n                        levels=c(\"bc\", \"wc\", \"prof\")) \nstr(Prestige)\n#&gt; 'data.frame':  102 obs. of  6 variables:\n#&gt;  $ education: num  13.1 12.3 12.8 11.4 14.6 ...\n#&gt;  $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...\n#&gt;  $ women    : num  11.16 4.02 15.7 9.11 11.68 ...\n#&gt;  $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...\n#&gt;  $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...\n#&gt;  $ type     : Factor w/ 3 levels \"bc\",\"wc\",\"prof\": 3 3 3 3 3 3 3 3 3 3 ...\n\nWe fit a main-effects model using all predictors (ignoring census, the Canadian Census occupational code):\n\nprestige.mod &lt;- lm(prestige ~ education + income + women + type,\n                   data=Prestige)\n\nplot(model) produces four separate plots. For a quick look, I like to arrange them in a single 2x2 figure.\n\nop &lt;- par(mfrow = c(2,2), \n          mar=c(4,4,3,1)+.1)\nplot(prestige.mod, lwd=2, cex.lab=1.4)\npar(op)\n\n\n\n\n\n\nFigure 6.4: Regression quartet of diagnostic plots for the Prestige data. Several possibly unusual observations are labeled in each plot.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#other-model-plots",
    "href": "06-linear_models-plots.html#other-model-plots",
    "title": "6  Plots for univariate response models",
    "section": "\n6.2 Other Model plots",
    "text": "6.2 Other Model plots\nTODO: What goes here?",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#sec-coefficient-displays",
    "href": "06-linear_models-plots.html#sec-coefficient-displays",
    "title": "6  Plots for univariate response models",
    "section": "\n6.3 Coefficient displays",
    "text": "6.3 Coefficient displays\nThe results of linear models are most often reported in tables and typically with “significance stars” (*, **, ***) to indicate the outcome of hypothesis tests. These are useful for looking up precise values and you can use this format to compare a small number of competing models side-by-side. However, as illustrated by Kastellec & Leoni (2007), plots of coefficients can increase the clarity of presentation and make it easier to draw correct conclusions. Yet, when you need to present tables, there is a variety of tools in R that can help make them attractive in publications.\nFor illustration, I’ll consider three models for the Prestige data of increasing complexity:\n\n\nmod1 fits the main effects of the three quantitative predictors;\n\nmod2 adds the categorical variable type of occupation;\n\nmod3 allows an interaction of income with type.\n\n\nmod1 &lt;- lm(prestige ~ education + income + women,\n           data=Prestige)\nmod2 &lt;- lm(prestige ~ education + women + income + type,\n           data=Prestige)\nmod3 &lt;- lm(prestige ~ education + women + income * type,\n           data=Prestige)\n\nFrom our earlier analyses (Section 3.2.3) we saw that the marginal relationship between income and prestige was nonlinear Figure 3.11), and was better represented in a linear model using log(income) (Section 3.2.3.1) shown in Figure 3.14. However, this possibly non-linear relationship could also be explained by stratifying (Section 3.2.3.2) the data by type of occupation (Figure 3.15).\n\n6.3.1 Displaying coefficients\nsummary() gives the complete precis of a fitted model, with information about the estimated coefficients, residuals and goodness-of fit statistics like \\(R^2\\). But if you only want to see the coefficients, standard errors, etc. lmtest::coeftest() gives these results in the familiar format for console output. broom::tidy() places these in a tidy format common to many modeling functions which is useful for futher processing (e.g., comparing models).\n\nlmtest::coeftest(mod1)\n#&gt; \n#&gt; t test of coefficients:\n#&gt; \n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -6.794334   3.239089   -2.10    0.039 *  \n#&gt; education    4.186637   0.388701   10.77  &lt; 2e-16 ***\n#&gt; income       0.001314   0.000278    4.73  7.6e-06 ***\n#&gt; women       -0.008905   0.030407   -0.29    0.770    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nbroom::tidy(mod1)\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept) -6.79     3.24        -2.10  3.85e- 2\n#&gt; 2 education    4.19     0.389       10.8   2.59e-18\n#&gt; 3 income       0.00131  0.000278     4.73  7.58e- 6\n#&gt; 4 women       -0.00891  0.0304      -0.293 7.70e- 1\n\nThe modelsummary package (Arel-Bundock, 2025b) is an easy to use, very general package to summarize data and statistical models in R. The main function modelsummary() can produce highly customizable tables of coefficients in a wide variety of output formats, including HTML, PDF, LaTeX, Markdown, and MS Word. You can select the statistics displayed for any model term with the estimate and statistic arguments.\n\n\nTable 6.1: Table of coefficients for the main effects model.\n\n# FIXME: There is something wrong with the latex output of this that texlive\n# 2025 does not like\nmodelsummary(list(\"Model1\" = mod1),\n  coef_omit = \"Intercept\",\n  shape = term ~ statistic,\n  estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n  statistic = c(\"std.error\", \"p.value\"),\n  fmt = fmt_statistic(\"estimate\" = 3, \"conf.low\" = 4, \"conf.high\" = 4),\n  gof_omit = \".\")\n\n\n\ngof_omit allows you to omit or select the goodness-of-fit statistics and other model information available from those listed by get_gof():\n\nget_gof(mod1)\n#&gt;   aic bic r.squared adj.r.squared rmse nobs   F logLik\n#&gt; 1 716 729     0.798         0.792 7.69  102 129   -353\n\n\n6.3.2 Visualizing coefficients\nmodelplot() is the companion function. It allows you to plot model estimates and confidence intervals. It makes it easy to subset, rename, reorder, and customize plots using same mechanics as in modelsummary().\n\ntheme_set(theme_minimal(base_size = 14))\n\nmodelplot(mod1, coef_omit=\"Intercept\", \n          color=\"red\", size=1, linewidth=2) +\n  labs(title=\"Raw coefficients for mod1\")\n\n\n\n\n\n\nFigure 6.5: Plot of coefficients and their standard error bar for the simple main effects model\n\n\n\n\nBut this plot is disappointing and misleading because it show the raw coefficients. From the plot, it looks like only education has a non-zero effect, but the effect of income is also highly significant. The problem is that the magnitude of the coefficient \\(\\hat{b}_{\\text{education}}\\) is more than 40,000 times that of the other coefficients, because education is measured years, while income is measured in dollars. The 95% confidence interval for \\(\\hat{b}_{\\text{income}} = [0.0008, 0.0019]\\), but this is invisible in the plot.\nBefore figuring out how to fix this issue, I show the comparable displays from modelsummary() and modelplot() for all three models. When you give modelsummary() a list of models, it displays their coefficients side-by-side as shown in Table 6.2.\n\nmodels &lt;- list(\"Model1\" = mod1, \"Model2\" = mod2, \"Model3\" = mod3)\nmodelsummary(models,\n     coef_omit = \"Intercept\",\n     fmt = 2,\n     stars = TRUE,\n     shape = term ~ statistic,\n     statistic = c(\"std.error\", \"p.value\"),\n     gof_map = c(\"rmse\", \"r.squared\")\n     )\n\n\nTable 6.2: Table of coefficients for three models.\n\n\n\n\n\n\n\n \nModel1\nModel2\nModel3\n\n\n \nEst.\nS.E.\np\nEst. \nS.E. \np \nEst.  \nS.E.  \np  \n\n\n\n\neducation\n4.19***\n0.39\n&lt;0.01\n3.66***\n0.65\n&lt;0.01\n2.80***\n0.59\n&lt;0.01\n\n\nincome\n0.00***\n0.00\n&lt;0.01\n0.00***\n0.00\n&lt;0.01\n0.00***\n0.00\n&lt;0.01\n\n\nwomen\n-0.01\n0.03\n0.77\n0.01\n0.03\n0.83\n0.08*\n0.03\n0.02\n\n\ntypewc\n\n\n\n-2.92\n2.67\n0.28\n3.43\n5.37\n0.52\n\n\ntypeprof\n\n\n\n5.91\n3.94\n0.14\n27.55***\n5.41\n&lt;0.01\n\n\nincome × typewc\n\n\n\n\n\n\n-0.00\n0.00\n0.21\n\n\nincome × typeprof\n\n\n\n\n\n\n-0.00***\n0.00\n&lt;0.01\n\n\nRMSE\n7.69\n\n\n6.91\n\n\n6.02\n\n\n\n\nR2\n0.798\n\n\n0.835\n\n\n0.875\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\nNote that a factor predictor (like type here) with \\(d\\) levels is represented by \\(d-1\\) coefficients in main effects and in interactions with quantitative variables. These levels are coded with treatment contrasts by default. Also by default, the first level is set as the reference level in alphabetical order. Here the reference level is blue collar (bc), so the coefficient typeprof = 5.91 indicates that professional occupations on average are rated 5.91 greater on the Prestige scale than blue collar workers.\nNote also that unlike the table, the coefficients in Figure 6.5 are ordered from bottom to top, because the Y axis starts at the lower left corner. In Figure 6.6 I use scale_y_discrete() to reverse the order. It is also useful to add a vertical reference line at \\(\\beta = 0\\).\n\nmodelplot(models, \n          coef_omit=\"Intercept\", \n          size=1.3, linewidth=2) +\n  ggtitle(\"Raw coefficients\") +\n  geom_vline(xintercept = 0, linewidth=1.5) +\n  scale_y_discrete(limits=rev) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.2))\n\n\n\n\n\n\nFigure 6.6: Plot of raw coefficients and their confidence intervals for all three models\n\n\n\n\n\n6.3.3 More useful coefficient plots\nThe problem with plots of raw coefficients shown in Figure 6.5 and Figure 6.6 is that the coefficients for different predictors are not directly comparable because they are measured in different units.\nOne alternative is to plot the standardized coefficients. Another way is to re-scale the predictors into more comparable and meaningful units. I illustrate these ideas below.\nStandardized coefficients\nThe simplest way to do this is to transform all variables to standardized (\\(z\\)) scores. The coefficients are then interpreted as the standardized change in prestige for a one standard deviation change in the predictors. The syntax below uses scale to transform all the numeric variables. Then, we re-fit the models using the standardized data.\n\nPrestige_std &lt;- Prestige |&gt;\n  as_tibble() |&gt;\n  mutate(across(where(is.numeric), scale))\n\nmod1_std &lt;- lm(prestige ~ education + income + women, \n               data=Prestige_std)\nmod2_std &lt;- lm(prestige ~ education + women + income + type, \n               data=Prestige_std)\nmod3_std &lt;- lm(prestige ~ education + women + income * type, \n               data=Prestige_std)\n\nThe plot in Figure 6.7 now shows the significant effect of income in all three models. As well, it offers a more sensitive comparison of the coefficients of other terms across models; for example women is not significant in models 1 and 2, but becomes significant in Model 3 when the interaction of income * type is included.\n\nmodels &lt;- list(\"Model1\" = mod1_std, \"Model2\" = mod2_std, \"Model3\" = mod3_std)\nmodelplot(models, \n          coef_omit=\"Intercept\", size=1.3) +\n  ggtitle(\"Standardized coefficients\") +\n  geom_vline(xintercept = 0, linewidth = 1.5) +\n  scale_y_discrete(limits=rev) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.2))\n\n\n\n\n\n\nFigure 6.7: Plot of standardized coefficients and their confidence intervals for all three models\n\n\n\n\nIt turns out there is an easier way to get plots of standardized coefficients. modelsummary() extracts coefficients from model objects using the parameters package, and that package offers several options for standardization: See model parameters documentation. We can pass the standardize=\"refit\" (or other) argument directly to modelsummary() or modelplot(), and that argument will be forwarded to parameters. The plot produced by the code below is identical to Figure 6.7 and is not shown.\n\nmodelplot(list(\"mod1\" = mod1, \"mod2\" = mod2, \"mod3\" = mod3),\n          standardize = \"refit\",\n          coef_omit=\"Intercept\", size=1.3) +\n  ggtitle(\"Standardized coefficients\") +\n  geom_vline(xintercept = 0, linewidth=1.5) +\n  scale_y_discrete(limits=rev) +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.2))\n\nThe ggstats package (Larmarange, 2025) provides even nicer versions of coefficient plots that handle factors in a more reasonable way, as levels within the factor. ggcoef_model() plots a single model and ggcoef_compare() plots a list of models using sensible defaults. A small but nice feature is that it explicitly shows the 0 value for the reference level of a factor (type = \"bc\" here) and uses better labels for factors and their interactions.\n\nmodels &lt;- list(\n  \"Base model\"      = mod1_std,\n  \"Add type\"        = mod2_std,\n  \"Add interaction\" = mod3_std)\n\nggcoef_compare(models) +\n  labs(x = \"Standarized Coefficient\")\n\n\n\n\n\n\nFigure 6.8: Model comparison plot from ggcoef_compare()\n\n\n\n\nMore meaningful units\nStandardizing the variables makes the coefficients directly comparable, but it may be harder to understand what they mean in terms of the variables. For example, the coefficient of income in mod2_std is 0.25. A literal interpretation is that occupational prestige is expected to increase 0.25 standard deviations for each standard deviation increase in income, but it may be difficult to appreciate what this means.\nA better substantive comparison of the coefficients could use understandable scales for the predictors, e.g., months of education, $100,000 of income or 10% of women’s participation. Note that the effect of this is just to multiply the coefficients and their standard errors by a factor. The statistical conclusions of significance are unchanged.\nFor simplicity, I do this just for Model 1.\n\nPrestige_scaled &lt;- Prestige |&gt;\n  mutate(education = 12 * education,\n         income = income / 100,\n         women = women / 10)\n\nmod1_scaled &lt;- lm(prestige ~ education + income + women,\n                  data=Prestige_scaled)\n\nWhen we plot this with ggcoef_model(), there are many options to control how variables are labeled and other details.\n\nggcoef_model(mod1_scaled,\n  signif_stars = FALSE,\n  variable_labels = c(education = \"education\\n(months)\",\n                      income = \"income\\n(/$100K)\",\n                      women = \"women\\n(/10%)\")) +\n  xlab(\"Coefficients for prestige with scaled predictors\")\n\n\n\n\n\n\nFigure 6.9: Plot of coefficients for prestige with scaled predictors for Model 1.\n\n\n\n\nSo, on average, each additional month of education increases the prestige rating by 0.34 units, while an additional $100,000 of income increases it by 0.13 units. While these are significant effects, they are not large in relation to the scale of prestige which ranges 14.8—87.2.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#sec-avplots",
    "href": "06-linear_models-plots.html#sec-avplots",
    "title": "6  Plots for univariate response models",
    "section": "\n6.4 Added-variable and related plots",
    "text": "6.4 Added-variable and related plots\nIn multiple regression problems, it is most often useful to construct a scatterplot matrix and examine the plot of the response vs. each of the predictors as well as those of the predictors against each other. However, the simple, marginal scatterplots of a response \\(y\\) against each of several predictors \\(x_1, x_2, \\dots\\) can be misleading because each one ignores the other predictors.\nTo see this consider a toy dataset, coffee, giving measures of coffee consumption, occupational stress and an index of heart problems in a sample of \\(n=20\\) graduate students and professors.\n\ndata(coffee, package=\"matlib\")\n\nscatterplotMatrix(~ Heart + Coffee + Stress, \n  data=coffee,\n  smooth = FALSE,\n  ellipse = list(levels=0.68, fill.alpha = 0.1),\n  pch = 19, cex.labels = 2.5)\n\n\n\n\n\n\nFigure 6.10: Scatterplot matrix showing pairwise relations among Heart (\\(y\\)), Coffee consumption (\\(x_1\\)) and Stress (\\(x_2\\)), with linear regression lines and 68% data ellipses for the bivariate relations\n\n\n\n\nThe message from these marginal plots in Figure 6.10 seems to be that coffee is bad for your heart, stress is bad for your heart, and stress is also strongly related to coffee consumption. Yet, when we fit a model with both variables together, we get the following results:\n\nfit.both   &lt;- lm(Heart ~ Coffee + Stress, data=coffee)\nlmtest::coeftest(fit.both)\n#&gt; \n#&gt; t test of coefficients:\n#&gt; \n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   -7.794      5.793   -1.35     0.20    \n#&gt; Coffee        -0.409      0.292   -1.40     0.18    \n#&gt; Stress         1.199      0.224    5.34  5.4e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe coefficients suggest that stress is indeed bad for your heart, but the negative (though non-significant) coefficient for coffee suggests that coffee is good for you.How can this be? Does that mean I should drink more coffee, while avoiding stress?\nThe reason for this apparent paradox is that the general linear model fit by lm() estimates all effects together and so the coefficients pertain to the partial effect of a given predictor, adjusting for the effects of all others. That is, the coefficient for coffee (\\(\\beta_{\\text{Coffee}} = -0.41\\)) estimates the effect of coffee for people with same level of stress. In the marginal scatterplot, the positive slope for coffee (1.10) ignores the correlation of coffee and stress.\nThis is an example of confounding in regression when an important predictor is omitted. Stress is positively associated with both coffee consumption and heart damage. When stress is omitted, the coefficient for coffee is biased because it “picks up” the relation with the omitted variable.\nA solution to this problem is the added-variable plot (“AV plot”, also called partial regression plot, MostellerTukey-1977). This is a multivariate analog of a a simple marginal scatterplot, designed to visualize directly the partial relation between \\(y\\) and the predictors \\(x_1, x_2, \\dots\\) in a multiple regression model.\nYou can think of this as a magic window that hides the relations of all other variables with each of the \\(y\\) and \\(x_i\\) shown in a given added-variable plot. This gives an unobstructed view of the net relation between \\(y\\) and \\(x_i\\) with the effect of all other variables removed. In effect, it reduces the problem of viewing the complete model in \\(p\\)-dimensional space to a sequence of \\(p\\) 2D plots, each of which tells the story of one predictor, unentangled from the others. This is essentially the same idea as the partial variables plot (Section 3.10.3) used to understand partial correlations.\nThe construction of an AV plot is conceptually very simple. For variable \\(x_i\\), imagine that we fit two supplementary regressions:\n\nRegress \\(\\mathbf{y}\\) on \\(\\mathbf{X_{(-i)}}\\), the model matrix of all of the regressors except \\(x_i\\). By definition, the residuals from this regression, \\(\\mathbf{y}^\\star \\equiv \\mathbf{y} \\,\\vert\\, \\text{others} = \\mathbf{y} - \\widehat{\\mathbf{y}} \\,\\vert\\, \\mathbf{X_{(-i)}}\\),  are the part of \\(\\mathbf{y}\\) that cannot be explained by all the other regression terms. These residuals are necessarily uncorrelated with the other predictors.\nRegress \\(x_i\\) on the other predictors, \\(\\mathbf{X_{(-i)}}\\) and again obtain the residuals. These residuals, \\(\\mathbf{x}_i^\\star \\equiv \\mathbf{x}_i \\,\\vert\\, \\text{others} = \\mathbf{x}_i - \\widehat{\\mathbf{x}}_i \\,\\vert\\, \\mathbf{X_{(-i)}}\\) give the part of \\(x_i\\) that cannot be explained by the others, and so are uncorrelated with them.\n\nThe AV plot is then just a simple scatterplot of these residuals, \\(\\mathbf{y}^\\star\\) on the vertical axis, and \\(\\mathbf{x}^\\star\\) on the horizontal. In practice, it is unnecessary to run the auxilliary regressions this way (Velleman & Welsh, 1981). Both can be calculated using stats::lsfit() roughly as follows:\n\nAVcalc &lt;- function(model, variable)\nX &lt;- model.matrix(model)\nresponse &lt;- model.response(model)\nx &lt;- X[, -variable]\ny &lt;- cbind(X[, variable], response)\nfit &lt;- lsfit(x, y, intercept = FALSE)\nresids &lt;- residuals(fit)\nreturn(resids)\n\nNote that y here contains both the current predictor, \\(\\mathbf{x}_i\\) and the response \\(\\mathbf{y}\\), so the residuals resids have two columns, one for \\(x_i \\,\\vert\\, \\text{others}\\) and one for \\(y \\,\\vert\\, \\text{others}\\).\nAdded-variable plots are produced using car::avPlot() for one predictor or avPlots() for any number of model terms. The id argument controls which points are identified in the plots; n=2 labels the two points that are furthest from the mean on the horizontal axis and the two with the largest absolute residuals. For instance, in Figure 6.11, observations 5 and 13 are flagged because their conditional \\(\\mathbf{x}_i^\\star\\) values are extreme; observation 17 has a large absolute residual, \\(\\mathbf{y}^\\star = \\text{Heart} \\,\\vert\\, \\text{others}\\).\n\navPlots(fit.both,\n  ellipse = list(levels = 0.68, fill=TRUE, fill.alpha = 0.1),\n  pch = 19,\n  id = list(n = 2),\n  cex.lab = 1.5,\n  main = \"Added-variable plots for Coffee data\")\n\n\n\n\n\n\nFigure 6.11: Added-variable plots for Coffee and Stress in the multiple regression model\n\n\n\n\nThe data ellipses for \\(\\mathbf{x}_i^\\star\\) and \\(\\mathbf{y}^\\star\\) summarize the conditional (or partial) relations of the response to each predictor controlling for all other predictors in each plot. The essential idea is that the data ellipse for \\((\\mathbf{x}_i^\\star, \\mathbf{y}^\\star)\\) has the identical relation to the estimate \\(\\hat{b}_i\\) in a multiple regression as the data ellipse of \\((\\mathbf{x}, \\mathbf{y})\\) has to the slope in a simple regression.\n\n6.4.1 Properties of AV plots\nAV plots are particularly interesting and useful for the following noteworthy properties:\n\nThe slope of the simple regression in the AV plot for variable \\(x_i\\) is identical to the slope \\(b_i\\) for that variable in the full multiple regression model.\nThe residuals in this plot are the same as the residuals using all predictors. This means you can see the degree of fit for observations directly in relation to the various predictors, which is not the case for marginal scatterplots.\nConsequentially, the standard deviation of the (vertical) residuals in the AV plot is the same as \\(s = \\sqrt(MSE)\\) in the full model and the standard error of a coefficient is \\(\\text{SE}(b_i) = s / \\sqrt{\\Sigma (\\mathbf{x}_i^\\star)^2}\\). This is shown by the size of the shadow of the data ellipses on the vertical axis in Figure 6.11.\nThe horizontal positions, \\(\\mathbf{x}_i^\\star\\), of points adjust for all other predictors, and so we can see points at the extreme left and right as unusual in relation to the others. If these points are also badly fitted (large residuals), we can see their influence on the fitted relation in the full model. AV plots thus provide visual displays of (partial) leverage and influence on each of the regression coefficients.\nThe correlation of \\(\\mathbf{x}_i^\\star\\) and \\(\\mathbf{y}^\\star\\) shown by the shape of the data ellipses is the partial correlation between \\(\\mathbf{x}_i\\) and \\(\\mathbf{y}_i\\) with other predictors partialled out.\n\n6.4.2 Marginal - conditional plots\nThe relation of the conditional data ellipses in AV plots to those in marginal plots of the same variables provides further insight into what it means to “control for” other variables. Figure 6.12 shows the same added-variable plots for Heart disease on Stress and Coffee as in Figure 6.11 (with a zoomed-out scaling), but here we also overlay the marginal data ellipses for \\((\\mathbf{x}_i, \\mathbf{y})\\) (centered at the means), and marginal regression lines for Stress and Coffee separately. Drawing arrows connecting the original data points to their positions in the AV plot shows what happens when we condition on or partial out the other variable.\nThese marginal - conditional plots are produced by car::mcPlot() (for one regressor) and car::mcPlots() (for several). The plots for the marginal and conditional relations can be compared separately using the same scales for both, or overlaid as shown here. The points labeled here are only those with large absolute residuals \\(\\mathbf{y}^\\star\\) in the vertical direction.\n\nmcPlots(fit.both, \n  ellipse = list(levels=0.68, fill=TRUE, fill.alpha=0.2),\n  id = list(n=2),\n  pch = c(16, 16),\n  col.marginal = \"red\", col.conditional = \"blue\",\n  col.arrows = \"black\",\n  cex.lab = 1.5)\n\n\n\n\n\n\nFigure 6.12: Marginal \\(+\\) conditional (added-variable) plots for Coffee and Stress in the multiple regression predicting Heart disease. Each panel shows the 68% conditional data ellipse for \\(x_i^\\star, y^\\star\\) residuals (shaded, blue) as well as the marginal 68% data ellipse for the \\((x_i, y)\\) variables, shifted to the origin. Arrows connect the mean-centered marginal points (red) to the residual points (blue).\n\n\n\n\nThe most obvious feature of Figure 6.12 is that Coffee has a negative slope in the conditional AV plot but a positive slope in the marginal plot. This is an example of Simpson’s paradox in a regression context: marginal and conditional relations can have opposite signs. \nLess obvious is the relation between the marginal and AVP ellipses. In 3D, the marginal data ellipse is the shadow of the ellipsoid for \\((\\mathbf{y}, \\mathbf{x}_1, \\mathbf{x}_2)\\) on one of the coordinate planes, while the AV plot is a slice through the ellipsoid where either \\(\\mathbf{x}_1\\) or \\(\\mathbf{x}_2\\) is held constant. Thus, the AVP ellipse must be contained in the marginal ellipse, as we can see in Figure 6.12. If there are only two \\(x\\)s, then the AVP ellipse must touch the marginal ellipse at two points.\nFinally, Figure 6.12 also shows how conditioning on other predictors works for individual observations, where each point of \\((\\mathbf{x}_i^\\star, \\mathbf{y}^\\star)\\) is the image of \\((\\mathbf{x}_i, \\mathbf{y})\\) along the path of the marginal regression. The variability in the response and in the focal predictor are both reduced, leaving only the uncontaminated relation of \\(\\mathbf{y}\\) with \\(\\mathbf{x}_i\\).\nThese plots are similar in spirit to the ARES plot (“Adding REgressors Smoothly”) proposed by Cook & Weisberg (1994), but their idea was an interactive animation, displaying a smooth transition between the fit of a marginal model and the fit of a larger model. They used linear interpolation, \\[\n(\\mathbf{x}_i, \\mathbf{y}_i)_{\\text{interp}} = (\\mathbf{x}_i, \\mathbf{y}_i) + \\lambda [(\\mathbf{x}_i^\\star, \\mathbf{y}_i^\\star) - (\\mathbf{x}_i, \\mathbf{y}_i)] \\:\\: ,\n\\] controlled by a slider whose value, \\(\\lambda \\in [0, 1]\\), was the weight given to the smaller marginal model. See this animation for an example using the Duncan data.\n\n6.4.3 Prestige data\nFor a substantive example, let’s return to the model for income, education and women in the Prestige data. The plot in Figure 6.13 shows the strong positive relations of income and education to prestige in the full model, and the negligible relation of percent women. But, in the plot for income, two occupations (physicians and general managers) with high income strongly pull the regression line down from what can be seen in the orientation of the conditional data ellipse.\n\nprestige.mod1 &lt;- lm(prestige ~ education + income + women,\n           data=Prestige)\n\navPlots(prestige.mod1, \n  ellipse = list(levels = 0.68),\n  id = list(n = 2, cex = 1.2),\n  pch = 19,\n  cex.lab = 1.5,\n  main = \"Added-variable plots for prestige\")\n\n\n\n\n\n\nFigure 6.13: Added-variable plot for the quantitative predictors in the Prestige data.\n\n\n\n\nThe influential points for physicians and general managers could just be unusual, or suggest that the relation of income to prestige is nonlinear. A rough test of this is to fit a smoothed curve through the points in the AV plot as shown in Figure 6.14.\n\nop &lt;- par(mar=c(4, 4, 1, 0) + 0.5)\nres &lt;- avPlot(prestige.mod1, \"income\",\n              ellipse = list(levels = 0.68),\n              pch = 19,\n              cex.lab = 1.5)\nsmooth &lt;- loess.smooth(res[,1], res[,2])\nlines(smooth, col = \"red\", lwd = 2.5)\n\n\n\n\n\n\nFigure 6.14: Added-variable plot for income, with a loess smooth.\n\n\n\n\nHowever, this use of AV plots to diagnose nonlinearity or suggest transformations can be misleading (Cook, 1996). Curvature in these plots is an indication of some model deficiency, but unless the predictors are uncorrelated, they cannot determine the form of a possible transformation of the predictors.\n\n6.4.4 Component + Residual plots\nA related method, the component + residual plot (“C+R plot”, also called partial residual plot, Larsen & McCleary (1972); Cook (1993)) gives a plot more suited to detecting the need to transform a predictor \\(\\mathbf{x}_i\\) to a form \\(f(\\mathbf{x}_i)\\) to make it’s relationship with the response \\(\\mathbf{y}\\) more nearly linear. This plot displays the partial residual \\(\\mathbf{e} + \\hat{b}_i \\mathbf{x}_i\\) on the vertical axis against \\(\\mathbf{x}_i\\) on the horizontal, where \\(\\mathbf{e}\\) are the residuals from the full model. A smoothed curve through the points will often suggest the form of the transformation \\(f()\\). The fact that the horizontal axis is \\(\\mathbf{x}_i\\) itself rather than \\(\\mathbf{x}^\\star_i\\) makes it easier to see the functional form.\nThe C+R plot has the same desirable properties as the AV plot: The slope \\(\\hat{b}_i\\) and residuals \\(\\mathbf{e}\\) in this plot are the same as those in the full model.\nC+R plots are produced by car::crPlots() and car::crPlot(). Figure 6.15 shows this just for income in the model prestige.mod1. (These plots for education and women show no strong evidence of curvilinearity.) The dashed blue line is the linear partial fit, \\(\\hat{b}_i \\mathbf{x}_i\\), whose slope \\(\\hat{b}_2 = 0.0013\\) is the same as that for income in prestige.mod1. The solid red curve is the loess smooth through the points. The same points are identified as noteworthy as in AV plot in Figure 6.14.\n\ncrPlot(prestige.mod1, \"income\",\n       smooth = TRUE,\n       order = 2,\n       pch = 19,\n       col.lines = c(\"blue\", \"red\"),\n       id = list(n=2, cex = 1.2),\n       cex.lab = 1.5) \n\n\n\n\n\n\nFigure 6.15: Component + residual plot for income in the model for the quantitative predictors of prestige. The dashed blue line is the partial linear fit for income. The solid red curve is the loess smooth.\n\n\n\n\nThe partial relation between prestige and income is clearly curved, so it would be appropriate to transform income or to include a polynomial (quadratic) term and refit the model. As suggested earlier (Section 3.2.3) it makes sense statistically and substantively to model the effect of income on a log scale, so then the slope for log(income) would measure the increment in prestige for a constant percentage increase in income.\nThe effect of percent women on prestige seen in Figure 6.13 appears very small and essentially linear. However, if we wished to examine this more closely, we could use the C+R plot in Figure 6.16.\n\ncrPlot(prestige.mod1, \"women\",\n       pch = 19,\n       col.lines = c(\"blue\", \"red\"),\n       id = list(n=2, cex = 1.2),\n       cex.lab = 1.5)\n\n\n\n\n\n\nFigure 6.16: Component + residual plot for women in the model for the quantitative predictors of prestige.\n\n\n\n\nThis shows a slight degree of curvature, with modestly larger values in the extremes. If we wished to test this statistically, we could fit a model with a quadratic effect of women, and compare that to the linear-only effect using anova().\n\nprestige.mod2 &lt;- lm(prestige ~ education + income + poly(women,2),\n           data=Prestige)\n\nanova(prestige.mod1, prestige.mod2)\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: prestige ~ education + income + women\n#&gt; Model 2: prestige ~ education + income + poly(women, 2)\n#&gt;   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n#&gt; 1     98 6034                         \n#&gt; 2     97 5907  1       127 2.08   0.15\n\nThis model ignores the type of occupation (“bc”, “wc”, “prof”) as well as any possible interactions of type with other predictors. We examine this next, using effect displays.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#sec-effect-displays",
    "href": "06-linear_models-plots.html#sec-effect-displays",
    "title": "6  Plots for univariate response models",
    "section": "\n6.5 Effect displays",
    "text": "6.5 Effect displays\nFor two predictors it is possible, even if awkward, to display the fitted response surface in a 3D plot or faceted 2D views in what I call a full model plot. For more than two predictors such displays become cumbersome if not impractical, particularly when there are interactions in the model, when some effects are curvilinear, or when the main substantive interest is focused understanding on one or more main effects or interaction terms in the presence of others. The method of effect displays, largely introduced by John Fox (Fox, 1987, 2003; Fox & Weisberg, 2018b) is a generally useful solution to this problem. 2 These plots are nearly always easier to understand than tables of coefficients.\nThe idea of effect displays is quite simple, but very general and handles models of arbitrary complexity. Imagine that in a model we have a particular subset of predictors (focal predictors) whose effects on the response variable we wish to visualize. The essence of an effect display is that we calculate the predicted values (and standard errors) of the response for the model term(s) involving the focal predictors (and all low-order relatives, e.g, main effects that are marginal to an interaction) as those predictors are allowed to vary over a grid covering their range.\nFor a given plot, the other, non-focal variables are “controlled” by being fixed at typical values. For example, a quantitative predictor could be fixed at it’s mean, median or some representative value. A factor could be fixed at equal proportions of its levels or its proportions in the data. The result, when plotted, shows the predicted effects of the focal variables, either with multiple lines or in a faceted display, but with all the other variables controlled, adjusted for or averaged over. For interaction effects all low-order relatives are typically included in the fitted values for the term being graphed.\nIn practical terms, a scoring matrix \\(\\mathbf{X}^\\bullet\\) is defined by the focal variables varied over their ranges and the other variables held fixed. The fitted values for a model term are then calculated as \\(\\widehat{\\mathbf{y}}^\\bullet = \\mathbf{X}^\\bullet \\; \\widehat{\\mathbf{b}}\\) using the equivalent of:\n\npredict(model, newdata = X, se.fit = TRUE) \n\nwhich also calculates the standard errors as the square roots of \\(\\mathrm{diag}\\, (\\mathbf{X}^\\bullet \\, \\widehat{\\mathbf{\\mathsf{Var}}} (\\mathbf{b}) \\, \\mathbf{X}^{\\bullet\\mathsf{T}} )\\) where \\(\\widehat{\\mathbf{\\mathsf{Var}}} (\\mathbf{b})\\) is the estimated covariance matrix of the coefficients. Consequently, predictor effect values can be obtained for any modelling function that has predict() and vcov() methods. To date, effect displays are available for over 100 different model types in various packages.\nTo illustrate the mechanics for the effect of education in the prestige.mod1 model, construct a data frame varying education, but fixing income and women at their means:\n\nX &lt;- expand.grid(\n      education = seq(8, 16, 2),\n      income = mean(Prestige$income),\n      women = mean(Prestige$women)) |&gt; \n  print(digits = 3)\n#&gt;   education income women\n#&gt; 1         8   6798    29\n#&gt; 2        10   6798    29\n#&gt; 3        12   6798    29\n#&gt; 4        14   6798    29\n#&gt; 5        16   6798    29\n\npredict() then gives the fitted values for a simple effect plot of prestige against education. predict.lm() returns list, so it is necessary to massage this to a data frame for graphing.\n\npred &lt;- predict(prestige.mod1, newdata=X, se.fit = TRUE)\ncbind(X, fit = pred$fit, se = pred$se.fit) |&gt; \n  print(digits=3)\n#&gt;   education income women  fit    se\n#&gt; 1         8   6798    29 35.4 1.318\n#&gt; 2        10   6798    29 43.7 0.828\n#&gt; 3        12   6798    29 52.1 0.919\n#&gt; 4        14   6798    29 60.5 1.487\n#&gt; 5        16   6798    29 68.9 2.188\n\nAs Fox & Weisberg (2018b) note, effect displays can be combined with partial residuals to visualize both fit and potential lack of fit simultaneously, by plotting residuals from a model around 2D slices of the fitted response surface. This adds the benefits of C+R plots, in that we can see the impact of unmodeled curvilinearity and interactions in addition to those of predictor effect displays.\nThere are several implementations of effect displays in R, whose details, terminology and ease of use vary. Among these,  ggeffects (Lüdecke, 2025) calculates adjusted predicted values under several methods for conditioning.  marginaleffects (Arel-Bundock, 2025a) is similar and also provides estimation of marginal slopes, contrasts, odds ratios, etc. Both have plot() methods based on ggplot2. My favorite is the  effects (Fox et al., 2022) package, which alone provides partial residuals, and is somewhat easier to use, though it uses lattice graphics. See the vignette Predictor Effects Graphics Gallery for details of the computations for effect displays.\nThe main functions for computing fitted effects are predictorEffect() (for one predictor) and predictorEffects() (for one or more). For a model mod with formula y ~ x1 + x2 + x3 + x1:x2, the call to predictorEffects(mod, ~ x1) recognizes that an interaction is present and calculates the fitted values for combinations of x1 and x2, holding x3 fixed at its average value. This returns an object of class \"eff\" which can be graphed using the plot.eff() method.\nThe effect displays for several predictors can be plotted together, as with avplots() (Figure 6.13) by including them in the plot formula, e.g., predictorEffects(mod, ~ x1 + x3). Another function, allEffects() calculates the effects for each high-order term in the model, so allEffects(mod) |&gt; plot() is handy for getting a visual overview of a fitted model.\n\n6.5.1 Prestige data\nTo illustrate effect plots, I consider a more complex model, allowing a quadratic effect of women, representing income on a \\(\\log_{10}\\) scale, and allowing this to interact with type of occupation. Anova() provides the Type II tests of each of the model terms.\n\nprestige.mod3 &lt;- lm(prestige ~ education + poly(women,2) +\n                       log10(income)*type, data=Prestige)\n\n# test model terms\nAnova(prestige.mod3)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: prestige\n#&gt;                    Sum Sq Df F value  Pr(&gt;F)    \n#&gt; education             994  1   25.87 2.0e-06 ***\n#&gt; poly(women, 2)        414  2    5.38 0.00620 ** \n#&gt; log10(income)        1523  1   39.63 1.1e-08 ***\n#&gt; type                  589  2    7.66 0.00085 ***\n#&gt; log10(income):type    221  2    2.88 0.06133 .  \n#&gt; Residuals            3420 89                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe fitted coefficients, standard errors and \\(t\\)-tests from coeftest() are shown below. The coefficient for education means that an increase of one year of education, holding other predictors fixed, gives an expected increase of 2.96 in prestige. The other coefficients are more difficult to understand. For example, the effect of women is represented by two coefficients for the linear and quadratic components of poly(women, 2). The interpretation of coefficients of terms involving type depend on the contrasts used. Here, with the default treatment contrasts, they represent comparisons with type = \"bc\" as the reference level. It is not obvious how to understand the interaction effects like log10(income):typeprof.\n\n\n\n\n\n\n\n\n\n\nlmtest::coeftest(prestige.mod3)\n#&gt; \n#&gt; t test of coefficients:\n#&gt; \n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            -137.500     23.522   -5.85  8.2e-08 ***\n#&gt; education                 2.959      0.582    5.09  2.0e-06 ***\n#&gt; poly(women, 2)1          28.339     10.190    2.78   0.0066 ** \n#&gt; poly(women, 2)2          12.566      7.095    1.77   0.0800 .  \n#&gt; log10(income)            40.326      6.714    6.01  4.1e-08 ***\n#&gt; typewc                    0.969     39.495    0.02   0.9805    \n#&gt; typeprof                 74.276     30.736    2.42   0.0177 *  \n#&gt; log10(income):typewc     -1.073     10.638   -0.10   0.9199    \n#&gt; log10(income):typeprof  -17.725      7.947   -2.23   0.0282 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt is easiest to produce effect displays for all terms in the model using allEffects(), accepting all defaults. This gives (Figure 6.17) effect plots for the main effects of education and income and the interaction of income with type, with the non-focal variables held fixed. Each plot shows the fitted regression relation and a default 95% pointwise confidence band using the standard errors. Rug plots at the bottom show the locations of observations for the horizontal focal variable, which is useful when the observations are not otherwise plotted.\n\nallEffects(prestige.mod3) |&gt;\n  plot()\n\n\n\n\n\n\nFigure 6.17: Predictor effect plot for all terms in the model with 95% confidence bands.\n\n\n\n\nThe effect for women, holding education, income and type constant looks to be quite strong and curved upwards. But note that these plots use different vertical scales for prestige in each plot and the range in the plot for women is much smaller than in the others. The interaction is graphed showing separate curves for the three levels of type.\nFor a more detailed look, it is useful to make separate plots for the predictors in the model, which allows customizing options for calculation and display. Partial residuals for the observations are computed by using residuals = TRUE in the call to predictorEffects(). The slope of the fitted line (in blue) is exactly coefficient for education in the full model. As with C+R plots, a smooth loess curve (in red) gives a visual assessment of linearity for a given predictor. A wide variety of graphing options are available in the call to plot(). Figure 6.18 shows the effect display for education with partial residuals and point identification of those points with the largest Mahalanobis distances from the centroid.\n\n\nlattice::trellis.par.set(par.xlab.text=list(cex=1.5),\n                         par.ylab.text=list(cex=1.5))\n\npredictorEffects(prestige.mod3, ~ education,\n                 residuals = TRUE) |&gt;\n  plot(partial.residuals = list(pch = 16, col=\"blue\"),\n       id=list(n=4, col=\"black\")) \n\n\n\n\n\n\nFigure 6.18: Predictor effect plot for education displaying partial residuals. The blue line shows the slice of the fitted regression surface where other variables are held fixed. The red curve shows a loess smooth of the partial residuals.\n\n\n\n\nThe effect plot for women in this model is shown in Figure 6.19. This uses the same vertical scale as in Figure 6.18, showing a more modest effect of percent women.\n\npredictorEffects(prestige.mod3, ~women,\n                 residuals = TRUE) |&gt;\n  plot(partial.residuals = list(pch = 16, col=\"blue\", cex=0.8),\n       id=list(n=4, col=\"black\"))\n\n\n\n\n\n\nFigure 6.19: Predictor effect plot for women with partial residuals\n\n\n\n\n\nBecause of the interaction with type, the fitted effects for income are calculated for the three types of occupation. It is easiest to compare these in the a single plot (using multiline = TRUE), rather than in separate panels as in Figure 6.17. Income is represented as log10(income) in the model prestige.mod3, and it is also easier to understand the interaction by plotting income on a log scale, using the axes argument to specify a transformation of the \\(x\\) axis. I use 68% confidence bands here to make the differences among type more apparent.\n\npredictorEffects(prestige.mod3, ~ income,\n                 confidence.level = 0.68) |&gt;\n  plot(lines=list(multiline=TRUE, lwd=3),\n       confint=list(style=\"bands\"),\n       axes=list(\n          x=list(income=list(transform=list(trans=log, inverse=exp)))),\n       key.args = list(x=.7, y=.35)) \n\n\n\n\n\n\nFigure 6.20: Predictor effect plot for income, plotted on a log scale.\n\n\n\n\nFigure 6.20 provides a clear interpretation of the interaction, represented by the coefficients shown above for log10(income):typewc and log10(income):typeprof in the model. Averaging over three occupation types, prestige increases linearly with log income with a coefficient of 40.33. This means that increasing income by 10% (say) gives an increase of \\(40.33 / 10 = 4.033\\) in prestige. The slope for professional workers is less steep: the coefficient for log10(income):typeprof is -17.725. For these workers compared with blue collar jobs, prestige increases 1.77 less with a 10% increase in income. The difference in slopes for blue collar and white collar jobs is negligible.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#sec-leverage",
    "href": "06-linear_models-plots.html#sec-leverage",
    "title": "6  Plots for univariate response models",
    "section": "\n6.6 Outliers, leverage and influence",
    "text": "6.6 Outliers, leverage and influence\nIn small to moderate samples, “unusual” observations can have dramatic effects on a fitted regression model, as we saw in the analysis of Davis’s data on reported and measured weight (Section 2.1.2) where one erroneous observations hugely altered the fitted line. As well, it turns out that two observations in Duncan’s data are unusual enough that removing them alters his conclusion that income and education have nearly equal effects on occupational prestige.\nAn observation can be unusual in three archetypal ways, with different consequences:\n\nUnusual in the response \\(y\\), but typical in the predictor(s), \\(\\mathbf{x}\\) — a badly fitted case with a large absolute residual, but with \\(x\\) not far from the mean, as in Figure 2.4. This case does not do much harm to the fitted model.\nUnusual in the predictor(s) \\(\\mathbf{x}\\), but typical in \\(y\\) — an otherwise well-fitted point. This case also does litle harm, and in fact can be considered to improve precision, a “good leverage” point.\nUnusual in both \\(\\mathbf{x}\\) and \\(y\\) — This is the case, a “bad leverage” point, revealed in the analysis of Davis’s data, Figure 2.3, where the one erroneous point for women was highly influential, pulling the regression line towards it and affecting the estimated coefficient as well as all the fitted values. In addition, subsets of observations can be jointly influential, in that their effects combine, or can mask each other’s influence.\n\nInfluential cases are the ones that matter most. As suggested above, to be influential an observation must be unusual in both \\(\\mathbf{x}\\) and \\(y\\), and affects the estimated coefficients, thereby also altering the predicted values for all observations. A heuristic formula capturing the relations among leverage, “outlyingness” on \\(y\\) and influence is\n\\[\n\\text{Influence}_{\\text{coefficients}} \\;=\\; X_\\text{leverage} \\;\\times\\; Y_\\text{residual}\n\\] As described below, leverage is proportional to the squared distance \\((x_i - \\bar{x})^2\\) of an observation \\(x_i\\) from its mean in simple regression and to the squared Mahalanobis distance in the general case. The \\(Y_\\text{residual}\\) is best measured by a studentized residual, obtained by omitting each case \\(i\\) in turn and calculating its residual from the coefficients obtained from the remaining cases.\n\n6.6.1 The leverage-influence quartet\nThese ideas can be illustrated in the “leverage-influence quartet” by considering a standard simple linear regression for a sample and then adding one additional point reflecting the three situations described above. Below, I generate a sample of \\(N = 15\\) points with \\(x\\) uniformly distributed between (40, 60) and \\(y \\sim 10 + 0.75 x + \\mathcal{N}(0, 1.25^2)\\), duplicated four times.\n\nlibrary(tidyverse)\nlibrary(car)\nset.seed(42)\nN &lt;- 15\ncase_labels &lt;- paste(1:4, c(\"OK\", \"Outlier\", \"Leverage\", \"Influence\"))\nlevdemo &lt;- tibble(\n  case = rep(case_labels, \n             each = N),\n  x = rep(round(40 + 20 * runif(N), 1), 4),\n  y = rep(round(10 + .75 * x + rnorm(N, 0, 1.25), 4)),\n  id = \" \"\n)\n\nmod &lt;- lm(y ~ x, data=levdemo)\ncoef(mod)\n#&gt; (Intercept)           x \n#&gt;      13.332       0.697\n\nThe additional points, one for each situation are set to the values below.\n\n\nOutlier: (52, 60) a low leverage point, but an outlier (O) with a large residual\n\nLeverage: (75, 65) a “good” high leverage point (L) that fits well with the regression line\n\nInfluence: (70, 40) a “bad” high leverage point (OL) with a large residual.\n\n\nextra &lt;- tibble(\n  case = case_labels,\n  x  = c(65, 52, 75, 70),\n  y  = c(NA, 65, 65, 40),\n  id = c(\"  \", \"O\", \"L\", \"OL\")\n)\n\n#' Join these to the data\nboth &lt;- bind_rows(levdemo, extra) |&gt;\n  mutate(case = factor(case))\n\nWe can plot these four situations with ggplot2 in panels faceted by case as shown below. The standard version of this plot shows the regression line for the original data and that for the ammended data with the additional point. Note that we use the levdemo dataset in geom_smooth() for the regression line with the original data, but specify data = both for that with the additional point.\n\nggplot(levdemo, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(data = both, \n              method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"red\", linewidth = 1.3, linetype = 1) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE,\n              color = \"blue\", linewidth = 1, linetype = \"longdash\" ) +\n  stat_ellipse(data = both, level = 0.5, color=\"blue\", type=\"norm\", linewidth = 1.4) +\n  geom_point(data=extra, color = \"red\", size = 4) +\n  geom_text(data=extra, aes(label = id), nudge_x = -2, size = 5) +\n  facet_wrap(~case, labeller = label_both) +\n  theme_bw(base_size = 14)\n\n\n\n\n\n\nFigure 6.21: Leverage influence quartet with data 50% ellipses. Case (1) original data; (2) adding one low-leverage outlier, “O”; (3) adding one “good” leverage point, “L”; (4) adding one “bad” leverage point, “OL”. The dashed blue line is the fitted line for the original data, while the solid red line reflects the additional point. The data ellipses show the effect of the additional point on precision.\n\n\n\n\nThe standard version of this graph shows only the fitted regression lines in each panel. As can be seen, the fitted line doesn’t change very much in panels (2) and (3); only the bad leverage point, “OL” in panel (4) is harmful. Adding data ellipses to each panel immediately makes it clear that there is another part to this story— the effect of the unusual point on precision (standard errors) of our estimates of the coefficients.\nNow, we see directly that there is a big difference in impact between the low-leverage outlier [panel (2)] and the high-leverage, small-residual case [panel (3)], even though their effect on coefficient estimates is negligible. In panel (2), the single outlier inflates the estimate of residual variance (the size of the vertical slice of the data ellipse at \\(\\bar{x}\\)), while in panel (3) this is decreased.\nTo allow direct comparison and make the added value of the data ellipse more apparent, we overlay the data ellipses from Figure 6.21 in a single graph, shown in Figure 6.22. Here, we can also see why the high-leverage point “L” (added in panel (c) of Figure 6.21) is called a “good leverage” point. By increasing the standard deviation of \\(x\\), it makes the data ellipse somewhat more elongated, giving increased precision of our estimates of \\(\\mathbf{\\beta}\\).\n\nCodecolors &lt;- c(\"black\", \"blue\", \"darkgreen\", \"red\")\nwith(both,\n     {dataEllipse(x, y, groups = case, \n          levels = 0.68,\n          plot.points = FALSE, add = FALSE,\n          center.pch = \"+\",\n          col = colors,\n          fill = TRUE, fill.alpha = 0.1)\n     })\n\ncase1 &lt;- both |&gt; filter(case == \"1 OK\")\npoints(case1[, c(\"x\", \"y\")], cex=1)\n\npoints(extra[, c(\"x\", \"y\")], \n       col = colors,\n       pch = 16, cex = 2)\n\ntext(extra[, c(\"x\", \"y\")],\n     labels = extra$id,\n     col = colors, pos = 2, offset = 0.5)\n\n\n\n\n\n\nFigure 6.22: Data ellipses in the Leverage-influence quartet. This graph overlays the data ellipses and additional points from the four panels of Figure 6.22. It can be seen that only the OL point affects the slope, while the O and L points affect precision of the estimates in opposite directions.\n\n\n\n\n\n6.6.1.1 Measuring leverage\nLeverage is thus an index of the potential impact of an observation on the model due to its’ atypical value in the X space of the predictor(s). It is commonly measured by the “hat” value, \\(h_i\\), so called because it puts the hat \\(\\mathbf{\\widehat{(\\bullet)}}\\) on \\(\\mathbf{y}\\), i.e., the vector of fitted values can be expressed as\n\\[\\begin{aligned}\n\\mathbf{\\hat{y}}\n   &= \\underbrace{\\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}}_\\mathbf{H}\\mathbf{y} \\\\[2ex]\n   &= \\mathbf{H\\:y} \\:\\: .\n\\end{aligned}\\]\n\n\n\n\nHere, \\(h_i \\equiv h_{ii}\\) are the diagonal elements of the Hat matrix \\(\\mathbf{H}\\). In simple regression, hat values are proportional to the squared distance of the observation \\(x_i\\) from the mean, \\(h_i \\propto (x_i - \\bar{x})^2\\), \\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\Sigma_i (x_i - \\bar{x})^2} \\; ,\n\\tag{6.1}\\]\nand range from \\(1/n\\) to 1, with an average value \\(\\bar{h} = 2/n\\). Consequently, observations with \\(h_i\\) greater than \\(2 \\bar{h}\\) or \\(3 \\bar{h}\\) are commonly considered to be of high leverage.\nWith \\(p \\ge 2\\) predictors, an analogous relationship holds, but the correlations among the predictors must be taken into account. It is demonstrated below that \\(h_i \\propto D^2 (\\mathbf{x} - \\bar{\\mathbf{x}})\\), the Mahalanobis squared distance of \\(\\mathbf{x}\\) from the centroid \\(\\bar{\\mathbf{x}}\\)3.\nThe generalized version of Equation 6.1 is\n\\[\nh_i = \\frac{1}{n} + \\frac{1}{n-1} D^2 (\\mathbf{x} - \\bar{\\mathbf{x}}) \\; ,\n\\tag{6.2}\\]\nwhere \\(D^2 (\\mathbf{x} - \\bar{\\mathbf{x}}) = (\\mathbf{x} - \\bar{\\mathbf{x}})^\\mathsf{T} \\mathbf{S}_X^{-1} (\\mathbf{x} - \\bar{\\mathbf{x}})\\). From Section 3.2, it follows that contours of constant leverage correspond to data ellipses or ellipsoids of the predictors in \\(\\mathbf{x}\\), whose boundaries, assuming normality, correspond to quantiles of the \\(\\chi^2_p\\) distribution\nExample:\nTo illustrate Equation 6.2, I generate \\(N = 100\\) points from a bivariate normal distribution with means \\(\\mu = (30, 30)\\), variances = 10, and a correlation \\(\\rho = 0.7\\) and add two noteworthy points that show an apparently paradoxical result.\n\nset.seed(421)\nN &lt;- 100\nr &lt;- 0.7\nmu &lt;- c(30, 30)\ncov &lt;- matrix(c(10,   10*r,\n                10*r, 10), ncol=2)\n\nX &lt;- MASS::mvrnorm(N, mu, cov) |&gt; as.data.frame()\ncolnames(X) &lt;- c(\"x1\", \"x2\")\n\n# add 2 points\nX &lt;- rbind(X,\n           data.frame(x1 = c(28, 38),\n                      x2 = c(42, 35)))\n\nThe Mahalanobis squared distances of these points can be calculated using heplots::Mahalanobis(), and their corresponding hatvalues found using hatvalues() for any linear model using both x1 and x2.\n\nX &lt;- X |&gt;\n  mutate(Dsq = heplots::Mahalanobis(X)) |&gt;\n  mutate(y = 2*x1 + 3*x2 + rnorm(nrow(X), 0, 5),\n         hat = hatvalues(lm(y ~ x1 + x2))) \n\nPlotting x1 and x2 with data ellipses shows the relation of leverage to squared distance from the mean. The blue point looks to be farther from the mean, but the red point is actually very much further by Mahalanobis squared distance, which takes the correlation into account; it thus has much greater leverage.\n\ndataEllipse(X$x1, X$x2, \n            levels = c(0.40, 0.68, 0.95),\n            fill = TRUE, fill.alpha = 0.05,\n            col = \"darkgreen\",\n            xlab = \"X1\", ylab = \"X2\")\npoints(X[1:nrow(X) &gt; N, 1:2], pch = 16, \n       col=c(\"red\", \"blue\"), cex = 2)\nX |&gt; slice_tail(n = 2) |&gt;      # last two rows\n  points(pch = 16, col=c(\"red\", \"blue\"), cex = 2)\n\n\n\n\n\n\nFigure 6.23: Data ellipses for a bivariate normal sample with correlation 0.7, and two additional noteworthy points. The blue point looks to be farther from the mean, but the red point is actually more than 5 times further by Mahalanobis squared distance, and thus has much greater leverage.\n\n\n\n\nThe fact that hatvalues are proportional to leverage can be seen by plotting one against the other. I highlight the two noteworthy points in their colors from Figure 6.23 to illustrate how much greater leverage the red point has compared to the blue point.\n\nplot(hat ~ Dsq, data = X,\n     cex = c(rep(1, N), rep(2, 2)), \n     col = c(rep(\"black\", N), \"red\", \"blue\"),\n     pch = 16,\n     ylab = \"Hatvalue\",\n     xlab = \"Mahalanobis Dsq\")\n\n\n\n\n\n\nFigure 6.24: Hat values are proportional to squared Mahalanobis distances from the mean.\n\n\n\n\nLook back at these two points in Figure 6.23. Can you guess how much further the red point is from the mean than the blue point? You might be surprised that its’ \\(D^2\\) and leverage are about five times as great!\n\nX |&gt; slice_tail(n=2)\n#&gt;   x1 x2   Dsq   y    hat\n#&gt; 1 28 42 25.65 179 0.2638\n#&gt; 2 38 35  4.95 175 0.0588\n\n\n6.6.1.2 Outliers: Measuring residuals\nFrom the discussion in Section 6.6, outliers for the response \\(y\\) are those observations for which the residual \\(e_i = y_i - \\hat{y}_i\\) are unusually large in magnitude. However, as demonstrated in Figure 6.21, a high-leverage point will pull the fitted line towards it, reducing its’ residual and thus making them look less unusual.\nThe standard approach (Cook & Weisberg, 1982; Hoaglin & Welsch, 1978) is to consider a deleted residual \\(e_{(-i)}\\), conceptually as that obtained by re-fitting the model with observation \\(i\\) omitted and obtaining the fitted value \\(\\hat{y}_{(-i)}\\) from the remaining \\(n-1\\) observations, \\[\ne_{(-i)} = y_i - \\hat{y}_{(-i)} \\; .\n\\] The (externally) studentized residual is then obtained by dividing \\(e_{(-i)}\\) by it’s estimated standard error, giving \\[\ne^\\star_{(-i)} = \\frac{e_{(-i)}}{\\text{sd}(e_{(-i)})} = \\frac{e_i}{\\sqrt{\\text{MSE}_{(-i)}\\; (1 - h_i)}} \\; .\n\\]\nThis is just the ordinary residual \\(e_i\\) divided by a factor that increases with the residual variance but decreases with leverage. It can be shown that these studentized residuals follow a \\(t\\) distribution with \\(n - p -2\\) degrees of freedom, so a value \\(|e^\\star_{(-i)}| &gt; 2\\) can be considered large enough to pay attention to.\nIn practice for classical linear models, it is unnecessary to actually re-fit the model \\(n\\) times. Velleman & Welsh (1981) show that all these leave-one-out quantities can be calculated from the model fitted to the full data set and the hat (projection) matrix \\(\\mathbf{H} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\) from which \\(\\widehat{\\mathbf{b}} = \\mathbf{H} \\mathbf{y}\\).\n\n6.6.1.3 Measuring influence\nAs described at the start of this section, the actual influence of a given case depends multiplicatively on its’ leverage and residual. But how can we measure it?\nThe essential idea introduced above, is to delete the observations one at a time, each time refitting the regression model on the remaining \\(n–1\\) observations. Then, for observation \\(i\\) compare the results using all \\(n\\) observations to those with the \\(i^{th}\\) observation deleted to see how much influence the observation has on the analysis.\nThe simplest such measure, called DFFITS, compares the predicted value for case \\(i\\) with what would be obtained when that observation is excluded.\n\\[\\begin{aligned}\n\\text{DFFITS}_i & = & \\frac{\\hat{y}_i - \\hat{y}_{(-i)}}{\\sqrt{\\text{MSE}_{(-i)}\\; h_i}} \\\\\n   & = & e^\\star_{(-i)} \\times \\sqrt{\\frac{h_i}{1-h_i}} \\;\\; .\n\\end{aligned}\\]\nThe first equation gives the signed difference in fitted values in units of the standard deviation of that difference weighted by leverage; the second version (Belsley et al., 1980) represents that as a product of residual and leverage. A rule of thumb is that an observation is deemed to be influential if \\(| \\text{DFFITS}_i | &gt; 2 \\sqrt{(p+1) / n}\\).\nInfluence can also be assessed in terms of the change in the estimated coefficients \\(\\mathbf{b} = \\widehat{\\mathbf{\\beta}}\\) versus their values \\(\\mathbf{b}_{(-i)}\\) when case \\(i\\) is removed. Cook’s distance, \\(D_i\\), summarizes the size of the difference as a weighted sum of squares of the differences \\(\\mathbf{d} =\\mathbf{b} - \\mathbf{b}_{(-i)}\\) (Cook, 1977).\n\\[\nD_i = \\mathbf{d}^\\mathsf{T}\\, (\\mathbf{X}^\\mathsf{T}\\mathbf{X}) \\,\\mathbf{d} / (p+1) \\hat{\\sigma}^2\n\\] This can be re-expressed in terms of the components of residual and leverage\n\\[\nD_i = \\frac{e^{\\star 2}_{(-i)}}{p+1} \\times \\frac{h_i}{(1- h_i)}\n\\tag{6.3}\\]\nCook’s distance is in the metric of an \\(F\\) distribution with \\(p\\) and \\(n − p\\) degrees of freedom, so values \\(D_i &gt; 4/n\\) are considered large.\n\n6.6.2 Influence plots\nThe most common plot to detect influence is a bubble plot of the studentized residuals versus hat values, with the size (area) of the plotting symbol proportional to Cook’s \\(D\\). These plots are constructed using car::influencePlot() which also fills the bubble symbols with color whose opacity is proportional to Cook’s \\(D\\).\nThis is shown in Figure 6.25 for the demonstration dataset constructed in Section 6.6.1. In this plot, notable cutoffs for hatvalues at \\(2 \\bar{h}\\) and \\(3 \\bar{h}\\) are shown by dashed vertical lines and horizontal cutoffs for studentized residuals are shown at values of \\(\\pm 2\\).\nThe demonstration data of Section 6.6.1 has four copies of the same \\((x, y)\\) data, three of which have an unusual observation. The influence plot in Figure 6.25 subsets the data to give the \\(19 = 15 + 4\\) unique observations, including the three unusual cases. As can be seen, the high “Leverage” point has has less influence than the point labeled “Influence”, which has moderate leverage but a large absolute residual.\n\nSee the codeonce &lt;- both[c(1:16, 62, 63, 64),]      # unique observations\nonce.mod &lt;- lm(y ~ x, data=once)\ninf &lt;- influencePlot(once.mod, \n                     id = list(cex = 0.01),\n                     fill.alpha = 0.5,\n                     cex.lab = 1.5)\n# custom labels\nunusual &lt;- bind_cols(once[17:19,], inf) |&gt; \n  print(digits=3)\n#&gt; # A tibble: 3 × 7\n#&gt;   case            x     y id    StudRes    Hat CookD\n#&gt; * &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2 Outlier      52    65 O        3.11 0.0591 0.201\n#&gt; 2 3 Leverage     75    65 L        1.52 0.422  0.784\n#&gt; 3 4 Influence    70    40 OL      -4.93 0.262  1.82\nwith(unusual, {\n  casetype &lt;- gsub(\"\\\\d \", \"\", case)\n  text(Hat, StudRes, label = casetype,\n       pos = c(4, 2, 3), cex=1.5)\n})\n\n\n\n\n\n\nFigure 6.25: Influence plot for the demonstration data. The areas of the bubble symbols are proportional to Cook’s \\(D\\). The impact of the three unusual points on Cook’s \\(D\\) is clearly seen.\n\n\n\n\n\n6.6.3 Duncan data\nLet’s return to the Duncan data used as an example in Section 6.1.1 where a few points stood out as unusual in the basic diagnostic plots (Figure 6.2). The influence plot in Figure 6.26 helps to make sense of these noteworthy observations. The default method for identifying points in influencePlot() labels points with any of large studentized residuals, hat-values or Cook’s distances.\n\ninf &lt;- influencePlot(duncan.mod, id = list(n=3),\n                     cex.lab = 1.5)\n\n\n\n\n\n\nFigure 6.26: Influence plot for the model predicting occupational prestige in Duncan’s data. Cases with large studentized residuals, hat-values or Cook’s distances are labeled.\n\n\n\n\ninfluencePlot() returns (invisibly) the influence diagnostics for the cases identified in the plot. It is often useful to look at data values for these cases to understand why each of these was flagged.\n\nmerge(Duncan, inf, by=\"row.names\", all.x = FALSE) |&gt; \n  arrange(desc(CookD)) |&gt; \n  print(digits=3)\n#&gt;     Row.names type income education prestige StudRes    Hat  CookD\n#&gt; 1    minister prof     21        84       87   3.135 0.1731 0.5664\n#&gt; 2   conductor   wc     76        34       38  -1.704 0.1945 0.2236\n#&gt; 3    reporter   wc     67        87       52  -2.397 0.0544 0.0990\n#&gt; 4 RR.engineer   bc     81        28       67   0.809 0.2691 0.0810\n#&gt; 5  contractor prof     53        45       76   2.044 0.0433 0.0585\n\n\nminister has by far the largest influence, because it has an extremely positive residual and a large hat value. Looking at the data, we see that ministers have very low income, so their prestige is under-predicted. The large hat value reflects the fact that ministers have low income combined with very high education.\nconductor has the next largest Cook’s \\(D\\). It has a large hat value because its combination of relatively high income and low education is unusual in the data.\nAmong the others, reporter has a relatively large negative residual—its prestige is far less than the model predicts—but its low leverage make it not highly influential. railroad engineer has an extremely large hat value because its income is very high in relation to education. But this case is well-predicted and has a small residual, so its leverage is not large.\n\n6.6.4 Influence in added-variable plots\nThe properties of added-variable plots discussed in Section 6.4 make them also useful for understanding why cases are influential because they control for other predictors in each plot, and therefore show the partial contributions of each observation to hat values and residuals. As a consequence, we can see directly the how individual cases become individually or jointly influential.\nThe Duncan data provides a particularly instructive example of this. Figure 6.27 shows the AV plots for both income and education in the model duncan.mod, with some annotations added. I want to focus here on the joint influence of the occupations minister and conductor which were seen to be the most influential in Figure 6.26. The green vertical lines show their residuals in each panel and the red lines show the regressions when these two observations are deleted.\nThe basic AV plots are produced using the call to avPlots() below. To avoid clutter, I use the argument id = list(method = \"mahal\", n=3) so that only the three points with the greatest Mahalanobis distances from the centroid in each plot are labeled. These are the cases with the largest leverage seen in Figure 6.26.\n\navPlots(duncan.mod,\n  ellipse = list(levels = 0.68, fill = TRUE, fill.alpha = 0.1),\n  id = list(method = \"mahal\", n=3),\n  pch = 16, cex = 0.9,\n  cex.lab = 1.5)\n\n\n\n\n\n\n\n\nFigure 6.27: Added variable plots for the Duncan model, highlighting the impact of the observations for minister and conductor in each plot. The green lines show the residuals for these observations. The red line in each panel shows the regression line omitting these observations.\n\n\n\n\nThe two cases—minister and conductor—are the most highly influential, but as we can see in Figure 6.27 their influence combines because they are at opposite sides of the horizontal axis and their residuals are of opposite signs. They act together to decrease the slope for income and increase that for education.\n\nCode for income AV plotres &lt;- avPlot(duncan.mod, \"income\",\n              ellipse = list(levels = 0.68),\n              id = list(method = \"mahal\", n=3),\n              pch = 16,\n              cex.lab = 1.5) |&gt;\n  as.data.frame()\nfit &lt;- lm(prestige ~ income, data = res)\ninfo &lt;- cbind(res, fitted = fitted(fit), \n             resids = residuals(fit),\n             hat = hatvalues(fit),\n             cookd = cooks.distance(fit))\n\n# noteworthy points in this plot\nbig &lt;- which(info$cookd &gt; .20)\nwith(info, {\n  arrows(income[big], fitted[big], income[big], prestige[big], \n         angle = 12, length = .18, lwd = 2, col = \"darkgreen\")\n  })\n\n# line w/o the unusual points\nduncan.mod2 &lt;- update(duncan.mod, subset = -c(6, 16))\nbs &lt;- coef(duncan.mod2)[\"income\"]\nabline(a=0, b=bs, col = \"red\", lwd=2)\n\n\nDuncan’s hypothesis that the slopes for income and education were equal thus fails when these two observations are deleted. The slope for income then becomes 2.6 times that of education.\n\nduncan.mod2 &lt;- update(duncan.mod, subset = -c(6, 16))\ncoef(duncan.mod2)\n#&gt; (Intercept)      income   education \n#&gt;      -6.409       0.867       0.332",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#what-we-have-learned",
    "href": "06-linear_models-plots.html#what-we-have-learned",
    "title": "6  Plots for univariate response models",
    "section": "\n6.7 What We Have Learned",
    "text": "6.7 What We Have Learned\nThis chapter unveils the sophisticated visual toolkit that transforms regression models from black boxes into transparent interpretable analyses. Here are the essential insights that will revolutionize how you understand and communicate model results:\n\nThe regression quartet reveals model reality beyond the summary statistics: The four diagnostic plots (residuals vs. fitted, Q-Q plot, scale-location, and leverage plots) form a comprehensive health check for your regression models. Like a medical examination, each plot diagnoses different potential problems—linearity violations, non-normality, heteroscedasticity, and influential observations. The enhanced versions in R packages like car and performance don’t just show problems; they label the troublemakers and guide you toward solutions.\nCoefficient plots make model comparisons effortless and honest: Raw coefficient tables hide the forest for the trees, especially when predictors have wildly different scales (years vs. dollars vs. percentages). Standardized coefficient plots and meaningful rescaling transform incomprehensible tables into intuitive visual comparisons. When Duncan’s income coefficient (0.0013) appears invisible next to education’s coefficient (4.19), the plot reveals they’re actually comparable forces shaping occupational prestige.\nAdded-variable plots expose the truth behind “controlling for other variables”: These magical plots perform visual surgery, removing the effects of all other predictors to show the pure relationship between response and focal predictor. They reveal Simpson’s paradox in action–—how coffee can appear harmful marginally but beneficial conditionally when stress is controlled. The relationship between marginal and conditional ellipses tells the complete story of confounding and adjustment.\nEffect displays translate complex models into understandable stories: When interactions, transformations, and multiple predictors make coefficient interpretation impossible, effect displays come to the rescue. They show exactly how prestige changes with income for different occupation types, or how the effects of education varies across meaningful ranges, controlling for other variables These plots transform “significant at p &lt; 0.05” into “here’s exactly what this means in practice.”\nInfluence diagnostics separate the signal from the statistical noise: The leverage-influence framework reveals which observations affect your model the most versus which are merely unusual. Through Cook’s distance, hat values, and studentized residuals, we learned that minister and conductor in Duncan’s data weren’t just outliers—they were the puppet masters controlling his key findings about the equal effects of income and education. Remove them, and the entire conclusion changes.\n\nThe chapter’s profound message is that regression modeling without visualization is like navigating without a compass. In our R-powered world of complex models and large datasets, sophisticated plotting techniques aren’t just helpful—they’re essential for understanding what your models actually say about the world.\nPackage summary\n\n29 packages used here: bayestestR, car, carData, correlation, datawizard, dplyr, easystats, effects, effectsize, forcats, ggeffects, ggplot2, ggstats, insight, knitr, lubridate, marginaleffects, modelbased, modelsummary, parameters, performance, purrr, readr, report, see, stringr, tibble, tidyr, tidyverse\n\n\n\n\n\n\nArel-Bundock, V. (2025a). Marginaleffects: Predictions, comparisons, slopes, marginal means, and hypothesis tests. https://marginaleffects.com/\n\n\nArel-Bundock, V. (2025b). Modelsummary: Summary tables and plots for statistical models and data: Beautiful, customizable, and publication-ready. https://modelsummary.com\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley; Sons.\n\n\nCook, R. D. (1977). Detection of influential observation in linear regression. Technometrics, 19(1), 15–18. http://links.jstor.org/sici?sici=0040-1706%28197702%2919%3A1%3C15%3ADOIOIL%3E2.0.CO%3B2-8\n\n\nCook, R. D. (1993). Exploring partial residual plots. Technometrics, 35(4), 351–362.\n\n\nCook, R. D. (1996). Added-variable plots and curvature in linear regression. Technometrics, 38(3), 275–278. https://doi.org/10.1080/00401706.1996.10484507\n\n\nCook, R. D., & Weisberg, S. (1982). Residuals and influence in regression. Chapman; Hall.\n\n\nCook, R. D., & Weisberg, S. (1994). ARES plots for generalized linear models. Computational Statistics & Data Analysis, 17(3), 303–315. https://doi.org/10.1016/0167-9473(92)00075-3\n\n\nDuncan, O. D. (1961). A socioeconomic index for all occupations. In Jr. A. J. Reiss, P. K. H. O. D. Duncan, & C. C. North (Eds.), Occupations and social status. The Free Press.\n\n\nFisher, R. A. (1925). Statistical methods for research workers (6th ed.). Oliver & Boyd.\n\n\nFox, J. (1987). Effect displays for generalized linear models. In C. C. Clogg (Ed.), Sociological methodology, 1987 (pp. 347–361). Jossey-Bass.\n\n\nFox, J. (2003). Effect displays in R for generalized linear models. Journal of Statistical Software, 8(15), 1–27.\n\n\nFox, J. (2020). Regression diagnostics (2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878651\n\n\nFox, J., & Weisberg, S. (2018a). An R companion to applied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFox, J., & Weisberg, S. (2018b). Visualizing fit and lack of fit in complex regression models with predictor effect plots and partial residuals. Journal of Statistical Software, 87(9). https://doi.org/10.18637/jss.v087.i09\n\n\nFox, J., Weisberg, S., & Price, B. (2023). Car: Companion to applied regression. https://CRAN.R-project.org/package=car\n\n\nFox, J., Weisberg, S., Price, B., Friendly, M., & Hong, J. (2022). Effects: Effect displays for linear, generalized linear, and other models. https://www.r-project.org\n\n\nHoaglin, D. C., & Welsch, R. E. (1978). The hat matrix in regression and ANOVA. The American Statistician, 32(1), 17–22. https://doi.org/10.1080/00031305.1978.10479237\n\n\nKastellec, J. P., & Leoni, E. L. (2007). Using graphs instead of tables in political science. Perspectives on Politics, 5(04), 755–771. https://doi.org/10.1017/S1537592707072209\n\n\nLarmarange, J. (2025). Ggstats: Extension to ggplot2 for plotting stats. https://larmarange.github.io/ggstats/\n\n\nLarsen, W. A., & McCleary, S. J. (1972). The use of partial residual plots in regression analysis. Technometrics, 14, 781–790.\n\n\nLüdecke, D. (2025). Ggeffects: Create tidy data frames of marginal effects for ggplot from model outputs. https://strengejacke.github.io/ggeffects/\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., & Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., & Makowski, D. (2022). Easystats: Framework for easy statistical modeling, visualization, and reporting. In CRAN. https://easystats.github.io/easystats/\n\n\nPineo, P. O., & Porter, J. (1967). Occupational prestige in canada*. Canadian Review of Sociology, 4(1), 24–40. https://doi.org/https://doi.org/10.1111/j.1755-618X.1967.tb00472.x\n\n\nSearle, S. R., Speed, F. M., & Milliken, G. A. (1980). Population marginal means in the linear model: An alternative to least squares means. The American Statistician, 34(4), 216–221.\n\n\nVelleman, P. F., & Welsh, R. E. (1981). Efficient computing of regression diagnostics. The American Statistician, 35(4), 234–242.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "06-linear_models-plots.html#footnotes",
    "href": "06-linear_models-plots.html#footnotes",
    "title": "6  Plots for univariate response models",
    "section": "",
    "text": "Note that the factor type in the dataset has its levels ordered alphabetically. For analysis and graphing it is useful to reorder the levels in the natural increasing order. An alternative is to make type an ordered factor, but this would represent it using polynomial contrasts for linear and quadratic trends, which seems unuseful in this context.↩︎\nEarlier, but less general expression of these ideas go back to the use of adjusted means in analysis of covariance (Fisher, 1925) or least squares means or population marginal means in analysis of variance (Searle et al., 1980)↩︎\nSee this Stats StackExchange discussion for a proof.↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Plots for univariate response models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html",
    "href": "07-lin-mod-topics.html",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "7.1 Ellipsoids in data space and \\(\\mathbf{\\beta}\\) space\nThe geometric and graphical approach of earlier chapters has already introduced some new ideas for thinking about multivariate data, models for explaining them, and graphical methods for understanding their results. These can be applied to better understand common problems that arise in data analysis.\nPackages\nIn this chapter I use the following packages. Load them now:\nIt is most common to look at data and fitted models in “data space,” where axes correspond to variables, points represent observations, and fitted models are plotted as lines (or planes) in this space. As we’ve suggested, data ellipsoids provide informative summaries of relationships in data space. For linear models, particularly regression models with quantitative predictors, there is another space—“\\(\\mathbf{\\beta}\\) space”—that provides deeper views of models and the relationships among them. This discussion extends Friendly et al. (2013), Sec. 4.6.\nIn \\(\\mathbf{\\beta}\\) space, the axes pertain to coefficients, for example \\((\\beta_0, \\beta_1)\\) in a simple linear regression. Points in this space are models (true, hypothesized, fitted) whose coordinates represent values of these parameters. For example, one point \\(\\widehat{\\mathbf{\\beta}}_{\\text{OLS}} = (\\hat{\\beta}_0, \\hat{\\beta}_1)\\) represents the least squares estimate; other points, \\(\\widehat{\\mathbf{\\beta}}_{\\text{WLS}}\\) and \\(\\widehat{\\mathbf{\\beta}}_{\\text{ML}}\\) would give weighted least squares and maximum likelihood estimates, and the line \\(\\beta_1 = 0\\) represents the null hypothesis that the slope is zero.\nIn the sense described below, data space and \\(\\boldsymbol{\\beta}\\) space are each dual to the other. In simple linear regression, for example:\nFigure 7.1: Duality of \\((x, y)\\) lines in data space (left) and points in \\(\\beta\\)-space (right). Each line in data space corresponds to a point, whose intercept and slope are shown in \\(\\beta\\)-space.\nThis is illustrated in Figure 7.1. The left panel shows three lines in data space, which can be expressed as linear equations in \\(\\mathbf{z} = (x, y)\\) of the form \\(\\mathbf{A} \\mathbf{z} = \\mathbf{d}\\). matlib::showEqn(A, d) prints these as equations in \\(x\\) and \\(y\\).\nA &lt;- matrix(c( 1, 1, 0,\n              -1, 1, 1), 3, 2) \nd &lt;- c(2, 1/2, 1)\nshowEqn(A, d, vars = c(\"x\", \"y\"), simplify = TRUE)\n#&gt;   x - 1*y  =    2 \n#&gt;   x   + y  =  0.5 \n#&gt; 0*x   + y  =    1\nThe first equation, \\(x - y = 2\\) can be expressed as the line \\(y = x - 2\\) and corresponds to the point \\((\\beta_0, \\beta_1) = (-2, 1)\\) in \\(\\beta\\) space, and similarly for the other two equations. The second equation, \\(x + y = \\frac{1}{2}\\), or \\(y = 0.5 - x\\) intersects the first at the point \\((x, y) = (1.25, 0.75)\\); this corresponds to the line connecting \\((-2, 1)\\) and \\((0.5, -1)\\) in \\(\\beta\\) space.\nThis lovely duality is an example of an important principle in modern mathematics which translates concepts and structures from one perspective to another and back again. We get two views of the same thing, whose dual nature provides greater insight.\nWe have seen (Section 3.2) how ellipsoids in data space summarize variance (lack of precision) and correlation of our data. For the purpose of understanding linear models, ellipsoids in \\(\\beta\\) space do the same thing for the estimates of parameters. These ellipsoids are dual and inversely related to each other, a point first made clear by Dempster (1969, Ch. 6):\nIt is useful to understand the underlying geometry here connecting the ellipses for a matrix and its inverse. This can be seen in Figure 7.2, which shows an ellipse for a covariance matrix \\(\\mathbf{S}\\), whose axes, as we saw in Chapter 4 are the eigenvectors \\(\\mathbf{v}_i\\) of \\(\\mathbf{S}\\) and whose radii are the square roots \\(\\sqrt{\\lambda_i}\\) of the corresponding eigenvalues. The comparable ellipse for \\(2 \\mathbf{S}\\) has radii multiplied by \\(\\sqrt{2}\\).\nFigure 7.2: Geometric properties of an ellipse \\(\\mathbf{S}\\) and its inverse, \\(\\mathbf{S}^{-1}\\). The principal axes (dotted lines) are given by the eigenvectors, which are the same for \\(\\mathbf{S}\\) and \\(\\mathbf{S}^{-1}\\). Multiplying \\(\\mathbf{S}\\) by 2 makes it’s ellipse larger by \\(\\sqrt{2}\\), while the same factor makes the ellipse for \\((2 \\mathbf{S})^{-1}\\) smaller by the same factor.\nAs long as \\(\\mathbf{S}\\) is of full rank, the eigenvectors of \\(\\mathbf{S}^{-1}\\) are identical, while the eigenvalues are \\(1 / \\lambda_i\\), so the radii are the reciprocals \\(1 / \\sqrt{\\lambda_i}\\). The analogous ellipse for \\((2 \\mathbf{S}^{-1})\\) is smaller by a factor of \\(\\sqrt{2}\\).\nThus, in two dimensions, the ellipse for \\(\\mathbf{S}^{-1}\\) is a \\(90^o\\) rotation of that for \\(\\mathbf{S}\\). It is small in directions where the ellipse for \\(\\mathbf{S}\\) is large, and vice-versa. In our statistical applications, this translates as: parameter estimates in \\(\\beta\\) space are more precise (have less variance) in the directions where the data are more widely dispersed, having more information about the relationship.\nWe illustrate these ideas in the example below.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#sec-betaspace",
    "href": "07-lin-mod-topics.html#sec-betaspace",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "each line, like \\(\\mathbf{y} = \\beta_0 + \\beta_1 \\mathbf{x}\\) with intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) in data space corresponds to a point \\((\\beta_0,\\beta_1)\\) in \\(\\mathbf{\\beta}\\) space, and conversely;\nthe set of points on any line \\(\\beta_1 = x + y \\beta_0\\) in \\(\\mathbf{\\beta}\\) space corresponds to a set of lines through a given point \\((x, y)\\) in data space, and conversely;\nthe geometric proposition that every pair of points defines a line in one space corresponds to the proposition that every two lines intersect in a point in the other space.\n\n\n\n\n\n\n\n\nIn data space, joint confidence intervals for the mean vector or joint prediction regions for the data are given by the ellipsoids \\((\\bar{x}_1, \\bar{x}_2)^\\mathsf{T} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}}\\), where the covariance matrix \\(\\mathbf{S}_{\\mathbf{X}}\\) depends on \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) (\\(\\oplus\\) here shifts the ellipsoid to one centered at \\((\\bar{x}_1, \\bar{x}_2)\\) here, as in Equation 3.2).\nIn the dual \\(\\mathbf{\\beta}\\) space, joint confidence regions for the coefficients of a response variable \\(y\\) on \\((x_1, x_2)\\) are given by ellipsoids of the form \\(\\widehat{\\mathbf{\\beta}} \\oplus c \\sqrt{\\mathbf{S}_{\\mathbf{X}}^{-1}}\\), and depend on \\(\\mathbf{(\\mathbf{X}^\\mathsf{T}\\mathbf{X})}^{-1}\\).\n\n\n\n\n\n\n\n7.1.1 Coffee, stress and heart disease\nConsider the dataset coffee, giving measures of Heart (\\(y\\)), an index of cardiac damage, Coffee (\\(x_1\\)), a measure of daily coffee consumption, and Stress (\\(x_2\\)), a measure of occupational stress, in a contrived sample of \\(n=20\\) university people.1 For the sake of the example we assume that the main goal is to determine whether or not coffee is good or bad for your heart, and stress represents one potential confounding variable among others (age, smoking, etc.) that might be useful to control statistically.\n\nset.seed(1234)\ndata(coffee, package=\"matlib\")\ncoffee |&gt; dplyr::sample_n(6)\n#&gt;          Group Coffee Stress Heart\n#&gt; 1 Grad_Student    104    117    92\n#&gt; 2      Student     52     86    63\n#&gt; 3 Grad_Student     76     92    58\n#&gt; 4      Student    100    123    92\n#&gt; 5      Student     64     74    63\n#&gt; 6    Professor    141    175   145\n\nFigure 7.3 shows the scatterplot matrix, giving the marginal relations between all pairs of variables. The marginal message seems to be that coffee is bad for your heart, stress is bad for your heart and coffee consumption is also related to occupational stress.\n\nShow the codescatterplotMatrix(~ Heart + Coffee + Stress, data=coffee,\n    smooth = FALSE,\n    pch = 16, col = \"brown\",\n    cex.axis = 1.3, cex.labels = 3,\n    ellipse = list(levels = 0.68, fill.alpha = 0.1))\n\n\n\n\n\n\nFigure 7.3: Scatterplot matrix for the coffee data showing the pairwise relationships among Heart damage (\\(y\\)), Coffee consumption (\\(x_1\\)), and Stress (\\(x_2\\)), with linear regression lines and 68% data ellipses.\n\n\n\n\nYet, when we fit both variables together, we obtain the following results, suggesting that coffee is good for you—the coefficient for coffee is now negative, though non-significant. How can this be?\n\ncoffee.mod &lt;- lm(Heart ~ Coffee + Stress, data=coffee)\nbroom::tidy(coffee.mod)\n#&gt; # A tibble: 3 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)   -7.79      5.79      -1.35 0.196    \n#&gt; 2 Coffee        -0.409     0.292     -1.40 0.179    \n#&gt; 3 Stress         1.20      0.224      5.34 0.0000536\n\nThe answer is that the marginal plots of Heart vs. Coffee and Stress in the first row of Figure 7.3 each ignore the the other predictor. In contrast, the coefficients for coffee and stress in the multiple regression model coffee.mod are partial coefficients, giving the estimated change in heart damage for a unit change in each predictor, but adjusting for (controlling for, or holding constant) the other predictor.\nWe can see these effects directly in added variable plots (Section 6.4), but here I consider the relationship of coffee and stress in data space and beta space and how their ellipses relate to each other and to hypothesis tests.\nThe left panel in Figure 7.4 is the same as that in the (3,2) cell of Figure 7.3 for the relation Stress ~ Coffee but with data ellipses of 40% and 60% coverage. The shadows of the 40% ellipse on any axis give univariate intervals of the mean \\(\\bar{x} \\pm 1 s_x\\) (standard deviation) shown by the thick red lines; the shadow of the 68% ellipse corresponds to an interval \\(\\bar{x} \\pm 1.5 s_x\\).\nThe right panel shows the joint 95% confidence region for the coefficients \\((\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}})\\) and individual confidence intervals in \\(\\mathbf{\\beta}\\) space. These are determined as\n\\[\n\\widehat{\\mathbf{\\beta}} \\oplus \\sqrt{d F^{.95}_{d, \\nu}} \\times s_e \\times \\mathbf{S}_X^{-1/2} \\:\\: .\n\\] where \\(d\\) is the number of dimensions for which we want coverage, \\(\\nu\\) is the residual degrees of freedom for \\(s_e\\), and \\(\\mathbf{S}_X\\) is the covariance matrix of the predictors.\n\n\n\n\n\n\n\nFigure 7.4: Data space and \\(\\mathbf{\\beta}\\) space representations of Coffee and Stress. Left: 40% and 68% data ellipses. Right: Joint 95% confidence ellipse (blue) for (\\(\\beta_{\\text{Coffee}}, \\beta_{\\text{Stress}}\\)), confidence interval generating ellipse (red) with 95% univariate shadows. \\(H_0\\) marks the joint hypothesis that both coefficients equal zero.\n\n\n\n\nThus, the blue ellipse in Figure 7.4 (right) is the ellipse of joint 95% coverage, using the factor \\(\\sqrt{2 F^{.95}_{2, \\nu}}\\), which covers the true values of (\\(\\beta_{\\mathrm{Stress}}, \\beta_{\\mathrm{Coffee}}\\)) in 95% of samples. Moreover:\n\nAny joint hypothesis (e.g., \\(\\mathcal{H}_0:\\beta_{\\mathrm{Stress}}=0, \\beta_{\\mathrm{Coffee}}=0\\)) can be tested visually, simply by observing whether the hypothesized point, \\((0, 0)\\) here, lies inside or outside the joint confidence ellipse. That hypothesis is rejected\nThe shadows of this ellipse on the horizontal and vertical axes give Scheff'e joint 95% confidence intervals for the parameters, with protection for simultaneous inference (“fishing”) in a 2-dimensional space.\nSimilarly, using the factor \\(\\sqrt{F^{1-\\alpha/d}_{1, \\nu}} = t^{1-\\alpha/2d}_\\nu\\) would give an ellipse whose 1D shadows are \\(1-\\alpha\\) Bonferroni confidence intervals for \\(d\\) posterior hypotheses.\n\nVisual hypothesis tests and \\(d=1\\) confidence intervals for the parameters separately are obtained from the red ellipse in Figure 7.4, which is scaled by \\(\\sqrt{F^{.95}_{1, \\nu}} = t^{.975}_\\nu\\). We call this the confidence-interval generating ellipse (or, more compactly, the “confidence-interval ellipse”). The shadows of the confidence-interval ellipse on the axes (thick red lines) give the corresponding individual 95% confidence intervals, which are equivalent to the (partial, Type III) \\(t\\)-tests for each coefficient given in the standard multiple regression output shown above.\nThus, controlling for Stress, the confidence interval for the slope for Coffee includes 0, so we cannot reject the hypothesis that \\(\\beta_{\\mathrm{Coffee}}=0\\) in the multiple regression model, as we saw above in the numerical output. On the other hand, the interval for the slope for Stress excludes the origin, so we reject the null hypothesis that \\(\\beta_{\\mathrm{Stress}}=0\\), controlling for Coffee consumption.\nFinally, consider the relationship between the data ellipse and the confidence ellipse. These have exactly the same shape, but (with equal coordinate scaling of the axes), the confidence ellipse is exactly a \\(90^o\\) rotation and rescaling of the data ellipse. In directions in data space where the slice of the data ellipse is wide—where we have more information about the relationship between Coffee and Stress—the projection of the confidence ellipse is narrow, reflecting greater precision of the estimates of coefficients. Conversely, where slice of the the data ellipse is narrow (less information), the projection of the confidence ellipse is wide (less precision).\nConfidence ellipses are drawn using car::confidenceEllipse(). Click the button to show the code.\n\nCode for confidence ellipsesconfidenceEllipse(coffee.mod, \n    grid = FALSE,\n    xlim = c(-2, 1), ylim = c(-0.5, 2.5),\n    xlab = expression(paste(\"Coffee coefficient,  \", beta[\"Coffee\"])),\n    ylab = expression(paste(\"Stress coefficient,  \", beta[\"Stress\"])),\n    cex.lab = 1.5)\nconfidenceEllipse(coffee.mod, add=TRUE, draw = TRUE,\n    col = \"red\", fill = TRUE, fill.alpha = 0.1,\n    dfn = 1)\nabline(h = 0, v = 0, lwd = 2)\n\n# confidence intervals\nbeta &lt;- coef( coffee.mod )[-1]\nCI &lt;- confint(coffee.mod)\nlines( y = c(0,0), x = CI[\"Coffee\",] , lwd = 6, col = 'red')\nlines( x = c(0,0), y = CI[\"Stress\",] , lwd = 6, col = 'red')\npoints( diag( beta ), col = 'black', pch = 16, cex=1.8)\n\nabline(v = CI[\"Coffee\",], col = \"red\", lty = 2)\nabline(h = CI[\"Stress\",], col = \"red\", lty = 2)\n\ntext(-2.1, 2.35, \"Beta space\", cex=2, pos = 4)\narrows(beta[1], beta[2], beta[1], 0, angle=8, len=0.2)\narrows(beta[1], beta[2], 0, beta[2], angle=8, len=0.2)\n\ntext( -1.5, 1.85, \"df = 2\", col = 'blue', adj = 0, cex=1.2)\ntext( 0.2, .85, \"df = 1\", col = 'red', adj = 0, cex=1.2)\n\nheplots::mark.H0(col = \"darkgreen\", pch = \"+\", lty = 0, pos = 4, cex = 3)",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#measurement-error",
    "href": "07-lin-mod-topics.html#measurement-error",
    "title": "\n7  Topics in Linear Models\n",
    "section": "\n7.2 Measurement error",
    "text": "7.2 Measurement error\n\n7.2.1 OLS is BLUE\nIn classical linear models, the predictors are often considered to be fixed variables, or, if random, to be measured without error and independent of the regression errors. Either condition, along with the assumption of linearity, guarantees that the standard OLS estimators are unbiased. That is, in a simple linear regression, \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), the estimated slope \\(\\hat{\\beta}_1\\) wiil have an average, expected value \\(\\mathcal{E} (\\hat{\\beta}_1)\\) equal to the true population value \\(\\beta_1\\) over repeated samples.\nNot only this, but the Gauss-Markov theorem guarantees that the OLS estimator is also the most efficient because it has the least variance among all linear and unbiased estimators. The classical OLS estimator is said to be BLUE: Best (lowest variance), Linear (among linear estimators), Unbiased, Estimator.\n\n7.2.2 Errors in predictors\nErrors in the response \\(y\\) are accounted for in the model and measured by the mean squared error, \\(\\text{MSE} = \\hat{\\sigma}_\\epsilon^2\\). But in practice, of course, predictor variables are often also observed indicators, subject to their own error. Indeed, in the behavioral sciences it is rare that predictors are perfectly reliable and measured exactly. This fact that is recognized in errors-in-variables regression models (Fuller, 2006) and in more general structural equation models, but often ignored otherwise. Ellipsoids in data space and \\(\\beta\\) space are well suited to showing the effect of measurement error in predictors on OLS estimates.\nThe statistical facts are well known, though perhaps counter-intuitive in certain details: measurement error in a predictor biases regression coefficients (towards 0), while error in the measurement in \\(y\\) increases the MSE and thus standard errors of the regression coefficients but does not introduce bias in the coefficients.\n\n7.2.2.1 Example\nAn illuminating example can be constructed by starting with the simple linear regression \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\; ,\n\\] where \\(x_i\\) is the true, fully reliable predictor and \\(y\\) is the response, with error variance \\(\\sigma_\\epsilon^2\\). Now consider that we don’t measure \\(x_i\\) exactly, but instead observe \\(x^\\star_i\\). \\[\nx^\\star_i = x_i + \\eta_i \\; ,\n\\] where the measurement error \\(\\eta_i\\) is independent of the true \\(x_i\\) with variance \\(\\sigma^2_\\eta\\). We can extend this example to also consider the effect of adding additional, independent error variance to \\(y\\), so that instead of \\(y_i\\) we observe\n\\[\ny^\\star_i = y_i + \\nu_i\n\\] with variance \\(\\sigma^2_\\nu\\).\nLet’s simulate an example where the true relation is \\(y = 0.2 + 0.3 x\\) with error standard deviation \\(\\sigma = 0.5\\). I’ll take \\(x\\) to be uniformly distributed in [0, 10] and calculate \\(y\\) as normally distributed around that linear relation.\n\n\nset.seed(123)\nn &lt;- 300\n\na &lt;- 0.2    # true intercept\nb &lt;- 0.3    # true slope\nsigma &lt;- 0.5 # baseline error standard deviation\n\nx &lt;- runif(n, 0, 10)\ny &lt;- rnorm(n, a + b*x, sigma)\ndemo &lt;- data.frame(x,y)\n\nThen, generate alternative values \\(x^\\star\\) and \\(y^\\star\\) with additional error standard deviations around \\(x\\) given by \\(\\sigma_\\eta = 4\\) and around \\(y\\) given by \\(\\sigma_\\nu = 1\\).\n\nerr_y &lt;- 1   # additional error stdev for y\nerr_x &lt;- 4   # additional error stdev for x\ndemo  &lt;- demo |&gt;\n  mutate(y_star = rnorm(n, y, err_y),\n         x_star = rnorm(n, x, err_x))\n\nThere are four possible models we could fit and compare, using the combinations of \\((x, x^\\star)\\) and \\((y, y^\\star)\\)\n\nfit_1 &lt;- lm(y ~ x,           data = demo)   # no additional error\nfit_2 &lt;- lm(y_star ~ x,      data = demo)   # error in y\nfit_3 &lt;- lm(y ~ x_star,      data = demo)   # error in x\nfit_4 &lt;- lm(y_star ~ x_star, data = demo)   # error in x and y\n\nHowever, to show the differences visually, we can simply plot the data for each pair and show the regression lines (with confidence bands) and the data ellipses. To do this efficiently with ggplot2, it is necessary to transform the demo data to long format with columns x and y, distinguished by name for the four combinations.\n\n# make the demo dataset long, with names for the four conditions\ndf &lt;- bind_rows(\n  data.frame(x=demo$x,      y=demo$y,      name=\"No measurement error\"),\n  data.frame(x=demo$x,      y=demo$y_star, name=\"Measurement error on y\"),\n  data.frame(x=demo$x_star, y=demo$y,      name=\"Measurement error on x\"),\n  data.frame(x=demo$x_star, y=demo$y_star, name=\"Measurement error on x and y\")) |&gt;\n  mutate(name = fct_inorder(name)) \n\nThen, we can plot the data in df with points, regression lines and a data ellipse, faceting by name to give the measurement error quartet.\n\n\nggplot(df, aes(x, y)) +\n  geom_point(alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", \n               color = \"blue\",fill= \"blue\", \n               alpha=0.05, linewidth = 1.1) +\n  geom_smooth(method=\"lm\", formula = y~x, fullrange=TRUE, level=0.995,\n              color = \"red\", fill = \"red\", alpha = 0.2) +\n  facet_wrap(~name) \n\n\n\n\n\n\nFigure 7.5: The measurement error quartet: Each plot shows the linear regression of y on x, but where additional error variance has been added to y or x or both. The widths of the confidence bands and the vertical extent of the data ellipses show the effect on precision.\n\n\n\n\nComparing the plots in the first row, you can see that when additional error is added to \\(y\\), the regression slope remains essentially unchanged, illustrating that the estimate is unbiased. However, the confidence bounds on the regression line become wider, and the data ellipse becomes fatter in the \\(y\\) direction, illustrating the loss of precision.\nThe effect of error in \\(x\\) is less kind. Comparing the first row of plots with the second row, you can see that the estimated slope decreases when errors are added to \\(x\\). This is called attenuation bias, and it can be shown that \\[\n\\widehat{\\beta}_{x^\\star} \\longrightarrow \\frac{\\beta}{1+\\sigma^2_\\eta /\\sigma^2_x} \\; ,\n\\] where \\(\\beta\\) here refers to the regression slope and \\(\\longrightarrow\\) means “converges to”, as the sample size gets large. Thus, as \\(\\sigma^2_\\eta\\) increases, \\(\\widehat{\\beta}_{x^\\star}\\) becomes less than \\(\\beta\\).\nBeyond plots like Figure 7.5, we can see the effects of error in \\(x\\) or \\(y\\) on the model summary statistics such as the correlation \\(r_{xy}\\) or MSE by extracting these from the fitted models. This is easily done using dplyr::nest_by(name) and fitting the regression model to each subset, from which we can obtain the model statistics using sigma(), coef() and so forth. A bit of dplyr::mutate() magic is used to construct indicators errX and errY giving whether or not error was added to \\(x\\) and/or \\(y\\).\n\nmodel_stats &lt;- df |&gt;\n  dplyr::nest_by(name) |&gt;\n  mutate(model = list(lm(y ~ x, data = data)),\n         sigma = sigma(model),\n         intercept = coef(model)[1],\n         slope = coef(model)[2],\n         r = sqrt(summary(model)$r.squared)) |&gt;\n  mutate(errX = stringr::str_detect(name, \" x\"),\n         errY = stringr::str_detect(name, \" y\")) |&gt;\n  mutate(errX = factor(errX, levels = c(\"TRUE\", \"FALSE\")),\n         errY = factor(errY, levels = c(\"TRUE\", \"FALSE\"))) |&gt;\n  relocate(errX, errY, r, .after = name) |&gt;\n  select(-data) |&gt;\n  print()\n#&gt; # A tibble: 4 × 8\n#&gt; # Rowwise:  name\n#&gt;   name                errX  errY      r model sigma intercept  slope\n#&gt;   &lt;fct&gt;               &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lis&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 No measurement err… FALSE FALSE 0.858 &lt;lm&gt;  0.495    0.244  0.294 \n#&gt; 2 Measurement error … FALSE TRUE  0.648 &lt;lm&gt;  1.09     0.0838 0.329 \n#&gt; 3 Measurement error … TRUE  FALSE 0.481 &lt;lm&gt;  0.844    1.22   0.0946\n#&gt; 4 Measurement error … TRUE  TRUE  0.401 &lt;lm&gt;  1.31     1.12   0.117\n\nWe plot the model \\(R = r_{xy}\\) and the estimated residual standard error in Figure 7.6 below. The lines connecting the points are approximately parallel, indicating that errors of measurement in \\(x\\) and \\(y\\) have nearly additive effects on model summaries.\n\n\np1 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = r, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model R \",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") +\n  legend_inside(c(0.25, 0.8))\n\np2 &lt;- ggplot(data=model_stats, \n             aes(x = errX, y = sigma, \n                 group = errY, color = errY, \n                 shape = errY, linetype = errY)) +\n  geom_point(size = 4) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Error on X?\",\n       y = \"Model residual standard error\",\n       color = \"Error on Y?\",\n       shape = \"Error on Y?\",\n       linetype = \"Error on Y?\") \n\np1 + p2\n\n\n\n\n\n\nFigure 7.6: Model statistics for the combinations of additional error variance in x or y or both. Left: model R; right: Residual standard error.\n\n\n\n\n\n7.2.3 Coffee data: \\(\\beta\\) space\nIn multiple regression the effects of measurement error in a predictor become more complex, because error variance in one predictor, \\(x_1\\), say, can affect the coefficients of other terms in the model.\nConsider the marginal relation between Heart disease and Stress in the coffee data. Figure 7.7 shows this with data ellipses in data space and the corresponding confidence ellipses in \\(\\beta\\) space. Each panel starts with the observed data (the darkest ellipse, marked \\(0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0.75, 1.0, 1.5\\}\\), to the value of Stress, while keeping the mean of Stress the same. All of the data ellipses have the same vertical shadows (\\(\\text{SD}_{\\textrm{Heart}}\\)), while the horizontal shadows increase with \\(\\delta\\), driving the slope for Stress toward 0.\nIn \\(\\beta\\) space, it can be seen that the estimated coefficients, \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) vary along a line and approach \\(\\beta_{\\textrm{Stress}}=0\\) as \\(\\delta\\) gets sufficiently large. The shadows of ellipses for \\((\\beta_0, \\beta_{\\textrm{Stress}})\\) along the \\(\\beta_{\\textrm{Stress}}\\) axis also demonstrate the effects of measurement error on the standard error of \\(\\beta_{\\textrm{Stress}}\\).\n\n\n\n\n\n\n\nFigure 7.7: Effects of measurement error in Stress on the marginal relationship between Heart disease and Stress. Each panel starts with the observed data (\\(\\delta = 0\\)), then adds random normal error, \\(\\mathcal{N}(0, \\delta \\times \\text{SD}_\\text{Stress})\\) with standard deviations multiplied by \\(\\delta\\) = 0.75, 1.0, 1.5, to the value of Stress. Increasing measurement error biases the slope for Stress toward 0. Left: 50% data ellipses; right: 50% confidence ellipses.\n\n\n\n\nPerhaps less well-known, but both more surprising and interesting, is the effect that measurement error in one variable, \\(x_1\\), has on the estimate of the coefficient for an other variable, \\(x_2\\), in a multiple regression model. Figure 7.8 shows the confidence ellipses for \\((\\beta_{\\textrm{Coffee}}, \\beta_{\\textrm{Stress}})\\) in the multiple regression predicting Heart disease, adding random normal error \\(\\mathcal{N}(0, \\delta \\times \\mathrm{SD}_{Stress})\\), with \\(\\delta = \\{0, 0.2, 0.4, 0.8\\}\\), to the value of Stress alone.\nAs can be plainly seen, while this measurement error in Stress attenuates its coefficient, it also has the effect of biasing the coefficient for Coffee toward that in the marginal regression of Heart disease on Coffee alone.\n\n\n\n\n\n\n\nFigure 7.8: Biasing effect of measurement error in one variable (Stress) on on the coefficient of another variable (Coffee) in a multiple regression. The coefficient for Coffee is driven towards its value in the marginal model using Coffee alone, as measurement error in Stress makes it less informative in the joint model.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#what-we-have-learned",
    "href": "07-lin-mod-topics.html#what-we-have-learned",
    "title": "\n7  Topics in Linear Models\n",
    "section": "\n7.3 What We Have Learned",
    "text": "7.3 What We Have Learned\n\nData space and \\(\\beta\\) space are dualities of each other - While we typically visualize regression models in data space (where points are observations), there’s a parallel \\(\\beta\\) space where points represent models and their coefficients. These spaces mirror each other in elegant ways: lines in one space become points in the other, and confidence ellipses in \\(\\beta\\) space are 90\\(^\\circ\\) rotations of data ellipses. This duality reveals that we gain precision in estimating coefficients precisely where our data spread the most.\nConfidence ellipses make hypothesis testing visual and intuitive - Instead of squinting at p-values in regression output, we can literally see whether hypotheses are supported by checking whether null hypothesis points fall inside or outside confidence ellipses. The shadows of these ellipses automatically give us individual confidence intervals, while the full ellipse captures joint uncertainty about multiple coefficients.\nMeasurement error in predictors is far more dangerous than measurement error in responses - While errors in your response variable (y) simply inflate standard errors without biasing coefficients, errors in predictors create attenuation bias that systematically pulls slope estimates toward zero. This “errors-in-variables” problem means that unreliable measurements of your predictors can make real effects appear weaker than they actually are.\nIn multiple regression, measurement error in one predictor contaminates estimates of other predictors - Perhaps most surprisingly, when one predictor in your model suffers from measurement error, it doesn’t just bias its own coefficient—it also distorts the coefficients of other variables in unpredictable ways. This distortion will cascade, meaning that measurement quality affects your entire model, not just individual variables.\nEllipses reveal the hidden geometry behind familiar statistical concepts - Data ellipses, confidence ellipses, and their mathematical relationships provide a geometric foundation for understanding correlation, regression coefficients, confidence intervals, and hypothesis tests. This visual approach transforms abstract statistical concepts into concrete geometric relationships that you can literally see and manipulate.\n\n\nPackages used here:\n10 packages used here: car, carData, dplyr, forcats, gganimate, ggplot2, knitr, matlib, patchwork, tidyr\n\n\n\n\n\nDempster, A. P. (1969). Elements of continuous multivariate analysis. Addison-Wesley.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFuller, W. (2006). Measurement error models (2nd ed.). John Wiley & Sons.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "07-lin-mod-topics.html#footnotes",
    "href": "07-lin-mod-topics.html#footnotes",
    "title": "\n7  Topics in Linear Models\n",
    "section": "",
    "text": "This example was developed by Georges Monette.↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topics in Linear Models</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html",
    "href": "08-collinearity-ridge.html",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "8.1 What is collinearity?\nIn univariate multiple regression models, we usually hope to have high correlations between the outcome \\(y\\) and each of the predictors, \\(\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x_2}, \\dots]\\), but high correlations among the predictors can cause problems in estimating and testing their effects. Exactly the same problems can exist in multivariate response models, because they involve only the relations among the predictor variables.\nThe problem of high correlations among the predictors in a model is called collinearity (or multicollinearity), referring to the situation when two or more predictors are very nearly linearly related to each other (collinear). This chapter illustrates the nature of collinearity geometrically, using data and confidence ellipsoids. It describes diagnostic measures to asses these effects, and presents some novel visual tools for these purposes using the VisCollin package.\nOne class of solutions for collinearity involves regularization methods such as ridge regression. Another collection of graphical methods, generalized ridge trace plots, implemented in the genridge package, sheds further light on what is accomplished by this technique. More generally, the methods of this chapter are further examples of how data and confidence ellipsoids can be used to visualize bias and precision of regression estimates.\nPackages\nIn this chapter we use the following packages. Load them now.\nResearchers who have studies standard treatments of linear models (e.g, Graybill (1961); Hocking (2013)) are often confused about what collinearity is, how to find its sources and how to take steps to resolve them. There are a number of important diagnostic measures that can help, but these are usually presented in a tabular display like Figure 8.1, which prompted this querry on an online forum:\nFigure 8.1: Collinearity diagnostics for a multiple regression model from SPSS. Source: Arndt Regorz, How to interpret a Collinearity Diagnostics table in SPSS, https://bit.ly/3YRB82b\nThe trouble with displays like Figure 8.1 is that the important information is hidden in a sea of numbers, some of which are bad when large, others bad when they are small and a large bunch which are irrelevant. In Friendly & Kwan (2009), we liken this problem to that of the reader of Martin Hansford’s successful series of books, Where’s Waldo. These consist of a series of full-page illustrations of hundreds of people and things and a few Waldos— a character wearing a red and white striped shirt and hat, glasses, and carrying a walking stick or other paraphernalia. Waldo was never disguised, yet the complex arrangement of misleading visual cues in the pictures made him very hard to find. Collinearity diagnostics often provide a similar puzzle: where should you look in traditional tabular displays?\nFigure 8.2: A scene from one of the Where’s Waldo books. Waldo wears a red-striped shirt, but far too many of the other figures in the scene have horizontal red stripes, making it very difficult to find him among all the distractors. This is often the problem with collinearity diagnostics. Source: Modified from https://bit.ly/48KPcOo\nRecall the standard classical linear model for a response variable \\(y\\) with a collection of predictors in \\(\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_p)\\)\n\\[\\begin{aligned}\n\\mathbf{y}  & =  \\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\cdots + \\beta_p \\mathbf{x}_p + \\boldsymbol{\\epsilon} \\\\\n            & =  \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\; ,\n\\end{aligned}\\]\nfor which the ordinary least squares solution is:\n\\[\n\\widehat{\\mathbf{b}} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\; \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\; .\n\\] The sampling variances and covariances of the estimated coefficients is \\(\\text{Var} (\\widehat{\\mathbf{b}}) = \\sigma_\\epsilon^2 \\times (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1}\\) and \\(\\sigma_\\epsilon^2\\) is the variance of the residuals \\(\\mathbf{\\epsilon}\\), estimated by the mean squared error (MSE).\nIn the limiting case, collinearity becomes particularly problematic when one \\(x_i\\) is perfectly predictable from the other \\(x\\)s, i.e., \\(R^2 (x_i | \\text{other }x) = 1\\). This is problematic because:\nThis extreme case reflects a situation when one or more predictors are effectively redundant, for example when you include two variables \\(x\\) and \\(y\\) and their sum \\(z = x + y\\) in a model. For instance, a dataset may include variables for income, expenses, and savings. But income is the sum of expenses and savings, so not all three should be used as predictors.\nA more subtle case is the use ipsatized, defined as scores that sum to a constant, such as proportions of a total. You might have scores on tests of reading, math, spelling and geography. With ipsatized scores, any one of these is necessarily 1 \\(-\\) sum of the others, i.e., if reading is 0.5, math and geography are both 0.15, then geography must be 0.2. Once thre of the four scores are known, the last provides no new information.\nMore generally, collinearity refers to the case when there are very high multiple correlations among the predictors, such as \\(R^2 (x_i | \\text{other }x) \\ge 0.9\\). Note that you can’t tell simply by looking at the simple correlations. A large correlation \\(r_{ij}\\) is sufficient for collinearity, but not necessary—you can have variables \\(x_1, x_2, x_3\\) for which the pairwise correlation are low, but the multiple correlation is high.\nThe consequences are:",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#what-is-collinearity",
    "href": "08-collinearity-ridge.html#what-is-collinearity",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "Some of my collinearity diagnostics have large values, or small values, or whatever they are not supposed to be\n\nWhat is bad?\nIf bad, what can I do about it?\n\n\n\n\n\n\n\n\n\n\n\n\nthere is no unique solution for the regression coefficients \\(\\mathbf{b} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X} \\mathbf{y}\\);\nthe standard errors \\(s (b_i)\\) of the estimated coefficients are infinite and t statistics \\(t_i = b_i / s (b_i)\\) are 0.\n\n\n\n\n\n\nThe estimated coefficients have large standard errors, \\(s(\\hat{b}_j)\\). They are multiplied by the square root of the variance inflation factor, \\(\\sqrt{\\text{VIF}}\\), discussed below.\nThe large standard errors deflate the \\(t\\)-statistics, \\(t = \\hat{b}_j / s(\\hat{b}_j)\\), by the same factor, so a coefficient that would significant if the predictors were uncorrelated becomes insignificant when collinearity is present.\nThus you may find a situation where an overall model is highly significant (large \\(F\\)-statistic), while no (or few) of the individual predictors are. This is a puzzlement!\nBeyond this, the least squares solution may have poor numerical accuracy (Longley, 1967), because the solution depends inversely on the determinant \\(|\\,\\mathbf{X}^\\mathsf{T} \\mathbf{X}\\,|\\), which approaches 0 as multiple correlations increase.\nThere is an interpretive problem as well. Recall that the coefficients \\(\\hat{b}\\) are partial coefficients, meaning that they estimate change \\(\\Delta y\\) in \\(y\\) when \\(x\\) changes by one unit \\(\\Delta x\\), but holding all other variables constant. Then, the model may be trying to estimate something that does not occur in the data. (For example: predicting strength from the highly correlated height and weight)\n\n\n8.1.1 Visualizing collinearity\nCollinearity can be illustrated in data space for two predictors in terms of the stability of the regression plane for a linear model Y = X1 + X2. Figure 8.3 (adapted from Fox (2016), Fig. 13.2) shows three cases as 3D plots of \\((X_1, X_2, Y)\\), where the correlation of predictors can be observed in the \\((X_1, X_2)\\) plane.\n\nshows a case where \\(X_1\\) and \\(X_2\\) are uncorrelated as can be seen in their scatter in the horizontal plane (+ symbols). The gray regression plane is well-supported; a small change in Y for one observation won’t make much difference.\nIn panel (b), \\(X_1\\) and \\(X_2\\) have a perfect correlation, \\(r (x_1, x_2) = 1.0\\). The regression plane is not unique; in fact there are an infinite number of planes that fit the data equally well. Note that, if all we care about is prediction (not the coefficients), we could use \\(X_1\\) or \\(X_2\\), or both, or any weighted sum of them in a model and get the same predicted values.\nShows a typical case where there is a strong correlation between \\(X_1\\) and \\(X_2\\). The regression plane here is unique, but is not well determined. A small change in Y can make quite a difference in the fitted value or coefficients, depending on the values of \\(X_1\\) and \\(X_2\\). Where \\(X_1\\) and \\(X_2\\) are far from their near linear relation in the botom plane, you can imagine that it is easy to tilt the plane substantially by a small change in \\(Y\\).\n\n\n\n\n\n\n\n\nFigure 8.3: Effect of collinearity on the least squares regression plane. (a) Small correlation between predictors; (b) Perfect correlation ; (c) Very strong correlation. The black points show the data Y values, white points are the fitted values in the regression plane, and + signs represent the values of X1 and X2. Source: Adapted from Fox (2016), Fig. 13.2\n\n\n\n\n\n8.1.2 Data space and \\(\\beta\\) space\nIt is also useful to visualize collinearity by comparing the representation in data space with the analogous view of the confidence ellipses for coefficients in beta space. To do so in this example, I generate data from a known model \\(y = 3 x_1 + 3 x_2 + \\epsilon\\) with \\(\\epsilon \\sim \\mathcal{N} (0, 100)\\) and various true correlations between \\(x_1\\) and \\(x_2\\), \\(\\rho_{12} = (0, 0.8, 0.97)\\) 1.\n\n\nR file: R/collin-data-beta.R\nFirst, I use MASS:mvrnorm() to construct a list of three data frames XY with the same means and standard deviations, but with different correlations. In each case, the variable \\(y\\) is generated with true coefficients beta \\(=(3, 3)\\), and the fitted model for that value of rho is added to a corresponding list of models, mods.\n\nCodelibrary(MASS)\nlibrary(car)\n\nset.seed(421)            # reproducibility\nN &lt;- 200                 # sample size\nmu &lt;- c(0, 0)            # means\ns &lt;- c(1, 1)             # standard deviations\nrho &lt;- c(0, 0.8, 0.97)   # correlations\nbeta &lt;- c(3, 3)          # true coefficients\n\n# Specify a covariance matrix, with standard deviations\n#   s[1], s[2] and correlation r\nCov &lt;- function(s, r){\n  matrix(c(s[1],        r * s[1]*s[2],\n         r * s[1]*s[2], s[2]), nrow = 2, ncol = 2)\n}\n\n# Generate a dataframe of X, y for each rho\n# Fit the model for each\nXY &lt;- vector(mode =\"list\", length = length(rho))\nmods &lt;- vector(mode =\"list\", length = length(rho))\nfor (i in seq_along(rho)) {\n  r &lt;- rho[i]\n  X &lt;- mvrnorm(N, mu, Sigma = Cov(s, r))\n  colnames(X) &lt;- c(\"x1\", \"x2\")\n  y &lt;- beta[1] * X[,1] + beta[2] * X[,2] + rnorm(N, 0, 10)\n\n  XY[[i]] &lt;- data.frame(X, y=y)\n  mods[[i]] &lt;- lm(y ~ x1 + x2, data=XY[[i]])\n}\n\n\nThe estimated coefficients can then be extracted using coef() applied to each model:\n\ncoefs &lt;- sapply(mods, coef)\ncolnames(coefs) &lt;- paste0(\"mod\", 1:3, \" (rho=\", rho, \")\")\ncoefs\n#&gt;             mod1 (rho=0) mod2 (rho=0.8) mod3 (rho=0.97)\n#&gt; (Intercept)         1.01        -0.0535           0.141\n#&gt; x1                  3.18         3.4719           3.053\n#&gt; x2                  1.68         2.9734           2.059\n\nThen, I define a function to plot the data ellipse (car::dataEllipse()) for each data frame and confidence ellipse (car::confidenceEllipse()) for the coefficients in the corresponding fitted model. In the plots in Figure 8.4, I specify the x, y limits for each plot so that the relative sizes of these ellipses are comparable, so that variance inflation can be assessed visually.\n\nCodedo_plots &lt;- function(XY, mod, r) {\n  X &lt;- as.matrix(XY[, 1:2])\n  dataEllipse(X,\n              levels= 0.95,\n              col = \"darkgreen\",\n              fill = TRUE, fill.alpha = 0.05,\n              xlim = c(-3, 3),\n              ylim = c(-3, 3), asp = 1)\n  text(0, 3, bquote(rho == .(r)), cex = 2, pos = NULL)\n\n  confidenceEllipse(mod,\n                    col = \"red\",\n                    fill = TRUE, fill.alpha = 0.1,\n                    xlab = expression(paste(\"x1 coefficient, \", beta[1])),\n                    ylab = expression(paste(\"x2 coefficient, \", beta[2])),\n                    xlim = c(-5, 10),\n                    ylim = c(-5, 10),\n                    asp = 1)\n  points(beta[1], beta[2], pch = \"+\", cex=2)\n  abline(v=0, h=0, lwd=2)\n}\n\nop &lt;- par(mar = c(4,4,1,1)+0.1,\n          mfcol = c(2, 3),\n          cex.lab = 1.5)\nfor (i in seq_along(rho)) {\n  do_plots(XY[[i]], mods[[i]], rho[i])\n}\npar(op)\n\n\n\n\n\n\nFigure 8.4: 95% Data ellipses for x1, x2 and the corresponding 95% confidence ellipses for their coefficients in the model predicting y. In the confidence ellipse plots, reference lines show the value (0,0) for the null hypothesis and “+” marks the true values for the coefficients. This figure adapts an example by John Fox (2022).\n\n\n\n\nRecall (Section 7.1) that the confidence ellipse for \\((\\beta_1, \\beta_2)\\) is just a 90 degree rotation (and rescaling) of the data ellipse for \\((x_1, x_2)\\): it is wide (more variance) in any direction where the data ellipse is narrow.\nThe shadows of the confidence ellipses on the coordinate axes in Figure 8.4 represent the standard errors of the coefficients, and get larger with increasing \\(\\rho\\). This is the effect of variance inflation, described in the following section.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-measure-collin",
    "href": "08-collinearity-ridge.html#sec-measure-collin",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.2 Measuring collinearity",
    "text": "8.2 Measuring collinearity\nThis section first describes the variance inflation factor (VIF) used to measure the effect of possible collinearity on each predictor and a collection of diagnostic measures designed to help interpret these. Then I describe some novel graphical methods to make these effects more readily understandable, to answer the “Where’s Waldo” question posed at the outset.\n\n8.2.1 Variance inflation factors\nHow can we measure the effect of collinearity? The essential idea is to compare, for each predictor the variance \\(s^2 (\\widehat{b_j})\\) that the coefficient that \\(x_j\\) would have if it was totally unrelated to the other predictors to the actual variance it has in the given model.\nFor two predictors such as shown in Figure 8.4 the sampling variance of \\(x_1\\) can be expressed as\n\\[\ns^2 (\\widehat{b_1}) = \\frac{MSE}{(n-1) \\; s^2(x_1)} \\; \\times \\; \\left[ \\frac{1}{1-r^2_{12}} \\right]\n\\] The first term here is the variance of \\(b_1\\) when the two predictors are uncorrelated. The term in brackets represents the variance inflation factor (Marquardt, 1970), the amount by which the variance of the coefficient is multiplied as a consequence of the correlation \\(r_{12}\\) of the predictors. As \\(r_{12} \\rightarrow 1\\), the variances approaches infinity.\nMore generally, with any number of predictors, this relation has a similar form, replacing the simple correlation \\(r_{12}\\) with the multiple correlation predicting \\(x_j\\) from all others,\n\\[\ns^2 (\\widehat{b_j}) = \\frac{MSE}{(n-1) \\; s^2(x_j)} \\; \\times \\; \\left[ \\frac{1}{1-R^2_{j | \\text{others}}} \\right]\n\\] So, we have that the variance inflation factors are:\n\\[\n\\text{VIF}_j = \\frac{1}{1-R^2_{j \\,|\\, \\text{others}}}\n\\] In practice, it is often easier to think in terms of the square root, \\(\\sqrt{\\text{VIF}_j}\\) as the multiplier of the standard errors. The denominator, \\(1-R^2_{j | \\text{others}}\\) is sometimes called tolerance, a term I don’t find particularly useful, but it is just the proportion of the variance of \\(x_j\\) that is not explainable from the others.\nFor the cases shown in Figure 8.4 the VIFs and their square roots are:\n\nvifs &lt;- sapply(mods, car::vif)\ncolnames(vifs) &lt;- paste(\"rho:\", rho)\nvifs\n#&gt;    rho: 0 rho: 0.8 rho: 0.97\n#&gt; x1      1     3.09      18.6\n#&gt; x2      1     3.09      18.6\n\nsqrt(vifs)\n#&gt;    rho: 0 rho: 0.8 rho: 0.97\n#&gt; x1      1     1.76      4.31\n#&gt; x2      1     1.76      4.31\n\nNote that when there are terms in the model with more than one degree of freedom, such as education with four levels (and hence 3 df) or a polynomial term specified as poly(age, 3), that variable, education or age is represented by three separate \\(x\\)s in the model matrix, and the standard VIF calculation gives results that vary with how those terms are coded in the model.\nTo allow for these cases, Fox & Monette (1992) define generalized, GVIFs as the inflation in the squared area of the confidence ellipse for the coefficients of such terms, relative to what would be obtained with uncorrelated data. Visually, this can be seen by comparing the areas of the ellipses in the bottom row of Figure 8.4. Because the magnitude of the GVIF increases with the number of degrees of freedom for the set of parameters, Fox & Monette suggest the analog \\(\\sqrt{\\text{GVIF}^{1/2 \\text{df}}}\\) as the measure of impact on standard errors. This is what car::vif() calculates for a factor or other term with more than 1 df.\nExample: This example uses the cars dataset in the VisCollin package containing various measures of size and performance on 406 models of automobiles from 1982. Interest is focused on predicting gas mileage, mpg.\n\ndata(cars, package = \"VisCollin\")\nstr(cars)\n#&gt; 'data.frame':  406 obs. of  10 variables:\n#&gt;  $ make    : Factor w/ 30 levels \"amc\",\"audi\",\"bmw\",..: 6 4 22 1 12 12 6 22 23 1 ...\n#&gt;  $ model   : chr  \"chevelle\" \"skylark\" \"satellite\" \"rebel\" ...\n#&gt;  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...\n#&gt;  $ cylinder: int  8 8 8 8 8 8 8 8 8 8 ...\n#&gt;  $ engine  : num  307 350 318 304 302 429 454 440 455 390 ...\n#&gt;  $ horse   : int  130 165 150 150 140 198 220 215 225 190 ...\n#&gt;  $ weight  : int  3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ...\n#&gt;  $ accel   : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n#&gt;  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...\n#&gt;  $ origin  : Factor w/ 3 levels \"Amer\",\"Eur\",\"Japan\": 1 1 1 1 1 1 1 1 1 1 ...\n\nWe fit a model predicting gas mileage (mpg) from the number of cylinders, engine displacement, horsepower, weight, time to accelerate from 0 – 60 mph and model year (1970–1982). Perhaps surprisingly, only weight and year appear to significantly predict gas mileage. What’s going on here?\n\ncars.mod &lt;- lm (mpg ~ cylinder + engine + horse + \n                      weight + accel + year, \n                data=cars)\nAnova(cars.mod)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: mpg\n#&gt;           Sum Sq  Df F value Pr(&gt;F)    \n#&gt; cylinder      12   1    0.99   0.32    \n#&gt; engine        13   1    1.09   0.30    \n#&gt; horse          0   1    0.00   0.98    \n#&gt; weight      1214   1  102.84 &lt;2e-16 ***\n#&gt; accel          8   1    0.70   0.40    \n#&gt; year        2419   1  204.99 &lt;2e-16 ***\n#&gt; Residuals   4543 385                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe check the variance inflation factors, using car::vif(). We see that most predictors have very high VIFs, indicating moderately severe multicollinearity.\n\nvif(cars.mod)\n#&gt; cylinder   engine    horse   weight    accel     year \n#&gt;    10.63    19.64     9.40    10.73     2.63     1.24\n\nsqrt(vif(cars.mod))\n#&gt; cylinder   engine    horse   weight    accel     year \n#&gt;     3.26     4.43     3.07     3.28     1.62     1.12\n\nAccording to \\(\\sqrt{\\text{VIF}}\\), the standard error of cylinder has been multiplied by \\(\\sqrt{10.63} = 3.26\\) and it’s \\(t\\)-value is divided by this number, compared with the case when all predictors are uncorrelated. engine, horse and weight suffer a similar fate.\nIf we also included the factor origin in the models, we would get the generalized GVIF:\n\ncars.mod2 &lt;- lm (mpg ~ cylinder + engine + horse + \n                       weight + accel + year + origin, \n                 data=cars)\nvif(cars.mod2)\n#&gt;           GVIF Df GVIF^(1/(2*Df))\n#&gt; cylinder 10.74  1            3.28\n#&gt; engine   22.94  1            4.79\n#&gt; horse     9.96  1            3.16\n#&gt; weight   11.07  1            3.33\n#&gt; accel     2.63  1            1.62\n#&gt; year      1.30  1            1.14\n#&gt; origin    2.10  2            1.20\n\n\n\n\n\n\n\nConnection with inverse of correlation matrix\n\n\n\nIn the linear regression model with standardized predictors, the covariance matrix of the estimated intercept-excluding parameter vector \\(\\mathbf{b}^\\star\\) has the simpler form, \\[\n\\mathcal{V} (\\mathbf{b}^\\star) = \\frac{\\sigma^2}{n-1} \\mathbf{R}^{-1}_{X} \\; .\n\\] where \\(\\mathbf{R}_{X}\\) is the correlation matrix among the predictors. It can then be seen that the VIF\\(_j\\) are just the diagonal entries of \\(\\mathbf{R}^{-1}_{X}\\).\nMore generally, the matrix \\(\\mathbf{R}^{-1}_{X} = (r^{ij})\\), when standardized to a correlation matrix as \\(-r^{ij} / \\sqrt{r^{ii} \\; r^{jj}}\\) gives the matrix of all partial correlations, \\(r_{ij} \\,|\\, \\text{others}\\). }\n\n\n\n8.2.2 Collinearity diagnostics\nOK, we now know that large VIF\\(_j\\) indicate predictor coefficients whose estimation is degraded due to large \\(R^2_{j \\,|\\, \\text{others}}\\). But for this to be useful, we need to determine:\n\nhow many dimensions in the space of the predictors are associated with nearly collinear relations?\nwhich predictors are most strongly implicated in each of these?\n\nAnswers to these questions are provided using measures developed by Belsley and colleagues (Belsley et al., 1980; Belsley, 1991). These measures are based on the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots \\lambda_p\\) of the correlation matrix \\(R_{X}\\) of the predictors (preferably centered and scaled, and not including the constant term for the intercept), and the corresponding eigenvectors in the columns of \\(\\mathbf{V}_{p \\times p}\\), given by the the eigen decomposition \\[\n\\mathbf{R}_{X} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\mathsf{T}\n\\] By elementary matrix algebra, the eigen decomposition of \\(\\mathbf{R}_{XX}^{-1}\\) is then \\[\n\\mathbf{R}_{X}^{-1} = \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\mathsf{T} \\; ,\n\\tag{8.1}\\] so, \\(\\mathbf{R}_{X}\\) and \\(\\mathbf{R}_{XX}^{-1}\\) have the same eigenvectors, and the eigenvalues of \\(\\mathbf{R}_{X}^{-1}\\) are just \\(\\lambda_i^{-1}\\). Using Equation 8.1, the variance inflation factors may be expressed as \\[\n\\text{VIF}_j = \\sum_{k=1}^p \\frac{V^2_{jk}}{\\lambda_k} \\; ,\n\\] which shows that only the small eigenvalues contribute to variance inflation, but only for those predictors that have large eigenvector coefficients on those small components. These facts lead to the following diagnostic statistics for collinearity:\n\n\nCondition indices: The smallest of the eigenvalues, those for which \\(\\lambda_j \\approx 0\\), indicate collinearity and the number of small values indicates the number of near collinear relations. Because the sum of the eigenvalues, \\(\\Sigma \\lambda_i = p\\) increases with the number of predictors \\(p\\), it is useful to scale them all in relation to the largest. This leads to condition indices, defined as \\(\\kappa_j = \\sqrt{ \\lambda_1 / \\lambda_j}\\). These have the property that the resulting numbers have common interpretations regardless of the number of predictors.\n\nFor completely uncorrelated predictors, all \\(\\kappa_j = 1\\).\n\n\\(\\kappa_j \\rightarrow \\infty\\) as any \\(\\lambda_k \\rightarrow 0\\).\n\n\nVariance decomposition proportions: Large VIFs indicate variables that are involved in some nearly collinear relations, but they don’t indicate which other variable(s) each is involved with. For this purpose, Belsley et al. (1980) and Belsley (1991) proposed calculation of the proportions of variance of each variable associated with each principal component as a decomposition of the coefficient variance for each dimension.\n\nThese measures can be calculated using VisCollin::colldiag(). For the current model, the usual display contains both the condition indices and variance proportions. However, even for a small example, it is often difficult to know what numbers to pay attention to.\n\n(cd &lt;- colldiag(cars.mod, center=TRUE))\n#&gt; Condition\n#&gt; Index  Variance Decomposition Proportions\n#&gt;           cylinder engine horse weight accel year \n#&gt; 1   1.000 0.005    0.003  0.005 0.004  0.009 0.010\n#&gt; 2   2.252 0.004    0.002  0.000 0.007  0.022 0.787\n#&gt; 3   2.515 0.004    0.001  0.002 0.010  0.423 0.142\n#&gt; 4   5.660 0.309    0.014  0.306 0.087  0.063 0.005\n#&gt; 5   8.342 0.115    0.000  0.654 0.715  0.469 0.052\n#&gt; 6  10.818 0.563    0.981  0.032 0.176  0.013 0.004\n\nBelsley (1991) recommends that the sources of collinearity be diagnosed (a) only for those components with large \\(\\kappa_j\\), and (b) for those components for which the variance proportion is large (say, \\(\\ge 0.5\\)) on two or more predictors. The print method for \"colldiag\" objects has a fuzz argument controlling this.\n\nprint(cd, fuzz = 0.5)\n#&gt; Condition\n#&gt; Index  Variance Decomposition Proportions\n#&gt;           cylinder engine horse weight accel year \n#&gt; 1   1.000  .        .      .     .      .     .   \n#&gt; 2   2.252  .        .      .     .      .    0.787\n#&gt; 3   2.515  .        .      .     .      .     .   \n#&gt; 4   5.660  .        .      .     .      .     .   \n#&gt; 5   8.342  .        .     0.654 0.715   .     .   \n#&gt; 6  10.818 0.563    0.981   .     .      .     .\n\nThe mystery is solved, if you can read that table with these recommendations in mind. There are two nearly collinear relations among the predictors, corresponding to the two smallest dimensions.\n\nDimension 5 reflects the high correlation between horsepower and weight,\nDimension 6 reflects the high correlation between number of cylinders and engine displacement.\n\nNote that the high variance proportion for year (0.787) on the second component creates no problem and should be ignored because (a) the condition index is low and (b) it shares nothing with other predictors.\n\n8.2.3 Tableplots\nThe default tabular display of condition indices and variance proportions from colldiag() is what triggered the comparison to “Where’s Waldo”. It suffers from the fact that the important information — (a) how many Waldos? (b) where are they hiding — is disguised by being embedded in a sea of mostly irrelevant numbers. The simple option of using a principled fuzz factor helps considerably, but not entirely.\nThe simplified tabular display above can be improved to make the patterns of collinearity more visually apparent and to signify warnings directly to the eyes. A tableplot (Kwan et al., 2009) is a semi-graphic display that presents numerical information in a table using shapes proportional to the value in a cell and other visual attributes (shape type, color fill, and so forth) to encode other information.\nFor collinearity diagnostics, these show:\n\nthe condition indices, using squares whose background color is red for condition indices &gt; 10, brown for values &gt; 5 and green otherwise, reflecting danger, warning and OK respectively. The value of the condition index is encoded within this using a white square whose side is proportional to the value (up to some maximum value, cond.max that fills the cell).\nVariance decomposition proportions are shown by filled circles whose radius is proportional to those values and are filled (by default) with shades ranging from white through pink to red. Rounded values of those diagnostics are printed in the cells.\n\nThe tableplot below (Figure 8.5) encodes all the information from the values of colldiag() printed above. To aid perception, it uses prop.col color breaks such that variance proportions &lt; 0.3 are shaded white. The visual message is that one should attend to collinearities with large condition indices and large variance proportions implicating two or more predictors.\n\n\n\nR file: R/cars-colldiag.R\n\ntableplot(cd, title = \"Tableplot of cars data\", \n          cond.max = 30 )\n\n\n\n\n\n\nFigure 8.5: Tableplot of condition indices and variance proportions for the cars data. In column 1, the square symbols are scaled relative to a maximum condition index of 30. In the remaining columns, variance proportions (times 100) are shown as circles scaled relative to a maximum of 100.\n\n\n\n\n\n8.2.4 Collinearity biplots\nAs we have seen, the collinearity diagnostics are all functions of the eigenvalues and eigenvectors of the correlation matrix of the predictors in the regression model, or alternatively, the SVD of the \\(\\mathbf{X}\\) matrix in the linear model (excluding the constant). The standard biplot (Gabriel, 1971; Gower & Hand, 1996) (see: Section 4.3) can be regarded as a multivariate analog of a scatterplot, obtained by projecting a multivariate sample into a low-dimensional space (typically of 2 or 3 dimensions) accounting for the greatest variance in the data.\nHowever the standard biplot is less useful for visualizing the relations among the predictors that lead to nearly collinear relations. Instead, biplots of the smallest dimensions show these relations directly, and can show other features of the data as well, such as outliers and leverage points. We use prcomp(X, scale.=TRUE) to obtain the PCA of the correlation matrix of the predictors:\n\ncars.X &lt;- cars |&gt;\n  select(where(is.numeric)) |&gt;\n  select(-mpg) |&gt;\n  tidyr::drop_na()\ncars.pca &lt;- prcomp(cars.X, scale. = TRUE)\ncars.pca\n#&gt; Standard deviations (1, .., p=6):\n#&gt; [1] 2.070 0.911 0.809 0.367 0.245 0.189\n#&gt; \n#&gt; Rotation (n x k) = (6 x 6):\n#&gt;             PC1     PC2    PC3    PC4     PC5     PC6\n#&gt; cylinder -0.454 -0.1869  0.168 -0.659 -0.2711 -0.4725\n#&gt; engine   -0.467 -0.1628  0.134 -0.193 -0.0109  0.8364\n#&gt; horse    -0.462 -0.0177 -0.123  0.620 -0.6123 -0.1067\n#&gt; weight   -0.444 -0.2598  0.278  0.350  0.6860 -0.2539\n#&gt; accel     0.330 -0.2098  0.865  0.143 -0.2774  0.0337\n#&gt; year      0.237 -0.9092 -0.335  0.025 -0.0624  0.0142\n\nThe standard deviations above are the square roots \\(\\sqrt{\\lambda_j}\\) of the eigenvalues of the correlation matrix, and are returned in the sdev component of the \"prcomp\" object. The eigenvectors are returned in the rotation component, whose directions are arbitrary. Because we are interested in seeing the relative magnitude of variable vectors, we are free to multiply them by any constant to make them more visible in relation to the scores for the cars.\n\ncars.pca$rotation &lt;- -2.5 * cars.pca$rotation    # reflect & scale the variable vectors\n\nggp &lt;- fviz_pca_biplot(\n  cars.pca,\n  axes = 6:5,\n  geom = \"point\",\n  col.var = \"blue\",\n  labelsize = 5,\n  pointsize = 1.5,\n  arrowsize = 1.5,\n  addEllipses = TRUE,\n  ggtheme = ggplot2::theme_bw(base_size = 14),\n  title = \"Collinearity biplot for cars data\")\n\n# add point labels for outlying points\ndsq &lt;- heplots::Mahalanobis(cars.pca$x[, 6:5])\nscores &lt;- as.data.frame(cars.pca$x[, 6:5])\nscores$name &lt;- rownames(scores)\n\nggp + geom_text_repel(data = scores[dsq &gt; qchisq(0.95, df = 6),],\n                aes(x = PC6,\n                    y = PC5,\n                    label = name),\n                vjust = -0.5,\n                size = 5)\n\n\n\n\n\n\nFigure 8.6: Collinearity biplot of the Cars data, showing the last two dimensions. The projections of the variable vectors on the coordinate axes are proportional to their variance proportions. To reduce graphic clutter, only the most outlying observations in predictor space are identified by case labels. An extreme outlier (case 20) appears in the lower right corner.\n\n\n\n\nAs with the tabular display of variance proportions, Waldo is hiding in the dimensions associated with the smallest eigenvalues (largest condition indices). As well, it turns out that outliers in the predictor space (also high leverage observations) can often be seen as observations far from the centroid in the space of the smallest principal components.\nThe projections of the variable vectors in Figure 8.6 on the Dimension 5 and Dimension 6 axes are proportional to their variance proportions shown above. The relative lengths of these variable vectors can be considered to indicate the extent to which each variable contributes to collinearity for these two near-singular dimensions.\nThus, we see again that Dimension 6 is largely determined by engine size, with a substantial (negative) relation to cylinder. Dimension 5 has its’ strongest relations to weight and horse.\nMoreover, there is one observation, #20, that stands out as an outlier in predictor space, far from the centroid. It turns out that this vehicle, a Buick Estate wagon, is an early-year (1970) American behemoth, with an 8-cylinder, 455 cu. in, 225 horse-power engine, and able to go from 0 to 60 mph in 10 sec. (Its MPG is only slightly under-predicted from the regression model, however.)\nWith PCA and the biplot, we are used to looking at the dimensions that account for the most variation, but the answer to Where’s Waldo? is that he is hiding in the smallest data dimensions, just as he does in Figure 8.2 where the weak signals of his stripped shirt, hat and glasses are embedded in a visual field of noise. As we just saw, outliers hide there also, hoping to escape detection. These small dimensions are also implicated in ridge regression as we will see shortly (Section 8.4).",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-remedies",
    "href": "08-collinearity-ridge.html#sec-remedies",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.3 Remedies for collinearity: What can I do?",
    "text": "8.3 Remedies for collinearity: What can I do?\nCollinearity is often a data problem, for which there is no magic cure. Nevertheless there are some general guidelines and useful techniques to address this problem.\n\nPure prediction: If we are only interested in predicting / explaining an outcome, and not the model coefficients or which are “significant”, collinearity can be largely ignored. The fitted values are unaffected by collinearity, even in the case of perfect collinearity as shown in Figure 8.3 (b).\n\nStructural collinearity: Sometimes collinearity results from structural relations among the variables that relate to how they have been defined.\n\nFor example, polynomial terms, like \\(x, x^2, x^3\\) or interaction terms like \\(x_1, x_2, x_1 * x_2\\) are necessarily correlated. A simple cure is to center the predictors at their means, using \\(x - \\bar{x}, (x - \\bar{x})^2, (x - \\bar{x})^3\\) or \\((x_1 - \\bar{x}_1), (x_2 - \\bar{x}_2), (x_1 - \\bar{x}_1) * (x_2 - \\bar{x}_2)\\). Centering removes the spurious ill-conditioning, thus reducing the VIFs. Note that in polynomial models, using y ~ poly(x, 3) to specify a cubic model generates orthogonal (uncorrelated) regressors, whereas in y ~ x + I(x^2) + I(x^3) the terms have built-in correlations.\nWhen some predictors share a common cause, as in GNP or population in time-series or cross-national data, you can reduce collinearity by re-defining predictors to reflect per capita measures. In a related example with sports data, when you have cumulative totals (e.g., runs, hits, homeruns in baseball) for players over years, expressing these measures as per year will reduce the common effect of longevity on these measures.\n\n\n\nModel re-specification:\n\nDrop one or more regressors that have a high VIF, if they are not deemed to be essential to understanding the model. Care must be taken here to not omit variables which should be controlled or accounted for in interpretation.\nReplace highly correlated regressors with less correlated linear combination(s) of them. For example, two related variables, \\(x_1\\) and \\(x_2\\) can be replaced without any loss of information by replacing them with their sum and difference, \\(z_1 = x_1 + x_2\\) and \\(z_2 = x_1 - x_2\\). For instance, in a dataset on fitness, we may have correlated predictors of resting pulse rate and pulse rate while running. Transforming these to average pulse rate and their difference gives new variables which are interpretable and less correlated.\n\n\n\nStatistical remedies:\n\nTransform the predictors \\(\\mathbf{X}\\) to uncorrelated principal component scores \\(\\mathbf{Z} = \\mathbf{X} \\mathbf{V}\\), and regress \\(\\mathbf{y}\\) on \\(\\mathbf{Z}\\). These will have the identical overall model fit without loss of information. A related technique is incomplete principal components regression, where some of the smallest dimensions (those causing collinearity) are omitted from the model. The trade-off is that it may be more difficult to interpret what the model means, but this can be countered with a biplot, showing the projections of the original variables into the reduced space of the principal components.\nUse regularization methods such as ridge regression and lasso, which correct for collinearity by introducing shrinking coefficients towards 0, introducing a small amount of bias, . See the genridge package and its pkgdown documentation for visualization methods.\nuse Bayesian regression; if multicollinearity prevents a regression coefficient from being estimated precisely, then a prior on that coefficient will help to reduce its posterior variance.\n\n\n\nExample: Centering\nTo illustrate the effect of centering a predictor in a polynomial model, we generate a perfect quadratic relationship, \\(y = x^2\\) and consider the correlations of \\(y\\) with \\(x\\) and with \\((x - \\bar{x})^2\\). The correlation of \\(y\\) with \\(x\\) is 0.97, while the correlation of \\(y\\) with \\((x - \\bar{x})^2\\) is zero.\n\nx &lt;- 1:20\ny1 &lt;- x^2\ny2 &lt;- (x - mean(x))^2\nXY &lt;- data.frame(x, y1, y2)\n\n(R &lt;- cor(XY))\n#&gt;        x    y1    y2\n#&gt; x  1.000 0.971 0.000\n#&gt; y1 0.971 1.000 0.238\n#&gt; y2 0.000 0.238 1.000\n\nThe effect of centering here is remove the linear association in what is a purely quadratic relationship, as can be seen by plotting y1 and y2 against x.\n\nr1 &lt;- R[1, 2]\nr2 &lt;- R[1, 3]\n\ngg1 &lt;-\nggplot(XY, aes(x = x, y = y1)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y~x, linewidth = 2, se = FALSE) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_bw(base_size = 16) +\n  annotate(\"text\", x = 5, y = 350, size = 6,\n           label = paste(\"X Uncentered\\nr =\", round(r1, 3)))\n\ngg2 &lt;-\n  ggplot(XY, aes(x = x, y = y2)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y~x, linewidth = 2, se = FALSE) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_bw(base_size = 16) +\n  annotate(\"text\", x = 5, y = 80, size = 6,\n           label = paste(\"X Centered\\nr =\", round(r2, 3)))\n\ngg1 + gg2         # show plots side-by-side\n\n\n\n\n\n\nFigure 8.7: Centering a predictor removes the nessessary correlation in a quadratic regression\n\n\n\n\nExample: Interactions\nThe dataset genridge::Acetylene gives data from Marquardt & Snee (1975) on the yield of a chemical manufacturing process to produce acetylene in relation to reactor temperature (temp), the ratio of two components and the contact time in the reactor. A naive response surface model might suggest that yield is quadratic in time and there are potential interactions among all pairs of predictors.\n\n\ndata(Acetylene, package = \"genridge\")\nacetyl.mod0 &lt;- lm(\n  yield ~ temp + ratio + time + I(time^2) + \n          temp:time + temp:ratio + time:ratio,\n  data=Acetylene)\n\n(acetyl.vif0 &lt;- vif(acetyl.mod0))\n#&gt;       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#&gt;        383      10555      18080        564       9719       9693 \n#&gt; ratio:time \n#&gt;        225\n\nThese results are horrible! How much does centering help? I first center all three predictors and then use update() to re-fit the same model using the centered data.\n\nAcetylene.centered &lt;-\n  Acetylene |&gt;\n  mutate(temp = temp - mean(temp),\n         time = time - mean(time),\n         ratio = ratio - mean(ratio))\n\nacetyl.mod1 &lt;- update(acetyl.mod0, data=Acetylene.centered)\n(acetyl.vif1 &lt;- vif(acetyl.mod1))\n#&gt;       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#&gt;      57.09       1.09      81.57      51.49      44.67      30.69 \n#&gt; ratio:time \n#&gt;      33.33\n\nThis is far better, although still not great in terms of VIF. But, how much have we improved the situation by the simple act of centering the predictors? The square roots of the ratios of VIFs tell us the impact of centering on the standard errors.\n\nsqrt(acetyl.vif0 / acetyl.vif1)\n#&gt;       temp      ratio       time  I(time^2)  temp:time temp:ratio \n#&gt;       2.59      98.24      14.89       3.31      14.75      17.77 \n#&gt; ratio:time \n#&gt;       2.60\n\nFinally, we use poly(time, 2) in the model for the centered data. Because there are multiple degree of freedom terms in the model, car::vif() calculates GVIFs here. The final column gives \\(\\sqrt{\\text{GVIF}^{1/2 \\text{df}}}\\), the remaining effect of collinearity on the standard errors of terms in this model.\n\nacetyl.mod2 &lt;- lm(yield ~ temp + ratio + poly(time, 2) + \n                          temp:time + temp:ratio + time:ratio,\n                  data=Acetylene.centered)\n\nvif(acetyl.mod2, type = \"term\")\n#&gt;                  GVIF Df GVIF^(1/(2*Df))\n#&gt; temp            57.09  1            7.56\n#&gt; ratio            1.09  1            1.05\n#&gt; poly(time, 2) 1733.56  2            6.45\n#&gt; temp:time       44.67  1            6.68\n#&gt; temp:ratio      30.69  1            5.54\n#&gt; ratio:time      33.33  1            5.77",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge",
    "href": "08-collinearity-ridge.html#sec-ridge",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.4 Ridge regression",
    "text": "8.4 Ridge regression\nRidge regression is an instance of a class of techniques designed to obtain more favorable predictions at the expense of some increase in bias, compared to ordinary least squares (OLS) estimation. These methods began as a way of solving collinearity problems in OLS regression with highly correlated predictors (Hoerl & Kennard, 1970). More recently ridge regression developed to a larger class of model selection methods, of which the LASSO method of Tibshirani (1996) and LAR method of Efron et al. (2004) are well-known instances. See, for example, the reviews in Vinod (1978) and McDonald (2009) for details and context omitted here. The case of ridge regression has also been extended to the case of two or more response variables (Brown & Zidek, 1980; Haitovsky, 1987).\nAn essential idea behind these methods is that the OLS estimates are constrained in some way, shrinking them, on average, toward zero, to achieve increased predictive accuracy at the expense of some increase in bias. Another common characteristic is that they involve some tuning parameter (\\(k\\)) or criterion to quantify the tradeoff between bias and variance. In many cases, analytical or computationally intensive methods have been developed to choose an optimal value of the tuning parameter, for example using generalized cross validation, bootstrap methods.\nA common means to visualize the effects of shrinkage in these problems is to make what are called univariate ridge trace plots (Section 8.5) showing how the estimated coefficients \\(\\widehat{\\boldsymbol{\\beta}}_k\\) change as the shrinkage criterion \\(k\\) increases. (An example is shown in Fig XX below.) But this only provides a view of bias. It is the wrong graphic form for a multivariate problem where we want to visualize bias in the coefficients \\(\\widehat{\\boldsymbol{\\beta}}_k\\) vs. their precision, as reflected in their estimated variances, \\(\\widehat{\\textsf{Var}} (\\widehat{\\boldsymbol{\\beta}}_k)\\). A more useful graphic plots the confidence ellipses for the coefficients, showing both bias and precision (Section 8.6). Some of the material below borrows from Friendly (2011) and Friendly (2013).\n\n8.4.1 Properties of ridge regression\nTo provide some context, I summarize the properties of ridge regression below, comparing the OLS estimates with their ridge counterparts. To avoid unnecessary details related to the intercept, assume the predictors have been centered at their means and the unit vector is omitted from \\(\\mathbf{X}\\). Further, to avoid scaling issues, we standardize the columns of \\(\\mathbf{X}\\) to unit length, so that \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is a also correlation matrix.\nThe ordinary least squares estimates of coefficients and their estimated variance covariance matrix take the (hopefully now) familiar form\n\\[\\begin{aligned}\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} = &\n    (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\mathbf{y} \\:\\: ,\\\\\n\\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}) = &\n    \\widehat{\\sigma}_{\\epsilon}^2 (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}.\n\\end{aligned} \\tag{8.2}\\]\nAs we saw ealier, one signal of the problem of collinearity is that the determinant \\(\\mathrm{det}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})\\) approaches zero as the predictors become more collinear. The inverse \\((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\) becomes numerically unstable, or does not exist if the determinant becomes zero in the case of exact dependency of one variable on the others.\nRidge regression uses a trick to avoid this. It adds a constant, \\(k\\) to the diagonal elements, replacing \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) with \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I}\\) in Equation 8.2. This drives the determinant away from zero as \\(k\\) increases. The ridge regression estimates then become,\n\\[\\begin{aligned}\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k = &\n    (\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T}\\mathbf{y}  \\\\\n                                    = & \\mathbf{G}_k \\, \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} \\:\\: ,\\\\\n\\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k) = &\n     \\widehat{\\sigma}^2  \\mathbf{G}_k (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{G}_k^\\mathsf{T}\\:\\: ,\n\\end{aligned} \\tag{8.3}\\]\nwhere \\(\\mathbf{G}_k = \\left[\\mathbf{I} + k (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\right] ^{-1}\\) is the \\((p \\times p)\\) shrinkage matrix. Thus, as \\(k\\) increases, \\(\\mathbf{G}_k\\) decreases, and drives \\(\\widehat{\\mathbf{\\beta}}^{\\mathrm{RR}}_k\\) toward \\(\\mathbf{0}\\) (Hoerl & Kennard, 1970).\nAnother insight, from the shrinkage literature, is that ridge regression can be formulated as least squares regression, minimizing a residual sum of squares, \\(\\text{RSS}(k)\\), which adds a penalty for large coefficients,\n\\[\n\\text{RSS}(k) = (\\mathbf{y}-\\mathbf{X} \\mathbf{\\beta}) ^\\mathsf{T}(\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\beta}) + k \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} \\quad\\quad (k \\ge 0)\n\\:\\: ,\n\\tag{8.4}\\] where the penalty restrict the coefficients to some squared length \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} = \\Sigma \\beta_i \\le t(k)\\).\nThe geometry of ridge regession is illustrated in Figure 8.8 for two coefficients \\(\\boldsymbol{\\beta} = (\\beta_1, \\beta_2)\\). The blue circles at the origin, having radii \\(\\sqrt{t_k}\\), show the constraint that the sum of squares of coefficients, \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta} = \\beta_1^2 + \\beta_2^2\\) be less than \\(k\\). The red ellipses show contours of the covariance ellipse of \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}\\). As the shrinkage constant \\(k\\) increases, the center of these ellipses travel along the path illustrated toward \\(\\boldsymbol{\\beta} = \\mathbf{0}\\) This path is called the locus of osculation, the path along which circles or ellipses first kiss as they expand, like the pattern of ripples from rocks dropped into a pond (Friendly et al., 2013).\n\n\n\n\n\n\n\nFigure 8.8: Geometric interpretation of ridge regression, using elliptical contours of the \\(\\text{RSS}(k)\\) function. The blue circles at the origin show the constraint that the sum of squares of coefficients, \\(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{\\beta}\\) be less than \\(k\\). The red ellipses show the covariance ellipse of two coefficients \\(\\boldsymbol{\\beta}\\). Ridge regression finds the point \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k\\) where the OLS contours just kiss the constraint region. _Source: Friendly et al. (2013).\n\n\n\n\n\nEquation 8.3 is computationally expensive, potentially numerically unstable for small \\(k\\), and it is conceptually opaque, in that it sheds little light on the underlying geometry of the data in the column space of \\(\\mathbf{X}\\). An alternative formulation can be given in terms of the singular value decomposition (SVD) of \\(\\mathbf{X}\\),\n\\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\mathsf{T}\n\\]\nwhere \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are respectively \\(n\\times p\\) and \\(p\\times p\\) orthonormal matrices, so that \\(\\mathbf{U}^\\mathsf{T}\\mathbf{U} = \\mathbf{V}^\\mathsf{T}\\mathbf{V} = \\mathbf{I}\\), and \\(\\mathbf{D} = \\mathrm{diag}\\, (d_1, d_2, \\dots d_p)\\) is the diagonal matrix of ordered singular values, with entries \\(d_1 \\ge d_2 \\ge \\cdots \\ge d_p \\ge 0\\).\nBecause \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X} = \\mathbf{V} \\mathbf{D}^2 \\mathbf{V}^\\mathsf{T}\\), the eigenvalues of \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) are given by \\(\\mathbf{D}^2\\) and therefore the eigenvalues of \\(\\mathbf{G}_k\\) can be shown (Hoerl & Kennard, 1970) to be the diagonal elements of\n\\[\n\\mathbf{D}(\\mathbf{D}^2 + k \\mathbf{I} )^{-1} \\mathbf{D} = \\mathrm{diag}\\,  \\left(\\frac{d_i^2}{d_i^2 + k}\\right) \\:\\: .\n\\]\nNoting that the eigenvectors, \\(\\mathbf{V}\\) are the principal component vectors, and that \\(\\mathbf{X} \\mathbf{V} = \\mathbf{U} \\mathbf{D}\\), the ridge estimates can be calculated more simply in terms of \\(\\mathbf{U}\\) and \\(\\mathbf{D}\\) as\n\\[\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k = (\\mathbf{D}^2 + k \\mathbf{I})^{-1} \\mathbf{D} \\mathbf{U}^\\mathsf{T}\\mathbf{y} = \\left( \\frac{d_i}{d_i^2 + k}\\right) \\: \\mathbf{u}_i^\\mathsf{T}\\mathbf{y}, \\quad i=1, \\dots p \\:\\: .\n\\]\nThe terms \\(d^2_i / (d_i^2 + k) \\le 1\\) are thus the factors by which the coordinates of \\(\\mathbf{u}_i^\\mathsf{T}\\mathbf{y}\\) are shrunk with respect to the orthonormal basis for the column space of \\(\\mathbf{X}\\). The small singular values \\(d_i\\) correspond to the directions which ridge regression shrinks the most. These are the directions which contribute most to collinearity, discussed earlier.\nThis analysis also provides an alternative and more intuitive characterization of the ridge tuning constant. By analogy with OLS, where the hat matrix, \\(\\mathbf{H} = \\mathbf{X} (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\) reflects degrees of freedom \\(\\text{df} = \\mathrm{tr} (\\mathbf{H}) = p\\) corresponding to the \\(p\\) parameters, the effective degrees of freedom for ridge regression (Hastie et al., 2009) is\n\\[\\begin{aligned}\n\\text{df}_k\n    = & \\text{tr}[\\mathbf{X} (\\mathbf{X}^\\mathsf{T}\\mathbf{X} + k \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T}] \\\\\n    = & \\sum_i^p \\text{df}_k(i) = \\sum_i^p \\left( \\frac{d_i^2}{d_i^2 + k} \\right) \\:\\: .\n\\end{aligned} \\tag{8.5}\\]\n\\(\\text{df}_k\\) is a monotone decreasing function of \\(k\\), and hence any set of ridge constants can be specified in terms of equivalent \\(\\text{df}_k\\). Greater shrinkage corresponds to fewer coefficients being estimated.\nThere is a close connection with principal components regression mentioned in Section 8.3. Ridge regression shrinks all dimensions in proportion to \\(\\text{df}_k(i)\\), so the low variance dimensions are shrunk more. Principal components regression discards the low variance dimensions and leaves the high variance dimensions unchanged.\n\n8.4.2 The genridge package\nRidge regression and other shrinkage methods are available in several packages including MASS (the lm.ridge() function), glmnet (Friedman et al., 2025), and penalized (Goeman et al., 2022), but none of these provides insightful graphical displays. glmnet::glmnet() also implements a method for multivariate responses with a `family=“mgaussian”.\nHere, I focus in the genridge package (Friendly, 2024), where the ridge() function is the workhorse and pca.ridge() transforms these results to PCA/SVD space. vif.ridge() calculates VIFs for class \"ridge\" objects and precision() calculates precision and shrinkage measures.\nA variety of plotting functions is available for univariate, bivariate and 3D plots:\n\n\ntraceplot() Traditional univariate ridge trace plots\n\nplot.ridge() Bivariate 2D ridge trace plots, showing the covariance ellipse of the estimated coefficients\n\npairs.ridge() All pairwise bivariate ridge trace plots\n\nplot3d.ridge() 3D ridge trace plots with ellipsoids\n\nbiplot.ridge() ridge trace plots in PCA/SVD space\n\nIn addition, the pca() method for \"ridge\" objects transforms the coefficients and covariance matrices of a ridge object from predictor space to the equivalent, but more interesting space of the PCA of \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) or the SVD of \\(\\mathbf{X}\\). biplot.pcaridge() adds variable vectors to the bivariate plots of coefficients in PCA space",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge-univar",
    "href": "08-collinearity-ridge.html#sec-ridge-univar",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.5 Univariate ridge trace plots",
    "text": "8.5 Univariate ridge trace plots\nA classic example for ridge regression is Longley’s (1967) data, consisting of 7 economic variables, observed yearly from 1947 to 1962 (n=16), in the dataset datasets::longley. The goal is to predict Employed from GNP, Unemployed, Armed.Forces, Population, Year, and GNP.deflator.\n\n\ndata(longley, package=\"datasets\")\nstr(longley)\n#&gt; 'data.frame':  16 obs. of  7 variables:\n#&gt;  $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...\n#&gt;  $ GNP         : num  234 259 258 285 329 ...\n#&gt;  $ Unemployed  : num  236 232 368 335 210 ...\n#&gt;  $ Armed.Forces: num  159 146 162 165 310 ...\n#&gt;  $ Population  : num  108 109 110 111 112 ...\n#&gt;  $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...\n#&gt;  $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...\n\nThese data were constructed to illustrate numerical problems in least squares software at the time, and they are (purposely) perverse, in that:\n\nEach variable is a time series so that there is clearly a lack of independence among predictors. Year is at least implicitly correlated with most of the others.\nWorse, there is also some structural collinearity among the variables GNP, Year, GNP.deflator, and Population; for example, GNP.deflator is a multiplicative factor to account for inflation.\n\nWe fit the regression model, and sure enough, there are some extremely large VIFs. The largest, for GNP represents a multiplier of \\(\\sqrt{1788.5} = 42.3\\) on the standard errors.\n\nlongley.lm &lt;- lm(Employed ~ GNP + Unemployed + Armed.Forces + \n                            Population + Year + GNP.deflator, \n                 data=longley)\nvif(longley.lm)\n#&gt;          GNP   Unemployed Armed.Forces   Population         Year \n#&gt;      1788.51        33.62         3.59       399.15       758.98 \n#&gt; GNP.deflator \n#&gt;       135.53\n\nShrinkage values can be specified using \\(k\\) (where \\(k = 0\\) corresponds to OLS) or the equivalent degrees of freedom $ _k$ (Equation 8.5). (The function uses the notation \\(\\lambda \\equiv k\\), so the argument is lambda.) Among other quantities, ridge() returns a matrix containing the coefficients for each predictor for each shrinkage value and other quantities.\n\nlambda &lt;- c(0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08)\nlridge &lt;- ridge(Employed ~ GNP + Unemployed + Armed.Forces + \n                           Population + Year + GNP.deflator, \n    data=longley, lambda=lambda)\nprint(lridge, digits = 2)\n#&gt; Ridge Coefficients:\n#&gt;        GNP     Unemployed  Armed.Forces  Population  Year  \n#&gt; 0.000  -3.447  -1.828      -0.696        -0.344       8.432\n#&gt; 0.002  -2.114  -1.644      -0.658        -0.713       7.466\n#&gt; 0.005  -1.042  -1.491      -0.623        -0.936       6.567\n#&gt; 0.010  -0.180  -1.361      -0.588        -1.003       5.656\n#&gt; 0.020   0.499  -1.245      -0.548        -0.868       4.626\n#&gt; 0.040   0.906  -1.155      -0.504        -0.523       3.577\n#&gt; 0.080   1.091  -1.086      -0.458        -0.086       2.642\n#&gt;        GNP.deflator\n#&gt; 0.000   0.157      \n#&gt; 0.002   0.022      \n#&gt; 0.005  -0.042      \n#&gt; 0.010  -0.026      \n#&gt; 0.020   0.098      \n#&gt; 0.040   0.321      \n#&gt; 0.080   0.570\n\nThe standard univariate plot, given by traceplot(), simply plots the estimated coefficients for each predictor against the shrinkage factor \\(k\\).\n\ntraceplot(lridge, \n          X = \"lambda\",\n          xlab = \"Ridge constant (k)\",\n          xlim = c(-0.02, 0.08), cex.lab=1.25)\n\n\n\n\n\n\nFigure 8.9: Univariate ridge trace plot for the coefficients of predictors of Employment in Longley’s data via ridge regression, with ridge constants k = (0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08). The dotted lines show optimal values for shrinkage by two criteria (HKB, LW).\n\n\n\n\nYou can see that the coefficients for Year and GNP are shrunk considerably. Differences from the \\(\\beta\\) value at \\(k =0\\) represent the bias (smaller \\(\\mid \\beta \\mid\\)) needed to achieve more stable estimates.\nThe dotted lines in Figure 8.9 show choices for the ridge constant by two commonly used criteria to balance bias against precision due to Hoerl et al. (1975) (HKB) and Lawless & Wang (1976) (LW). These values (along with a generalized cross-validation value GCV) are also stored in the “ridge” object as a vector criteria.\n\nlridge$criteria\n#&gt;    kHKB     kLW    kGCV \n#&gt; 0.00428 0.03230 0.00200\n\n\nThe shrinkage constant \\(k\\) doesn’t have much intrinsic meaning, so it is often easier to interpret the plot when coefficients are plotted against the equivalent degrees of freedom, \\(\\text{df}_k\\). OLS corresponds to \\(\\text{df}_k = 6\\) degrees of freedom in the space of six parameters, and the effect of shrinkage is to decrease the degrees of freedom, as if estimating fewer parameters. This more natural scale also makes the changes in coefficient with shrinkage more nearly linear.\n\ntraceplot(lridge, \n          X = \"df\",\n          xlim = c(4, 6.2), cex.lab=1.25)\n\n\n\n\n\n\nFigure 8.10: Univariate ridge trace plot using equivalent degrees of freedom, \\(\\text{df}_k\\) to specify shrinkage. This scale is easier to understand and makes the traces of prarameters more nearly linear.\n\n\n\n\nBut a bigger problem is that these univariate plots are the wrong kind of plot! They show the trends in increased bias (toward smaller \\(\\mid \\beta \\mid\\)) associated with larger \\(k\\), but they do not show the accompanying increase in precision (decrease in variance) achieved by allowing a bit of bias.\nFor that, we need to consider the variances and covariances of the estimated coefficients. The univariate trace plot is the wrong graphic form for what is essentially a multivariate problem, where we would like to visualize how both coefficients and their variances change with \\(k\\).",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#sec-ridge-bivar",
    "href": "08-collinearity-ridge.html#sec-ridge-bivar",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.6 Bivariate ridge trace plots",
    "text": "8.6 Bivariate ridge trace plots\nThe bivariate analog of the trace plot suggested by Friendly (2013) plots bivariate confidence ellipses for pairs of coefficients. Their centers, \\((\\widehat{\\beta}_i, \\widehat{\\beta}_j)\\) compared to the OLS values show the bias induced for each coefficient, and also how the change in the ridge estimate for one parameter is related to changes for other parameters.\nThe size and shapes of the covariance ellipses show directly the effect on precision of the estimates as a function of the ridge tuning constant. and their size and shape indicate sampling variance, \\(\\widehat{\\text{Var}} (\\mathbf{\\widehat{\\beta}}_{ij})\\). Here, I plot those for GNP against four of the other predictors. The plot() method for \"ridge\" objects plots these ellipses for a pair of variables.\n\nclr &lt;-  c(\"black\", \"red\", \"brown\", \"darkgreen\",\"blue\", \"cyan4\", \"magenta\")\npch &lt;- c(15:18, 7, 9, 12)\nlambdaf &lt;- c(expression(~widehat(beta)^OLS), as.character(lambda[-1]))\n\nfor (i in 2:5) {\n  plot(lridge, variables=c(1,i), \n       radius=0.5, cex.lab=1.5, col=clr, \n       labels=NULL, fill=TRUE, fill.alpha=0.2)\n  text(lridge$coef[1,1], lridge$coef[1,i], \n       expression(~widehat(beta)^OLS), cex=1.5, pos=4, offset=.1)\n  text(lridge$coef[-1,c(1,i)], lambdaf[-1], pos=3, cex=1.3)\n}\n\n\n\n\n\n\nFigure 8.11: Bivariate ridge trace plots for the coefficients of four predictors against the coefficient for GNP in Longley’s data, with k = 0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08. In most cases, the coefficients are driven toward zero, but the bivariate plot also makes clear the reduction in variance, as well as the bivariate path of shrinkage.\n\n\n\n\nAs can be seen, the coefficients for each pair of predictors trace a path generally in toward the origin (0,0), and the covariance ellipses get smaller, indicating increased precision. Sometimes, these paths are rather direct, but it takes a peculiar curvilinear route in the case of population and GNP.\nThe pairs() method for \"ridge\" objects shows all pairwise views in scatterplot matrix form. radius sets the base size of the ellipse-generating circle for the covariance ellipses.\n\npairs(lridge, radius=0.5, diag.cex = 2, \n      fill = TRUE, fill.alpha = 0.1)\n\n\n\n\n\n\nFigure 8.12: Scatterplot matrix of bivariate ridge trace plots. Each panel shows the effect of shrinkage on the covariance ellipse for a pair of predictors.\n\n\n\n\n\n8.6.1 Visualizing the bias-variance tradeoff\nThe function precision() calculates a number of measures of the effect of shrinkage of the coefficients in relation to the “size” of the covariance matrix \\(\\boldsymbol{\\mathcal{V}}_k \\equiv \\widehat{\\mathsf{Var}} (\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{RR}}_k)\\). Larger shrinkage \\(k\\) should lead to a smaller ellipsoid for \\(\\boldsymbol{\\mathcal{V}}_k\\), indicating increased precision.\n\npdat &lt;- precision(lridge) |&gt; print()\n#&gt;       lambda   df   det  trace max.eig norm.beta norm.diff\n#&gt; 0.000  0.000 6.00 -12.9 18.119  15.419     1.000     0.000\n#&gt; 0.002  0.002 5.70 -13.6 11.179   8.693     0.857     0.695\n#&gt; 0.005  0.005 5.42 -14.4  6.821   4.606     0.741     1.276\n#&gt; 0.010  0.010 5.14 -15.4  4.042   2.181     0.637     1.783\n#&gt; 0.020  0.020 4.82 -16.8  2.218   1.025     0.528     2.262\n#&gt; 0.040  0.040 4.48 -18.7  1.165   0.581     0.423     2.679\n#&gt; 0.080  0.080 4.13 -21.1  0.587   0.260     0.337     3.027\n\nHere, the first three terms described below are (inverse) measures of precision; the last two quantify shrinkage:\n\ndet \\(=\\log{| \\mathcal{V}_k |}\\) is an overall measure of variance of the coefficients. It is the (linearized) volume of the covariance ellipsoid and corresponds conceptually to Wilks’ Lambda criterion.\ntrace \\(=\\text{trace} (\\boldsymbol{\\mathcal{V}}_k)\\) is the sum of the variances and also the sum of the eigenvalues of \\(\\boldsymbol{\\mathcal{V}}_k\\), conceptually similar to Pillai’s trace criterion.\nmax.eig is the largest eigenvalue measure of size, an analog of Roy’s maximum root test.\nnorm.beta \\(= \\left \\Vert \\boldsymbol{\\beta}\\right \\Vert / \\max{\\left \\Vert \\boldsymbol{\\beta}\\right \\Vert}\\) is a summary measure of shrinkage, the normalized root mean square of the estimated coefficients. It starts at 1.0 for \\(k=0\\) and decreases with the penalty for large coefficients.\ndiff.beta is the root mean square of the difference from the OLS estimate \\(\\lVert \\mathbf{\\beta}_{\\text{OLS}} - \\mathbf{\\beta}_k \\rVert\\). This measure is inversely related to norm.beta.\n\nPlotting shrinkage against a measure of variance gives a direct view of the tradeoff between bias and precision. Here I plot norm.beta against det, and join the points with a curve. You can see that in this example the HKB criterion prefers a smaller degree of shrinkage, but achieves only a modest decrease in variance. But variance decreases more sharply thereafter and the LW choice achieves greater precision.\n\nShow the codelibrary(splines)\nwith(pdat, {\n  plot(norm.beta, det, type=\"b\", \n       cex.lab=1.25, pch=16, \n       cex=1.5, col=clr, lwd=2,\n       xlab='shrinkage: ||b|| / max(||b||)',\n       ylab='variance: log |Var(b)|')\n  text(norm.beta, det, \n       labels = lambdaf, \n       cex = 1.25, \n       pos = c(rep(2,length(lambda)-1),4))\n  text(min(norm.beta), max(det), \n       labels = \"log |Variance| vs. Shrinkage\", \n       cex=1.5, pos=4)\n  })\n# find locations for optimal shrinkage criteria\nmod &lt;- lm(cbind(det, norm.beta) ~ bs(lambda, df=5), \n          data=pdat)\nx &lt;- data.frame(lambda=c(lridge$kHKB, \n                         lridge$kLW))\nfit &lt;- predict(mod, x)\npoints(fit[,2:1], pch=15, \n       col=gray(.50), cex=1.6)\ntext(fit[,2:1], c(\"HKB\", \"LW\"), \n     pos=3, cex=1.5, col=gray(.50))\n\n\n\n\n\n\nFigure 8.13: The tradeoff between bias and precision. Bias increases as we move away from the OLS solution, but precision increases.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#low-rank-views",
    "href": "08-collinearity-ridge.html#low-rank-views",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.7 Low-rank views",
    "text": "8.7 Low-rank views\nJust as principal components analysis gives low-dimensional views of a data set, PCA can be useful to understand ridge regression, just as it did for the problem of collinearity.\nThe pca method transforms a \"ridge\" object from parameter space, where the estimated coefficients are \\(\\beta_k\\) with covariance matrices \\(\\boldsymbol{\\mathcal{V}}_k\\), to the principal component space defined by the right singular vectors, \\(\\mathbf{V}\\), of the singular value decomposition \\(\\mathbf{U} \\mathbf{D} \\mathbf{V}^\\mathsf{T}\\) of the scaled predictor matrix, \\(\\mathbf{X}\\). In PCA space the total variance of the predictors remains the same, but it is distributed among the linear combinations that account for successively greatest variance.\n\nplridge &lt;- pca(lridge)\nplridge\n#&gt; Ridge Coefficients:\n#&gt;        dim1     dim2     dim3     dim4     dim5     dim6   \n#&gt; 0.000  1.51541  0.37939  1.80131  0.34595  5.97391  6.74225\n#&gt; 0.002  1.51537  0.37935  1.80021  0.34308  5.69497  5.06243\n#&gt; 0.005  1.51531  0.37928  1.79855  0.33886  5.32221  3.68519\n#&gt; 0.010  1.51521  0.37918  1.79579  0.33205  4.79871  2.53553\n#&gt; 0.020  1.51500  0.37898  1.79031  0.31922  4.00988  1.56135\n#&gt; 0.040  1.51459  0.37858  1.77944  0.29633  3.01774  0.88291\n#&gt; 0.080  1.51377  0.37778  1.75810  0.25915  2.01876  0.47238\n\nThen, a traceplot() of the resulting \"pcaridge\" object shows how the dimensions are affected by shrinkage, shown on the scale of degrees of freedom in Figure 8.14.\n\ntraceplot(plridge, X=\"df\", \n          cex.lab = 1.2, lwd=2)\n\n\n\n\n\n\nFigure 8.14: Ridge traceplot for the longley regression viewed in PCA space. The dimensions are the linear combinations of the predictors which account for greatest variance.\n\n\n\n\nWhat may be surprising at first is that the coefficients for the first 4 components are not shrunk at all. These large dimensions are immune to ridge tuning. Rather, the effect of shrinkage is seen only on the last two dimensions. But those also are the directions that contribute most to collinearity as we saw earlier.\n\nA pairs() plot gives a dramatic representation bivariate effects of shrinkage in PCA space: the principal components of X are uncorrelated, so the ellipses are all aligned with the coordinate axes and the ellipses largely coincide for dimensions 1 to 4. You can see them shrink in one direction in the last two columns and rows.\n\npairs(plridge)\n\n\n\n\n\n\nFigure 8.15: All pairwise bivariate ridge plots shown in PCA space.\n\n\n\n\nIf we focus on the plot of dimensions 5:6, we can see where all the shrinkage action is in this representation. Generally, the predictors that are related to the smallest dimension (6) are shrunk quickly at first.\n\nplot(plridge, variables=5:6, \n     fill = TRUE, fill.alpha=0.15, cex.lab = 1.5)\ntext(plridge$coef[, 5:6], \n     label = lambdaf, \n     cex=1.5, pos=4, offset=.1)\n\n\n\n\n\n\nFigure 8.16: Bivariate ridge trace plot for the smallest two dimensions …\n\n\n\n\n\n8.7.1 Biplot view\nThe question arises how to relate this view of shrinkage in PCA space to the original predictors. The biplot is again your friend. You can project variable vectors for the predictor variables into the PCA space of the smallest dimensions, where the shrinkage action mostly occurs to see how the predictor variables relate to these dimensions.\nbiplot.pcaridge() supplements the standard display of the covariance ellipsoids for a ridge regression problem in PCA/SVD space with labeled arrows showing the contributions of the original variables to the dimensions plotted. Recall from Section 4.3 that these reflect the correlations of the variables with the PCA dimensions. The lengths of the arrows reflect the proportion of variance that each predictors shares with the components.\n\nbiplot(plridge, radius=0.5, \n       ref=FALSE, asp=1, \n       var.cex=1.15, cex.lab=1.3, col=clr,\n       fill=TRUE, fill.alpha=0.15, \n       prefix=\"Dimension \")\n#&gt; Vector scale factor set to  5.25\ntext(plridge$coef[,5:6], lambdaf, pos=2, cex=1.3)\n\n\n\n\n\n\nFigure 8.17: Biplot view of the ridge trace plot for the smallest two dimensions, where the effects of shrinkage are most apparent.\n\n\n\n\nThe biplot view in Figure 8.17 showing the two smallest dimensions is particularly useful for understanding how the predictors contribute to shrinkage in ridge regression. Here, Year and Population largely contribute to dimension 5; a contrast between (Year, Population) and GNP contributes to dimension 6.",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#what-have-we-learned",
    "href": "08-collinearity-ridge.html#what-have-we-learned",
    "title": "8  Collinearity & Ridge Regression",
    "section": "\n8.8 What have we learned?",
    "text": "8.8 What have we learned?\nThis chapter has considered the problems in regression models which stem from high correlations among the predictors. We saw that collinearity results in unstable estimates of coefficients with larger uncertainty, often dramatically more so than would be the case if the predictors were uncorrelated. Collinearity can be seen as merely a “data problem” which can safely be ignored if we are only interested in prediction. When we want to understand a model, ridge regression can tame the collinearity beast by shrinking the coefficients slightly to gain greater precision in the estimates.\nBeyond these statistical considerations, the methods of this chapter highlight the roles of multivariate thinking and visualization in understanding these phenomena and the methods developed for solving them. Data ellipses and confidence ellipses for coefficients again provide tools for visualizing what is concealed in numerical summaries. A perhaps surprising feature of both collinearity and ridge regression is that the important information usually resides in the smallest PCA dimensions and biplots help again to understand these dimensions.\n\nPackages used here:\n12 packages used here: car, carData, dplyr, factoextra, genridge, ggplot2, ggrepel, knitr, MASS, patchwork, splines, VisCollin\n\n\n\n\n\n\nBelsley, D. A. (1991). Conditioning diagnostics: Collinearity and weak data in regression. Wiley.\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley; Sons.\n\n\nBrown, P. J., & Zidek, J. V. (1980). Adaptive multivariate ridge regression. The Annals of Statistics, 8(1), 64–74. http://www.jstor.org/stable/2240743\n\n\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. The Annals of Statistics, 32(2), 407–499.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (Third edition.). SAGE.\n\n\nFox, J., & Monette, G. (1992). Generalized collinearity diagnostics. Journal of the American Statistical Association, 87(417), 178–183.\n\n\nFriedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K., Simon, N., & Yang, J. (2025). Glmnet: Lasso and elastic-net regularized generalized linear models. https://glmnet.stanford.edu\n\n\nFriendly, M. (2011). Generalized ridge trace plots: Visualizing bias and precision with the genridge R package. SCS Seminar.\n\n\nFriendly, M. (2013). The generalized ridge trace plot: Visualizing bias and precision. Journal of Computational and Graphical Statistics, 22(1), 50–68. https://doi.org/10.1080/10618600.2012.681237\n\n\nFriendly, M. (2024). Genridge: Generalized ridge trace plots for ridge regression. https://github.com/friendly/genridge\n\n\nFriendly, M., & Kwan, E. (2009). Where’s Waldo: Visualizing collinearity diagnostics. The American Statistician, 63(1), 56–65. https://doi.org/10.1198/tast.2009.0012\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with application to principal components analysis. Biometrics, 58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGoeman, J., Meijer, R., Chaturvedi, N., & Lueder, M. (2022). Penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the cox model. https://CRAN.R-project.org/package=penalized\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman & Hall.\n\n\nGraybill, F. A. (1961). An introduction to linear statistical models. McGraw-Hill.\n\n\nHaitovsky, Y. (1987). On multivariate ridge regression. Biometrika, 74(3), 563–570. https://doi.org/10.1093/biomet/74.3.563\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference and prediction (2nd ed.). Springer. http://www-stat.stanford.edu/~tibs/ElemStatLearn/\n\n\nHocking, R. R. (2013). Methods and applications of linear models: Regression and the analysis of variance. Wiley. https://books.google.ca/books?id=iq2J-1iS6HcC\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12, 55–67.\n\n\nHoerl, A. E., Kennard, R. W., & Baldwin, K. F. (1975). Ridge regression: Some simulations. Communications in Statistics, 4(2), 105–123. https://doi.org/10.1080/03610927508827232\n\n\nKwan, E., Lu, I. R. R., & Friendly, M. (2009). Tableplot: A new tool for assessing precise predictions. Zeitschrift für Psychologie / Journal of Psychology, 217(1), 38–48. https://doi.org/10.1027/0044-3409.217.1.38\n\n\nLawless, J. F., & Wang, P. (1976). A simulation study of ridge and other regression estimators. Communications in Statistics, 5, 307–323.\n\n\nLongley, J. W. (1967). An appraisal of least squares programs for the electronic computer from the point of view of the user. Journal of the American Statistical Association, 62, 819–841. https://doi.org/https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896\n\n\nMarquardt, D. W. (1970). Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation. Technometrics, 12, 591–612.\n\n\nMarquardt, D. W., & Snee, R. D. (1975). Ridge regression in practice. The American Statistician, 29(1), 3–20. https://doi.org/10.1080/00031305.1975.10479105\n\n\nMcDonald, G. C. (2009). Ridge regression. Wiley Interdisciplinary Reviews: Computational Statistics, 1(1), 93–100. https://doi.org/10.1002/wics.14\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B: Methodological, 58, 267–288.\n\n\nVinod, H. D. (1978). A survey of ridge regression and related techniques for improvements over ordinary least squares. The Review of Economics and Statistics, 60(1), 121–131. http://www.jstor.org/stable/1924340",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "08-collinearity-ridge.html#footnotes",
    "href": "08-collinearity-ridge.html#footnotes",
    "title": "8  Collinearity & Ridge Regression",
    "section": "",
    "text": "This example is adapted from one by John Fox (2022), Collinearity Diagnostics↩︎",
    "crumbs": [
      "Univariate Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collinearity & Ridge Regression</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html",
    "href": "09-hotelling.html",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "",
    "text": "9.1 \\(T^2\\) as a generalized \\(t\\)-test\nJust as the one- and two- sample univariate \\(t\\)-test is the gateway drug for understanding analysis of variance, so too Hotelling’s \\(T^2\\) test provides an entry point to multivariate analysis of variance. This simple case provides an entry point to understanding the collection of methods I call the HE plot framework for visualizing effects in multivariate linear models, which are a main focus of this book.\nThe essential idea is that Hotelling’s \\(T^2\\) provides a test of the difference in means between two groups on a collection of variables, \\(\\mathbf{x} = x_1, x_2, \\dots x_p\\) simultaneously, rather than one by one. This has the advantages that it:\nAfter describing it’s features, I use an example of a two-group \\(T^2\\) test to illustrate the basic ideas behind multivariate tests and hypothesis error plots. Then, we’ll dip our toes into the visual ideas for representing the statistical quantities involved in such tests.\nPackages\nIn this chapter we use the following packages. Load them now.\nHotelling’s \\(T^2\\) (Hotelling, 1931) is an analog of the square of a univariate \\(t\\) statistic, extended to the case of two or more response variables tested together. Consider the basic one-sample \\(t\\)-test, where we wish to test the hypothesis that the mean \\(\\bar{x}\\) of a set of \\(N\\) measures on a test of basic math, with standard deviation \\(s\\) does not differ from an assumed mean \\(\\mu_0 = 150\\) for a population. The \\(t\\) statistic for testing \\(\\mathcal{H}_0 : \\mu = \\mu_0\\) against the two-sided alternative, \\(\\mathcal{H}_0 : \\mu \\ne \\mu_0\\) is \\[\nt = \\frac{(\\bar{x} - \\mu_0)}{s / \\sqrt{N}} = \\frac{(\\bar{x} - \\mu_0)\\sqrt{N}}{s}\n\\]\nSquaring this gives\n\\[\nt^2 = \\frac{N (\\bar{x} - \\mu_0)^2}{s^2} = N (\\bar{x} - \\mu_0)(s^2)^{-1} (\\bar{x} - \\mu_0)\n\\]\nNow consider we also have measures on a test of solving word problems for the same sample. Then, a hypothesis test for the means on basic math (BM) and word problems (WP) is the test of the means of these two variables jointly equal some specified values, say, \\((\\mu_{0,BM}=150,\\; \\mu_{0,WP} =100)\\):\n\\[\n\\mathcal{H}_0 : \\mathbf{\\mu} = \\mathbf{\\mu_0} =\n  \\begin{pmatrix}\n    \\mu_{0, \\text{BM}} \\\\ \\mu_{0,\\text{WP}}\n  \\end{pmatrix}\n  =\n  \\begin{pmatrix}\n    150 \\\\ 100\n  \\end{pmatrix}\n\\]\nHotelling’s \\(T^2\\) is then the analog of \\(t^2\\), with the variance-covariance matrix \\(\\mathbf{S}\\) of the scores on (BM, WP) replacing the variance of a single score. This is nothing more than the squared Mahalanobis \\(D^2_M\\) distance between the sample mean vector \\((\\bar{x}_\\text{BM}, \\bar{x}_\\text{WP})^\\mathsf{T}\\) and the hypothesized means \\(\\mathbf{\\mu}_0\\), in the metric of \\(\\mathbf{S}\\), as shown in Figure 9.1.\n\\[\\begin{aligned}\nT^2 &= N (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)^\\mathsf{T} \\; \\mathbf{S}^{-1} \\; (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0) \\\\\n    &= N D^2_M (\\bar{\\mathbf{x}}, \\mathbf{\\mu}_0)\n\\end{aligned}\\]\nFigure 9.1: Hotelling’s T^2 statistic as the squared distance between the sample means and hypothesized means relative to the variance-covariance matrix. Source: Author",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#sec-t2-properties",
    "href": "09-hotelling.html#sec-t2-properties",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.2 \\(T^2\\) properties",
    "text": "9.2 \\(T^2\\) properties\nAside from it’s elegant geometric interpretation Hotelling’s \\(T^2\\) has simple properties that aid in understanding the extension to more complex multivariate tests.\n\nMaximum \\(t^2\\) : Consider constructing a new variable \\(w\\) as a linear combination of the scores in a matrix \\(\\mathbf{X} = [ \\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}]\\) with weights \\(\\mathbf{a}\\), \\[\nw = a_1 \\mathbf{x_1} + a_2 \\mathbf{x_2} + \\dots + a_p \\mathbf{x_p} = \\mathbf{X} \\mathbf{a}\n\\] Hotelling’s \\(T^2\\) is then the maximum value of a univariate \\(t^2 (\\mathbf{a})\\) over all possible choices of the weights in \\(\\mathbf{a}\\). In this way, Hotellings test reduces a multivariate problem to a univariate one.\nEigenvalue : Hotelling showed that \\(T^2\\) is the one non-zero eigenvalue (latent root) \\(\\lambda\\) of the matrix \\(\\mathbf{Q}_H = N (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)^\\mathsf{T}  (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)\\) relative to \\(\\mathbf{Q}_E = \\mathbf{S}\\) that solves the equation \\[\n(\\mathbf{Q}_H - \\lambda \\mathbf{Q}_E) \\mathbf{a} = 0\n\\tag{9.1}\\] In more complex MANOVA problems, there are more than one non-zero latent roots, \\(\\lambda_1, \\lambda_2, \\dots \\lambda_s\\), and test statistics (Wilks’ \\(\\Lambda\\), Pillai and Hotelling-Lawley trace criteria, Roy’s maximum root test) are functions of these.\nEigenvector : The corresponding eigenvector is \\(\\mathbf{a} = \\mathbf{S}^{-1} (\\bar{\\mathbf{x}} - \\mathbf{\\mu}_0)\\). These are the (raw) discriminant coefficients, giving the relative contribution of each variable to \\(T^2\\).\nCritical values : For a single response, the square of a \\(t\\) statistic with \\(N-1\\) degrees of freedom is an \\(F (1, N-1)\\) statistic. But we chose \\(\\mathbf{a}\\) to give the maximum \\(t^2 (\\mathbf{a})\\); this can be taken into account with a transformation of \\(T^2\\) to give an exact \\(F\\) test with the correct sampling distribution: \\[\nF^* = \\frac{N - p}{p (N-1)} T^2 \\; \\sim \\; F (p, N - p)\n\\tag{9.2}\\]\nInvariance under linear transformation : Just as a univariate \\(t\\)-test is unchanged if we apply a linear transformation to the variable, \\(x \\rightarrow a x + b\\), \\(T^2\\) is invariant under all linear (affine) transformations, \\[\n\\mathbf{x}_{p \\times 1} \\rightarrow \\mathbf{C}_{p \\times p} \\mathbf{x} + \\mathbf{b}\n\\] So, you get the same results if you convert penguins flipper lengths from millimeters to centimeters or inches. The same is true for all MANOVA tests.\nTwo-sample tests : With minor variations in notation, everything above applies to the more usual test of equality of multivariate means in a two sample test of \\(\\mathcal{H}_0 : \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2\\). \\[\nT^2 = N (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)^\\mathsf{T} \\; \\mathbf{S}_p^{-1} \\; (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)\n\\] where \\(\\mathbf{S}_p\\) is the pooled within-sample variance covariance matrix.\n\nExample\nThe data set heplots::mathscore gives (fictitious) scores on a test of basic math skills (BM) and solving word problems (WP) for two groups of \\(N=6\\) students in an algebra course, each taught by different instructors. The null hypothesis is that the means are equal for both variables, \\(\\mathcal{H}_0: \\mu_{\\text{BM}} = \\mu_{\\text{WP}}\\).\n\ndata(mathscore, package = \"heplots\")\nstr(mathscore)\n#&gt; 'data.frame':  12 obs. of  3 variables:\n#&gt;  $ group: Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 2 2 2 2 ...\n#&gt;  $ BM   : int  190 170 180 200 150 180 160 190 150 160 ...\n#&gt;  $ WP   : int  90 80 80 120 60 70 120 150 90 130 ...\n\nYou can carry out the test that the means for both variables are jointly equal across groups using either Hotelling::hotelling.test() (Curran & Hersh, 2021) or car::Anova(), but the latter is more generally useful\n\nhotelling.test(cbind(BM, WP) ~ group, data=mathscore) |&gt; print()\n#&gt; Test stat:  64.174 \n#&gt; Numerator df:  2 \n#&gt; Denominator df:  9 \n#&gt; P-value:  0.0001213\n\nmath.mod &lt;- lm(cbind(BM, WP) ~ group, data=mathscore)\nAnova(math.mod)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;       Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; group  1     0.865     28.9      2      9 0.00012 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWhat’s wrong with just doing the two \\(t\\)-tests (or equivalent \\(F\\)-test with lm())?\n\nAnova(mod1 &lt;- lm(BM ~ group, data=mathscore))\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: BM\n#&gt;           Sum Sq Df F value Pr(&gt;F)  \n#&gt; group       1302  1    4.24  0.066 .\n#&gt; Residuals   3071 10                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnova(mod2 &lt;- lm(WP ~ group, data=mathscore))\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: WP\n#&gt;           Sum Sq Df F value Pr(&gt;F)   \n#&gt; group       4408  1    10.4  0.009 **\n#&gt; Residuals   4217 10                  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFrom this, we might conclude that the two groups do not differ significantly on Basic Math but strongly differ on Word problems. But the two univariate tests do not take the correlation among the mean differences into account.\nIf you want to just extract the \\(t\\)-tests, here’s a handy trick using broom::tidy.mlm(), which summarizes the test statistics for each response and each term in a MLM. The mean difference shown below is that for group 2 - group 1.\n\ntidy(math.mod) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  select(-term) |&gt;\n  rename(Mean_diff = estimate,\n         t = statistic) |&gt;\n  mutate(signif = noquote(gtools::stars.pval(p.value)))\n#&gt; # A tibble: 2 × 6\n#&gt;   response Mean_diff std.error     t p.value signif   \n#&gt;   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;noquote&gt;\n#&gt; 1 BM           -20.8      10.1 -2.06 0.0665  .        \n#&gt; 2 WP            38.3      11.9  3.23 0.00897 **\n\nTo see the differences between the groups on both variables together, we draw their data (68%) ellipses, using heplots::covEllipses(). (Setting pooled=FALSE here omits drawing the the ellipse for the pooled covariance matrix \\(\\mathbf{S}_p\\).)\n\n\ncolors &lt;- c(\"darkgreen\", \"blue\")\ncovEllipses(mathscore[,c(\"BM\", \"WP\")], mathscore$group,\n            pooled = FALSE, \n            col = colors,\n            fill = TRUE, \n            fill.alpha = 0.05,\n            cex = 2, cex.lab = 1.5,\n            asp = 1,\n            xlab=\"Basic math\", ylab=\"Word problems\")\n# plot points\npch &lt;- ifelse(mathscore$group==1, 15, 16)\ncol &lt;- ifelse(mathscore$group==1, colors[1], colors[2])\npoints(mathscore[,2:3], pch=pch, col=col, cex=1.25)\n\n\n\n\n\n\nFigure 9.2: Data ellipses for the mathscore data, enclosing approximately 68% of the observations in each group\n\n\n\n\nWe can see that:\n\nGroup 1 &gt; Group 2 on Basic Math, but worse on Word Problems\nGroup 2 &gt; Group 1 on Word Problems, but worse on Basic Math\nWithin each group, those who do better on Basic Math also do better on Word Problems\n\nWe can also see why the univariate test, at least for Basic math is non-significant: the scores for the two groups overlap considerably on the horizontal axis. They are slightly better separated along the vertical axis for word problems. The plot also reveals why Hotelling’s \\(T^2\\) reveals such a strongly significant result: the two groups are very widely separated along an approximately 45\\(^o\\) line between them.\nA relatively simple interpretation is that the groups don’t really differ in overall math ability, but perhaps the instructor in Group 1 put more focus on basic math skills, while the instructor for Group 2 placed greater emphasis on solving word problems.\nIn Hotelling’s \\(T^2\\), the “size” of the difference between the means (labeled “1” and “2”) is assessed relative to the pooled within-group covariance matrix \\(\\mathbf{S}_p\\), which is just a size-weighted average of the two within-sample matrices, \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\),\n\\[\n\\mathbf{S}_p = [ (n_1 - 1) \\mathbf{S}_1 + (n_2 - 1) \\mathbf{S}_2 ] / (n_1 + n_2 - 2) \\:\\: .\n\\]\nVisually, imagine sliding the the separate data ellipses to the grand mean, \\((\\bar{x}_{\\text{BM}}, \\bar{x}_{\\text{WP}})\\) and finding their combined data ellipse. This is just the data ellipse of the sample of deviations of the scores from their group means, or that of the residuals from the model lm(cbind(BM, WP) ~ group, data=mathscore)\nTo see this, we plot \\(\\mathbf{S}_1\\), \\(\\mathbf{S}_2\\) and \\(\\mathbf{S}_p\\) together,\n\ncovEllipses(mathscore[,c(\"BM\", \"WP\")], mathscore$group,\n            col = c(colors, \"red\"),\n            fill = c(FALSE, FALSE, TRUE), \n            fill.alpha = 0.3,\n            cex = 2, cex.lab = 1.5,\n            asp = 1,\n            xlab=\"Basic math\", ylab=\"Word problems\")\n\n\n\n\n\n\nFigure 9.3: Data ellipses and the pooled covariance matrix mathscore data.\n\n\n\n\nOne of the assumptions of the \\(T^2\\) test (and of MANOVA) is that the within-group variance covariance matrices, \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\), are the same. In Figure 9.3, you can see how the shapes of \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\) are very similar, differing in that the variance of word Problems is slightly greater for group 2. In Chapter XX we take of the topic of visualizing tests of this assumption, based on Box’s \\(M\\)-test.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#sec-t2-heplot",
    "href": "09-hotelling.html#sec-t2-heplot",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.3 HE plot and discriminant axis",
    "text": "9.3 HE plot and discriminant axis\nAs we describe in detail in Chapter 11, all the information relevant to the \\(T^2\\) test and MANOVA can be captured in the remarkably simple Hypothesis Error plot, which shows the relative size of two data ellipses,\n\n\n\\(\\mathbf{H}\\): the data ellipse of the fitted values, which are just the group means on the two variables, \\(\\bar{\\mathbf{x}}\\), corresponding to \\(\\mathbf{Q}_H\\) in Equation 9.1. In case of \\(T^2\\), the \\(\\mathbf{H}\\) matrix is of rank 1, so the “ellipse” plots as a line.\n\n\n# calculate H directly\nfit &lt;- fitted(math.mod)\nxbar &lt;- colMeans(mathscore[,2:3])\nN &lt;- nrow(mathscore)\ncrossprod(fit) - N * outer(xbar, xbar)\n#&gt;       BM    WP\n#&gt; BM  1302 -2396\n#&gt; WP -2396  4408\n\n# same as: SSP for group effect from Anova\nmath.aov &lt;- Anova(math.mod)\n(H &lt;- math.aov$SSP)\n#&gt; $group\n#&gt;       BM    WP\n#&gt; BM  1302 -2396\n#&gt; WP -2396  4408\n\n\n\n\\(\\mathbf{E}\\): the data ellipse of the residuals, the deviations of the scores from the group means, \\(\\mathbf{x} - \\bar{\\mathbf{x}}\\), corresponding to \\(\\mathbf{Q}_E\\).\n\n\n# calculate E directly\nresids &lt;- residuals(math.mod)\ncrossprod(resids)\n#&gt;      BM   WP\n#&gt; BM 3071 2808\n#&gt; WP 2808 4217\n\n# same as: SSPE from Anova\n(E &lt;- math.aov$SSPE)\n#&gt;      BM   WP\n#&gt; BM 3071 2808\n#&gt; WP 2808 4217\n\n\n9.3.1 heplot()\n\nheplots::heplot() takes the model object, extracts the \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) matrices (from summary(Anova(math.mod))) and plots them. There are many options to control the details.\n\nheplot(math.mod, \n       fill=TRUE, lwd = 3,\n       asp = 1,\n       cex=2, cex.lab=1.8,\n       xlab=\"Basic math\", ylab=\"Word problems\")\n\n\n\n\n\n\nFigure 9.4: Hypothesis error plot of the mathscore data. The line through the group means is the H ellipse, which plots as a line here. The red ellipse labeled ‘Error’ represents the pooled within-group covariance matrix.\n\n\n\n\nBut the HE plot offers more:\n\nA visual test of significance: the \\(\\mathbf{H}\\) ellipse is scaled so that it projects anywhere outside the \\(\\mathbf{E}\\) ellipse, if and only if the test is significant at a given \\(\\alpha\\) level (\\(\\alpha = 0.05\\) by default)\nThe \\(\\mathbf{H}\\) ellipse, which appears as a line, goes through the means of the two groups. This is also the discriminant axis, the direction in the space of the variables which maximally discriminates between the groups. That is, if we project the data points onto this line, we get the linear combination \\(w\\) which has the maximum possible univariate \\(t^2\\).\n\nYou can see how the HE plot relates to the plots of the separate data ellipses by overlaying them in a single figure. We also plot the scores on the discriminant axis, by using this small function to find the orthogonal projection of a point \\(\\mathbf{a}\\) on the line joining two points, \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\), which in math is \\(\\mathbf{p}_1 + \\frac{\\mathbf{d}^\\mathsf{T} (\\mathbf{a} - \\mathbf{p}_1)} {\\mathbf{d}^\\mathsf{T} \\mathbf{d}}\\), letting \\(\\mathbf{d} = \\mathbf{p}_1 - \\mathbf{p}_2\\).\n\ndot &lt;- function(x, y) sum(x*y)       # dot product of two vectors\nproject_on &lt;- function(a, p1, p2) {\n  a &lt;- as.numeric(a)\n  p1 &lt;- as.numeric(p1)\n  p2 &lt;- as.numeric(p2)\n  t &lt;- dot(p2-p1, a-p1) / dot(p2-p1, p2-p1)\n  C &lt;- p1 + t*(p2-p1)\n  C\n}\n\nThen, we run the same code as before to plot the data ellipses, and follow this with a call to heplot() using the option add=TRUE which adds to an existing plot. Following this, we find the group means and draw lines projecting the points on the line between them.\n\ncovEllipses(mathscore[,c(\"BM\", \"WP\")], mathscore$group,\n            pooled=FALSE, \n            col = colors,\n            cex=2, cex.lab=1.5,\n            asp=1, \n            xlab=\"Basic math\", ylab=\"Word problems\"\n            )\npch &lt;- ifelse(mathscore$group==1, 15, 16)\ncol &lt;- ifelse(mathscore$group==1, \"red\", \"blue\")\npoints(mathscore[,2:3], pch=pch, col=col, cex=1.25)\n\n# overlay with HEplot (add = TRUE)\nheplot(math.mod, \n       fill=TRUE, \n       cex=2, cex.lab=1.8, \n       fill.alpha=0.2, lwd=c(1,3),\n       add = TRUE, \n       error.ellipse=TRUE)\n\n# find group means\nmeans &lt;- mathscore |&gt;\n  group_by(group) |&gt;\n  summarize(BM = mean(BM), WP = mean(WP))\n\nfor(i in 1:nrow(mathscore)) {\n  gp &lt;- mathscore$group[i]\n  pt &lt;- project_on( mathscore[i, 2:3], means[1, 2:3], means[2, 2:3]) \n  segments(mathscore[i, \"BM\"], mathscore[i, \"WP\"], pt[1], pt[2], lwd = 1.2)\n}\n\n\n\n\n\n\nFigure 9.5: HE plot overlaid on top of the within-group data ellipses, with lines showing the projection of each point on the discriminant axis.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#sec-t2-discrim",
    "href": "09-hotelling.html#sec-t2-discrim",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.4 Discriminant analysis",
    "text": "9.4 Discriminant analysis\nDiscriminant analysis for two-group designs or for one-way MANOVA essentially turns the problem around: Instead of asking whether the mean vectors for two or more groups are equal, discriminant analysis tries to find the linear combination \\(w\\) of the response variables that has the greatest separation among the groups, allowing cases to be best classified.\n\nFor the mathscore data, you can perform the discriminant analysis as follows, using the MASS function lda():\n\n(math.lda &lt;- MASS::lda(group ~ ., data=mathscore))\n#&gt; Call:\n#&gt; lda(group ~ ., data = mathscore)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;   1   2 \n#&gt; 0.5 0.5 \n#&gt; \n#&gt; Group means:\n#&gt;    BM    WP\n#&gt; 1 178  83.3\n#&gt; 2 158 121.7\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;        LD1\n#&gt; BM -0.0835\n#&gt; WP  0.0753\n\nThe coefficients give \\(w = -0.084 \\;\\text{BM} + 0.075 \\;\\text{WP}\\). This is exactly the direction given by the line for the \\(\\mathbf{H}\\) ellipse in Figure 9.5.\nTo round this out, we can calculate the discriminant scores by multiplying the matrix \\(\\mathbf{X}\\) by the vector \\(\\mathbf{a} = \\mathbf{S}^{-1} (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)\\) of the discriminant weights. These were shown in Figure 9.5 as the projections of the data points on the line joining the group means,\n\nmath.lda$scaling\n#&gt;        LD1\n#&gt; BM -0.0835\n#&gt; WP  0.0753\n\nscores &lt;- cbind(group = mathscore$group,\n                as.matrix(mathscore[, 2:3]) %*% math.lda$scaling) |&gt;\n  as.data.frame()\nscores |&gt;\n  group_by(group) |&gt;\n  slice(1:3)\n#&gt; # A tibble: 6 × 2\n#&gt; # Groups:   group [2]\n#&gt;   group   LD1\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1 -9.09\n#&gt; 2     1 -8.17\n#&gt; 3     1 -9.01\n#&gt; 4     2 -4.33\n#&gt; 5     2 -4.58\n#&gt; 6     2 -5.75\n\nThen a \\(t\\)-test on these scores gives the same value as Hotelling’s \\(T\\); it is accessed via the statistic component of t.test()\n\nt &lt;- t.test(LD1 ~ group, data=scores)$statistic\nc(t, T2 =t^2)\n#&gt;     t  T2.t \n#&gt; -8.01 64.17\n\nFinally, it is instructive to compare violin plots for the three measures, BM, WP and LD1. To do this with ggplot2 requires reshaping the data from wide to long format so the plots can be faceted.\n\nscores &lt;- mathscore |&gt;\n  bind_cols(LD1 = scores[, \"LD1\"]) \n\nscores |&gt;\n  tidyr::gather(key = \"measure\", value =\"Score\", BM:LD1) |&gt;\n  mutate(measure = factor(measure, levels = c(\"BM\", \"WP\", \"LD1\"))) |&gt;\n  ggplot(aes(x = group, y = Score, color = group, fill = group)) +\n    geom_violin(alpha = 0.2) +\n    geom_jitter(width = .2, size = 2) +\n    facet_wrap( ~ measure, scales = \"free\", labeller = label_both) +\n    scale_fill_manual(values = c(\"darkgreen\", \"blue\")) +\n    scale_color_manual(values = c(\"darkgreen\", \"blue\")) +\n    theme_bw(base_size = 14) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 9.6: Violin plots comparing group 1 and 2 for the two observed measures and the linear discriminant score.\n\n\n\n\nYou can readily see how well the groups are separated on the discriminant axes, relative to the two individual variables.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#sec-t2-more-variables",
    "href": "09-hotelling.html#sec-t2-more-variables",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.5 More variables",
    "text": "9.5 More variables\nThe mathscore data gave a simple example with two outcomes to explain the essential ideas behind Hotelling’s \\(T^2\\) and multivariate tests. Multivariate methods become increasingly useful as the number of response variables increases because it is harder to show them all together and see how they relate to differences between groups.\nA classic example is the dataset mbclust::banknote, containing six size measures made on 100 genuine and 100 counterfeit old-Swiss 1000-franc bank notes (Flury & Riedwyl, 1988). The goal is to see how well the real and fake banknotes can be distinguished. The measures are the Length and Diagonal lengths of a banknote and the Left, Right, Top and Bottom edge margins in mm.\nBefore considering hypothesis tests, let’s look at some exploratory graphics. Figure 9.7 shows univariate violin and boxplots of each of the measures. To make this plot, faceted by measure, I first reshape the data from wide to long and make measure a factor with levels in the order of the variables in the data set.\n\n\ndata(banknote, package= \"mclust\")\nbanknote |&gt;\n  tidyr::gather(key = \"measure\", \n                value = \"Size\", \n                Length:Diagonal) |&gt; \n  mutate(measure = factor(measure, \n                          levels = c(names(banknote)[-1]))) |&gt; \n\n  ggplot(aes(x = Status, y = Size, color = Status)) +\n  geom_violin(aes(fill = Status), alpha = 0.2) +           # (1)\n  geom_jitter(width = .2, size = 1.2) +                    # (2)\n  geom_boxplot(width = 0.25,                               # (3)\n               linewidth = 1.1, \n               color = \"black\", \n               alpha = 0.5) +\n  labs(y = \"Size (mm)\") +\n  facet_wrap( ~ measure, scales = \"free\", labeller = label_both) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nFigure 9.7: Overlaid violin and boxplots of the banknote variables. The violin plots give a sense of the shapes of the distributions, while the boxplots highlight the center and spread.\n\n\n\n\nA quick glance at Figure 9.7 shows that the counterfeit and genuine bills differ in their means on most of the measures, with the counterfeit ones slightly larger on Left, Right, Bottom and Top margins. But univariate plots don’t give an overall sense of how these variables are related to one another.\n\n\n\n\n\n\nGraph craft: Layers and transparency\n\n\n\nFigure 9.7 is somewhat complex, so it is useful to understand the steps needed to make this figure show what I wanted. The plot in each panel contains three layers:\n\nthe violin plot based on a density estimate, showing the shape of each distribution;\nthe data points, but they are jittered horizontally using geom_jiter() because otherwise they would all overlap on the X axis;\nthe boxplot, showing the center (median) and spread (IQR) of each distribution.\n\nIn composing graphs with layers, order matters, and also does the alpha transparency, because each layer adds data ink on top of earlier ones. I plotted these in the order shown because I wanted the violin plot to provide the background, and the boxplot to show a simple univariate summary, not obscured by the other layers. The alpha values allow the data ink to be blended for each layer, and in this case, alpha = 0.5 for the boxplot let the earlier layers show through.\n\n\n\n9.5.1 Biplots\nMultivariate relations among these six variables could be explored in data space using scatterplots or other methods, but I turn to my trusty multivariate juicer, a biplot, to give a 2D summary. Two dimensions account for 70% of the total variance of all the banknotes, while three would give 85%.\n\nbanknote.pca &lt;- prcomp(banknote[, -1], scale = TRUE)\nsummary(banknote.pca)\n#&gt; Importance of components:\n#&gt;                          PC1   PC2   PC3   PC4    PC5    PC6\n#&gt; Standard deviation     1.716 1.131 0.932 0.671 0.5183 0.4346\n#&gt; Proportion of Variance 0.491 0.213 0.145 0.075 0.0448 0.0315\n#&gt; Cumulative Proportion  0.491 0.704 0.849 0.924 0.9685 1.0000\n\nThe biplot in Figure 9.8 gives a nicely coherent overview, at least in two dimensions. The first component shows the positive correlations among the measures of the margins, where the counterfeit bills are larger than the real ones and a negative correlation of the Diagonal with the other measures. The length of bills only distinguishes the types of banknotes on the second dimension.\n\nbanknote.pca &lt;- ggbiplot::reflect(banknote.pca)\nggbiplot(banknote.pca,\n   obs.scale = 1, var.scale = 1,\n   groups = banknote$Status,\n   ellipse = TRUE, \n   ellipse.level = 0.5, \n   ellipse.alpha = 0.1, \n   ellipse.linewidth = 0,\n   varname.size = 4,\n   varname.color = \"black\") +\n  labs(fill = \"Status\", \n       color = \"Status\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = 'top')\n\n\n\n\n\n\nFigure 9.8: Biplot of the banknote variables, showing how the size measurements are related to each other. The points and data ellipses for the component scores are colored by Status, showing how the counterfeit and genuine bills are distinguished by these measures.\n\n\n\n\n\n9.5.2 Testing mean differences\nAs noted above, Hotelling’s \\(T^2\\) is equivalent to a one-way MANOVA, fitting the size measures to the Status of the banknotes. Anova() reports only the \\(F\\)-statistic based on Pillai’s trace criterion.\n\nbanknote.mlm &lt;- lm(cbind(Length, Left, Right, Bottom, Top, Diagonal) ~ Status,\n                    data = banknote)\nAnova(banknote.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;        Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Status  1     0.924      392      6    193 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nYou can see all the multivariate test statistics with the summary() method for \"Anova.mlm\" objects. With two groups, and hence a 1 df test, these all translate into identical \\(F\\)-statistics.\n\nsummary(Anova(banknote.mlm)) |&gt; print(SSP = FALSE)\n#&gt; \n#&gt; Type II MANOVA Tests:\n#&gt; \n#&gt; ------------------------------------------\n#&gt;  \n#&gt; Term: Status \n#&gt; \n#&gt; Multivariate Tests: Status\n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1      0.92      392      6    193 &lt;2e-16 ***\n#&gt; Wilks             1      0.08      392      6    193 &lt;2e-16 ***\n#&gt; Hotelling-Lawley  1     12.18      392      6    193 &lt;2e-16 ***\n#&gt; Roy               1     12.18      392      6    193 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIf you wish, you can extract the univariate \\(t\\)-tests or equivalent \\(F = t^2\\) statistics from the \"mlm\" object using broom::tidy.mlm(). What is given as the estimate is the difference in the mean for the genuine banknotes relative to the counterfeit ones.\n\nbroom::tidy(banknote.mlm) |&gt; \n  filter(term != \"(Intercept)\") |&gt;\n  dplyr::select(-term) |&gt;\n  rename(t = statistic) |&gt;\n  mutate(F = t^2) |&gt;\n  relocate(F, .after = t)\n#&gt; # A tibble: 6 × 6\n#&gt;   response estimate std.error      t      F  p.value\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 Length      0.146    0.0524   2.79   7.77 5.82e- 3\n#&gt; 2 Left       -0.357    0.0445  -8.03  64.5  8.50e-14\n#&gt; 3 Right      -0.473    0.0464 -10.2  104.   6.84e-20\n#&gt; 4 Bottom     -2.22     0.130  -17.1  292.   7.78e-41\n#&gt; 5 Top        -0.965    0.0909 -10.6  113.   3.85e-21\n#&gt; 6 Diagonal    2.07     0.0715  28.9  836.   5.35e-73\n\nThe individual \\(F_{(1, 198)}\\) statistics can be compared to the \\(F_{(6, 193)} = 392\\) value for the overall multivariate test. While all of the individual tests are highly significant, the average of the univariate \\(F\\)s is only 236. The multivariate test gains power by taking the correlations of the size measures into account.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#variance-accounted-for-eta-square-eta2",
    "href": "09-hotelling.html#variance-accounted-for-eta-square-eta2",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.6 Variance accounted for: Eta square (\\(\\eta^2\\))",
    "text": "9.6 Variance accounted for: Eta square (\\(\\eta^2\\))\nIn a univariate multiple regression model, the coefficient of determination \\(R^2 = \\text{SS}_H / \\text{SS}_\\text{Total}\\) gives the proportion of variance accounted for by hypothesized terms in \\(H\\) relative to the total variance. An analog for ANOVA-type models with categorical, group factors as predictors is \\(\\eta^2\\) (Pearson, 1903), defined as \\[\n\\eta^2 = \\frac{\\text{SS}_\\text{Between groups}}{\\text{SS}_\\text{Total}}\n\\] For multivariate response models, the generalization of \\(\\eta^2\\) uses multivariate analogs of these sums of squares, \\(\\mathbf{Q}_H\\) and \\(\\mathbf{Q}_T = \\mathbf{Q}_H + \\mathbf{Q}_E\\), and there are different calculations for a single measure corresponding to the various test statistics (Wilks’ \\(\\Lambda\\), etc.), as described in Chapter 10.\nLet’s calculate the \\(\\eta^2\\) for the multivariate model banknote.mlm with Status as the only predictor, giving \\(\\eta^2 = 0.92\\), or 92% of the total variance.\n\nheplots::etasq(banknote.mlm)\n#&gt;        eta^2\n#&gt; Status 0.924\n\nThis can be compared to the principal components analysis and the biplot in Figure 9.8, where two components (less favorably) accounted for 70% of total variance and it took four PCA dimensions to account for over 90%. The goals of PCA and MANOVA are different, of course, but they are both concerned with accounting for variance of multivariate data. We will meet another multivariate juicer, canonical discriminant analysis in Chapter 11.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#what-weve-learned",
    "href": "09-hotelling.html#what-weve-learned",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.7 What we’ve learned",
    "text": "9.7 What we’ve learned\nThis chapter was designed to illustrate the main ideas for visualizing differences between means on multiple response variables in a two-group design. Hotelling’s \\(T^2\\) is the generalization of a simple univariate \\(t\\)-test and works by combining the responses into a weighted sum that has the maximum possible univariate \\(t\\) for all choices of weights.\nFigure 9.9 summarizes what was shown in Section 9.3 and Section 9.4. The data ellipses for the two groups in the mathscore data summarize the information about means and within-group variances. In the HE plot, the difference between the means is itself summarized by the line through them, which represents the \\(\\mathbf{H} =\\mathbf{Q}_H\\) matrix and within-group variation is represented by the “Error” ellipse which is the \\(\\mathbf{E} = \\mathbf{S}_p = \\mathbf{Q}_E\\) matrix.\n\n\n\n\n\n\n\nFigure 9.9: The Hypothesis Error plot framework for a two-group design. Above: Data ellipses can be summarized in an HE plot showing the pooled within-group error (\\(\\mathbf{E}\\)) ellipse and the \\(\\mathbf{H}\\) ‘ellipse’ for the group means. Below: Observations projected on the line joining the means give discriminant scores which correpond to a one-dimensional canonical space, represented by a boxplot of their scores and arrows reflecting the variable weights.\n\n\n\n\nAs we will see later (Chapter 11), the \\(\\mathbf{H}\\) ellipse is scaled so that it provides a visual test of significance: it projects somewhere outside the \\(\\mathbf{E}\\) ellipse if and only if the means differ significantly. The direction of the line between the means is also the discriminant axis and scores on this axis are weighted sum of the responses that have the greatest possible mean difference.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "09-hotelling.html#exercises",
    "href": "09-hotelling.html#exercises",
    "title": "9  Hotelling’s \\(T^2\\)",
    "section": "\n9.8 Exercises",
    "text": "9.8 Exercises\n\nExercise 9.1 The value of Hotelling’s \\(T^2\\) found by hotelling.test() is 64.17. The value of the equivalent \\(F\\) statistic found by Anova() is 28.9. Verify that Equation 9.2 gives this result.\n\n\nPackages used here:\n11 packages used here: broom, car, carData, corpcor, dplyr, ggbiplot, ggplot2, heplots, Hotelling, knitr, tidyr\n\n\n\n\n\n\nCurran, J., & Hersh, T. (2021). Hotelling: Hotelling’s t^2 test and variants. https://CRAN.R-project.org/package=Hotelling\n\n\nFlury, B., & Riedwyl, H. (1988). Multivariate statistics: A practical approach. Chapman & Hall.\n\n\nHotelling, H. (1931). The generalization of Student’s ratio. The Annals of Mathematical Statistics, 2(3), 360–378. https://doi.org/10.1214/aoms/1177732979\n\n\nPearson, K. (1903). I. Mathematical contributions to the theory of evolution. —XI. On the influence of natural selection on the variability and correlation of organs. Philosophical Transactions of the Royal Society of London, 200(321–330), 1–66. https://doi.org/10.1098/rsta.1903.0001",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hotelling's $T^2$</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html",
    "href": "10-mlm-review.html",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "10.1 Structure of the MLM\nChapter 9 introduced the essential ideas of multivariate analysis in the context of a two-group design using Hotelling’s \\(T^2\\). Through the magical power of multivariate thinking, I extend this here to a “Holy of Holies”, inner sanctuary of the Tabernacle, the awed general Multivariate Linear Model (MLM).1\nThis can be understood as a simple extension of the univariate linear model, with the main difference being that there are multiple response variables considered together, instead of just one, analysed alone. These outcomes might reflect several different ways or scales for measuring an underlying theoretical construct, or they might represent different aspects of some phenomenon that we hope to better understand when they are studied jointly.\nFor example, in the case of different measures, there are numerous psychological scales used to assess depression or anxiety and it may be important to include more than one measure to ensure that the construct has been measured adequately. It would add considerably to our understanding to know if the different outcome measures all had essentially the same relations to the predictor variables, or if they differ across measures.\nIn the second case of various aspects, student “aptitude” or “achievement” reflects competency in different various subjects (reading, math, history, science, …) that are better studied together. We get a better understanding of the factors that influence each of aspects by testing them jointly.\nJust as in univariate analysis there are variously named techniques (ANOVA, regression) that can be applied to several outcomes, depending on the structure of the predictors at hand. For instance, with one or more continuous predictors and multiple response variables, you could use multivariate multiple regression (MMRA) to obtain estimates useful for prediction.\nInstead, if the predictors are categorical factors, multivariate analysis of variance (MANOVA) can be applied to test for differences between groups. Again, this is akin to ANOVA in the univariate context— the same underlying model is utilized, but the tests for terms in the model are multivariate ones for the collection of all response variables, rather than univariate ones for a single response.\nThe main goal of this chapter is to describe the details of the extension of univariate linear models to the case of multiple outcome measures. But the larger goal is to set the stage for the visualization methods using HE plots and low-D views discussed separately in Chapter 11. Some of the example datasets used here will re-appear there, and also in Chapter 12 which concerns some model-diagnostic graphical methods.\nHowever, before considering the details and examples that apply separately to MANOVA and MMRA, it is useful to consider the general features of the multivariate linear model of which these cases are examples.\nTODO: Offer defense against Huang (2019) and others here; cite Huberty & Morris (1989). Or, maybe not!\nPackages\nIn this chapter I use the following packages. Load them now:\nWith \\(p\\) response variables, the multivariate linear model is most easily appreciated as the collection of \\(p\\) linear models, one for each response. We have \\(p\\) outcomes, so why not just consider a separate model for each?\n\\[\\begin{aligned}\n\\mathbf{y}_1 =& \\mathbf{X}\\boldsymbol{\\beta}_1 + \\boldsymbol{\\epsilon}_1 \\\\\n\\mathbf{y}_2 =& \\mathbf{X}\\boldsymbol{\\beta}_2 + \\boldsymbol{\\epsilon}_2 \\\\\n  \\vdots      &  \\\\\n\\mathbf{y}_p =& \\mathbf{X}\\boldsymbol{\\beta}_p + \\boldsymbol{\\epsilon}_p \\\\\n\\end{aligned} \\tag{10.1}\\]\nBut the problems with fitting separate univariate models are that:\nThe model matrix \\(\\mathbf{X}\\) in Equation 10.1 is the same for all responses, but each one gets its own vector \\(\\boldsymbol{\\beta}_j\\) of coefficients for how the predictors in \\(\\mathbf{X}\\) fit a given response \\(\\mathbf{y}_j\\).\nAmong the beauties of multivariate thinking is that we can put these separate equations together in single equation by joining the responses \\(\\mathbf{y}_j\\) as columns in a matrix \\(\\mathbf{Y}\\) and similarly arranging the vectors of coefficients \\(\\boldsymbol{\\beta}_j\\) as columns in a matrix \\(\\mathbf{B}\\).2\nTODO Revise notation here, to be explicit about inclusion of \\(\\boldsymbol{\\beta}_0\\)\nThe MLM then becomes:\n\\[\n\\mathord{\\mathop{\\mathbf{Y}}\\limits_{n \\times p}} =\n\\mathord{\\mathop{\\mathbf{X}}\\limits_{n \\times (q+1)}} \\, \\mathord{\\mathop{\\mathbf{B}}\\limits_{(q+1) \\times p}} + \\mathord{\\mathop{\\mathbf{\\boldsymbol{\\Large\\varepsilon}}}\\limits_{n \\times p}} \\:\\: ,\n\\tag{10.2}\\]\nwhere:\nWriting Equation 10.2 in terms of its elements, we have\n# FIXME: TeXLive 2025 does not like the immediately following latex code\n# Package amsmath Error: Erroneous nesting of equation structures;\n# (amsmath)                trying to recover with `aligned'.\n# \n# See the amsmath package documentation for explanation.\nThe structure of the model matrix \\(\\mathbf{X}\\) is exactly the same as the univariate linear model, and may therefore contain,",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#structure-of-the-mlm",
    "href": "10-mlm-review.html#structure-of-the-mlm",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "They don’t give simultaneous tests for all regressions. The situation is similar to that in a one-way ANOVA, where an overall test for group differences is usually applied before testing individual comparisons to avoid problems of multiple testing: \\(g\\) groups gives \\(g \\times (g-1)/2\\) pairwise tests.\n\nMore importantly, fitting separate univariate models does not take correlations among the \\(\\mathbf{y}\\)s into account.\n\nIt might be the case that the response variables are all essentially measuring the same thing, but perhaps weakly. If so, the multivariate approach can pool strength across the outcomes to detect their common relations to the predictors, giving greater power.\nOn the other hand, perhaps the responses are related in different ways to the predictors. A multivariate approach can help you understand how many different ways there are, and characterize each.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{Y} = (\\mathbf{y}_1 , \\mathbf{y}_2, \\dots ,  \\mathbf{y}_p )\\) is the matrix of \\(n\\) observations on \\(p\\) responses, with typical column \\(\\mathbf{y}_j\\);\n\n\\(\\mathbf{X}\\) is the model matrix with columns \\(\\mathbf{x}_i\\) for \\(q\\) regressors, which typically includes an initial column \\(\\mathbf{x}_0\\) of 1s for the intercept;\n\n\\(\\mathbf{B} = ( \\mathbf{b}_1 , \\mathbf{b}_2 , \\dots,  \\mathbf{b}_p )\\) is a matrix of regression coefficients, one column \\(\\mathbf{b}_j\\) for each response variable;\n\n\\(\\boldsymbol{\\Large\\varepsilon}\\) is a matrix of errors in predicting \\(\\mathbf{Y}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nquantitative predictors, such as age, income, years of education\n\n\ntransformed predictors like \\(\\sqrt{\\text{age}}\\) or \\(\\log{(\\text{income})}\\)\n\n\npolynomial terms: \\(\\text{age}^2\\), \\(\\text{age}^3, \\dots\\) (using poly(age, k) in R)\n\ncategorical predictors (“factors”), such as treatment (Control, Drug A, drug B), or sex; internally a factor with k levels is transformed to k-1 dummy (0, 1) variables, representing comparisons with a reference level, typically the first.\n\ninteraction terms, involving either quantitative or categorical predictors, e.g., age * sex, treatment * sex.\n\n\n10.1.1 Assumptions\nJust as in univariate models, the assumptions of the multivariate linear model almost entirely concern the behavior of the errors (residuals). Let \\(\\mathbf{\\epsilon}_{i}^{\\prime}\\) represent the \\(i\\)th row of \\(\\boldsymbol{\\Large\\varepsilon}\\). Then it is assumed that:\n\n\nNormality: The residuals, \\(\\mathbf{\\epsilon}_{i}^{\\prime}\\) are distributed as multivariate normal, \\(\\mathcal{N}_{p}(\\mathbf{0},\\boldsymbol{\\Sigma})\\), where \\(\\mathbf{\\Sigma}\\) is a non-singular error-covariance matrix.\n\nStatistical tests of multivariate normality of the residuals include the Shapiro-Wilk (Shapiro & Wilk, 1965) and Mardia (1970, 1974) tests (and others, in the MVN package). \nHowever, as in univariate models, the MLM is relatively robust however this is often better assessed visually using a \\(\\chi^2\\) QQ plot of Mahalanobis squared distance against their corresponding \\(\\chi^2_p\\) values for \\(p\\) degrees of freedom using heplots::cqplot().\n\n\nHomoscedasticity: The error-covariance matrix \\(\\mathbf{\\Sigma}\\) is constant across all observations and grouping factors. Graphical methods to show if this assumption is met are described in Chapter 12.\nIndependence: \\(\\mathbf{\\epsilon}_{i}^{\\prime}\\) and \\(\\mathbf{\\epsilon}_{j}^{\\prime}\\) are independent for \\(i\\neq j\\), so knowing the data for case \\(i\\) gives no information about case \\(j\\) (as would be true if the data consisted of pairs of husbands and wives);\nThe predictors, \\(\\mathbf{X}\\), are fixed and measured without error or at least they are independent of the errors, \\(\\boldsymbol{\\Large\\varepsilon}\\).\n\nThese statements are simply the multivariate analogs of the assumptions of normality, constant variance and independence of the errors in univariate models. Note that it is unnecessary to assume that the predictors (regressors, columns of \\(\\mathbf{X}\\)) are normally distributed.\nImplicit in the above is perhaps the most important assumption—that the model has been correctly specified. This means:\n\nLinearity: The form of the relations between each \\(\\mathbf{y}\\) and the \\(\\mathbf{x}\\)s is correct. Typically this means that the relations are linear, but if not, we have specified a correct transformation of \\(\\mathbf{y}\\) and/or \\(\\mathbf{x}\\).\nCompleteness: No relevant predictors have been omitted from the model. For example in the coffee, stress example (Section 7.1.1), omitting stress from the model biases the effect of coffee on heart disease.\nAdditive effects: The combined effect of different predictors is the sum of their individual effects.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#fitting-the-model",
    "href": "10-mlm-review.html#fitting-the-model",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.2 Fitting the model",
    "text": "10.2 Fitting the model\nThe least squares (and also maximum likelihood) solution for the coefficients \\(\\mathbf{B}\\) is given by\n\n\n\n\\[\n\\widehat{\\mathbf{B}} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{Y} \\:\\: .\n\\]\nThis is precisely the same as fitting the separate responses \\(\\mathbf{y}_1 , \\mathbf{y}_2 , \\dots , \\mathbf{y}_p\\), and placing the estimated coefficients \\(\\widehat{\\mathbf{b}}_i\\) as columns in \\(\\widehat{\\mathbf{B}}\\)\n\\[\n\\widehat{\\mathbf{B}} = [ \\widehat{\\mathbf{b}}_1, \\widehat{\\mathbf{b}}_2, \\dots , \\widehat{\\mathbf{b}}_p] \\:\\: .\n\\] In R, we fit the multivariate linear model with lm() simply by giving a collection of response variables y1, y2, ... on the left-hand side of the model formula, wrapped in cbind() which combines them to form a matrix response.\n\nlm(cbind(y1, y2, y3) ~ x1 + x2 + ..., data=)\n\nIn the presence of possible outliers, robust methods are available for univariate linear models (e.g., MASS::rlm()). So too, heplots::robmlm() provides robust estimation in the multivariate case.\n\n10.2.1 Example: Dog food data\nAs a toy example to make these ideas concrete, consider the dataset heplots::dogfood. Here, a dogfood manufacturer wanted to study preference for different dogfood formulas, two of their own (“Old”, “New”) and two from other manufacturers (“Major”, “Alps”).\nIn a between-dog design, each of \\(n=4\\) dogs were presented with a bowl of one formula and the time to start eating and amount eaten were recorded. Greater preference would be seen in a shorter delay to start eating and a greater amount, so these responses are expected to be negatively correlated.\n\ndata(dogfood, package = \"heplots\")\nstr(dogfood)\n#&gt; 'data.frame':  16 obs. of  3 variables:\n#&gt;  $ formula: Factor w/ 4 levels \"Old\",\"New\",\"Major\",..: 1 1 1 1 2 2 2 2 3 3 ...\n#&gt;  $ start  : int  0 1 1 0 0 1 2 3 1 5 ...\n#&gt;  $ amount : int  100 97 88 92 95 85 82 89 77 84 ...\n\nFor this data, boxplots for the two responses provide an initial look, shown in Figure 10.1. Putting these side-by-side makes it easy to see the inverse relation between the medians on the two response variables.\n\nCodedog_long &lt;- dogfood |&gt;\n  pivot_longer(c(start, amount),\n               names_to = \"variable\")\nggplot(data = dog_long, \n       aes(x=formula, y = value, fill = formula)) +\n  geom_boxplot(alpha = 0.2) +\n  geom_point(size = 2.5) +\n  facet_wrap(~ variable, scales = \"free\") +\n  theme_bw(base_size = 14) + \n  theme(legend.position=\"none\")\n\n\n\n\n\n\nFigure 10.1: Boxplots for time to start eating and amount eaten by dogs given one of four dogfood formulas.\n\n\n\n\nAs suggested above, the multivariate model for testing mean differences due to the dogfood formula is fit using lm() on the matrix \\(\\mathbf{Y}\\) constructed with cbind(start, amount).\n\ndogfood.mod &lt;- lm(cbind(start, amount) ~ formula, \n                  data=dogfood) |&gt; \n  print()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cbind(start, amount) ~ formula, data = dogfood)\n#&gt; \n#&gt; Coefficients:\n#&gt;               start   amount\n#&gt; (Intercept)     0.50   94.25\n#&gt; formulaNew      1.00   -6.50\n#&gt; formulaMajor    2.00  -12.25\n#&gt; formulaAlps     1.75  -16.00\n\nBy default, the factor formula is represented by three columns in the \\(\\mathbf{X}\\) matrix that correspond to treatment contrasts, which are comparisons of the Old formula (a baseline level) with each of the others. The coefficients, for example formulaNEW, are the difference in means from those for Old.\nThen, the overall multivariate test that means on both variables do not differ is carried out using car::Anova().\n\ndogfood.aov &lt;- Anova(dogfood.mod) |&gt;\n  print()\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;         Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; formula  3     0.702     2.16      6     24  0.083 .\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe details of these analysis steps are explained below.\n\n10.2.2 Sums of squares\nIn univariate response models, statistical tests and model summaries (like \\(R^2\\)) are based on the familiar decomposition of the total sum of squares \\(SS_T\\) into regression or hypothesis (\\(SS_H\\)) and error (\\(SS_E\\)) sums of squares. In the multivariate linear model each of these becomes a \\(p \\times p\\) matrix \\(SSP\\) containing sums of squares for the \\(p\\) responses on the diagonal and sums of cross products in the off-diagonal. elements. For the MLM this is expressed as:\n\\[\\begin{aligned}\n\\underset{(p\\times p)}{\\mathbf{SSP}_{T}}\n& =  \\mathbf{Y}^{\\top} \\mathbf{Y} - n \\overline{\\mathbf{y}}\\,\\overline{\\mathbf{y}}^{\\top} \\\\\n& =  \\left(\\widehat {\\mathbf{Y}}^{\\top}\\widehat{\\mathbf{Y}} - n\\overline{\\mathbf{y}}\\,\\overline{\\mathbf{y}}^{\\top} \\right) + \\widehat{\\boldsymbol{\\Large\\varepsilon}}^{\\top}\\widehat{\\boldsymbol{\\Large\\varepsilon}} \\\\\n& =   \\mathbf{SSP}_{H} + \\mathbf{SSP}_{E} \\\\\n& \\equiv  \\mathbf{H} + \\mathbf{E} \\:\\: ,\n\\end{aligned} \\tag{10.3}\\]\nwhere,\n\n\n\\(\\overline{\\mathbf{y}}\\) is the \\((p\\times 1)\\) vector of means for the response variables;\n\n\\(\\widehat{\\mathbf{Y}} = \\mathbf{X}\\widehat{\\mathbf{B}}\\) is the matrix of fitted values; and\n\n\\(\\widehat{\\boldsymbol{\\Large\\varepsilon}} = \\mathbf{Y} -\\widehat{\\mathbf{Y}}\\) is the matrix of residuals.\n\nWe can visualize this decomposition in the simple case of a two-group design (for the mathscore data in Section 9.2) as shown in Figure 10.2. Let \\(\\mathbf{y}_{ij}\\) be the vector of \\(p\\) responses for subject \\(j\\) in group \\(i, i=1,\\dots g\\) for \\(j = 1, \\dots n_i\\). Then, using \\(.\\) to represent a subscript averaged over, Equation 10.3 comes from the identity\n\\[\n\\underbrace{(\\mathbf{y}_{ij} - \\mathbf{y}_{\\cdot \\cdot})}_T =\n\\underbrace{(\\overline{\\mathbf{y}}_{i \\cdot} - \\mathbf{y}_{\\cdot \\cdot})}_H +\n\\underbrace{(\\mathbf{y}_{ij} - \\overline{\\mathbf{y}}_{i \\cdot})}_E\n\\tag{10.4}\\]\nwhere each side of Equation 10.4 is squared and summed over observations to give Equation 10.3. In Figure 10.2,\n\nThe total variance \\(\\mathbf{SSP}_T\\) reflects the deviations of the observations \\(\\mathbf{y}_{ij}\\) from the grand mean \\(\\overline{\\mathbf{y}}_{. .}\\) and has the data ellipse shown in gray.\nIn the middle panel, all the observations are represented at their group means, \\(\\overline{\\mathbf{y}}_{i .}\\), the fitted values. Their variance and covariance is then reflected by deviations of the group means (weighted for the number of observations per group) around the grand mean.\nThe right panel then shows the residual variance, which is the variation of the observations \\(\\mathbf{y}_{ij}\\) around their group means, \\(\\overline{\\mathbf{y}}_{i .}\\). Centering the two data ellipses at the centroid \\(\\overline{\\mathbf{y}}_{. .}\\) then gives the ellipse for the \\(\\mathbf{SSP}_E\\), also called the pooled within-group covariance matrix.\n\n\n\n\n\n\n\n\nFigure 10.2: Breakdown of the total \\(\\mathbf{SSP}_{T}\\) into sums of squares and products for between-group hypothesis variance (\\(\\mathbf{SSP}_{H}\\)) and within-group, error variance (\\(\\mathbf{SSP}_{E}\\)).\n\n\n\n\nThe formulas for these sum of squares and products matrices can be shown explicitly as follows, where the notation \\(\\mathbf{z} \\mathbf{z}^\\top\\) generates the \\(p \\times p\\) outer product of a vector \\(\\mathbf{z}\\), giving \\(z_k \\times z_\\ell\\) for all pairs of elements. \\[\n\\mathbf{SSP}_T =\n\\sum_{i=1}^{g} \\sum_{j=1}^{n_{i}}\\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)\\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)^{\\top}\n\\]\n\\[\n\\mathbf{SSP}_H =\n\\sum_{i=1}^{g} \\mathbf{n}_{i}\\left(\\overline{\\mathbf{y}}_{i \\cdot}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)\\left(\\overline{\\mathbf{y}}_{i \\cdot}-\\overline{\\mathbf{y}}_{\\cdot \\cdot}\\right)^{\\top}\n\\]\n\\[\n\\mathbf{SSP}_E =\n\\sum_{i=1}^{g} \\sum_{j=1}^{n_{i}} \\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{i \\cdot}\\right) \\left(\\mathbf{y}_{ij}-\\overline{\\mathbf{y}}_{i \\cdot}\\right)^{\\top}\n\\]\nThis is the decomposition that we visualize in HE plots, where the size and direction of \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) can be represented as ellipsoids.\nBut first, let’s find these results for the example. The easy way is to get them from the result returned by car::Anova(), where the hypothesis \\(\\mathbf{SSP}_{H}\\) for each term in the model is returned as an element in a named list SSP and the error \\(\\mathbf{SSP}_{E}\\) is returned as the matrix SSPE.\n\nSSP_H &lt;- dogfood.aov$SSP |&gt; print()\n#&gt; $formula\n#&gt;         start amount\n#&gt; start    9.69  -70.9\n#&gt; amount -70.94  585.7\nSSP_E &lt;- dogfood.aov$SSPE |&gt; print()\n#&gt;        start amount\n#&gt; start   25.7   11.7\n#&gt; amount  11.7  390.3\n\nYou can calculate these directly as shown below. sweep() is used to subtract the colMeans() from \\(\\mathbf{Y}\\) and \\(\\widehat{\\mathbf{Y}}\\) and crossprod() premultiplies a matrix by its’ transpose.\n\nY &lt;- dogfood[, c(\"start\", \"amount\")]\nYdev &lt;- sweep(Y, 2, colMeans(Y)) |&gt; as.matrix()\nSSP_T &lt;- crossprod(as.matrix(Ydev)) |&gt; print()\n#&gt;        start amount\n#&gt; start   35.4  -59.2\n#&gt; amount -59.2  975.9\n\nfitted &lt;- fitted(dogfood.mod)\nYfit &lt;- sweep(fitted, 2, colMeans(fitted)) |&gt; as.matrix()\nSSP_H &lt;- crossprod(Yfit) |&gt; print()\n#&gt;         start amount\n#&gt; start    9.69  -70.9\n#&gt; amount -70.94  585.7\n\nresiduals &lt;- residuals(dogfood.mod)\nSSP_E &lt;- crossprod(residuals) |&gt; print()\n#&gt;        start amount\n#&gt; start   25.7   11.7\n#&gt; amount  11.7  390.3\n\nThe decomposition of the total sum of squares and products in Equation 10.3 can be shown as:\n\\[\n\\overset{\\mathbf{SSP}_T}\n  {\\begin{pmatrix}\n   35.4 & -59.2 \\\\\n  -59.2 & 975.9 \\\\\n  \\end{pmatrix}}\n=\n\\overset{\\mathbf{SSP}_H}\n  {\\begin{pmatrix}\n    9.69 & -70.94 \\\\\n  -70.94 & 585.69 \\\\\n  \\end{pmatrix}}\n+\n\\overset{\\mathbf{SSP}_E}\n  {\\begin{pmatrix}\n   25.8 &  11.8 \\\\\n   11.8 & 390.3 \\\\\n  \\end{pmatrix}}\n\\]\n\n10.2.3 How big is \\(SS_H\\) compared to \\(SS_E\\)?\nIn a univariate response model, \\(SS_H\\) and \\(SS_E\\) are both scalar numbers and the univariate \\(F\\) test statistic, \\[F = \\frac{\\text{SS}_H/\\text{df}_h}{\\text{SS}_E/\\text{df}_e} = \\frac{\\mathsf{Var}(H)}{\\mathsf{Var}(E)} \\:\\: ,\n\\] assesses “how big” \\(\\text{SS}_H\\) is, relative to \\(\\text{SS}_E\\), the variance accounted for by a hypothesized model or model terms relative to error variance. The measure \\(R^2 = SS_H / (SS_H + SS_E) = SS_H / SS_T\\) gives the proportion of total variance accounted for by the model terms.\nIn the multivariate analog \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) are both \\(p \\times p\\) matrices, and \\(\\mathbf{H}\\) “divided by” \\(\\mathbf{E}\\) becomes \\(\\mathbf{H}\\mathbf{E}^{-1}\\). The answer, “how big” \\(\\text{SS}_H\\) is compared to \\(\\text{SS}_E\\) is expressed in terms of the \\(p\\) eigenvalues \\(\\lambda_i, i = 1, 2, \\dots p\\) of \\(\\mathbf{H}\\mathbf{E}^{-1}\\). These are the \\(p\\) values \\(\\lambda\\) which solve the determinant equation \\[\n\\mathrm{det}(\\mathbf{H}\\mathbf{E}^{-1} - \\lambda \\mathbf{I}) = 0 \\:\\: .\n\\] The solution also gives the \\(\\lambda_i\\) as the eigenvalues, with vectors \\(\\mathbf{v}_i\\) as the corresponding eigenvectors, \\[\n\\mathbf{H}\\mathbf{E}^{-1} \\; \\lambda_i = \\lambda_i \\mathbf{v}_i \\:\\: .\n\\tag{10.5}\\]\nThis can also be expressed in terms of the size of \\(\\mathbf{H}\\) relative to total variation \\((\\mathbf{H}+ \\mathbf{E})\\) as\n\\[\n\\mathbf{H}(\\mathbf{H}+\\mathbf{E}+)^{-1} \\; \\rho_i = \\rho_i \\mathbf{v}_i \\:\\: ,\n\\tag{10.6}\\]\nwhich has the same eigenvectors as Equation 10.5 and the eigenvalues are \\(\\rho_i = \\lambda_i / (1 + \\lambda_i)\\).\nHowever, when the hypothesized model terms have \\(\\text{df}_h\\) degrees of freedom (columns of the \\(\\mathbf{X}\\) matrix for that term), \\(\\mathbf{H}\\) is of rank \\(\\text{df}_h\\), so only \\(s=\\min(p, \\text{df}_h)\\) eigenvalues can be non-zero. For example, a test for a hypothesis about a single quantitative predictor \\(\\mathbf{x}\\), has \\(\\text{df}_h = 1\\) degree of freedom and \\(\\mathrm{rank} (\\mathbf{H}) = 1\\); for a factor with \\(g\\) groups, \\(\\text{df}_h = \\mathrm{rank} (\\mathbf{H}) = g-1\\).\nFor the dogfood data, we get the following results:\n\nHEinv &lt;- SSP_H %*% solve(SSP_E) |&gt; print()\n#&gt;         start amount\n#&gt; start   0.466 -0.196\n#&gt; amount -3.488  1.606\neig &lt;- eigen(HEinv)\neig$values\n#&gt; [1] 2.0396 0.0317\n\n# as proportions\neig$values / sum(eig$values)\n#&gt; [1] 0.9847 0.0153\n\nThe factor formula has four levels and therefore \\(\\text{df}_h = 3\\) degrees of freedom. But there are only \\(p = 2\\) responses, so there are \\(s=\\min(p, \\text{df}_h) = 2\\) eigenvalues (and corresponding eigenvectors). The eigenvalues tell us that 98.5% of the hypothesis variance due to formula can be accounted for by a single dimension.\n\n\n\n\n\n\n\n\nThe overall multivariate test for the model in Equation 10.2 is essentially a test of the hypothesis \\(\\mathcal{H}_0: \\mathbf{B} = 0\\) (excluding the row for the intercept). Equivalently, this is a test based on the incremental \\(\\mathbf{SSP}_{H}\\) for the hypothesized terms in the model—that is, the difference between the \\(\\mathbf{SSP}_{H}\\) for the full model and the null, intercept-only model. The same idea can be applied to test the difference between any pair of nested models—the added contribution of terms in a larger model relative to a smaller model containing a subset of terms.\nThe eigenvectors \\(\\mathbf{v}_i\\) in Equation 10.5 are also important. These are the weights for the variables in a linear combination \\(v_{i1} \\mathbf{y}_1 + v_{i2} \\mathbf{y}_2 + \\cdots + v_{ip} \\mathbf{y}_p\\) which produces the largest univariate \\(F\\) statistic for the \\(i\\)-th dimension. We exploit this in canonical discriminant analysis and the corresponding canonical HE plots (Section 11.7).\nThe eigenvectors of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) for the dogfood model are shown below:\n\nrownames(eig$vectors) &lt;- rownames(HEinv)\ncolnames(eig$vectors) &lt;- paste(\"Dim\", 1:2)\neig$vectors\n#&gt;         Dim 1  Dim 2\n#&gt; start   0.123 -0.411\n#&gt; amount -0.992 -0.911\n\nThe first column corresponds to the weighted sum \\(0.12 \\times\\text{start} - 0.99 \\times \\text{amount}\\), which as we saw above accounts for 95.5% of the differences in the group means.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#sec-multivar-tests",
    "href": "10-mlm-review.html#sec-multivar-tests",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.3 Multivariate test statistics",
    "text": "10.3 Multivariate test statistics\nIn the univariate case, the overall \\(F\\)-test of \\(\\mathcal{H}_0: \\boldsymbol{\\beta} = \\mathbf{0}\\) is the uniformly most powerful invariant test when the assumptions are met. There is nothing better. This is not the case in the MLM.\nThe reason is that when there are \\(p &gt; 1\\) response variables, and we are testing a hypothesis comprising \\(\\text{df}_h &gt;1\\) coefficients or degrees of freedom, there are \\(s &gt; 1\\) possible dimensions in which \\(\\mathbf{H}\\) can be large relative to \\(\\mathbf{E}\\), each measured by the eigenvalue \\(\\lambda_i\\). There are several test statistics that combine these into a single measure, shown in Table 10.1.\n\n\n\n\n\n\n\n\n\n\nTable 10.1: Test statistics for multivariate tests combine the size of dimensions of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) into a single measure.\n\n\n\n\nCriterion\nFormula\nPartial \\(\\eta^2\\)\n\n\n\n\n\nWilks’s \\(\\Lambda\\)\n\n\\(\\Lambda = \\prod^s_i \\frac{1}{1+\\lambda_i}\\)\n\\(\\eta^2 = 1-\\Lambda^{1/s}\\)\n\n\n\nPillai trace\n\\(V = \\sum^s_i \\frac{\\lambda_i}{1+\\lambda_i}\\)\n\\(\\eta^2 = \\frac{V}{s}\\)\n\n\n\nHotelling-Lawley trace\n\\(H = \\sum^s_i \\lambda_i\\)\n\\(\\eta^2 = \\frac{H}{H+s}\\)\n\n\n\nRoy maximum root\n\\(R = \\lambda_1\\)\n\\(\\eta^2 = \\frac{\\lambda_1}{1+\\lambda_1}\\)\n\n\n\n\n\n\n\n\nThese correspond to different kinds of “means” of the \\(\\lambda_i\\): geometric (Wilks), arithmetic (Pillai), harmonic (Hotelling-Lawley) and supremum (Roy). See Friendly et al. (2013) for the geometry behind these measures.\nEach of these statistics have different sampling distributions under the null hypothesis, but they can all be converted to \\(F\\) statistics. These are exact when \\(s \\le 2\\), and approximations otherwise. As well, each has an analog of the \\(R^2\\)-like partial \\(\\eta^2\\) measure, giving the partial association accounted for by each term in the MLM.\n\n10.3.1 Testing contrasts and linear hypotheses\nEven more generally, these multivariate tests apply to every linear hypothesis concerning the coefficients in \\(\\mathbf{B}\\). Suppose we want to test the hypothesis that a subset of rows (predictors) and/or columns (responses) simultaneously have null effects. This can be expressed in the general linear test, \\[\n\\mathcal{H}_0 : \\mathbf{C}_{h \\times q} \\, \\mathbf{B}_{q \\times p} = \\mathbf{0}_{h \\times p} \\:\\: ,\n\\] where \\(\\mathbf{C}\\) is a full rank \\(h \\le q\\) hypothesis matrix of constants, that selects subsets or linear combinations (contrasts) of the coefficients in \\(\\mathbf{B}\\) to be tested in a \\(h\\) degree-of-freedom hypothesis.\nIn this case, the SSP matrix for the hypothesis has the form \\[\n\\mathbf{H}  =\n(\\mathbf{C} \\widehat{\\mathbf{B}})^\\mathsf{T}\\,\n[\\mathbf{C} (\\mathbf{X}^\\mathsf{T}\\mathbf{X} )^{-1} \\mathbf{C}^\\mathsf{T}]^{-1} \\,\n(\\mathbf{C} \\widehat{\\mathbf{B}}) \\:\\: ,\n\\tag{10.7}\\]\nwhere there are \\(s = \\min(h, p)\\) non-zero eigenvalues of \\(\\mathbf{H}\\mathbf{E}^{-1}\\). In Equation 10.7, \\(\\mathbf{H}\\) measures the (Mahalanobis) squared distances (and cross products) among the linear combinations \\(\\mathbf{C} \\widehat{\\mathbf{B}}\\) from the origin under the null hypothesis.  \nFor example, with three responses \\(y_1, y_2, y_3\\) and three predictors \\(x_1, x_2, x_3\\), we can test the hypothesis that neither \\(x_2\\) nor \\(x_3\\) contribute at all to predicting the \\(y\\)s in terms of the hypothesis that the coefficients for the corresponding rows of \\(\\mathbf{B}\\) are zero using a 1-row \\(\\mathbf{C}\\) matrix that simply selects those rows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\mathcal{H}_0 : \\mathbf{C} \\mathbf{B} & =\n\\begin{bmatrix}\n0 & 1 & 1 & 0\n\\end{bmatrix}\n\\begin{pmatrix}\n  \\beta_{0,y_1} & \\beta_{0,y_2} & \\beta_{0,y_3} \\\\\n  \\beta_{1,y_1} & \\beta_{1,y_2} & \\beta_{1,y_3} \\\\\n  \\beta_{2,y_1} & \\beta_{2,y_2} & \\beta_{2,y_3} \\\\\n  \\beta_{3,y_1} & \\beta_{3,y_2} & \\beta_{3,y_3}\n\\end{pmatrix} \\\\ \\\\\n& =\n\\begin{bmatrix}\n  \\beta_{1,y_1} & \\beta_{1,y_2} & \\beta_{1,y_3} \\\\\n  \\beta_{2,y_1} & \\beta_{2,y_2} & \\beta_{2,y_3} \\\\\n\\end{bmatrix}\n=\n\\mathbf{0}_{(2 \\times 3)}\n\\end{aligned}\\]\nIn MANOVA designs, it is often desirable to follow up a significant effect for a factor with subsequent tests to determine which groups differ. While you can simply test for all pairwise differences among groups (using Bonferonni or other corrections for multiplicity), a more substantively-driven approach uses planned comparisons or contrasts among the factor levels as described in Section 10.3.1.\nTODO: This stuff was copied to Section 10.3.1. Delete it here\nFor a factor with \\(g\\) groups, a contrast is simply a comparison of the mean of one subset of groups against the mean of another subset. This is specified as a weighted sum, \\(L\\) of the means with weights \\(\\mathbf{c}\\) that sum to zero,\n\\[\nL = \\mathbf{c}^\\mathsf{T} \\boldsymbol{\\mu} = \\sum_i c_i \\mu_i \\quad\\text{such that}\\quad \\Sigma c_i = 0\n\\] Two contrasts, \\(\\mathbf{c}_1\\) and \\(\\mathbf{c}_2\\) are orthogonal if the sum of products of their weights is zero, i.e., \\(\\mathbf{c}_1^\\mathsf{T} \\mathbf{c}_2 = \\Sigma c_{1i} \\times c_{2i} = 0\\). When contrasts are placed as columns of a matrix \\(\\mathbf{C}\\), they are all mutually orthogonal if each pair is orthogonal, which means \\(\\mathbf{C}^\\top \\mathbf{C}\\) is a diagonal matrix. Orthogonal contrasts correspond to statistically independent tests. This is nice because they reflect separate, non-overlapping research questions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor example, with the \\(g=4\\) groups for the dogfood data, the company might want to test the following comparisons among the formulas Old, New, Major and Alps: (a) Ours vs. Theirs: The average of (Old, New) compared to (Major, Alps); (b) Old vs. New; (c) Major vs. Alps. The contrasts that do this are:\n\\[\\begin{aligned}\nL_1 & = \\textstyle{\\frac12} (\\mu_O + \\mu_N) -\n        \\textstyle{\\frac12} (\\mu_M + \\mu_A) & \\rightarrow\\: & \\mathbf{c}_1 =\n\\textstyle{\\frac12}\n     \\begin{pmatrix}\n      1 &  1 & -1 & -1\n     \\end{pmatrix} \\\\\nL_2 & = \\mu_O - \\mu_N                     & \\rightarrow\\: & \\mathbf{c}_2 =\n    \\begin{pmatrix}\n     1 &  -1 & 0 & 0\n    \\end{pmatrix} \\\\\nL_3 & = \\mu_M - \\mu_A                     & \\rightarrow\\: & \\mathbf{c}_3 =\n    \\begin{pmatrix}\n     0 &  0 & 1 & -1\n    \\end{pmatrix}\n\\end{aligned}\\]\nNote that these correspond to nested dichotomies among the four groups: first we compare groups (Old and New) against groups (Major and Alps), then subsequently within each of these sets. Nested dicontomy contrasts are always orthogonal, and therefore correspond to statistically independent tests. We are effectively taking a three degree-of-freedom question, “do the means differ?” and breaking it down into three separate 1 df tests that answer specific parts of that overall question.\nIn R, contrasts for a factor are specified as columns of matrix, each of which sums to zero. For this example, we can set this up by creating each as a vector and joining them as columns using cbind():\n\nc1 &lt;- c(1,  1, -1, -1)/2    # Old,New vs. Major,Alps\nc2 &lt;- c(1, -1,  0,  0)      # Old vs. New\nc3 &lt;- c(0,  0,  1, -1)      # Major vs. Alps\nC &lt;- cbind(c1,c2,c3) \nrownames(C) &lt;- levels(dogfood$formula)\n\nC\n#&gt;         c1 c2 c3\n#&gt; Old    0.5  1  0\n#&gt; New    0.5 -1  0\n#&gt; Major -0.5  0  1\n#&gt; Alps  -0.5  0 -1\n\n# show they are mutually orthogonal\nt(C) %*% C\n#&gt;    c1 c2 c3\n#&gt; c1  1  0  0\n#&gt; c2  0  2  0\n#&gt; c3  0  0  2\n\nFor the dogfood data, with formula as the group factor, you can set up the analyses to use these contrasts by assigning the matrix C to contrasts() for that factor in the dataset itself. When the contrasts are changed, it is necessary to refit the model. The estimated coefficients then become the estimated mean differences for the contrasts.\n\ncontrasts(dogfood$formula) &lt;- C\ndogfood.mod &lt;- lm(cbind(start, amount) ~ formula, \n                  data=dogfood)\ncoef(dogfood.mod)\n#&gt;              start amount\n#&gt; (Intercept)  1.688  85.56\n#&gt; formulac1   -1.375  10.88\n#&gt; formulac2   -0.500   3.25\n#&gt; formulac3    0.125   1.88\n\nFor example, Ours vs. Theirs estimated by formulac1 takes 0.69 less time to start eating and eats 5.44 more on average.\nFor multivariate tests, when all contrasts are pairwise orthogonal, the overall test of a factor with \\(\\text{df}_h = g-1\\) degrees of freedom can be broken down into \\(g-1\\) separate 1 df tests. This gives rise to a set of \\(\\text{df}_h\\) rank 1 \\(\\mathbf{H}\\) matrices that additively decompose the overall hypothesis SSCP matrix,\n\\[\n\\mathbf{H} = \\mathbf{H}_1 + \\mathbf{H}_2 + \\cdots + \\mathbf{H}_{\\text{df}_h} \\:\\: ,\n\\tag{10.8}\\]\nexactly as the univariate \\(\\text{SS}_H\\) can be decomposed using orthogonal contrasts in an ANOVA.\nYou can test such contrasts or any other hypotheses involving linear combinations of the coefficients using car::linearHypothesis. Here, \"formulac1\" refers to the contrast c1 for the difference between Ours and Theirs. Note that because this is a 1 df test, all four test statistics yield the same \\(F\\) values.\n\nhyp &lt;- rownames(coef(dogfood.mod))[-1] |&gt; print()\n#&gt; [1] \"formulac1\" \"formulac2\" \"formulac3\"\nH1 &lt;- linearHypothesis(dogfood.mod, hyp[1], title=\"Ours vs. Theirs\") |&gt; \n  print()\n#&gt; \n#&gt; Sum of squares and products for the hypothesis:\n#&gt;         start amount\n#&gt; start    7.56  -59.8\n#&gt; amount -59.81  473.1\n#&gt; \n#&gt; Sum of squares and products for error:\n#&gt;        start amount\n#&gt; start   25.8   11.7\n#&gt; amount  11.7  390.3\n#&gt; \n#&gt; Multivariate Tests: Ours vs. Theirs\n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; Pillai            1     0.625     9.18      2     11 0.0045 **\n#&gt; Wilks             1     0.375     9.18      2     11 0.0045 **\n#&gt; Hotelling-Lawley  1     1.669     9.18      2     11 0.0045 **\n#&gt; Roy               1     1.669     9.18      2     11 0.0045 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSimilarly we can test the other two contrasts within these each of these two subsets, but I don’t print the results\n\nH2 &lt;- linearHypothesis(dogfood.mod, hyp[2], \n                       title=\"Old vs. New\")\nH3 &lt;- linearHypothesis(dogfood.mod, hyp[3], \n                       title=\"Alps vs. Major\")\n\nThen, we can illustrate Equation 10.8 by extracting the 1 df \\(\\mathbf{H}\\) matrices (SSPH) from the results of linearHypothesis.\n\n\\[\n\\overset{\\mathbf{H}}\n{\\begin{pmatrix}\n  9.7 & -70.9 \\\\\n-70.9 & 585.7 \\\\\n\\end{pmatrix}}\n=\n\\overset{\\mathbf{H}_1}\n{\\begin{pmatrix}\n  7.6 & -59.8 \\\\\n-59.8 & 473.1 \\\\\n\\end{pmatrix}}\n+\n\\overset{\\mathbf{H}_2}\n{\\begin{pmatrix}\n0.13 &  1.88 \\\\\n1.88 & 28.12 \\\\\n\\end{pmatrix}}\n+\n\\overset{\\mathbf{H}_3}\n{\\begin{pmatrix}\n  2 & -13 \\\\\n-13 &  84 \\\\\n\\end{pmatrix}}\n\\]",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#anova-rightarrow-manova",
    "href": "10-mlm-review.html#anova-rightarrow-manova",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.4 ANOVA \\(\\rightarrow\\) MANOVA",
    "text": "10.4 ANOVA \\(\\rightarrow\\) MANOVA\nMultivariate analysis of variance (MANOVA) generalizes the familiar ANOVA model to situations where there are two or more response variables. Unlike ANOVA, which focuses on discerning statistical differences in one continuous dependent variable influenced by an independent variable (or grouping variable), MANOVA considers several dependent variables at once. It integrates these variables into a single, composite variable through a weighted linear combination, allowing for a comprehensive analysis of how these dependent variables collectively vary with respect to the levels of the independent variable. Essentially, MANOVA investigates whether the grouping variable explains significant variations in the combined dependent variables.\nThe situation is illustrated in Figure 10.3 where there are two response measures, \\(Y_1\\) and \\(Y_2\\) with data collected for three groups. For concreteness, \\(Y_1\\) might be a score on a math test and \\(Y_2\\) might be a reading score. Let’s also say that group 1 has been studying Shakespeare, while group 2 has concentrated on physics, but group 3 has done nothing beyond the normal curriculum.\n\n\n\n\n\n\n\nFigure 10.3: Data from simple MANOVA design involving three groups and two response measures, \\(Y_1\\) and \\(Y_2\\), summarized by their data ellipses.\n\n\n\n\nAs shown in the figure, the centroids, \\((\\mu_{1g}, \\mu_{2g})\\), clearly differ—the data ellipses barely overlap. A multivariate analysis would show a highly difference among groups. From a rough visual inspection, it seems that means differ on the math test \\(Y_1\\), with the physics group out-performing the other two. On the reading test \\(Y_2\\) however it might turn out that the three group means don’t differ significantly in an ANOVA, but the Shakespeare and physics groups appear to outperform the normal curriculum group. Doing separate ANOVAs on these variables would miss what is so obvious from Figure 10.3: there is wide separation among the groups in the two tests considered jointly.\nFigure 10.4 illustrates a second important advantage of performing a multivariate analysis over separate ANOVAS: that of determining the number of dimensions or aspects along which groups differ. In the panel on the left, the means of the three groups increase nearly linearly on the combination of \\(Y_1\\) and \\(Y_2\\), so their differences can be ascribed to a single dimension, which simplifies the interpretation.\nFor example, the groups here might be patients diagnosed as normal, mild schizophrenia and profound schizophrenia, and the measures could be tests of memory and attention. The obvious multivariate interpretation from the figure is that of increasing impairment of cognitive functioning across the groups, comprised by memory and attention. Note also the positive association within each group: those who perform better on the memory task also do better on attention.\n\n\n\n\n\n\n\nFigure 10.4: A simple MANOVA design involving three groups and two response measures, \\(Y_1\\) and \\(Y_2\\), but with different patterns of the differences among the group means. The red arrows suggest interpretations in terms of dimensions or aspects of the response variables.\n\n\n\n\nIn contrast, the right panel of Figure 10.4 shows a situation where the group means have a low correlation. Data like this might arise in a study of parental competency, where there are are measures of both the degree of caring (\\(Y_1\\)) and time spent in play (\\(Y_2\\)) by fathers and groups consisting of fathers of children with no disability, or a physical disability or a mental ability.\nAs can be seen in Figure 10.4 fathers of the disabled children differ from those of the not disabled group in two different directions corresponding to being higher on either \\(Y_1\\) or \\(Y_2\\). The red arrows suggest that the differences among groups could be interpreted in terms of two uncorrelated dimensions, perhaps labeled overall competency and emphasis on physical activity. (The pattern in Figure 10.4 (right) is contrived for the sake of illustration; it does not reflect the data analyzed in the example below.)\n\n10.4.1 Example: Father parenting data\nI use a simple example of a three-group multivariate design to illustrate the basic ideas of fitting MLMs in R and testing hypotheses. Visualization methods using HE plots are discussed in Chapter 11.\nThe dataset heplots::Parenting come from an exercise (10B) in Meyers et al. (2006) and are probably contrived, but are modeled on a real study in which fathers were assessed on three subscales of a Perceived Parenting Competence Scale,\n\n\ncaring, caretaking responsibilities;\n\nemotion, emotional support provided to the child; and\n\nplay, recreational time spent with the child.\n\nThe dataset Parenting comprises 60 fathers selected from three groups of \\(n = 20\\) each: (a) fathers of a child with no disabilities (\"Normal\"); (b) fathers with a physically disabled child; (c) fathers with a mentally disabled child. The design is thus a three-group MANOVA, with three response variables.\n\nThe main questions concern whether the group means differ on these scales, and the nature of these differences. That is, do the means differ significantly on all three measures? Is there a consistent order of groups across these three aspects of parenting?\nMore specific questions are: (a) Do the fathers of typical children differ from the other two groups on average? (b) Do the physical and mental groups differ? These questions can be tested using contrasts, and are specified by assigning a matrix to contrasts(Parenting$group); each column is a contrast whose values sum to zero. They are given labels \"group1\" (normal vs. other) and \"group2\" (physical vs. mental) in some output.\n\ndata(Parenting, package=\"heplots\")\nC &lt;- matrix(c(1, -.5, -.5,\n              0,  1,  -1), \n            nrow = 3, ncol = 2) |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,]  1.0    0\n#&gt; [2,] -0.5    1\n#&gt; [3,] -0.5   -1\ncontrasts(Parenting$group) &lt;- C\n\nExploratory plots\nBefore setting up a model and testing, it is well-advised to examine the data graphically. The simplest plots are side-by-side boxplots (or violin plots) for the three responses. With ggplot2, this is easily done by reshaping the data to long format and using faceting. In Figure 10.5, I’ve also plotted the group means with white dots.\n\nSee the ggplot codeparenting_long &lt;- Parenting |&gt;\n  tidyr::pivot_longer(cols=caring:play, \n                      names_to = \"variable\")\n\nggplot(parenting_long, \n       aes(x=group, y=value, fill=group)) +\n  geom_boxplot(outlier.size=2.5, \n               alpha=.5, \n               outlier.alpha = 0.9) + \n  stat_summary(fun=mean, \n               color=\"white\", \n               geom=\"point\", \n               size=2) +\n  scale_fill_hue(direction = -1) +     # reverse default colors\n  labs(y = \"Scale value\", x = \"Group\") +\n  facet_wrap(~ variable) +\n  theme_bw(base_size = 14) + \n  theme(legend.position=\"top\") +\n  theme(axis.text.x = element_text(angle = 15,\n                                   hjust = 1)) \n\n\n\n\n\n\nFigure 10.5: Faceted boxplots of scores on the three parenting scales, showing also the mean for each.\n\n\n\n\nIn this figure, differences among the groups on play are most apparent, with fathers of non-disabled children scoring highest. Differences among the groups on emotion are very small, but one high outlier for the fathers of mentally disabled children is apparent. On caring, fathers of children with a physical disability stand out as highest.\nFor exploratory purposes, you might also make a scatterplot matrix. Here, because the MLM assumes homogeneity of the variances and covariance matrices \\(\\mathbf{S}_i\\), I show only the data ellipses in scatterplot matrix format, using heplots:covEllipses() (with 50% coverage, for clarity):\n\ncolors &lt;- scales::hue_pal()(3) |&gt; rev()  # match color use in ggplot\ncovEllipses(cbind(caring, play, emotion) ~ group, \n  data=Parenting,\n  variables = 1:3,\n  fill = TRUE, fill.alpha = 0.2,\n  pooled = FALSE,\n  level = 0.50, \n  col = colors)\n\n\n\n\n\n\nFigure 10.6: Bivariate data ellipses for pairs of the three responses, showing the means, correlations and variances for the three groups.\n\n\n\n\nIf the covariance matrices were all the same, the data ellipses would have roughly the same size and orientation, but that is not the case in Figure 10.6. The normal group shows greater variability overall and the correlations among the measures differ somewhat from group to group. We’ll assess later whether this makes a difference in the conclusions that can be drawn (Chapter 12). The group centroids also differ, but the pattern is not particularly clear. We’ll see an easier to understand view in HE plots and their canonical discriminant cousins.\n\n10.4.1.1 Testing the model\nLet’s proceed to fit the multivariate model predicting all three scales from the group factor. lm() for a multivariate response returns an object of class \"mlm\", for which there are many methods (use methods(class=\"mlm\") to find them).\n\nparenting.mlm &lt;- lm(cbind(caring, play, emotion) ~ group, \n                    data=Parenting) |&gt; print()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cbind(caring, play, emotion) ~ group, data = Parenting)\n#&gt; \n#&gt; Coefficients:\n#&gt;              caring   play     emotion\n#&gt; (Intercept)   5.8833   4.6333   5.9167\n#&gt; group1       -0.3833   2.4167  -0.0667\n#&gt; group2        1.7750  -0.2250  -0.6000\n\nThe coefficients in this model are the values of the contrasts set up above. group1 is the mean of the typical group minus the average of the other two, which is negative on caring and emotion but positive for play. group2 is the difference in means for the physical vs. mental groups.\nBefore doing multivariate tests, it is useful to see what would happen if we ran univariate ANOVAs on each of the responses. These can be extracted from an MLM using stats::summary.aov() and they give tests of the model terms for each response variable separately:\n\nsummary.aov(parenting.mlm)\n#&gt;  Response caring :\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; group        2    130    65.2    18.6  6e-07 ***\n#&gt; Residuals   57    200     3.5                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Response play :\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; group        2    177    88.6    27.6  4e-09 ***\n#&gt; Residuals   57    183     3.2                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Response emotion :\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; group        2     15    7.27    1.02   0.37\n#&gt; Residuals   57    408    7.16\n\nFor a more condensed summary, you can instead extract the univariate model fit statistics from the \"mlm\" object using the heplots::glance() method for a multivariate model object. The code below selects just the \\(R^2\\) and \\(F\\)-statistic for the overall model for each response, together with the associated \\(p\\)-value.\n\nglance(parenting.mlm) |&gt;\n  select(response, r.squared, fstatistic, p.value)\n#&gt; # A tibble: 3 × 4\n#&gt;   response r.squared fstatistic       p.value\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 caring      0.395       18.6  0.000000602  \n#&gt; 2 play        0.492       27.6  0.00000000405\n#&gt; 3 emotion     0.0344       1.02 0.369\n\nFrom this, one might conclude that there are differences only in caring and play and therefore ignore emotion, but this would be short-sighted. car::Anova(), shown below, gives the overall multivariate test \\(\\mathcal{H}_0: \\mathbf{B} = 0\\) of the group effect, that the groups don’t differ on any of the response variables. Note that this has a much smaller \\(p\\)-value than any of the univariate \\(F\\) tests.\n\nAnova(parenting.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;       Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; group  2     0.948     16.8      6    112  9e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova() returns an object of class \"Anova.mlm\" which has various methods. The summary() method for this gives more details, including all four test statistics. With \\(p=3\\) responses, these tests have \\(s = \\min(p, \\text{df}_h) = \\min(3,2) = 2\\) dimensions and the \\(F\\) approximations are not equivalent here. All four tests are highly significant, with Roy’s test giving the largest \\(F\\) statistic.\n\nparenting.summary &lt;- Anova(parenting.mlm) |&gt;  summary() \nprint(parenting.summary, SSP=FALSE)\n#&gt; \n#&gt; Type II MANOVA Tests:\n#&gt; \n#&gt; ------------------------------------------\n#&gt;  \n#&gt; Term: group \n#&gt; \n#&gt; Multivariate Tests: group\n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            2     0.948     16.8      6    112 9.0e-14 ***\n#&gt; Wilks             2     0.274     16.7      6    110 1.3e-13 ***\n#&gt; Hotelling-Lawley  2     1.840     16.6      6    108 1.8e-13 ***\n#&gt; Roy               2     1.108     20.7      3     56 3.8e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe summary() method by default prints the SSH = \\(\\mathbf{H}\\) and SSE = \\(\\mathbf{E}\\) matrices, but I suppressed them above. The data structure returned contains nested elements which can be extracted more easily from the object using purrr::pluck():\n\nH &lt;- parenting.summary |&gt; \n  purrr::pluck(\"multivariate.tests\", \"group\", \"SSPH\") |&gt; \n  print()\n#&gt;         caring    play emotion\n#&gt; caring   130.4 -43.767 -41.833\n#&gt; play     -43.8 177.233   0.567\n#&gt; emotion  -41.8   0.567  14.533\nE &lt;- parenting.summary |&gt; \n  purrr::pluck(\"multivariate.tests\", \"group\", \"SSPE\") |&gt; \n  print()\n#&gt;         caring  play emotion\n#&gt; caring   199.8 -45.8    35.3\n#&gt; play     -45.8 182.7    80.6\n#&gt; emotion   35.3  80.6   408.1\n\nLinear hypotheses & contrasts\nWith three or more groups or with a more complex MANOVA design, contrasts provide a way of testing questions of substantive interest regarding differences among group means.\nThe test of the contrast comparing the typical group to the average of the others is the test using the contrast \\(c_1 = (1, -\\frac12, -\\frac12)\\) which produces the coefficients labeled \"group1\". The function car::linearHypothesis() carries out the multivariate test that this difference is zero. This is a 1 df test, so all four test statistics produce the same \\(F\\) and \\(p\\)-values.\n\ncoef(parenting.mlm)[\"group1\",]\n#&gt;  caring    play emotion \n#&gt; -0.3833  2.4167 -0.0667\nlinearHypothesis(parenting.mlm, \"group1\") |&gt; \n  print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            1     0.521     19.9      3     55 7.1e-09 ***\n#&gt; Wilks             1     0.479     19.9      3     55 7.1e-09 ***\n#&gt; Hotelling-Lawley  1     1.088     19.9      3     55 7.1e-09 ***\n#&gt; Roy               1     1.088     19.9      3     55 7.1e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSimilarly, the difference between the physical and mental groups uses the contrast \\(c_2 = (0, 1, -1)\\) and the test that these means are equal is given by linearHypothesis() applied to group2.\n\ncoef(parenting.mlm)[\"group2\",]\n#&gt;  caring    play emotion \n#&gt;   1.775  -0.225  -0.600\nlinearHypothesis(parenting.mlm, \"group2\") |&gt; \n  print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1     0.429     13.8      3     55  8e-07 ***\n#&gt; Wilks             1     0.571     13.8      3     55  8e-07 ***\n#&gt; Hotelling-Lawley  1     0.752     13.8      3     55  8e-07 ***\n#&gt; Roy               1     0.752     13.8      3     55  8e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlinearHypothesis() is very general. The second argument (hypothesis.matrix) corresponds to \\(\\mathbf{C}\\), and can be specified as numeric matrix giving the linear combinations of coefficients by rows to be tested, or a character vector giving the hypothesis in symbolic form; \"group1\" is equivalent to \"group1 = 0\".\nBecause the two contrasts used here are orthogonal, they add together to give the overall test of \\(\\mathbf{B} = \\mathbf{0}\\), which implies that the means of the three groups are all equal. The test below gives the same results as Anova(parenting.mlm).\n\nlinearHypothesis(parenting.mlm, c(\"group1\", \"group2\")) |&gt; \n  print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            2     0.948     16.8      6    112 9.0e-14 ***\n#&gt; Wilks             2     0.274     16.7      6    110 1.3e-13 ***\n#&gt; Hotelling-Lawley  2     1.840     16.6      6    108 1.8e-13 ***\n#&gt; Roy               2     1.108     20.7      3     56 3.8e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n10.4.2 Ordered factors\nWhen groups are defined by an ordered factor, such as level of physical fitness (rated 1–5) or grade in school, it is tempting to treat that as a numeric variable and use a multivariate regression model. This would assume that the effect of that factor is linear and if not, we might consider adding polynomial terms. A different strategy, often preferable, is to make the group variable an ordered factor, for which R assigns polynomial contrasts. This gives separate tests of the linear, quadratic, cubic, … trends of the response, without the need to specify them separately in the model\n\n10.4.3 Example: Adolescent mental health\nThe dataset heplots::AddHealth contains a large cross-sectional sample of participants from grades 7–12 from the National Longitudinal Study of Adolescent Health, described by Warne (2014). It contains responses to two Likert-scale (1–5) items, anxiety and depression. grade is an ordered factor, which means that the default contrasts are taken as orthogonal polynomials with linear (grade.L), quadratic (grade.Q), up to 5th degree (grade^5) trends, which decompose the total effect of grade.\n\ndata(AddHealth, package=\"heplots\")\nstr(AddHealth)\n#&gt; 'data.frame':  4344 obs. of  3 variables:\n#&gt;  $ grade     : Ord.factor w/ 6 levels \"7\"&lt;\"8\"&lt;\"9\"&lt;\"10\"&lt;..: 5 4 6 1 2 2 2 3 3 3 ...\n#&gt;  $ depression: int  0 0 0 0 0 0 0 0 1 2 ...\n#&gt;  $ anxiety   : int  0 0 0 1 1 0 0 1 1 0 ...\n\nThe research questions are:\n\nHow do the means for anxiety and depression vary separately with grade? Is there evidence for linear and nonlinear trends?\nHow do anxiety and depression vary jointly with grade?\nHow does the association of anxiety and depression vary with age?\n\nThe first question can be answered by fitting separate linear models for each response (e.g., lm(anxiety ~ grade))). However the second question is more interesting because it considers the two responses together and takes their correlation into account. This would be fit as the MLM:\n\\[\n\\mathbf{y} = \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1 x + \\boldsymbol{\\beta}_2 x^2 + \\cdots \\boldsymbol{\\beta}_5 x^5\n\\tag{10.9}\\]\nor, expressed in terms of the variables,\n\\[\\begin{aligned}\n\\begin{bmatrix} y_{\\text{anx}} \\\\y_{\\text{dep}} \\end{bmatrix} & =\n\\begin{bmatrix} \\beta_{0,\\text{anx}} \\\\ \\beta_{0,\\text{dep}} \\end{bmatrix} +\n\\begin{bmatrix} \\beta_{1,\\text{anx}} \\\\ \\beta_{1,\\text{dep}} \\end{bmatrix} \\text{grade} +\n\\begin{bmatrix} \\beta_{2,\\text{anx}} \\\\ \\beta_{2,\\text{dep}} \\end{bmatrix} \\text{grade}^2 \\\\\n& + \\cdots +\n\\begin{bmatrix} \\beta_{5,\\text{anx}} \\\\ \\beta_{5,\\text{dep}} \\end{bmatrix} \\text{grade}^5\n\\end{aligned} \\tag{10.10}\\]\nWithgrade represented as an ordered factor, the values of \\(x\\) in Equation 10.9 are those of the orthogonal polynomials given by poly(grade,5).\nExploratory plots\nSome exploratory analysis is useful before fitting and visualizing models. As a first step, we find the means, standard deviations, and standard errors of the means.\n\nmeans &lt;- AddHealth |&gt;\n  group_by(grade) |&gt;\n  summarise(\n    n = n(),\n    dep_sd = sd(depression, na.rm = TRUE),\n    anx_sd = sd(anxiety, na.rm = TRUE),\n    dep_se = dep_sd / sqrt(n),\n    anx_se = anx_sd / sqrt(n),\n    depression = mean(depression),\n    anxiety = mean(anxiety) ) |&gt; \n  relocate(depression, anxiety, .after = grade) |&gt;\n  print()\n#&gt; # A tibble: 6 × 8\n#&gt;   grade depression anxiety     n dep_sd anx_sd dep_se anx_se\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 7          0.881   0.751   622   1.11   1.05 0.0447 0.0420\n#&gt; 2 8          1.08    0.804   664   1.19   1.06 0.0461 0.0411\n#&gt; 3 9          1.17    0.934   778   1.19   1.08 0.0426 0.0387\n#&gt; 4 10         1.27    0.956   817   1.23   1.11 0.0431 0.0388\n#&gt; 5 11         1.37    1.12    790   1.20   1.16 0.0428 0.0411\n#&gt; 6 12         1.34    1.10    673   1.14   1.11 0.0439 0.0426\n\nNow, plot the means with \\(\\pm 1\\) error bars. It appears that average level of both depression and anxiety increase steadily with grade, except for grades 11 and 12 which don’t differ much. Alternatively, we could describe this as relationships that seem largely linear, with a hint of curvature at the upper end.\n\np1 &lt;-ggplot(data = means, aes(x = grade, y = anxiety)) +\n  geom_point(size = 4) +\n  geom_line(aes(group = 1), linewidth = 1.2) +\n  geom_errorbar(aes(ymin = anxiety - anx_se, \n                   ymax = anxiety + anx_se),\n                width = .2) \n\np2 &lt;-ggplot(data = means, aes(x = grade, y = depression)) +\n  geom_point(size = 4) +\n  geom_line(aes(group = 1), linewidth = 1.2) +\n  geom_errorbar(aes(ymin = depression - dep_se, \n                    ymax = depression + dep_se),\n                width = .2) \n\np1 + p2\n\n\n\n\n\n\nFigure 10.7: Means of anxiety and depression by grade, with \\(\\pm 1\\) standard error bars.\n\n\n\n\nIt is also useful to within-group correlations using covEllipses(), as shown in Figure 10.8. This also plots the bivariate means showing the form of the association , treating anxiety and depression as multivariate outcomes. (Because the variability of the scores within groups is so large compared to the range of the means, I show the data ellipses with coverage of only 10%.)\n\ncovEllipses(AddHealth[, 3:2], group = AddHealth$grade,\n            pooled = FALSE, level = 0.1,\n            center.cex = 2.5, cex = 1.5, cex.lab = 1.5,\n            fill = TRUE, fill.alpha = 0.05)\n\n\n\n\n\n\nFigure 10.8: Within-group covariance ellipses for the grade groups.\n\n\n\n\nFit the MLM\nNow, let’s fit the MLM for both responses jointly in relation to grade. The null hypothesis is that the means for anxiety and depression are the same at all six grades, \\[\n\\mathcal{H}_0: \\mathbf{\\mu}_7 = \\mathbf{\\mu}_8 = \\cdots = \\mathbf{\\mu}_{12} \\; ,\n\\] or equivalently, that all coefficients except the intercept in the model Equation 10.9 are zero, \\[\n\\mathcal{H}_0: \\boldsymbol{\\beta}_1 =  \\boldsymbol{\\beta}_2  = \\cdots =  \\boldsymbol{\\beta}_5 = \\boldsymbol{0} \\; .\n\\] We fit the MANOVA model, and test the grade effect using car::Anova(). The effect of grade is highly significant, as we could tell from Figure 10.7.\n\nAH.mlm &lt;- lm(cbind(anxiety, depression) ~ grade, data = AddHealth)\n\n# overall test of `grade`\nAnova(AH.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;       Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; grade  5    0.0224     9.83     10   8676 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHowever, the overall test, with 5 degrees of freedom is diffuse, in that it can be rejected if any pair of means differ. Given that grade is an ordered factor, it makes sense to examine narrower hypotheses of linear and nonlinear trends, car::linearHypothesis() on the coefficients of model AH.mlm.\n\ncoef(AH.mlm) |&gt; rownames()\n#&gt; [1] \"(Intercept)\" \"grade.L\"     \"grade.Q\"     \"grade.C\"    \n#&gt; [5] \"grade^4\"     \"grade^5\"\n\nThe joint test of the linear coefficients \\(\\boldsymbol{\\beta}_1 = (\\beta_{1,\\text{anx}},  \\beta_{1,\\text{dep}})^\\mathsf{T}\\) for anxiety and depression, \\(\\mathcal{H}_0 : \\boldsymbol{\\beta}_1 = \\boldsymbol{0}\\) is highly significant,\n\n## linear effect\nlinearHypothesis(AH.mlm, \"grade.L\") |&gt; print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1     0.019     42.5      2   4337 &lt;2e-16 ***\n#&gt; Wilks             1     0.981     42.5      2   4337 &lt;2e-16 ***\n#&gt; Hotelling-Lawley  1     0.020     42.5      2   4337 &lt;2e-16 ***\n#&gt; Roy               1     0.020     42.5      2   4337 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe test of the quadratic coefficients \\(\\mathcal{H}_0 : \\boldsymbol{\\beta}_2 = \\boldsymbol{0}\\) indicates significant curvature in trends across grade, as we saw in the plots of their means in Figure 10.7. One interpretation might be that depression and anxiety after increasing steadily up to grade eleven could level off thereafter.\n\n## quadratic effect\nlinearHypothesis(AH.mlm, \"grade.Q\") |&gt; print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; Pillai            1     0.002     4.24      2   4337  0.014 *\n#&gt; Wilks             1     0.998     4.24      2   4337  0.014 *\n#&gt; Hotelling-Lawley  1     0.002     4.24      2   4337  0.014 *\n#&gt; Roy               1     0.002     4.24      2   4337  0.014 *\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAn advantage of linear hypotheses is that we can test several terms jointly. Of interest here is the hypothesis that all higher order terms beyond the quadratic are zero, \\(\\mathcal{H}_0 : \\boldsymbol{\\beta}_3 =  \\boldsymbol{\\beta}_4 =  \\boldsymbol{\\beta}_5 = \\boldsymbol{0}\\). Using linearHypothesis you can supply a vector of coefficient names to be tested for their joint effect when dropped from the model.\n\ncoefs &lt;- rownames(coef(AH.mlm)) |&gt; print()\n#&gt; [1] \"(Intercept)\" \"grade.L\"     \"grade.Q\"     \"grade.C\"    \n#&gt; [5] \"grade^4\"     \"grade^5\"\n## joint test of all higher terms\nlinearHypothesis(AH.mlm, coefs[3:5],\n                 title = \"Higher-order terms\") |&gt; \n  print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: Higher-order terms\n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; Pillai            3     0.002     1.70      6   8676   0.12  \n#&gt; Wilks             3     0.998     1.70      6   8674   0.12  \n#&gt; Hotelling-Lawley  3     0.002     1.70      6   8672   0.12  \n#&gt; Roy               3     0.002     2.98      3   4338   0.03 *\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n10.4.4 Factorial MANOVA\nWhen there are two or more categorical factors, the general linear model provides a way to investigate the effects (differences in means) of each simultaneously. More importantly, this allows you to determine if factors interact, so the effect of one factor varies with the levels of another factor …\nExample: Penguins data\nIn Chapter 3 we examined the Palmer penguins data graphically, using a mosaic plot (Figure 3.34) of the frequencies of the three factors, species, island and sex and then ggpairs() scatterplot matrix (Figure 3.35).",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#mra-rightarrow-mmra",
    "href": "10-mlm-review.html#mra-rightarrow-mmra",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.5 MRA \\(\\rightarrow\\) MMRA",
    "text": "10.5 MRA \\(\\rightarrow\\) MMRA\nWhen all predictor variables are quantitative, the MLM Equation 10.2 becomes the extension of univariate multiple regression to the situation where there are \\(p\\) response variables. Just as in univariate models, we might want to test hypotheses about subsets of the predictors, for example when some predictors are meant as controls or things you might want to adjust for in assessing the effects of predictors of main interest.\nBut first, there are a couple of aspects of statistical practice that should be mentioned.\nModel selection is one topic where univariate and multivariate approaches differ. When there are more than a few predictors, approaches like hierarchical regression, LASSO (Tibshirani, 1996) and stepwise selection can be used to eliminate uninformative predictors for each response.3 But this gives a different models for each response, based on the predictors included, each with its own interpretation. In contrast, the multivariate approach considers the outcome variables collectively. You can eliminate predictors that are unimportant, but the mechanics are geared toward removing them from the models for all responses.\nOverall tests In a one-way ANOVA, to control for multiple testing, it is common practice to carry out an overall \\(F\\)-test to see if the group means differ collectively before testing comparison between specific groups. Similarly, in univariate multiple regression, researchers sometimes report an overall \\(F\\)-test or test of the \\(R^2\\) so they can reject the hypothesis that all predictors have no effect, before considering them individually.\nSimilarly, the the case of multivariate linear models, some consider it necessary to reject the multivariate null hypothesis for a predictor term before considering how it contributes to each of the response variables. Some further suggest that the individual univariate models be tested after an overall significant effect. I believe the first of these is wise, but the second might be too much to require when the general goal is to understand the data.\n\n10.5.1 Example: NLSY data\nThe dataset heplots::NLSY comes from a small part of the National Longitudinal Survey of Youth, a series of annual surveys conducted by the U.S. Department of Labor to examine the transition of young people into the labor force. This particular subset gives measures of 243 children on mathematics and reading achievement and also measures of behavioral problems (antisocial, hyperactivity). Also available are the yearly income and education of the child’s father.\nIn this analysis the math and read scores are taken at the outcome variables.4 Among the remaining predictors, income and educ might be considered as background variables necessary to control for. Interest might then be focused on whether the behavioral variables antisoc and hyperact contribute beyond that.\n\ndata(NLSY, package = \"heplots\")\nstr(NLSY)\n#&gt; 'data.frame':  243 obs. of  6 variables:\n#&gt;  $ math    : num  50 28.6 50 32.1 21.4 ...\n#&gt;  $ read    : num  45.2 28.6 53.6 34.5 22.6 ...\n#&gt;  $ antisoc : int  4 0 2 0 0 1 0 1 1 4 ...\n#&gt;  $ hyperact: int  3 0 2 2 2 0 1 4 3 5 ...\n#&gt;  $ income  : num  52.52 42.6 50 6.08 7.41 ...\n#&gt;  $ educ    : int  14 12 12 12 14 12 12 12 12 9 ...\n\nExploratory plots\nTo begin, I would examine some scatterplots and univariate displays. I’ll start with density plots for all the variables to see the shapes of their distributions, wikh rug plots at the bottom to show where the observations are located. From Figure 10.9 we see that math and reading scores are positively skewed, anti-social and hyperactivity have distributions highly concentrated in the lower scores. As we would suspect, father’s income is quite positively skewed. Father’s education is reasonably symmetric, but highly peaked at 12 years of schooling in this sample. The spikes reflect the fact that education is measured in discrete years.\n\nNLSY_long &lt;- NLSY |&gt; \n  tidyr::pivot_longer(math:educ, names_to = \"variable\") |&gt;\n  dplyr::mutate(variable = forcats::fct_inorder(variable))\n\nggplot(NLSY_long, aes(x=value, fill=variable)) +\n  geom_density(alpha = 0.5) +\n  geom_rug() +\n  facet_wrap(~variable, scales=\"free\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\nFigure 10.9: Density plots for the variables in the NLSY dataset.\n\n\n\n\nIn terms of an analysis focused on math and read as outcomes, a scatterplot of one against the other is useful, as is collection of scatterplots of each against the remaining variables. The second of these is left as an exercise to the reader.\n\nset.seed(47)\nggplot(NLSY, aes(x = read, y = math)) +\n  geom_jitter()+\n  geom_smooth(method = lm, formula = y~x, fill = \"blue\", alpha = 0.2) +\n  geom_smooth(method = loess, se = FALSE, color = \"red\", linewidth = 2)\n\n\n\n\n\n\nFigure 10.10: Scatterplot of mathematics score against reading score in the NLSY data\n\n\n\n\nThe non-linear trend in Figure 10.10 may be due to the sparsity of data in the upper range of reading, and there are also a few unusual points shown in this plot. The function heplots::noteworthy() provides a variety of methods to identify such noteworthy points in scatterplots. The default method uses Mahalanobis \\(D^2\\). The plot below labels the five largest observations.\n\nids &lt;- heplots::noteworthy(NLSY[, 1:2], method = \"mahal\", n=5)\nggplot(NLSY, aes(x = read, y = math)) +\n  geom_jitter()+\n  geom_smooth(method = lm, formula = y~x, fill = \"blue\", alpha = 0.2) +\n  geom_text(data = NLSY[ids, ], label = ids, size = 5, nudge_y = 2) \n\n\n\n\n\n\nFigure 10.11: Scatterplot of mathematics score against reading score in the NLSY data, highlighting noteworthy points\n\n\n\n\nFitting models\nWe could of course include all of the predictors in a single model, and perhaps be done with it. To develop some model-thinking, it is more useful to proceed in smaller steps to see what we can learn from each. If we view parents’ income and education as the most obvious predictors of reading and mathematics scores, those are the variables to fit first.\nTODO: Using log2(income) is more appropriate, but give HE plots in Ch 11 that are less dramatic. Consider using just income.\n\n\n\n\nNLSY.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY)\n\nAnova(NLSY.mod1)  # Type II, partial test\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;        Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; income  1    0.0345     4.27      2    239 0.0151 * \n#&gt; educ    1    0.0515     6.49      2    239 0.0018 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nOverall test\nThe Anova() results above give multivariate tests of the contributions of each predictor separately to explaining reading and math and how they vary together. To get an overall test of the global null hypothesis \\(\\mathcal{H}_0 : \\mathbf{B} =(\\boldsymbol{\\beta}_{\\text{inc}}, \\boldsymbol{\\beta}_{\\text{educ}}) =\\mathbf{0}\\) for all predictors together, you can use linearHypothesis():\n\ncoefs &lt;- rownames(coef(NLSY.mod1))[-1]\nlinearHypothesis(NLSY.mod1, coefs, title = \"income, educ = 0\") |&gt; \n  print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: income, educ = 0\n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            2     0.117     7.44      4    480 8.1e-06 ***\n#&gt; Wilks             2     0.884     7.59      4    478 6.2e-06 ***\n#&gt; Hotelling-Lawley  2     0.130     7.75      4    476 4.7e-06 ***\n#&gt; Roy               2     0.123    14.79      2    240 8.7e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis joint multivariate test is more highly significant than either of those for the separate effects of the predictors, again because it pools strength.\nCoefficient plots\nAs usual, you can display the coefficients using coef(). The tidy method for \"mlm\" objects defined in heplots shows these in a tidy format with \\(t\\)-tests for each coefficient., arranged by the response variable.\n\ncoef(NLSY.mod1)\n#&gt;                read   math\n#&gt; (Intercept) 15.8848 8.7829\n#&gt; income       0.0137 0.0893\n#&gt; educ         0.9495 1.2755\n\ntidy(NLSY.mod1)\n#&gt; # A tibble: 6 × 6\n#&gt;   response term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 read     (Intercept)  15.9       4.25       3.74  0.000233\n#&gt; 2 read     income        0.0137    0.0325     0.420 0.675   \n#&gt; 3 read     educ          0.949     0.360      2.64  0.00894 \n#&gt; 4 math     (Intercept)   8.78      4.33       2.03  0.0437  \n#&gt; 5 math     income        0.0893    0.0331     2.70  0.00749 \n#&gt; 6 math     educ          1.28      0.367      3.47  0.000607\n\nHowever, a bivariate plot of these coefficients is more useful, because it provides visual tests of multivariate hypotheses. heplots::coefplot.mlm() gives displays of the coefficients for a given pair of response variables. For interpretation, it adds the bivariate confidence ellipse for the coefficients, as well as univariate confidence intervals for each response. The univariate intervals are simply the horizontal and vertical shadows of the ellipses on the response variable axes.\nA wrinkle here, as in Section 6.3, is that the coefficients are measured in different units and so coefficient plots for different predictors are more easily compared for standardized variables. To do this, I first re-fit the model using scale(NLSY) …\n\nNLSY_std &lt;- scale(NLSY) |&gt;\n  as.data.frame()\n\nNLSY_std.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY_std)\n\n\ncoefplot(NLSY_std.mod1, fill = TRUE,\n   col = c(\"darkgreen\", \"brown\"),\n   lwd = 2,\n   cex.lab = 1.5,\n   ylim = c(-0.1, 0.5),\n   xlab = \"read coefficient (std)\",\n   ylab = \"math coefficient (std)\")\n\n\n\n\n\n\nFigure 10.12: Bivariate coefficient plot for reading and math with 95% confidence ellipses. The variables have been standardized to make their units comparable.\n\n\n\n\nIn Figure 10.12, the confidence ellipses for income and educ both exclude the origin, which represents the multivariate hypothesis \\(\\mathcal{H}_0 : ( \\beta_\\textrm{read}, \\beta_\\textrm{math} ) = (0, \\, 0)\\), so this hypothesis is rejected. Note that if we only examined the univariate tests for each of the four parameters, we would conclude that for reading, income is not a significant predictor. The orientation of the confidence ellipses indicates the positive correlation between reading and mathematics scores.\n\n10.5.1.1 Behavioral measures\nGiven that the parental background variables are highly predictive of student performance, we might want to know if the behavioral measures antisoc and hyperact add importantly to this. One way to do this is to add these predictors to the model and test for their additional contributions over and above the baseline model.\nYou can do this using update(). In the model formula, “.” on the left hand side corresponds to the previous \\(y\\) variables; on the right-hand side it refers to the \\(x\\)s in the previous model, so I just add the new predictors to that.\n\nNLSY.mod2 &lt;- update(NLSY.mod1, . ~ . + antisoc + hyperact)\nAnova(NLSY.mod2)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;          Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; income    1    0.0383     4.72      2    237 0.0098 **\n#&gt; educ      1    0.0532     6.65      2    237 0.0015 **\n#&gt; antisoc   1    0.0193     2.34      2    237 0.0988 . \n#&gt; hyperact  1    0.0144     1.74      2    237 0.1784   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nEach of these new predictors are individually non-significant according to the Type II tests. Using linearHypothesis() you can test them jointly:\n\ncoefs &lt;- rownames(coef(NLSY.mod2))[-1] |&gt; print()\n#&gt; [1] \"income\"   \"educ\"     \"antisoc\"  \"hyperact\"\n\nlinearHypothesis(NLSY.mod2, coefs[3:4], \n                 title = \"NLSY.mod2 | NLSY.mod1\") |&gt;\n  print(SSP = FALSE)\n#&gt; \n#&gt; Multivariate Tests: NLSY.mod2 | NLSY.mod1\n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; Pillai            2     0.024     1.45      4    476  0.218  \n#&gt; Wilks             2     0.976     1.44      4    474  0.218  \n#&gt; Hotelling-Lawley  2     0.024     1.44      4    472  0.218  \n#&gt; Roy               2     0.022     2.64      2    238  0.073 .\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n10.5.2 Example: School data\nCharnes et al. (1981) describe a large scale “social experiment” in public school education. Seventy school sites across the U.S. participated and a number of variables related to attributes of parents and teachers were used to predict aspects of students’ success in academic indicators (reading, mathematics), but also in their self-esteem. It was conceived in the late 1960’s in relation to a federally sponsored program charged with providing remedial assistance to educationally disadvantaged early primary school students.\nThe study was focused on the management styles used to guide educational planning across schools. In particular, it was primarily designed to compare schools using Program Follow Through (PFT) management methods of taking actions to achieve goals with those of Non Follow Through (NFT).\nHere, I simply focus on the relations between outcome scores on tests of reading, mathematics and self-esteem in relation to five explanatory variables related to parents and teachers:\n\n\neducation level of mother as measured by the percentage of high school graduates among female parents.\n\noccupation, highest occupation of a family member on a rating scale.\n\nvisit, an index of the number of parental visits to the school site.\n\ncounseling, a measure calculated from data on time spent with child on school-related topics such as reading together, etc.\n\nteacher, number of teachers at the given site.\n\nThe dataset, given in heplots::schooldata contains observations for 70 schools.5\nExploratory plots\nThere are eight variables in this example, so a scatterplot matrix or even a corrgram might not be sufficiently revealing. As usual, I tried a number of different methods and found a couple that were interesting and useful.\nMultivariate normality is not required for all the variables in \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\)— it is only required for the residuals, \\(\\boldsymbol{\\Large\\varepsilon}= \\mathbf{Y} - \\widehat{\\mathbf{Y}}\\). Yet, for MMRA problems, sometimes an initial \\(\\chi^2\\) QQ plot provides a handy way to flag possibly unusual values to pay attention to as the analysis proceeds. In Figure 10.13 we see five cases outside the 95% confidence envelope.\n\ndata(schooldata, package = \"heplots\")\nres &lt;- cqplot(schooldata, id.n = 5) |&gt; print()\n#&gt;     DSQ quantile       p\n#&gt; 59 44.6     21.0 0.00714\n#&gt; 44 38.8     18.0 0.02143\n#&gt; 33 27.9     16.5 0.03571\n#&gt; 66 24.0     15.5 0.05000\n#&gt; 35 21.7     14.7 0.06429\n\n# save the case ID numbers\noutliers &lt;- rownames(res) |&gt; as.numeric()\n\n\n\n\n\n\nFigure 10.13: \\(\\chi^2\\) QQ plot of the schooldata variables.\n\n\n\n\nRather than a complete \\(8 \\times 8\\) scatterplot matrix, it is useful here to examine the scatterplots only for each \\(y\\) variable against each of the predictors in \\(\\mathbf{X}\\).6 I’ll take steps to flag some of these possibly unusual cases to see where they appear in these pairwise relations.\nTo prepare for this with ggplot2, it is necessary to reshape the data to long format twice—once for the (\\(q=5\\)) \\(x\\) variables and again for the (\\(p=3\\)) \\(y\\) responses to get all of their \\(q \\times p\\) combinations. That way, we get a data set with variables x and y whose variable names are by xvar and yvar.\n\n# plot predictors vs each response\nxvars &lt;- names(schooldata)[1:5]\nyvars &lt;- names(schooldata)[6:8]\n\nschool_long &lt;- schooldata |&gt;\n  tibble::rownames_to_column(var = \"site\") |&gt;\n  pivot_longer(cols = all_of(xvars), \n               names_to = \"xvar\", values_to = \"x\") |&gt;\n  pivot_longer(cols = all_of(yvars), \n               names_to = \"yvar\", values_to = \"y\") |&gt;\n  mutate(xvar = factor(xvar, xvars), \n         yvar = factor(yvar, yvars))\n\ncar::some(school_long, n=8)\n#&gt; # A tibble: 8 × 5\n#&gt;   site  xvar           x yvar            y\n#&gt;   &lt;chr&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n#&gt; 1 1     teacher     9    reading     54.5 \n#&gt; 2 18    education  28    mathematics 38.2 \n#&gt; 3 26    teacher     7    selfesteem  31.2 \n#&gt; 4 49    occupation  5.29 reading     12.2 \n#&gt; 5 53    counseling 26.3  mathematics 22.0 \n#&gt; 6 61    occupation  2.59 mathematics  7.1 \n#&gt; 7 62    visit       9.89 reading      9.35\n#&gt; 8 64    occupation  8.91 mathematics 24.5\n\nWith this data structure, each scatterplot is a plot of a y against and x, and we can facet this using facet_grid(yvar ~ xvar), giving Figure 10.14.\n\np1 &lt;- ggplot(school_long, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x) +\n  stat_ellipse(geom = \"polygon\", \n               level = 0.95, fill = \"blue\", alpha = 0.2) +\n  facet_grid(yvar ~ xvar, scales = \"free\") +\n  labs(x = \"predictor\", y = \"response\") +\n  theme_bw(base_size = 16)\n\n# label the 3 most unusual points in each panel\np1 + geom_text_repel(data = school_long |&gt; \n                       filter(site %in% outliers[1:3]), \n                     aes(label = site))\n\n\n\n\n\n\nFigure 10.14: Scatterplots of each of the three response variables against each of the five predictors in the schooldata dataset. Three of the points identified as possible multivariate outliers are labeled.\n\n\n\n\nAll of the predictors except for number of teachers show very strong linear relations with the outcome scores. Among the identified points, cases 44 and 59 stand out in all the plots, case 59 being particularly high on all the measures. As well, there is a small cluster of unusual points in the plots for number of teachers.\nFiting models\nLet’s proceed to fit the multivariate regression model. Here “.” on the right-hand side of the model formula means all the other variables in the dataset.\n\nschool.mod &lt;- lm(cbind(reading, mathematics, selfesteem) ~ ., \n                 data=schooldata)\ncar::Anova(school.mod)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;            Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; education   1     0.376    12.43      3     62 1.8e-06 ***\n#&gt; occupation  1     0.567    27.02      3     62 2.7e-11 ***\n#&gt; visit       1     0.260     7.27      3     62 0.00029 ***\n#&gt; counseling  1     0.065     1.43      3     62 0.24297    \n#&gt; teacher     1     0.049     1.07      3     62 0.37003    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThese multivariate tests have a seemingly simple interpretation: parent’s education and occupation and their visits to the schools are highly predictive of student’s outcomes; their counseling efforts and the number of teachers in the schools, not so much.\nYou can get an assessment of the strength of multivariate association from the \\(R^2\\) for each of the responses using glance() for the MLM. All of these are very high.\n\nglance(school.mod)\n#&gt; # A tibble: 3 × 8\n#&gt;   response    r.squared sigma fstatistic numdf dendf  p.value  nobs\n#&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 reading         0.929  4.83       167.     5    64 2.34e-35    70\n#&gt; 2 mathematics     0.917  6.16       141.     5    64 3.37e-33    70\n#&gt; 3 selfesteem      0.993  1.17      1852.     5    64 8.47e-68    70\n\nSimilarly etasq() for an MLM gives an \\(R^2\\)-like measure called \\(\\eta^2\\) of the partial association accounted for each of the predictor terms in the model. These are analogous to the Type II tests from Anova(), which test the additional contribution of each term in the model beyond all the others.\nIn spite of the overwhelming significance of the first three predictors, their variance accounted for is more modest. It is highest for parent’s occupation, followed by education. Parent counseling and teachers contribute very little.\n\netasq(school.mod)\n#&gt;             eta^2\n#&gt; education  0.3756\n#&gt; occupation 0.5666\n#&gt; visit      0.2603\n#&gt; counseling 0.0647\n#&gt; teacher    0.0491",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#model-diagnostics-for-mlms",
    "href": "10-mlm-review.html#model-diagnostics-for-mlms",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.6 Model diagnostics for MLMs",
    "text": "10.6 Model diagnostics for MLMs\nModel building, visualization and interpretation is often an iterative process. You fit a model and calculate some goodness of fit measures (\\(R^2\\) for responses, \\(\\eta^2\\) for predictors). If these are reasonably strong, you feel happy and proceed to graphical displays to help you understand what you’ve found and explain it to others.\nBut wait: did you check the assumptions of the MLM? As in univariate models, diagnostic plots can help you spot problems in the data (unusual cases) or in the model (nonlinear relations, omitted predictors or interactions). You sometimes need to go circle back and fit a revised model, starting the process again.\n\n\nFor multivariate regression models, I consider the assumed multivariate normality of residuals and multivariate influence here.\n\n10.6.1 Multivariate normality of residuals\nOne easy thing to do is to check for multivariate normality of the residuals. Given that we found a few noteworthy points in Figure 10.13, a \\(\\chi^2\\) QQ plot of the residuals in the model will tell us if any of these are really problematic. The pattern of points relative to the confidence band gives a rough indication of overall multivariate normality.\n\ncqplot(school.mod, id.n = 5)\n\n\n\n\n\n\nFigure 10.15: \\(\\chi^2\\) QQ plot of the residuals in the schooldat multivariate regression model.\n\n\n\n\nSo, you can see that among the cases that stood out in the cqplot() of the observed variables (Figure 10.13), only case 35 attracts attention here, and it is well within the confidence band. Case 59, which was the largest in all the pairwise scatterplots (Figure 10.14) seems not unusual in the fitted model. It is a high-leverage point, but appeared to be well-fitted in all the simple regressions, except in those for teacher.\nIt is useful to contrast this with what we get from formal tests that the residuals are strictly multivariate normal. The MVN package (Korkmaz et al., 2025) provides mvn() for this, which performs a wide variety of normality tests. The most widely used of these is due to Mardia (1974), which gives multivariate tests of skewness (lack of symmetry) and kurtosis (length of the tails).\nApplying this to the residuals from the schools multivariate regression shows that multivariate normality is rejected here. Based on other evidence, this doesn’t seem particularly troubling.\n\nschool.mvn &lt;- mvn(residuals, mvn_test = \"mardia\")\n# print multivariate and univariate tests\nsummary(school.mvn, select = \"mvn\")\n#&gt;              Test Statistic p.value     Method      MVN\n#&gt; 1 Mardia Skewness      1.92   0.750 asymptotic ✓ Normal\n#&gt; 2 Mardia Kurtosis     -1.17   0.244 asymptotic ✓ Normal\nsummary(school.mvn, select = \"univariate\")\n#&gt;               Test Variable Statistic p.value Normality\n#&gt; 1 Anderson-Darling    start     0.307   0.524  ✓ Normal\n#&gt; 2 Anderson-Darling   amount     0.625   0.085  ✓ Normal\n\nmvn() also provides a variety of tests for univariate normality for each of the response variables. These are all OK.\n\n10.6.2 Multivariate influence\nAgain, what we see in simple scatterplots can be misleading because they ignore all the other variables in a model. But looking back after fitting a model and examining diagnostic plots can often be illuminating. Among the ideas we inherit from univariate models, the influence of particular observations on the results of analysis should be high on your list.\nThe multivariate extension of the diagnostic measures of leverage and influence, and influence plots (Section 6.6) is provided by the mvinfluence package (Friendly, 2025). The theory behind this is due to Barrett & Ling (1992) and better illustrated in Barrett (2003). Mathematical details of this generalization are given in help(\"mvinfluence-package\").\nFigure 10.16 shows one form of an influence plot for the school.mod model. Because multiple response variables are involved, this plots a measure of the squared studentized residuals for each observation against a generalized version of hat values, so potentially “bad” observations appear in the upper left corner. The size of the bubble symbol is proportional to a generalization of Cook’s distance, the measure of influence based on the change in all coefficients if each case was deleted from the analysis.\n\ninfluencePlot(school.mod, id.n=4, \n              type=\"stres\",\n              cex.lab = 1.5)\n#&gt;        H      Q  CookD     L      R\n#&gt; 33 0.328 0.2017 0.7054 0.488 0.3001\n#&gt; 35 0.101 0.2825 0.3038 0.112 0.3142\n#&gt; 44 0.503 0.2984 1.6022 1.014 0.6009\n#&gt; 59 0.568 0.4937 2.9938 1.317 1.1441\n#&gt; 66 0.355 0.0174 0.0657 0.551 0.0269\n\n\n\n\n\n\nFigure 10.16: Influence plot for the schooldat multivariate regression model. Five cases are labeled as “noteworthy” on either axis.\n\n\n\n\nThat does not look good! You can see that cases 44 and 59 are actually quite troublesome here, but it turns out for different reasons. Take another look at Figure 10.14. You can see that case 59 is the most extreme on all the predictors, giving it very high leverage and therefore pulling the regression lines toward it in most of the plots, except those for number of teachers, where it also has large residuals. Case 44, on the other hand, stands out as high-leverage on only a few of the predictor–response combinations, but enough to give it a large multivariate hat value. It is also a point that is furthest from the regression lines.\nWhat should be done? An appropriate action would be to re-fit the model, reducing the impact of these cases, in what I call a sensitivity test:\n\nDo the main conclusions change with those cases removed? That is, do any model terms change in significance tests or do coefficients change sign?\nDo the relative sizes of effects for predictors change enough to affect interpretation? That is, how much do the coefficients change?\n\nThe easiest solution is to just omit these troubling cases. You can do this using update(), specifying the data argument to be the subset of rows without the bad boys.\n\nbad &lt;- c(44, 59)\nOK &lt;- (1:nrow(schooldata)) |&gt; setdiff(bad)\nschool.mod2 &lt;- update(school.mod, data = schooldata[OK,])\nAnova(school.mod2)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;            Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; education   1     0.211     5.35      3     60  0.0025 ** \n#&gt; occupation  1     0.422    14.62      3     60 2.9e-07 ***\n#&gt; visit       1     0.191     4.73      3     60  0.0050 ** \n#&gt; counseling  1     0.053     1.13      3     60  0.3459    \n#&gt; teacher     1     0.064     1.37      3     60  0.2618    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe results of Anova() on this model tell us that the three significant predictors— occupation, education and visit— are still so, but slightly less so for the last two of these. To compare the coefficients in the new model compared to the old you can calculate the relative difference, \\(| \\mathbf{B}_1 - \\mathbf{B}_2 | / \\mathbf{B}_1\\), which are\n\nreldiff &lt;- function(x, y, pct=TRUE) {\n  res &lt;- abs(x - y) / x\n  if (pct) res &lt;- 100 * res\n  res\n}\n\nreldiff(coef(school.mod)[-1,], coef(school.mod2)[-1,]) |&gt;\n  round(1)\n#&gt;            reading mathematics selfesteem\n#&gt; education     10.1        19.8      -75.7\n#&gt; occupation    11.0        10.2       19.4\n#&gt; visit       -103.6       -76.8        2.8\n#&gt; counseling  -332.2       305.3     -115.4\n#&gt; teacher       -7.5        -5.6        7.0\n\nAs you can see, the effects on the coefficients for visit and counseling` are dramatic.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#sec-ANCOVA-MANCOVA",
    "href": "10-mlm-review.html#sec-ANCOVA-MANCOVA",
    "title": "10  Multivariate Linear Models",
    "section": "\n10.7 ANCOVA \\(\\rightarrow\\) MANCOVA",
    "text": "10.7 ANCOVA \\(\\rightarrow\\) MANCOVA\nTODO: Consider moving this to Chapter 11 and use much of the heplots MMRA vignette.\nIn univariate linear models, analysis of covariance (ANCOVA) is most often used in a situation where we want to compare the mean response, \\(\\bar{y}_j\\) for different groups (defined by one or more factors), but where there are one or more quantitative predictors \\(\\mathbf{x}_1, \\dots\\) that should be taken into account for our comparisons to make sense. The simplest case is when \\(\\mathbf{x}\\) is a pre-test score on the same measure, or when it is a background measure like age or level of education that we want to control for, to adjust for differences among the groups.\nMore generally, ANCOVA and its’ multivariate MANCOVA brother are used for situations where the model matrix \\(\\mathbf{X}\\) contains a mixture of factor variables and quantitative predictors, called “covariates”. In this wider context, there are two flavors of analysis with different emphasis on the factors or the covariates:\n\ntrue ANCOVA/MANOVA: Attention is centered on the differences between the group means, but controlling for any difference in the covariate(s). This requires assuming that the slopes for the groups are all the same.\nhomogeneity of regression: Here the focus is on the regression relations between the \\(\\mathbf{y}\\)s and the predictor \\(\\mathbf{x}\\)s, but we might also want to determine if the regression slopes are the same for all groups defined by the factors.\n\nIn the ANCOVA flavor, the model fits additive effects of the group factor(s) and the covariate(s), while the homogeneity of regression flavor adds interaction terms between groups and the \\(\\mathbf{x}\\)s. The test for homogeneity of regression is the added effect of the interaction terms:\nmod1 &lt;- lm(y ~ Group + x)            # ANCOVA model\nmod2 &lt;- lm(y ~ Group + x + Group:x)  # allow separate slopes\nmod2 &lt;- lm(y ~ Group * x)            # same as above\n\nanova(mod1, mod2)            # test homogeneity of regression\n\nFigure 10.17 illustrates these cases for a hypothetical two-group design studying the the effect of an exercise program treatment on weight, recorded pre- (\\(x\\)) and post- (\\(y\\)) compared to a control group given no treatment. In panel (a) the slopes for the two groups are approximately equal, so the effect of treatment can be estimated by the difference in the fitted values of \\(\\hat{y}_i\\) at the average value of \\(x\\). In panel (b), the slope for the treated group is considerably greater than that for the control group, so the difference between the groups varies with \\(x\\).\n\n\n\n\n\n\n\nFigure 10.17: Two possible outcome patterns for a two-group design assessing the effect of a treatment on weight, measured pre- and post-treatment. (a) Additive effects of Group and \\(x\\); (b) Different slopes for the two groups. Plus signs show the means \\((\\bar{x}_i, \\bar{y}_i)\\) for the two groups.\n\n\n\n\n\n\n10.7.1 Example: Paired-associate tasks and academic performance\nTo what extent can simple tests of paired-associate learning7 predict measures of aptitude and achievement in kindergarten children? This was the question behind an experiment by William Rohwer at the University of California, Berkeley.\nThere were three outcome measures, one verbal and two visually-based:\n\nA student achievement test (SAT),\nPeabody Picture Vocabulary test (PPVT),\nRaven Progressive Matrices test (Raven).\n\nFour paired-associate tasks were used, which differed in the syntactic and semantic relationship between the stimulus and response terms in each pair. These are called named (n), still (s), named still (ns), named action (na), and sentence still (ss).\nRohwer’s data, taken from Timm (1975), is given in heplots::Rohwer. But there’s a MANCOVA wrinkle: Performance on the academic tasks is well-known to vary with socioeconomic status of the parents or the school they attend. A simple design was to collect data from children in two schools, one in a low SES neighborhood (\\(n=37\\)) and the other an upper-class high SES one (\\(n=32\\)). The data look like this:\n\n\ndata(Rohwer, package = \"heplots\")\nset.seed(42)\nRohwer |&gt; dplyr::sample_n(6)\n#&gt;    group SES SAT PPVT Raven n  s ns na ss\n#&gt; 49     2  Hi  88  105    21 2 11 10 26 22\n#&gt; 65     2  Hi  50   96    13 5  8 20 28 26\n#&gt; 25     1  Lo   6   57    10 0  1 16 15 17\n#&gt; 18     1  Lo  45   54    10 0  6  6 14 16\n#&gt; 69     2  Hi  50   78    19 5 10 18 27 26\n#&gt; 64     2  Hi  24  102    16 4 17 21 27 31\n\nFollowing the scheme for reshaping the data used in Figure 10.14, a set of scatterplots of each predictor against each response will give a useful initial look at the data. There’s a lot to see here, so the plot in Figure 10.18 focuses attention on the regression lines for the two groups and their data ellipses.\n\nCodeyvars &lt;- c(\"SAT\", \"PPVT\", \"Raven\" )      # outcome variables\nxvars &lt;- c(\"n\", \"s\", \"ns\", \"na\", \"ss\")   # predictors\n\nRohwer_long &lt;- Rohwer %&gt;%\n  dplyr::select(-group) |&gt;\n  tidyr::pivot_longer(cols = all_of(xvars), \n                      names_to = \"xvar\", values_to = \"x\") |&gt;\n  tidyr::pivot_longer(cols = all_of(yvars), \n                      names_to = \"yvar\", values_to = \"y\") |&gt;\n  dplyr::mutate(xvar = factor(xvar, levels = xvars),\n                yvar = factor(yvar, levels = yvars))\n\nggplot(Rohwer_long, aes(x, y, color = SES, shape = SES, fill = SES)) +\n  geom_jitter(size=0.8) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              formula = y ~ x, \n              linewidth = 1.5) +\n  stat_ellipse(geom = \"polygon\", alpha = 0.1) +\n  labs(x = \"Predictor (PA task)\",\n       y = \"Response (Academic)\") +\n  facet_grid(yvar ~ xvar,            # plot matrix of Y by X\n             scales = \"free\") +\n  theme_bw(base_size = 16) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 10.18: Scatterplots of each of the three response variables against each of the five predictors in the Rohwer dataset.\n\n\n\n\nYou can see here that the high-SES group generally performs better than the low group. The regression lines have similar slopes in some of the panels, but not all. The low SES group also appears to have larger variance on most of the PA tasks.\nMANCOVA model\nNevertheless, I fit the MANCOVA model that allows a test of different means for the two SES groups on the responses, but constrains the slopes for the PA covariates to be equal. Only two of the PA tasks (na and ns) show individually significant effects in the multivariate tests.\n\n# Make SES == 'Lo' the reference category\nRohwer$SES &lt;- relevel(Rohwer$SES, ref = \"Lo\")\n\nRohwer.mod1 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES + n + s + ns + na + ss, \n                  data=Rohwer)\nAnova(Rohwer.mod1)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;     Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; SES  1     0.379    12.18      3     60 2.5e-06 ***\n#&gt; n    1     0.040     0.84      3     60  0.4773    \n#&gt; s    1     0.093     2.04      3     60  0.1173    \n#&gt; ns   1     0.193     4.78      3     60  0.0047 ** \n#&gt; na   1     0.231     6.02      3     60  0.0012 ** \n#&gt; ss   1     0.050     1.05      3     60  0.3770    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nYou can also examine the tests of the univariate ANCOVA models for each of the responses using glance() or heplots::uniStats(). All are significantly related, but the PPVT measure has the largest \\(R^2\\) by far.\n\nuniStats(Rohwer.mod1)\n#&gt; Univariate tests for responses in the multivariate linear model Rohwer.mod1 \n#&gt; \n#&gt;         R^2     F df1 df2 Pr(&gt;F)    \n#&gt; SAT   0.295  4.33   6  62  0.001 ** \n#&gt; PPVT  0.628 17.47   6  62  1e-11 ***\n#&gt; Raven 0.211  2.76   6  62  0.019 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTo help interpret these effects, bivariate coefficient plots for the paired associate tasks are shown in Figure 10.19. (The coefficients for the group variable SES are on a different scale and so are omitted here.) From this you can see that the named still and named action tasks have opposite signs: contrary to expectations, ns is negatively associated with the measures of aptitude and achievement (when the other predictors are adjusted for). ::: {.cell layout-align=“center”}\ncoefplot(Rohwer.mod1, parm = 2:6,\n         fill = TRUE,\n         level = 0.68,\n         cex.lab = 1.5)\ncoefplot(Rohwer.mod1, parm = 2:6, variables = c(1,3),\n         fill = TRUE,\n         level = 0.68,\n         cex.lab = 1.5)\n\n\n\n\n\n\nFigure 10.19: Bivariate coefficient plots for the MANCOVA model with confidence ellipses of 68% coverage.\n\n\n\n:::\nTODO: For interpretation, it would be nice to know how many items were used for each of the PA tasks. The range of na goes up to 35, but others are less.\nAdjusted means\nFrom the analysis of covariance perspective, interest is often centered on estimating the differences between the group means, but adjusting or controlling for differences on the covariates. From the table of means below, you can see that the high SES group performs better on all three response variables, but this group also has higher scores on the paired associate tasks.\n\nmeans &lt;- Rohwer |&gt;\n  group_by(SES) |&gt;\n  summarise_all(mean) |&gt;\n  print()\n#&gt; # A tibble: 2 × 10\n#&gt;   SES   group   SAT  PPVT Raven     n     s    ns    na    ss\n#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Lo        1  31.3  62.6  13.2  2.54  6.92  13.5  22.4  18.4\n#&gt; 2 Hi        2  47.7  83.1  15    4.59  7.25  14.5  24.3  21.5\n\nThe adjusted mean differences are simply the values estimated by the coefficients for SES in the model. These are smaller than the differences between the observed means. ::: {.cell layout-align=“center”}\nmeans[2, 3:5] - means[1, 3:5]  \n#&gt;    SAT PPVT Raven\n#&gt; 1 16.4 20.4  1.76\n\n# adjusted means\ncoef(Rohwer.mod1)[2,]\n#&gt;   SAT  PPVT Raven \n#&gt;  8.80 16.88  1.59\n:::\nTODO: do this with a CI for the effects\nHomogeneity of regression\nThe MANCOVA model, Rohwer.mod1, has relatively simple interpretations (a large effect of SES, with ns and na as the major predictors) but the test of the SES effect relies on the assumption of homogeneity of slopes for the predictors. We can test this assumption as follows, by adding interactions of SES with each of the covariates:\n\nRohwer.mod2 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES * (n + s + ns + na + ss),\n                  data=Rohwer)\nAnova(Rohwer.mod2)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;        Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; SES     1     0.391    11.78      3     55 4.5e-06 ***\n#&gt; n       1     0.079     1.57      3     55 0.20638    \n#&gt; s       1     0.125     2.62      3     55 0.05952 .  \n#&gt; ns      1     0.254     6.25      3     55 0.00100 ***\n#&gt; na      1     0.307     8.11      3     55 0.00015 ***\n#&gt; ss      1     0.060     1.17      3     55 0.32813    \n#&gt; SES:n   1     0.072     1.43      3     55 0.24417    \n#&gt; SES:s   1     0.099     2.02      3     55 0.12117    \n#&gt; SES:ns  1     0.118     2.44      3     55 0.07383 .  \n#&gt; SES:na  1     0.148     3.18      3     55 0.03081 *  \n#&gt; SES:ss  1     0.057     1.12      3     55 0.35094    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt appears from the above that there is only weak evidence of unequal slopes from the separate SES: terms; only that for SES:na is individually significant. The evidence for heterogeneity is stronger, however, when these terms are tested collectively using linearHypothesis(). I use a small grep() trick here to find the interaction terms, which have a “:” in their names.\n\n# test interaction terms jointly\ncoefs &lt;- rownames(coef(Rohwer.mod2)) \ninteractions &lt;- coefs[grep(\":\", coefs)]\n\nprint(linearHypothesis(Rohwer.mod2, interactions), SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; Pillai            5     0.418     1.85     15    171 0.0321 * \n#&gt; Wilks             5     0.624     1.89     15    152 0.0277 * \n#&gt; Hotelling-Lawley  5     0.539     1.93     15    161 0.0240 * \n#&gt; Roy               5     0.385     4.38      5     57 0.0019 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSeparate models\nModel Rohwer.mod2 with all interaction terms essentially fits a separate slope for each of the low and high SES groups for all responses with each of the predictor PA tasks. This is similar in spirit to what we would get if we fit a separate multivariate regression model for each of the groups, but parameterized differently: The heterogeneous regression model gives, for the interaction terms estimates of the difference in slopes between groups, while the separate-regressions approach gives separate slope estimates for each of the groups. These are equivalent, in the sense that the estimates for each approach can be derived from the other.\nThey are not equivalent in testing however, because the full model uses a combined pooled within-group error covariance, allows hypotheses about equality of slopes and intercepts to be tested directly and has greater power because it uses the total sample size. Here, I simply illustrate the mechanics of fitting separate models using the subset argument to lm().\n\nRohwer.sesHi &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Hi\")\nAnova(Rohwer.sesHi)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df Pr(&gt;F)   \n#&gt; n   1     0.202     2.02      3     24 0.1376   \n#&gt; s   1     0.310     3.59      3     24 0.0284 * \n#&gt; ns  1     0.358     4.46      3     24 0.0126 * \n#&gt; na  1     0.465     6.96      3     24 0.0016 **\n#&gt; ss  1     0.089     0.78      3     24 0.5173   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRohwer.sesLo &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Lo\")\nAnova(Rohwer.sesLo)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df Pr(&gt;F)  \n#&gt; n   1    0.0384     0.39      3     29  0.764  \n#&gt; s   1    0.1118     1.22      3     29  0.321  \n#&gt; ns  1    0.2252     2.81      3     29  0.057 .\n#&gt; na  1    0.2675     3.53      3     29  0.027 *\n#&gt; ss  1    0.1390     1.56      3     29  0.220  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe strength of evidence for the predictors na and ns is weaker here than when tested in the full heterogeneous model.\n\n\nPackages used here:\n13 packages used here: broom, car, carData, dplyr, ggplot2, ggrepel, heplots, knitr, matlib, mvinfluence, MVN, patchwork, tidyr\n\n\n\n\n\nBarrett, B. E. (2003). Understanding influence in multivariate regression. Communications in Statistics - Theory and Methods, 32(3), 667–680. https://doi.org/10.1081/STA-120018557\n\n\nBarrett, B. E., & Ling, R. F. (1992). General classes of influence measures for multivariate regression. Journal of the American Statistical Association, 87(417), 184–191. https://www.jstor.org/stable/i314301\n\n\nCharnes, A., Cooper, W. W., & Rhodes, E. (1981). Evaluating program and managerial efficiency: An application of data envelopment analysis to program follow through. Management Science, 27(6), 668–697. http://www.jstor.org/stable/2631155\n\n\nFriendly, M. (2025). Mvinfluence: Influence measures and diagnostic plots for multivariate linear models. https://github.com/friendly/mvinfluence\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with R: Visualization and modeling techniques for categorical and count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nHarrell, F. E. (2015). Regression modeling strategies: With applications to linear models, logistic and ordinal regression, and survival analysis. Springer International Publishing. https://books.google.ca/books?id=sQ90rgEACAAJ\n\n\nHuang, F. L. (2019). MANOVA: A procedure whose time has passed? Gifted Child Quarterly, 64(1), 56–60. https://doi.org/10.1177/0016986219887200\n\n\nHuberty, C. J., & Morris, J. D. (1989). Multivariate analysis versus multiple univariate analyses. Psychological Bulletin, 105(2), 302–308. https://doi.org/10.1037/0033-2909.105.2.302\n\n\nKorkmaz, S., Goksuluk, D., & Zararsiz, G. (2025). MVN: Multivariate normality tests. https://selcukorkmaz.github.io/mvn-tutorial/\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis with applications. Biometrika, 57(3), 519–530. https://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMardia, K. V. (1974). Applications of some measures of multivariate skewness and kurtosis in testing normality and robustness studies. Sankhya: The Indian Journal of Statistics, Series B, 36(2), 115–128. http://www.jstor.org/stable/25051892\n\n\nMeyers, L. S., Gamst, G., & Guarino, A. J. (2006). Applied multivariate research: Design and interpretation. SAGE Publications.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52(3–4), 591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B: Methodological, 58, 267–288.\n\n\nTimm, N. H. (1975). Multivariate analysis with applications in education and psychology. Wadsworth (Brooks/Cole).\n\n\nWarne, F. T. (2014). A primer on multivariate analysis of variance(MANOVA) for behavioral scientists. Practical Assessment, Research & Evaluation, 19(1). https://scholarworks.umass.edu/pare/vol19/iss1/17/\n\n\nYee, T. W. (2015). Vector generalized linear and additive models: With an implementation in r. Springer.\n\n\nYee, T. W. (2025). VGAM: Vector generalized linear and additive models. https://CRAN.R-project.org/package=VGAM",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "10-mlm-review.html#footnotes",
    "href": "10-mlm-review.html#footnotes",
    "title": "10  Multivariate Linear Models",
    "section": "",
    "text": "There’s a bit of a puzzle here, and therefore a gap in methods and therefore an opportunity. The classical linear models fit by lm() extend naturally to non-gaussian data via glm() which provides for other families (binary: Bernoulli, count data: Poisson). The standard lm() extends quite naturally to a multivariate responses. Yet the combination of these ideas— non-gaussian multivariate models— remains elusive. The VGAM (Yee (2015), Yee (2025)) handles the bivariate cases of logistic (and probit) regression, but not much more. See Friendly & Meyer (2016), Sec. 10.4 for an example and graphs of odds ratios in these models. There is a lot more to do on this topic.↩︎\nA slight hiccup in notation is that the uppercase for the Greek Beta (\\(\\boldsymbol{\\beta}\\)) is the same as the uppercase Roman \\(\\mathbf{B}\\), so I use \\(\\mathbf{b}_1 , \\mathbf{b}_2 , \\dots\\) below to refer to its’ columns.↩︎\n“Automatic” model selection procedures like stepwise regression, while seemingly attractive are dangerous in that can increase false positives or drop variables that should, on logical grounds, be included in the model. See Stepwise selection of variables in regression is Evil and Harrell (Harrell, 2015).↩︎\nOther choices are possible: we could instead try to model the behavioral variables, antisocial and hyperact first, and then determine if the parental variables add appreciably to this. Modeling choices aren’t arbitrary. They should reflect the aims of a study and the story you want to tell about the result.↩︎\nIn this, schools 1–49 were PFT sites and the remaining sites 50–70 were NFT. A separate dataset heplots::schoolsites provides other information on the schools, such as the general education style, region of the U.S., size of the city and that of the student population.↩︎\nGGally has a function ggduo() that does something similar, plotting each of one set of variables against another. Like ggpairs() it allows for generalizations of a scatterplot where combinations of discrete factors and continuous variables can be displayed with appropriate visualizations for each.↩︎\nPaired-associate learning are among the simplest tests of memory and learning. The subject is given a list of pairs of words or nonsense syllables, like “banana - house” or “YYZ - Toronto” to learn. On subsequent trials she is given the stimulus term of each pair (“banana”, “YYZ”) and asked to reply with the correct response (“house”, “Toronto”).↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate Linear Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html",
    "href": "11-mlm-viz.html",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "11.1 HE plot framework\nThe methods discussed in Chapter 10 provide the basis for a rather complete multivariate analysis of traditional univariate methods for the same designs. You can carry out multiple regression, ANOVA, or indeed, any classical linear model with the standard collection of analysis tools you use for a single outcome variable, but naturally extended in most cases to having several outcomes to analyse together. The key points are:\nAs nice as these mathematical and statistical ideas might be, the fact that the analysis is conducted for the response variables collectively, means that it may be harder to interpret and explain what this means about the separate responses. Here’s where multivariate model visualization comes to the rescue!\nPackages\nIn this chapter we use the following packages. I load them now.\nChapter 9 illustrated the basic ideas of the framework for visualizing multivariate linear models in the context of a simple two group design, using Hotelling’s \\(T^2\\). The main ideas were illustrated in Figure 9.9.\nHaving described the statistical ideas behind the MLM in Chapter 10, we can proceed to extend this framework to larger designs. Figure 11.1 illustrates these ideas using the simple one-way MANOVA design of the dogfood data from Section 10.2.1.\nFigure 11.1: Dogfood quartet: Illustration of the conceptual ideas of the HE plot framework for the dogfood data. (a) Scatterplot of the data; (b) Summary using data ellipses; (c) HE plot shows the variation in the means in relation to pooled within group variance; (d) Transformation from data space to canonical space\nFor more complex models such as MANOVA with multiple factors or multivariate multivariate regression, there is one sum of squares and products matrix (SSP), and therefore one \\(\\mathbf{H}\\) ellipse for each term in the model. For example, in a two-way MANOVA design with the model formula (y1, y2) ~ A + B + A*B and equal sample sizes in the groups, the total sum of squares accounted for by the model is \\[\\begin{aligned}\n\\mathbf{SSP}_{\\text{Model}} & = \\mathbf{SSP}_{A} + \\mathbf{SSP}_{B} + \\mathbf{SSP}_{AB} \\\\\n                            & = \\mathbf{H}_A + \\mathbf{H}_B + \\mathbf{H}_{AB} \\:\\: .\n\\end{aligned}\\]",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-he-framework",
    "href": "11-mlm-viz.html#sec-he-framework",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "In data space (a), each group is summarized by its data ellipse (b), representing the means and covariances.\nVariation against the hypothesis of equal means can be seen by the \\(\\mathbf{H}\\) ellipse in the HE plot (c), representing the data ellipse of the fitted values. Error variance is shown in the \\(\\mathbf{E}\\) ellipse, representing the pooled within-group covariance matrix, \\(\\mathbf{S}_p\\) and the data ellipse of the residuals from the model. For the dogfood data, the group means have a negative relation: longer time to start eating is associated with a smaller amount eaten.\nThe MANOVA (or Hotelling’s \\(T^2\\)) is formally equivalent to a discriminant analysis, predicting group membership from the response variables which can be seen in data space. (The main difference is emphasis and goals: MANOVA seeks to test differences among group means, while discriminant analysis aims at classification of the observations into groups.)\nThis effectively projects the \\(p\\)-dimensional space of the predictors into the smaller canonical space (d) that shows the greatest differences among the groups. As in a biplot, vectors show the relations of the response variables with the canonical dimensions.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#he-plot-construction",
    "href": "11-mlm-viz.html#he-plot-construction",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.2 HE plot construction",
    "text": "11.2 HE plot construction\nThe HE plot is constructed to allow a direct visualization of the “size” of hypothesized terms in a multivariate linear model in relation to unexplained error variation. These can be displayed in 2D or 3D plots, so I use the term “ellipsoid” below to cover all cases.\nError variation is represented by a standard 68% data ellipsoid of the \\(\\mathbf{E}\\) matrix of the residuals in \\(\\boldsymbol{\\Large\\varepsilon}\\). This is divided by the residual degrees of freedom, so the size of \\(\\mathbf{E} / \\text{df}_e\\) is analogous to a mean square error in univariate tests. The choice of 68% coverage allows you to ``read’’ the residual standard deviation as the half-length of the shadow of the \\(\\mathbf{E}\\) ellipsoid on any axis (see Figure 3.10). The \\(\\mathbf{E}\\) ellipsoid is then translated to the overall (grand) means \\(\\bar{\\mathbf{y}}\\) of the variables plotted, which allows us to show the means for factor levels on the same scale, facilitating interpretation. In the notation of Equation 3.2, the error ellipsoid is given by \\[\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{E}) = \\bar{\\mathbf{y}} \\; \\oplus \\; c\\,\\mathbf{E}^{1/2} \\:\\: ,\n\\tag{11.1}\\] where \\(c = \\sqrt{2 F_{2, n-2}^{0.68}}\\) for 2D plots and \\(c = \\sqrt{3 F_{3, n-3}^{0.68}}\\) for 3D for standard 68% coverage.\nAn ellipsoid representing variation in the means of a factor (or any other term reflected in a general linear hypothesis test, Equation 10.7) in the \\(\\mathbf{H}\\) matrix is simply the data ellipse of the fitted values for that term.\nDividing the hypothesis matrix by the error degrees of freedom, giving \\(\\mathbf{H} / \\text{df}_e\\), puts this on the same scale as the \\(\\mathbf{E}\\) ellipse.  I refer to this as effect size scaling, because it is similar to an effect size index used in univariate models, e.g., \\(ES = (\\bar{y}_1 - \\bar{y}_2) / s_e\\) in a two-group, univariate design.\n\n\n11.2.1 Example: Iris data\nPerhaps the most famous (or infamous) dataset in the history of multivariate data analysis is that of measurements on three species of Iris flowers collected by Edgar Anderson (1935) in the Gaspé Peninsula of Québec, Canada. Anderson wanted to quantify the outward appearance (“morphology”: shape, structure, color, pattern, size) of species as a method to study variation within and between such groups. Although Anderson published in the obscure Bulletin of the American Iris Society, R. A. Fisher (1936) saw this as a challenge and opportunity to introduce the method now called discriminant analysis—how to find a weighted composite of variables to best discriminate among existing groups.\n\n\n\n\n\n\n\nHistory corner\n\n\n\nI said “infamous” above because Fisher published in the Annals of Eugenics, was an ardent eugenicist himself, and the work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. Through guilt by association, the Iris data, having mistakenly been called “Fisher’s Iris Data”, has become deprecated, even called “racist data”.1 The voices of the Setosa, Versicolor and Virginica of Gaspé protest: we don’t have a racist bone in out body and nor prejudice against any other species.\nBodmer et al. (2021) present a careful account of Fisher’s views on eugenics within the context of his time and his contributions to modern statistical theory and practice. Fisher’s views on race were largely formed by Darwin and Galton, but “nearly all of Fisher’s statements were about populations, groups of populations, or the human species as a whole”. Regardless, the iris data were Anderson’s and should not be blamed. After all, if Anderson had lent his car to Fisher, would the car be tainted by Fisher’s eugenicist leanings?\n\n\n\n\n\n\n\n\n\n\nFigure 11.2: Diagram of an iris flower showing the measurements of petal and sepal size. Each flower has three sepals and three alternating petals. The sepals have brightly colored central sections. Source: Gayan De Silva (2020)\n\n\n\n\nSo that we understand what the measurements represent, Figure 11.2 superposes labels on a typical iris flower. Sepals are like ostentatious petals, with attractive decorations in the central section. Length is the distance from the center to the tip and width is the transverse dimension.\nAs always, it is useful to start with overview displays to see the data. A scatterplot matrix (Figure 11.3) shows that versicolor and virginica are more similar to each other than either is to setosa, both in their pairwise means (setosa are smaller) and in the slopes of regression lines. Further, the ellipses suggest that the assumption of constant within-group covariance matrices is problematic: While the shapes and sizes of the concentration ellipses for versicolor and virginica are reasonably similar, the shapes and sizes of the ellipses for setosa are different from the other two.\n\niris_colors &lt;-c(\"blue\", \"darkgreen\", \"brown4\")\nscatterplotMatrix(~ Sepal.Length + Sepal.Width + \n                    Petal.Length + Petal.Width | Species,\n  data = iris,\n  col = iris_colors,\n  pch = 15:17,\n  smooth=FALSE,\n  regLine = TRUE,\n  ellipse=list(levels=0.68, fill.alpha=0.1),\n  diagonal = FALSE,\n  legend = list(coords = \"bottomleft\", \n                cex = 1.3, pt.cex = 1.2))\n\n\n\n\n\n\nFigure 11.3: Scatterplot matrix of the iris dataset. The species are summarized by 68% data ellipses and linear regression lines in each pairwise plot.\n\n\n\n\n\n11.2.2 MANOVA model\nWe proceed nevertheless to fit a multivariate one-way ANOVA model to the iris data. The MANOVA model for these data addresses the question: “Do the means of the Species differ significantly for the sepal and petal variables taken together?” \\[\n\\mathcal{H}_0 : \\mathbf{\\mu}_\\textrm{setosa} = \\mathbf{\\mu}_\\textrm{versicolor} = \\mathbf{\\mu}_\\textrm{virginica}\n\\]\nBecause there are three species, the test involves \\(s = \\min(p, g-1) =2\\) degrees of freedom, and we are entitled to represent this by two 1-df contrasts, or sub-questions. From the separation among the groups shown in Figure 11.3 (or more botanical knowledge), it makes sense to compare:\n\nSetosa vs. others: \\(\\mathbf{c}_1 = (1,\\: -\\frac12, \\: -\\frac12)\\)\n\nVersicolor vs. Virginica: : \\(\\mathbf{c}_1 = (0,\\: 1, \\: -1)\\)\n\n\nYou can do this by putting these vectors as columns in a matrix and assigning this to the contrasts() of Species. It is important to do this before fitting with lm(), because the contrasts in effect determine how the \\(\\mathbf{X}\\) matrix is setup, and hence the names of the coefficients representing Species.\n\nC &lt;- matrix(c(1,-1/2,-1/2,  \n              0,   1,  -1), nrow=3, ncol=2)\ncontrasts(iris$Species) &lt;- C\ncontrasts(iris$Species)\n#&gt;            [,1] [,2]\n#&gt; setosa      1.0    0\n#&gt; versicolor -0.5    1\n#&gt; virginica  -0.5   -1\n\nNow let’s fit the model. As you would expect from Figure 11.3, the differences among groups are highly significant.\n\niris.mod &lt;- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~\n                 Species, data=iris)\nAnova(iris.mod)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;         Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Species  2      1.19     53.5      8    290 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAs a quick follow-up, it is useful to examine the univariate tests for each of the iris variables, using heplots::glance() or heplots::uniStats(). It is of interest that the univariate \\(R^2\\) values are much larger for the petal variables than the sepal length and width.2. For comparison, heplots::etasq() gives the overall \\(\\eta^2\\) proportion of variance accounted for in all responses.\n\nglance(iris.mod)\n#&gt; # A tibble: 4 × 8\n#&gt;   response     r.squared sigma fstatistic numdf dendf  p.value  nobs\n#&gt;   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Sepal.Length     0.619 0.515      119.      2   147 1.67e-31   150\n#&gt; 2 Sepal.Width      0.401 0.340       49.2     2   147 4.49e-17   150\n#&gt; 3 Petal.Length     0.941 0.430     1180.      2   147 2.86e-91   150\n#&gt; 4 Petal.Width      0.929 0.205      960.      2   147 4.17e-85   150\n\netasq(iris.mod)\n#&gt;         eta^2\n#&gt; Species 0.596\n\nBut these statistics don’t help to understand how the species differ. For this, we turn to HE plots.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#he-plots",
    "href": "11-mlm-viz.html#he-plots",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.3 HE plots",
    "text": "11.3 HE plots\nThe heplot() function takes a \"mlm\" object and produces an HE plot for one pair of variables specified by the variables argument. By default, it plots the first two. Figure 11.4 shows the HE plots for the two sepal and the two petal variables.\n\n\nheplot(iris.mod, size = \"effect\",\n       cex = 1.5, cex.lab = 1.5,\n       fill = TRUE, fill.alpha = c(0.3, 0.1))\nheplot(iris.mod, size = \"effect\", variables = 3:4,\n       cex = 1.5, cex.lab = 1.5,\n       fill = TRUE, fill.alpha = c(0.3, 0.1))\n\n\n\n\n\n\n\n\nFigure 11.4: HE plots for the multivariate model iris.mod. The left panel shows the plot for the Sepal variables; the right panel plots the Petal variables.\n\n\n\n\nThe interpretation of the plots in Figure 11.4 is as follows:\n\nFor the Sepal variables, length and width are positively correlated within species (the \\(\\mathbf{E}\\) = “Error” ellipsoid). The means of the groups (the \\(\\mathbf{H}\\) = “Species” ellipsoid), however, are negatively correlated. This plot is the HE plot representation of the data shown in row 2, column 1 of Figure 11.3. It reflects the relative shape of the iris sepals: shorter and wider for setosa than the other two species.\nFor the Petal variables length and width are again positively correlated within species, but now the means of the groups are positively correlated: longer petals go with wider ones across species. This reflects the relative size of the iris petals. The analogous data plot appears in row 4, column 3 of Figure 11.3.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#significance-scaling",
    "href": "11-mlm-viz.html#significance-scaling",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.4 Significance scaling",
    "text": "11.4 Significance scaling\nThe geometry of ellipsoids and multivariate tests allow us to go further with another re-scaling of the \\(\\mathbf{H}\\) ellipsoid that gives a visual test of significance for any term in a MLM. This is done simply by dividing \\(\\mathbf{H} / df_e\\) further by the \\(\\alpha\\)-critical value of the corresponding test statistic to show the strength of evidence against the null hypothesis.\nAmong the various multivariate test statistics, Roy’s maximum root test, based on the largest eigenvalue \\(\\lambda_1\\) of \\(\\mathbf{H} \\mathbf{E}^{-1}\\), gives \\(\\mathbf{H} / (\\lambda_\\alpha df_e)\\) which has the visual property that the scaled \\(\\mathbf{H}\\) ellipsoid will protrude somewhere outside the standard \\(\\mathbf{E}\\) ellipsoid if and only if Roy’s test is significant at significance level \\(\\alpha\\). The critical value \\(\\lambda_\\alpha\\) for Roy’s test is \\[\n\\lambda_\\alpha = \\left(\\frac{\\text{df}_1}{\\text{df}_2}\\right) \\; F_{\\text{df}_1, \\text{df}_2}^{1-\\alpha} \\:\\: ,\n\\] where \\(\\text{df}_1 = \\max(p, \\text{df}_h)\\) and \\(\\text{df}_2 = \\text{df}_h + \\text{df}_e - \\text{df}_1\\).\nFor these data, the HE plot using significance scaling is shown in the right panel of Figure 11.5. The left panel is the same as that shown for sepal width and length in Figure 11.4, but with axis limits to make the two plots directly comparable.\n\n\n\n\n\n\n\nFigure 11.5: HE plots for sepal width and sepal length in the iris dataset. Left: effect scaling of the \\(\\mathbf{H}\\) matrix; right: significance scaling, where protrusion of \\(\\mathbf{H}\\) outside \\(\\mathbf{E}\\) indicates a significant effect by Roy’s test.\n\n\n\n\nYou can interpret the plot using effect scaling to indicate that the overall “size” of variation of the group means is roughly the same as that of within-group variation for the two sepal variables. Significance scaling weights the evidence against the null hypothesis that a given effect is zero.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#visualizing-contrasts-and-linear-hypotheses",
    "href": "11-mlm-viz.html#visualizing-contrasts-and-linear-hypotheses",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.5 Visualizing contrasts and linear hypotheses",
    "text": "11.5 Visualizing contrasts and linear hypotheses\nAs described in Section 10.3.1, tests of linear hypotheses and contrasts represented by the general linear test \\(\\mathcal{H}_0: \\mathbf{C} \\;\\mathbf{B} = \\mathbf{0}\\) provide a powerful way to probe the specific effects represented within the global null hypothesis, \\(\\mathcal{H}_0: \\mathbf{B} = \\mathbf{0}\\), that all effects are zero.\nIn this example the contrasts \\(\\mathbf{c}_1\\) (Species1) and \\(\\mathbf{c}_2\\) (Species2) among the iris species are orthogonal, i.e., \\(\\mathbf{c}_1^\\top \\mathbf{c}_2 = 0\\). Therefore, their tests are statistically independent, and their \\(\\mathbf{H}\\) matrices are additive. They fully decompose the general question of differences among the groups into two independent questions regarding the contrasts.\n\\[\n\\mathbf{H}_\\text{Species} = \\mathbf{H}_\\text{Species1} + \\mathbf{H}_\\text{Species2}\n\\tag{11.2}\\]\ncar::linearHypothesis() is the means for testing these statistically, and heplot() provides the way to show these tests visually. Using the contrasts set up in Section 11.2.2, \\(\\mathbf{c}_1\\), representing the difference between setosa and the other species is labeled Species1 and the comparison of versicolor with virginica is Species2. The coefficients for these in \\(\\mathbf{B}\\) give the differences in the means. The line for (Intercept) gives grand means of the variables.\n\ncoef(iris.mod)\n#&gt;             Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; (Intercept)        5.843       3.057        3.758       1.199\n#&gt; Species1          -0.837       0.371       -2.296      -0.953\n#&gt; Species2          -0.326      -0.102       -0.646      -0.350\n\nNumerical tests of hypotheses using linearHypothesis() can be specified in a very general way: A matrix (or vector) \\(\\mathbf{C}\\) giving linear combinations of coefficients by rows, or a character vector giving the hypothesis in symbolic form. A character variable or vector tests whether the named coefficients are different from zero for all responses.\n\nlinearHypothesis(iris.mod, \"Species1\") |&gt; print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1      0.97     1064      4    144 &lt;2e-16 ***\n#&gt; Wilks             1      0.03     1064      4    144 &lt;2e-16 ***\n#&gt; Hotelling-Lawley  1     29.55     1064      4    144 &lt;2e-16 ***\n#&gt; Roy               1     29.55     1064      4    144 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlinearHypothesis(iris.mod, \"Species2\") |&gt; print(SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)    \n#&gt; Pillai            1     0.745      105      4    144 &lt;2e-16 ***\n#&gt; Wilks             1     0.255      105      4    144 &lt;2e-16 ***\n#&gt; Hotelling-Lawley  1     2.925      105      4    144 &lt;2e-16 ***\n#&gt; Roy               1     2.925      105      4    144 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe various test statistics are all equivalent here—they give the same \\(F\\) statistics— because they they have 1 degree of freedom.\nIn passing, from Equation 11.2, note that the joint test of these contrasts is exactly equivalent to the overall test of Species (results not shown).\n\nlinearHypothesis(iris.mod, c(\"Species1\", \"Species2\"))\n\nWe can show these contrasts in an HE plot by supplying a named list for the hypotheses argument. The names are used as labels in the plot. In the case of a 1-df multivariate test, the \\(\\mathbf{H}\\) ellipses plot as a degenerate line.\n\nhyp &lt;- list(\"S:Vv\" = \"Species1\", \"V:v\" = \"Species2\")\nheplot(iris.mod, hypotheses=hyp,\n       cex = 1.5, cex.lab = 1.5,\n       fill = TRUE, fill.alpha = c(0.3, 0.1),\n       col = c(\"red\", \"blue\", \"darkgreen\", \"darkgreen\"),\n       lty = c(0,0,1,1), label.pos = c(3, 1, 2, 1),\n       xlim = c(2, 10), ylim = c(1.4, 4.6))\n\n\n\n\n\n\nFigure 11.6: HE plot for sepal length and width in the iris data showing the tests of the two contrasts\n\n\n\n\nThis HE plot shows that, for the two sepal variables, the greatest between-species variation is accounted for by the contrast (S:Vv) between setosa and the others, for which the effect is very large in relation to error variation. The second contrast (V:v), between the versicolor and virginica species is relatively smaller, but still explains significant variation of the sepal variables among the species.\nThe directions of these hypotheses in a given plot show how the group means differ in terms of a given contrast.3 For example, the contrast S:Vv is the line that separates setosa from the others and indicates that setosa flowers have shorter but wider sepals.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-HEplot-matrices",
    "href": "11-mlm-viz.html#sec-HEplot-matrices",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.6 HE plot matrices",
    "text": "11.6 HE plot matrices\nIn base R graphics, 2D scatterplots are extended to all pairwise views of multivariate data with a pairs() method. For multivariate linear models, the heplots defines a pairs.mlm() method to display HE plots for all pairs of the response variables.\n\npairs(iris.mod,\n      fill=TRUE, fill.alpha=c(0.3, 0.1))\n\n\n\n\n\n\nFigure 11.7: All pairwise HE plots for the iris data.\n\n\n\n\nFigure 11.7 provides a fairly complete visualization of the results of the multivariate tests and answers the question: how do the species differ? Sepal length and the two petal variables have the group means nearly perfectly correlated, in the order setosa &lt; versicolor &lt; virginica. For Sepal width, however, setosa has the largest mean, and so the \\(\\mathbf{H}\\) ellipses show a negative correlation in the second row and column.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-candisc",
    "href": "11-mlm-viz.html#sec-candisc",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.7 Low-D views: Canonical analysis",
    "text": "11.7 Low-D views: Canonical analysis\nThe HE plot framework so far provides views of all the effects in a MLM in variable space. We can view this in 2D for selected pairs of response variables, or for all pairwise views in scatterplot matrix format. There is also an heplot3d() function giving plots for three response variables together. The 3D plots are interactive, in that they can be rotated and zoomed by mouse control, and dynamic, in that they can be made to spin and saved as movies. To save space, these plots are not shown here.\nHowever in a one-way MANOVA design with more than response three variables, it is difficult to visualize how the groups vary on all responses together, and how the different variables contribute to discrimination among groups. In this situation, canonical discriminant analysis (CDA) is often used, to provide a low-D visualization of between-group variation. When the predictors are also continuous, the analogous term is canonical correlation analysis (CCA). The advantage here is that we can also show the relations of the response variables to these dimensions, similar to a biplot (Section 4.3) for a PCA of purely quantitative variables.\nThe key to this is the eigenvalue decomposition, \\(\\mathbf{H}\\mathbf{E}^{-1} \\lambda_i = \\lambda_i \\mathbf{v}_i\\) (Equation 10.5) of \\(\\mathbf{H}\\) relative to \\(\\mathbf{E}\\). The eigenvalues, \\(\\lambda_i\\), give the “size” of each \\(s\\) orthogonal dimensions on which the multivariate tests are based. But the corresponding eigenvectors, \\(\\mathbf{v}_i\\), give the weights for the response variables in \\(s\\) linear combinations that maximally discriminate among the groups or equivalently maximize the (canonical) \\(R^2\\) of a linear combination of the predictor \\(\\mathbf{X}\\)s with a linear combination of the response \\(\\mathbf{Y}\\)s.\nThus, CDA amounts to a transformation of the \\(p\\) responses, \\(\\mathbf{Y}_{n \\times p}\\) into the canonical space, \\[\n\\mathbf{Z}_{n \\times s} = \\mathbf{Y} \\; \\mathbf{E}^{-1/2} \\; \\mathbf{V} \\;,\n\\] where \\(\\mathbf{V}\\) contains the eigenvectors of \\(\\mathbf{H}\\mathbf{E}^{-1}\\) and \\(s=\\min ( p, df_h )\\). It is well-known (e.g., Gittins (1985)) that canonical discriminant plots of the first two (or three, in 3D) columns of \\(\\mathbf{Z}\\) corresponding to the largest canonical correlations provide an optimal low-D display of the variation between groups relative to variation within groups.\nCanonical discriminant analysis is typically carried out in conjunction with a one-way MANOVA design. The candisc package (Friendly & Fox, 2025) generalizes this to multi-factor designs in the candisc() function. For any given term in a \"mlm\", the generalized canonical discriminant analysis amounts to a standard discriminant analysis based on the \\(\\mathbf{H}\\) matrix for that term in relation to the full-model \\(\\mathbf{E}\\) matrix.4\nTests based on the eigenvalues \\(\\lambda_i\\), initially stated by Bartlett (1938), use Wilks’ \\(\\Lambda\\) likelihood ratio tests of these. This allow you to determine the number of significant canonical dimensions, or the number of different aspects to consider for the relations between the responses and predictors. These take the form of sequential global tests of the hypothesis that the canonical correlation in the current row and all that follow are zero. Thus, if you find The canonical \\(R^2\\), CanRsq, gives the R-squared value of fitting the \\(i\\)th response canonical variate to the corresponding \\(i\\)th canonical variate for the predictors.\nFor the iris data, we get the following printed summary:\n\niris.can &lt;- candisc(iris.mod) |&gt; print()\n#&gt; \n#&gt; Canonical Discriminant Analysis for Species:\n#&gt; \n#&gt;   CanRsq Eigenvalue Difference Percent Cumulative\n#&gt; 1  0.970     32.192       31.9  99.121       99.1\n#&gt; 2  0.222      0.285       31.9   0.879      100.0\n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;   LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1        0.023    199.1     8   288 &lt; 2e-16 ***\n#&gt; 2        0.778     13.8     3   145 5.8e-08 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis analysis shows a very simple result: The differences among the iris species can be nearly entirely accounted for by the first canonical dimension (99.1%). Interestingly, the second dimension is also highly significant, even though it accounts for only 0.88%.\n\n11.7.1 Coeficients\nThe coef() method for “candisc” objects returns a matrix of weights for the response variables in the canonical dimensions. By default, these are given for the response variables standardized to \\(\\bar{y}=0\\) and \\(s^2_y = 1\\). The type argument also allows for raw score weights (type = \"raw\") used to compute the observation scores on Can1, Can2, … . type = \"structure\" gives the canonical structure coefficients, which are the correlations between each response and the canonical scores.\n\ncoef(iris.can, type = \"std\")\n#&gt;                Can1    Can2\n#&gt; Sepal.Length  0.427  0.0124\n#&gt; Sepal.Width   0.521  0.7353\n#&gt; Petal.Length -0.947 -0.4010\n#&gt; Petal.Width  -0.575  0.5810\ncoef(iris.can, type = \"structure\")\n#&gt;                Can1  Can2\n#&gt; Sepal.Length -0.792 0.218\n#&gt; Sepal.Width   0.531 0.758\n#&gt; Petal.Length -0.985 0.046\n#&gt; Petal.Width  -0.973 0.223\n\nThus, the two weighted sums for the canonical variates, using mean-centered standardized scores, can be shown as follows:\n\nCan1 = 0.43 x S.len + 0.52 x S.wid + -0.95 x P.len + -0.58 x P.wid Can2 = 0.01 x S.len + 0.74 x S.wid + -0.4 x P.len + 0.58 x P.wid\n\n\n11.7.2 Canonical scores plot\nThe plot() method for \"candisc\" objects gives a plot of these observation scores. ellipse=TRUE overlays this with their standard data ellipses for each species, as shown in Figure 11.8. The response variables are shown as vectors, using the structure coefficients, as in a biplot. Thus, the relative size of the projection of these vectors on the canonical axes reflects the correlation of the observed response on the canonical dimension. For ease of interpretation I flipped the sign of the first canonical dimension, so that the positive Can1 direction corresponds to larger flowers.\nTODO: var.labels not a graphical parameter\n\nvars &lt;- names(iris)[1:4] |&gt; \n  stringr::str_replace(\"\\\\.\", \"\\n\")\nplot(iris.can,\n     var.labels = vars,\n     var.col = \"black\",\n     var.lwd = 2,\n     ellipse=TRUE,\n     scale = 9,\n     col = iris_colors,\n     pch = 15:17,\n     cex = 0.7, var.cex = 1.25,\n     rev.axes = c(TRUE, FALSE),\n     xlim = c(-10, 10),\n     cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.8: Plot of canonical scores for the iris data.\n\n\n\n\nThe interpretation of this plot is simple: in canonical space, variation of the means for the iris species is essentially one-dimensional (99.1% of the effect of Species), and this dimension corresponds to overall size of the iris flowers. All variables except for Sepal.Width are positively aligned with this axis, but the two petal variables show the greatest discrimination. The negative direction for Sepal.Width reflects the pattern seen in Figure 11.7, where setosa have wider sepals.\nFor the second dimension, look at the projections of the variable vectors on the Can2 axis. All are positive, but this is dominated by Sepal.Width. We could call this a flower shape dimension.\n\n11.7.3 Canonical HE plot\nFor a one-way design, the canonical HE plot is simply the HE plot of the canonical scores in the analogous MLM model that substitutes \\(\\mathbf{Z}\\) for \\(\\mathbf{Y}\\). In effect, it is a more compact visual summary of the plot shown in Figure 11.8.\nThis is shown in Figure 11.9 for the iris data. In canonical space, the residuals are always uncorrelated, so the \\(\\mathbf{E}\\) ellipse plots as a circle. The \\(\\mathbf{H}\\) ellipse for Species here reflects a data ellipse for the fitted values— group means— shown as labeled points in the plot. The differences among Species are so large that this plot uses size = \"effect\" scaling, making the axes comparable to those in Figure 11.8.5\nThe vectors for each predictor are the same structure coefficients as in the ordinary canonical plot. They can again be reflected for interpretation and scaled in length to fill the plot window.\n\n\nheplot(iris.can,\n       size = \"effect\",\n       scale = 8,\n       var.labels = vars,\n       var.col = \"black\",\n       var.lwd = 2,\n       fill = TRUE, fill.alpha = 0.2,\n       rev.axes = c(TRUE, FALSE),\n       xlim = c(-10, 10))\n\n\n\n\n\n\nFigure 11.9: Canonical HE plot for the iris data.\n\n\n\n\nThe collection of plots shown for the iris data here can be seen as progressive visual summaries of the data:\n\nThe scatterplot matrix in Figure 11.3 shows the iris flowers in the data space of the sepal and petal variables.\nCanonical analysis substitutes for these the two linear combinations reflected in Can1 and Can2. The plot in Figure 11.8 portrays exactly the same relations among the species, but in the reduced canonical space of only two dimensions.\nThe HE plot version, shown in Figure 11.9 summarizes the separate data ellipses for the species with pooled, within-group variance of the \\(\\mathbf{E}\\) matrix for the canonical variables, which are always uncorrelated. The variation among the group means is reflected in the size and shape of the ellipse for the \\(\\mathbf{E}\\) matrix.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#quantitative-predictors-mmra",
    "href": "11-mlm-viz.html#quantitative-predictors-mmra",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.8 Quantitative predictors: MMRA",
    "text": "11.8 Quantitative predictors: MMRA\nThe ideas behind HE plots extend naturally to multivariate multiple regression (MMRA).  A purely visual feature of HE plots in these cases is that the \\(\\mathbf{H}\\) ellipse for a quantitative predictor with 1 df appears as a degenerate line. But consequently, the angles between these for different predictors has a simple interpretation as as the correlation between their predicted effects. Moreover, it is easy to show visual overall tests of joint linear hypotheses for two or more predictors together.\nTODO: Use these examples below\n\n\nheplots::NLSY: National Longitudinal Survey of Youth Data\n\nheplots::schooldata: School Data -&gt; R/schooldata-ex.R\n\n\nheplots::Hernior: Recovery from Elective Herniorrhaphy -&gt; HE_mmra vignette\n\n\n11.8.1 Example: NLSY data\n\nHere I’ll continue the analysis of the NLSY data from Section 10.5.1. In the model NLSY.mod1 I used only father’s income and education to predict scores in reading and math, and both of these demographic variables were highly significant.\n\ndata(NLSY, package = \"heplots\")\nNLSY.mod1 &lt;- lm(cbind(read, math) ~ income + educ, \n                data = NLSY)\n\nheplot(NLSY.mod1, \n  fill=TRUE, fill.alpha = 0.2, \n  cex = 1.5, cex.lab = 1.5,\n  lwd=c(2, 3, 3),\n  label.pos = c(\"bottom\", \"top\", \"top\")\n  )\n\n\n\n\n\n\nFigure 11.10: HE plot for the simple model for the NLSY data fitting reading and math scores from income and education.\n\n\n\n\nFathers income and education are positively correlated in their effects on the outcome scores. From the angles in the plot, income is most related to the math score, while education is related to both, but slightly more to the reading score.\nThe overall joint test for both predictors can then visualized as the test of the linear hypothesis \\(\\mathcal{H}_0 : \\mathbf{B} = [\\boldsymbol{\\beta}_\\text{income}, \\boldsymbol{\\beta}_\\text{educ}] = \\mathbf{0}\\). For heplot(), we specify the names of the coefficients to be tested with the hypotheses argument.\n\ncoefs &lt;- rownames(coef(NLSY.mod1))[-1] |&gt; print()\n#&gt; [1] \"income\" \"educ\"\n\nheplot(NLSY.mod1, \n       hypotheses = list(\"Overall\" = coefs),\n       fill=TRUE, fill.alpha = 0.2, \n       cex = 1.5, cex.lab = 1.5,\n       lwd=c(2, 3, 3, 2),\n       label.pos = c(\"bottom\", \"top\"))\n\n\n\n\n\n\nFigure 11.11: HE plot adding the \\(\\mathbf{H}\\) ellipse for the overall test that both predictors have no effect on the outcome scores.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#sec-cancor",
    "href": "11-mlm-viz.html#sec-cancor",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.9 Canonical correlation analysis",
    "text": "11.9 Canonical correlation analysis\nJust as we saw for MANOVA designs, a canonical analysis for multivariate regression involves finding a low-D view of the relations between predictors and outcomes that maximally explains their relations in terms of linear combinations of each. That is, the goal is to find weights for one set of variables, say \\(\\mathbf{X}\\) not to predict each of the other set \\(\\mathbf{Y} =[\\mathbf{y}_1, \\mathbf{y}_2, \\dots]\\) individually, but rather to also find weights for the \\(\\mathbf{y}\\)s which is most highly correlated with the linear combination of the \\(\\mathbf{x}\\)s.\nIn this sense, canonical correlation analysis (CCA) is symmetric in the \\(x\\) and \\(y\\) variables: the \\(y\\) set is not considered responses. Rather the goal is simply to explain the correlations between the two sets. For a thorough treatment of this topic, see Gittins (1985).\nGeometrically, these linear combinations are vectors representing projections in the observation space of the \\(x\\) and \\(y\\) variables, and CCA can also be thought of as minimizing the angle between these vectors or maximizing the cosine of this angle. This is illustrated in Figure 11.12.\n\n\n\n\n\n\n\nFigure 11.12: Diagram illustrating canonical correlation. For two \\(y\\) variables, all linear combinations are vectors in their plane, and similarly for the \\(x\\) variables. Maximizing the correlation between linear combinations of each is equivalent to making the angle \\(\\phi\\) between them as small as possible, or maximizing \\(\\cos({\\theta})\\), shown in the diagram at the right. The thick grey arrow indignates that the two planes should be overlaid at a common origin. Source: Re-drawn by Udi Alter following a Cross-Validated discussion by user ‘ttnphns’, https://bit.ly/4dgq2cp\n\n\n\n\nSpecifically, we want to find one set of weights \\(\\mathbf{a}_1\\) for the \\(x\\) variables and another for the \\(y\\) variables to give the linear combinations \\(\\mathbf{u}_1\\) and \\(\\mathbf{v}_1\\),\n\\[\\begin{aligned}\n\\mathbf{u}_1 & = \\mathbf{X} \\ \\mathbf{a}_1 = a_{11} \\mathbf{x}_1 + a_{12} \\mathbf{x}_2 + \\cdots + a_{11} \\mathbf{x}_q \\\\\n\\mathbf{v}_1 & = \\mathbf{Y} \\ \\mathbf{b}_1 = b_{11} \\mathbf{y}_1 + b_{12} \\mathbf{y}_1 + \\cdots + b_{11} \\mathbf{y}_p \\; ,\n\\end{aligned}\\]\nsuch that the correlation \\(\\rho_1 = \\textrm{corr}(\\mathbf{u}_1, \\mathbf{v}_1)\\) is maximized, or equivalently, minimizing the angle between them.\nUsing \\(\\mathbf{S}_{xx}\\), \\(\\mathbf{S}_{yy}\\) to represent the covariance matrices of the \\(x\\) and \\(y\\) variables, and \\(\\mathbf{S}_{xy}\\) for the cross-covariances between the two sets, the correlation between the linear combinations of each can be expressed as\n\\[\\begin{aligned}\n\\rho_1 & = \\textrm{corr}(\\mathbf{u}_1, \\mathbf{v}_1)\n         = \\textrm{corr}(\\mathbf{X} \\ \\mathbf{a}_1, \\mathbf{Y} \\ \\mathbf{b}_1) \\\\\n       & = \\frac{\\mathbf{a}_1^\\top \\ \\mathbf{S}_{xy} \\ \\mathbf{b}_1 }{\\sqrt{\\mathbf{a}_1^\\top \\ \\mathbf{S}_{xx} \\  \\mathbf{a}_1 } \\sqrt{\\mathbf{b}_1^\\top \\ \\mathbf{S}_{yy} \\ \\mathbf{b}_1 }}\n\\end{aligned}\\]\nBut, the \\(y\\) variables lie in a \\(p\\)-dimensional (observation) space, and the \\(x\\) in \\(q\\) dimensions, so what they have common is a space of \\(s = \\min(p, q)\\) dimensions. Therefore, we can find additional pairs of canonical variables,\n\\[\\begin{aligned}\n\\mathbf{u}_2 = \\mathbf{X} \\ \\mathbf{a}_2 & \\quad\\quad \\mathbf{v}_2 = \\mathbf{Y} \\ \\mathbf{b}_2 \\\\\n                                         & \\vdots \\\\\n\\mathbf{u}_s = \\mathbf{X} \\ \\mathbf{a}_s & \\quad\\quad \\mathbf{v}_s = \\mathbf{Y} \\ \\mathbf{b}_s \\\\\n\\end{aligned}\\]\nsuch that each pair \\((\\mathbf{u}_i, \\mathbf{v}_i)\\) has the maximum possible correlation and all distinct pairs are uncorrelated:\n\\[\\begin{aligned}\n\\rho_i & =\\max _{\\mathbf{a}_i, \\mathbf{b}_i}\\left\\{\\mathbf{u}_i^{\\top} \\mathbf{v}_i\\right\\} = \\\\\n\\left\\|\\mathbf{u}_i\\right\\| & =1, \\quad\\left\\|\\mathbf{v}_i\\right\\|=1, \\\\\n\\mathbf{u}_i^{{\\top}} \\mathbf{u}_j & =0, \\quad \\mathbf{v}_i^{\\top} \\mathbf{v}_j=0 \\quad\\quad \\forall j \\neq i: i, j \\in\\{1,2, \\ldots, s\\} \\ .\n\\end{aligned}\\]\nIn words, the correlations among canonical variables are zero except when when they are associated with the same canonical correlation or the weights \\((\\mathbf{a}_i, \\mathbf{b}_i)\\) for the same pair. Alternatively, all \\(p \\times q\\) correlations the variables in \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\) are fully summarized in the \\(s = \\min(p, q)\\) canonical correlations \\(\\rho_i\\) for \\(i = 1, 2, \\dots, s\\).\nThe solution, developed by Hotelling (1936), is a form of a generalized eigenvalue problem, that can be stated in two equivalent ways,\n\n\n\n\\[\n\\begin{aligned}\n& \\left(\\mathbf{S}_{y x} \\ \\mathbf{S}_{x x}^{-1} \\ \\mathbf{S}_{x y} - \\rho^2 \\ \\mathbf{S}_{y y}\\right) \\mathbf{b} = \\mathbf{0} \\\\\n& \\left(\\mathbf{S}_{x y} \\ \\mathbf{S}_{y y}^{-1} \\ \\mathbf{S}_{y x} - \\rho^2 \\ \\mathbf{S}_{x x}\\right) \\mathbf{a} = \\mathbf{0} \\ .\n\\end{aligned}\n\\] Both equations have the same form and have the same eigenvalues. And, given the eigenvectors for one of these equations, we can find the eigenvectors for the other.\nTODO: Fill in details of canonical correlations\n\n11.9.1 Example: School data\n\nThe schooldata dataset analyzed in Section 10.5.2 can also be illuminated by the methods of this chapter. There I fit the multivariate regression model predicting students scores on reading, mathematics and a measure of self-esteem using as predictors measures of parents’ education, occupation, school visits, counseling help with school assignments and number of teachers per school. But I also found two highly influential observations (cases 44, 59; see Figure 10.16) whose effect on the coefficients is rather large; so, I remove them from the analysis here.6\n\ndata(schooldata, package = \"heplots\")\n\nbad &lt;- c(44, 59)\nOK &lt;- (1:nrow(schooldata)) |&gt; setdiff(bad)\nschool.mod2 &lt;- lm(cbind(reading, mathematics, selfesteem) ~ ., \n                  data=schooldata[OK, ])\n\nIn this model, parent’s education and occupation and their visits to the schools were highly predictive of student’s outcomes but their counseling efforts and the number of teachers in the schools did not contribute much. However, the nature of these relationships was largely uninterpreted in that analysis.\nHere is where HE plots can help. You can think of this as a way to visualize what is entailed in the coefficients for this model by showing the magnitude of the predictor effects by their size and their relations to the outcome variable by their direction. The table of raw score coefficients isn’t very helpful in this regard.\n\ncoef(school.mod2)\n#&gt;             reading mathematics selfesteem\n#&gt; (Intercept)  2.7096       3.561    0.39751\n#&gt; education    0.2233       0.132   -0.01088\n#&gt; occupation   3.3336       4.284    1.79574\n#&gt; visit        0.0101      -0.123    0.20005\n#&gt; counseling  -0.3953      -0.293    0.00868\n#&gt; teacher     -0.1945      -0.360    0.01129\n\nFigure 11.13 shows the HE plot for reading and mathematics scores in this model, using the default significance scaling.\n\nheplot(school.mod2, \n       fill=TRUE, fill.alpha=0.1,\n       cex = 1.5,\n       cex.lab = 1.5,\n       label.pos = c(rep(\"top\", 4), \"bottom\", \"bottom\"))\n\n\n\n\n\n\nFigure 11.13: HE plot for reading and mathematics scores in the multivariate regression model…\n\n\n\n\nParent’s occupation and education are both significant in this view, but what is more important is their orientation. Both are positively associated with reading and math scores, but education is somewhat more related to reading than to mathematics. Number of teachers and degree of parental counseling have a similar orientation, with teachers having a greater relation to mathematics scores. Visits to school and number of teachers are not significant in this plot, but both are positively correlated with reading and math and are coincident in the plot. The parent time counseling measure, while also insignificant, tilts in the opposite direction, having different signs for reading and math.\nIn the pairs() plot for all three responses (Figure 11.14), we see something different in the relations for self-esteem. While occupation has a large positive relation in all the plots in the third row and column, education, counseling and teachers have negative relations in these plots, particularly with mathematics scores.\n\npairs(school.mod2, \n      fill=TRUE, fill.alpha=0.1,\n      var.cex = 2.5,\n      cex = 1.3)\n\n\n\n\n\n\nFigure 11.14: Pairwise HE plots for the three outcome variables in the multivariate regression model …\n\n\n\n\n\n11.9.2 Canonical analysis\nWith \\(p = 3\\) responses and \\(q = 5\\) predictors there are three possible sets of canonical variables which together account for 100% of the total linear relations between them. heplots::cancor() gives the percentage associated with each of the eigenvalues and the canonical correlations.\nFor this dataset, the first canonical variates, with Can \\(R = 0.995\\), accounts for 98.6%, so you might think that that is sufficient. Yet the likelihood ratio tests show that the second set, with Can \\(R = 0.774\\), is also significant, even though it only accounts for 1.3%.\n\n# bad &lt;- c(44, 59)\n# OK &lt;- (1:nrow(schooldata)) |&gt; setdiff(bad)\nschool.can2 &lt;- cancor(cbind(reading, mathematics, selfesteem) ~\n                        education + occupation + visit + counseling + teacher,\n                     data=schooldata[OK, ])\nschool.can2\n#&gt; \n#&gt; Canonical correlation analysis of:\n#&gt;   5   X  variables:  education, occupation, visit, counseling, teacher \n#&gt;   with  3   Y  variables:  reading, mathematics, selfesteem \n#&gt; \n#&gt;     CanR CanRSQ    Eigen  percent    cum\n#&gt; 1 0.9946 0.9892 91.41999 98.57540  98.58\n#&gt; 2 0.7444 0.5541  1.24267  1.33994  99.92\n#&gt; 3 0.2698 0.0728  0.07852  0.08466 100.00\n#&gt;                          scree\n#&gt; 1 ****************************\n#&gt; 2                             \n#&gt; 3                             \n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;    CanR LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1 0.995        0.004     67.5    15   166 &lt; 2e-16 ***\n#&gt; 2 0.744        0.413      8.5     8   122 4.1e-09 ***\n#&gt; 3 0.270        0.927      1.6     3    62    0.19    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe virtue of CCA is that all correlations between the X and Y variables are completely captured in the correlations between the pairs of canonical scores: The \\(p \\times q\\) correlations between the sets are entirely represented by the \\(s = \\min(p, q)\\) canonical ones. Whether the second dimension is useful here depends on whether it adds some interpretable increment to what is going on in these relations. One could be justifiably happy with an explanation based on the first dimension that accounts for nearly all the total association between the sets.\nThe class \"cancor\" object returned by cancor() contains the canonical coefficients, for which there is a coef() method as in candisc(), and also a scores() method to return the scores on the canonical variables, called Xcan1, Xcan2, … and Ycan1, Ycan2.\n\nnames(school.can2)\n#&gt;  [1] \"cancor\"    \"names\"     \"ndim\"      \"dim\"       \"coef\"     \n#&gt;  [6] \"scores\"    \"X\"         \"Y\"         \"weights\"   \"structure\"\n#&gt; [11] \"call\"      \"terms\"\n\nYou can use the plot() method or heplot() method to visualize and help interpret the results. The plot() method plots the canonical scores$X against the scores$Y for a given dimension (selected by the which argument). The id.n argument gives a way to flag noteworthy observations.\n\ntext(-2, 1.5, paste(\"Can R =\", round(school.can2$cancor[1], 3)), \n     cex = 1.4, pos = 4)\n\nplot(school.can2, which = 2, \n     pch=16, id.n = 3,\n     cex.lab = 1.5, id.cex = 1.5,\n     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))\ntext(-3, 3, paste(\"Can R =\", round(school.can2$cancor[2], 3)), \n     cex = 1.4, pos = 4)\npar(op)\n\n\n\n\n\n\nFigure 11.15: Plots of canonical scores for the first two canonical dimensions of the schooldata dataset, omitting the two highly influential cases.\n\n\n\n\nIt is worthwhile to look at an analogous plot of canonical scores for the original dataset including the two highly influential cases. As you can see in Figure 11.16, cases 44 and 59 are way outside the range of the rest of the data. Their influence increases the canonical correlation to a near perfect \\(\\rho = 0.997\\).\n\nschool.can &lt;- cancor(cbind(reading, mathematics, selfesteem) ~\n                       education + occupation + visit + counseling + teacher,\n                     data=schooldata)\nplot(school.can, \n     pch=16, id.n = 3,\n     cex.lab = 1.5, id.cex = 1.5,\n     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))\ntext(-5, 1, paste(\"Can R =\", round(school.can$cancor[1], 3)), \n     cex = 1.4, pos = 4)\n\n\n\n\n\n\nFigure 11.16: Plots of canonical scores on the first canonical dimension for the schooldata, including the influential cases, which stand out as so far frome the rest of the observations.\n\n\n\n\nPlots of canonical scores tell us of the strength of the canonical dimensions, but do not help interpreting the analysis in relation to the original variables. The HE plot version for canonical correlation analysis re-fits a multivariate regression model for the Y variables against the Xs, but substitutes the canonical scores for each, essentially projecting the data into canonical space.\nTODO: Check out signs of structure coefs from cancor(). Would be better to reflect the vectors for Ycan1.\n\nheplot(school.can2,\n       fill = TRUE, fill.alpha = 0.2,\n       var.col = \"red\", \n       asp = NA, scale = 0.25,\n       cex.lab = 1.5, cex = 1.25,\n       prefix=\"Y canonical dimension \")\n\n\n\n\n\n\nFigure 11.17: HE plot for the canonical correlation analysis of the schooldata. Vectors for the variables indicate their correlations with the canonical dimensions.\n\n\n\n\nThe red variable vectors shown in these plots are intended only to show the correlations of Y variables with the canonical dimensions. The fact that they are so closely aligned reflects the fact that the first dimension accounts for nearly all of their associations with the predictors. The orientation of the \\(\\mathbf{H}\\) ellipses/lines reflects the projection of those from Figure 11.14 into canonical space\nOnly their relative lengths and angles with respect to the Y canonical dimensions have meaning in these plots. Relative lengths correspond to proportions of variance accounted for in the Y canonical dimensions plotted; angles between the variable vectors and the canonical axes correspond to the structure correlations. The absolute lengths of these vectors are arbitrary and are typically manipulated by the scale argument to provide better visual resolution and labeling for the variables.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#mancova-models",
    "href": "11-mlm-viz.html#mancova-models",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.10 MANCOVA models",
    "text": "11.10 MANCOVA models\nHE plots for designs containing a collection of quantitative predictors and one or more factors are quite simple in MANCOVA models where the effects are additive, i.e., don’t involve interactions. They are a bit more challenging when you allow separate slopes for groups on all quantitative variables, because there get to be too many terms to usefully display. But these models are more complicated!\nIf the evidence for heterogeneity of regressions is not very strong, it is still useful to fit the MANCOVA model and display it in an HE plot.\nAn alternative is to fit separate models for the groups and display these as HE plots. As noted earlier (Section 10.7.1), this is not ideal for testing hypotheses, but provides a useful and informative display of the relations between the predictors and responses and the groups effect. I illustrate these approaches for the Rohwer data, encountered in Section 10.7.1, below.\n\n11.10.1 Example: Rohwer data\nIn Section 10.7.1 I fit several models for Rohwer’s data on the relations between paired-associate tasks and scholastic performance. The first model was the MANCOVA model testing the difference between the high and low SES groups, controlling for, or taking into account differences on the paired-associate task.\n\nRohwer.mod1 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES + n + s + ns + na + ss, \n                 data=Rohwer)\n\nHE plots for this model for the pairs (SAT, PPVT) and (SAT, Raven) is shown in Figure 11.18. The result of an overall test for all predictors, \\(\\mathcal{H}_0 : \\mathbf{B} = \\mathbf{0}\\), is added to the basic plot using the hypotheses argument.\n\ncolors &lt;- c(\"red\", \"blue\", rep(\"black\",5), \"#969696\")\ncovariates &lt;- rownames(coef(Rohwer.mod1))[-(1:2)]\npairs(Rohwer.mod1, \n       col=colors,\n       hypotheses=list(\"Regr\" = covariates),\n       fill = TRUE, fill.alpha = 0.1,\n       cex=1.5, cex.lab = 1.5, var.cex = 3,\n       lwd=c(2, rep(3,5), 4))\n\n\n\n\n\n\nFigure 11.18: All-pairs HE plot for SAT, PPVT and Raven using the MANCOVA model. The ellipses labeled ‘Regr’ show the test of the overall effect of the quantitative predictors.\n\n\n\n\nThe positive effect of SES on the outcome measures is seen in all pairwise plots: the high SES group is better on all responses. The positive orientation of the Regr ellipses for the covariates shows that the predicted values for all three responses are positively correlated (more so for SAT and PPVT): higher performance on the paired associate tasks, in general, is associated with higher academic performance. The two significant predictors, na and ns are the only ones that extend outside the error ellipses, but their orientations differ.\nHomogeneity of regression\nA second model tested the assumption of homogeneity of regression by adding interactions of SES with the PA tasks, allowing separate slopes for the two groups on each of the other predictors.\n\nRohwer.mod2 &lt;- lm(cbind(SAT, PPVT, Raven) ~ SES * (n + s + ns + na + ss),\n                  data=Rohwer)\n\nThis model has 11 terms, excluding the intercept: SES, plus 5 main effects (\\(x\\)s) for the predictors and 5 interactions (slope differences), too many for an understandable display. To visualize this in an HE plot (Figure 11.19), I simplify, by showing the interaction terms collectively by a single ellipse, representing their joint effect, and specified as a linear hypothesis called slopes that picks out the interaction effects.\nThe argument terms limits the \\(\\mathbf{H}\\) ellipses for the right-hand-side of the model which are shown to just those terms specified. The combined effect of the interaction terms is specified as an hypothesis (slopes) testing the interaction terms (which have a “:” in their name). Because SES is “treatment-coded” in this model, the interaction terms reflect the difference in slopes for the high SES group compared to the low.\n\n(coefs &lt;- rownames(coef(Rohwer.mod2)))\n#&gt;  [1] \"(Intercept)\" \"SESLo\"       \"n\"           \"s\"          \n#&gt;  [5] \"ns\"          \"na\"          \"ss\"          \"SESLo:n\"    \n#&gt;  [9] \"SESLo:s\"     \"SESLo:ns\"    \"SESLo:na\"    \"SESLo:ss\"\n\ncolors &lt;- c(\"red\", \"blue\", rep(\"black\",5), \"#969696\")\nheplot(Rohwer.mod2, col=c(colors, \"darkgreen\"), \n       terms=c(\"SES\", \"n\", \"s\", \"ns\", \"na\", \"ss\"), \n       hypotheses=list(\"Regr\" = c(\"n\", \"s\", \"ns\", \"na\", \"ss\"),\n                       \"Slopes\" = coefs[grep(\":\", coefs)]),\n       fill = TRUE, fill.alpha = 0.2, cex.lab = 1.5)\n\n\n\n\n\n\nFigure 11.19: HE plot for SAT and PPVT using the heterogeneous regression model. The ellipse labeled ‘Regr’ shows the test of the covariates combined, and the ellipse labeled ‘slopes’ shows the combined difference in slopes between the two groups.\n\n\n\n\nSeparate models\nWhen there is heterogeneity of regressions, using submodels for each of the groups has the advantage that you can easily visualize the slopes for the predictors in each of the groups, particularly if you overlay the individual HE plots. In this example, I’m using the models Rohwer.sesLo and Rohwer.sesLo fit to each of the groups.\n\nRohwer.sesLo &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Lo\")\nRohwer.sesHi &lt;- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, \n                   data=Rohwer, subset = SES==\"Hi\")\n\nHere I make use of the fact that several HE plots can be overlaid using the option add=TRUE as shown in Figure 11.20. The axis limits may need adjustment in the first plot so that the second one will fit.\n\nheplot(Rohwer.sesLo, \n       xlim = c(0,100),               # adjust axis limits\n       ylim = c(40,110), \n       col=c(\"red\", \"black\"), \n       fill = TRUE, fill.alpha = 0.1,\n       lwd=2, cex=1.2, cex.lab = 1.5)\nheplot(Rohwer.sesHi, \n       add=TRUE, \n       col=c(\"brown\", \"black\"), \n       grand.mean=TRUE, \n       error.ellipse=TRUE,            # not shown by default when add=TRUE\n       fill = TRUE, fill.alpha = 0.1,\n       lwd=2, cex=1.2)\n\n\n\n\n\n\nFigure 11.20: Overlaid HE plots for SAT and PPVT, for the low and high SES groups, when each group is fit separately.\n\n\n\n\nWe can readily see the difference in means for the two SES groups (Hi has greater scores on both variables) and it also appears that the slopes of the s and n predictor ellipses are shallower for the High than the Low group, indicating greater relation with the SAT score. As well, the error ellipses show that on these measures, error variation is somewhat smaller in the low SES group.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#what-we-have-learned",
    "href": "11-mlm-viz.html#what-we-have-learned",
    "title": "11  Visualizing Multivariate Models",
    "section": "\n11.11 What We Have Learned",
    "text": "11.11 What We Have Learned\n\nHE plots clarify complex multivariate models into enlightening visualizations - The HE plot framework brilliantly addresses the interpretability problem of multivariate models by visualizing hypothesis (H) ellipses against error (E) ellipses. Rather than navigate a confusing maze of tables of coefficients and test statistics, with HE plots you can see which effects matter, how they relate to each other, and whether they’re statistically significant. All of these benefits are given in a single, intuitive plot that reveals the geometric structure underlying your multivariate analysis.\nCanonical space is your secret weapon for high-dimensional visualization - When you have many response variables, canonical discriminant analysis and canonical correlation analysis project the complex multivariate relationships into a lower-dimensional space that captures the essential patterns. This isn’t just dimension reduction—it’s insight amplification, revealing the fundamental directions of variation that matter most while showing how your original variables contribute to these meaningful dimensions.\nVisual hypothesis testing beats p-value hunting every time - HE plots make hypothesis testing immediate and intuitive: if a hypothesis ellipse extends outside the error ellipse (under significance scaling), the effect is significant. No more scanning tables of p-values or wrestling with multiple comparisons—the geometry tells the story directly. You can even decompose overall effects into meaningful contrasts and see their individual contributions as separate ellipses.\nEllipse orientations reveal the hidden correlational structure of your effects - The angles between hypothesis ellipses in HE plots directly show how different predictors relate to your response variables and to each other. When effect ellipses point in similar directions, those predictors have similar multivariate signatures; when they’re orthogonal, they capture independent aspects of variation. This geometric insight goes far beyond what correlation matrices can reveal.\nMultivariate models become tractable through progressive visual summaries - The chapter demonstrates a powerful visualization strategy: start with scatterplot matrices to see the raw data structure, move to HE plots to understand model effects, then project to canonical space for the clearest possible view of multivariate relationships. Each step preserves the essential information while making it more interpretable, turning the complexity of multivariate analysis into a comprehensible visual narrative.\n\nPackages:\n\n#&gt; Writing packages to  /home/mstruong/Documents/RStudio/collab/Vis-MLM-book/bib/pkgs.txt\n#&gt; 9  packages used here:\n#&gt;  broom, candisc, car, carData, dplyr, ggplot2, heplots, knitr, tidyr\n\n\n\n\n\nAnderson, E. (1935). The irises of the Gaspé peninsula. Bulletin of the American Iris Society, 35, 2–5.\n\n\nBartlett, M. S. (1938). Further aspects of the theory of multiple regression. Mathematical Proceedings of the Cambridge Philosophical Society, 34(1), 33–40. https://doi.org/10.1017/s0305004100019897\n\n\nBodmer, W., Bailey, R. A., Charlesworth, B., Eyre-Walker, A., Farewell, V., Mead, A., & Senn, S. (2021). The outstanding scientist, r.a. Fisher: His views on eugenics and race. Heredity, 126(4), 565–576. https://doi.org/10.1038/s41437-020-00394-6\n\n\nFisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x\n\n\nFriendly, M., & Fox, J. (2025). Candisc: Visualizing generalized canonical discriminant and canonical correlation analysis. https://github.com/friendly/candisc/\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGayan De Silva. (2020). Exploring the world of artificial neural networks -a beginner’s overview. https://doi.org/10.13140/RG.2.2.14790.14406\n\n\nGittins, R. (1985). Canonical analysis: A review with applications in ecology. Springer-Verlag.\n\n\nHotelling, H. (1936). Relations between two sets of variates. Biometrika, 28(3/4), 321. https://doi.org/10.2307/2333955\n\n\nHuang, F. L. (2019). MANOVA: A procedure whose time has passed? Gifted Child Quarterly, 64(1), 56–60. https://doi.org/10.1177/0016986219887200",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "11-mlm-viz.html#footnotes",
    "href": "11-mlm-viz.html#footnotes",
    "title": "11  Visualizing Multivariate Models",
    "section": "",
    "text": "For example, Megan Stodel in a blog post Stop using iris says, “It is clear to me that knowingly using work that was itself used in pursuit of racist ideals is totally unacceptable.” A Reddit discussion on this topic, Is it socially acceptable to use the Iris dataset? has some interesting replies.↩︎\nRecall that \\(R^2\\) for a linear model is the the proportion of variation in the response that is explained by the model, calculated as \\(R^2 = \\text{SS}_H / \\text{SS}_T = \\text{SS}_H / (\\text{SS}_H + \\text{SS}_E )\\). For a multivariate model, these are obtained from the diagonal elements of \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\).↩︎\nThat the \\(\\mathbf{H}\\) ellipses for the contrasts subtend that for the overall test of Species is no accident. In fact, this is true in \\(p\\)-dimensional space for any linear hypothesis, and orthogonal contrasts have the additional geometric property that they form conjugate axes for the overall \\(\\mathbf{H}\\) ellipsoid relative to the \\(\\mathbf{E}\\) ellipsoid (Friendly et al., 2013).↩︎\ncandiscList() performs a generalized canonical discriminant analysis for all terms in a multivariate linear model, returning a list of the results for each factor.↩︎\nIf significance scaling was used the interpretation of the canonical HE plot plot would the same as before: if the hypothesis ellipse extends beyond the error ellipse, then that dimension is significant.↩︎\nAn alternative to fitting the model removing specific cases deemed troublesome is to use a robust method, such as heplots::roblm() which uses re-weighted least squares to down-weight observations with large residuals or other problems.↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing Multivariate Models</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html",
    "href": "12-eqcov.html",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "",
    "text": "12.1 Homogeneity of Variance in Univariate ANOVA\nThis chapter concerns the extension of tests of homogeneity of variance from the classical univariate ANOVA setting to the analogous multivariate (MANOVA) setting. Such tests are a routine but important aspect of data analysis, as particular violations can drastically impact model estimates and appropriate conclusions that can be drawn (Lix & Keselman, 1996).\nBeyond issues of model assumptions, the question of equality of covariance matrices is often of general interest itself. For instance, variability is often an important issue in studies of strict equivalence in laboratories comparing across multiple patient measurements and in other applied contexts (see Gastwirth et al., 2009 for other exemplars).\nMoreover the outcome of such tests often have important consequences for the details of a main method of analysis. Just as the Welsh \\(t\\)-test (Welch, 1947) is now commonly used and reported for a two-group test of differences in means under unequal variances, a preliminary test of equality of covariance matrices is often used in discriminant analysis to decide whether linear (LDA) or quadratic discriminant analysis (QDA) should be applied in a given problem. In such cases, the data at hand should inform the choice of statistical analysis to utilize.\nWe provide some answers to the following questions:\nThe following subsections provide a capsule summary of the issues in this topic. Most of the discussion is couched in terms of a one-way design for simplicity, but the same ideas can apply to two-way (and higher) designs, where a “group” factor is defined as the product combination (interaction) of two or more factor variables.\nWhen there are also numeric covariates, this topic can also be extended to the multivariate analysis of covariance (MANCOVA) setting. This is accomplished simply by applying these techniques to the residuals from predictions by the covariates alone.\nPackages\nIn this chapter we use the following packages. Load them now\nIn classical (Gaussian) univariate ANOVA models, the main interest is typically on tests of mean differences in a response \\(y\\) according to one or more factors. The validity of the typical \\(F\\) test, however, relies on the assumption of homogeneity of variance: all groups have the same (or similar) variance, \\[\n\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_g^2 \\; .\n\\]\nIt turns out that the \\(F\\) test for differences in means is relatively robust to violation of this assumption (Harwell et al., 1992), as long as the group sample sizes are roughly equal.1 This applies to Type I error \\(\\alpha\\) rates, which are not much affected. However, unequal variance makes the ANOVA tests less efficient: you lose power to detect significant differences.\nA variety of classical test statistics for homogeneity of variance are available, including Hartley’s \\(F_{max}\\) (Hartley, 1950), Cochran’s C (Cochran, 1941),and Bartlett’s test (Bartlett, 1937), but these have been found to have terrible statistical properties (Rogan & Keselman, 1977), which prompted Box’s famous quote.\nLevene (1960) introduced a different form of test, based on the simple idea that when variances are equal across groups, the average absolute values of differences between the observations and group means will also be equal, i.e., substituting an \\(L_1\\) norm for the \\(L_2\\) norm of variance. In a one-way design, this is equivalent to a test of group differences in the means of the auxilliary variable \\(z_{ij} = | y_{ij} - \\bar{y}_i |\\).\nMore robust versions of this test were proposed by Brown & Forsythe (1974). These tests substitute the group mean by either the group median or a trimmed mean in the ANOVA of the absolute deviations. Some suggest these should be almost always preferred to Levene’s version using the mean deviation. See Conover et al. (1981) for an early review and Gastwirth et al. (2009) for a general discussion of these tests. In what follows, we refer to this class of tests as “Levene-type” tests and suggest a multivariate extension described below (Section 12.2).\nThese deviations from a group central can be calculated using heplots::colDevs() and the central value can be a function, like mean, median or an anonymous one like function(x) mean(x, trim = 0.1)) that trims 10% off each side of the distribution. With a response Y Levene-type tests then be performed “by hand” as follows:\n# Levine\nZ.mean &lt;- abs( colDevs(Y, group) )\nlm(Z.mean ~ group)\n\n# Brown-Forsythe\nZ.med &lt;- abs( colDevs(Y, group, median) )\nlm(Z.med ~ group)\nThe function car::leveneTest() does this, so we could examine whether the variances are equal in the Penguin variables, one at a time, like so:\ndata(peng, package = \"heplots\")\nleveneTest(bill_length ~ species, data=peng)\n#&gt; Levene's Test for Homogeneity of Variance (center = median)\n#&gt;        Df F value Pr(&gt;F)\n#&gt; group   2    2.29    0.1\n#&gt;       330\n\nleveneTest(bill_depth ~ species, data=peng)\n#&gt; Levene's Test for Homogeneity of Variance (center = median)\n#&gt;        Df F value Pr(&gt;F)\n#&gt; group   2    1.91   0.15\n#&gt;       330\n\n# ...\n\nleveneTest(body_mass ~ species, data=peng)\n#&gt; Levene's Test for Homogeneity of Variance (center = median)\n#&gt;        Df F value Pr(&gt;F)   \n#&gt; group   2    5.13 0.0064 **\n#&gt;       330                  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nMore conveniently, heplots:leveneTests() with an “s”, does this for each of a set of response variables, specified in a data frame, a model formula or a \"mlm\" object. It also formats the results in a more pleasing way:\npeng.mod &lt;- lm(cbind(bill_length, bill_depth, flipper_length, body_mass) ~ species, \n               data = peng)\nleveneTests(peng.mod)\n#&gt; Levene's Tests for Homogeneity of Variance (center = median)\n#&gt; \n#&gt;                df1 df2 F value Pr(&gt;F)   \n#&gt; bill_length      2 330    2.29 0.1033   \n#&gt; bill_depth       2 330    1.91 0.1494   \n#&gt; flipper_length   2 330    0.44 0.6426   \n#&gt; body_mass        2 330    5.13 0.0064 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nSo, this tells us that the groups do not differ in variances on first three variables, but they do for body_mass.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#sec-mlevene",
    "href": "12-eqcov.html#sec-mlevene",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.2 Visualizing Levene’s test",
    "text": "12.2 Visualizing Levene’s test\nTo gain some insight into the problem of homogeneity of variance it is helpful so see how the situation looks in terms of data. For the Penguin data, it might be simplest just just to look at boxplots of the variables and try to see whether the widths of the central 50% boxes seem to be the same, as in Figure 12.1. However, it is perceptually difficult to focus on differences with widths of the boxes within each panel when their centers also differ from group to group.\n\nSee the codesource(\"R/penguin/penguin-colors.R\")\ncol &lt;- peng.colors(\"dark\")\nclr &lt;- c(col, gray(.20))\npeng_long &lt;- peng |&gt; \n  pivot_longer(bill_length:body_mass, \n               names_to = \"variable\", \n               values_to = \"value\") \n\npeng_long |&gt;\n  group_by(species) |&gt; \n  ggplot(aes(value, species, fill = species)) +\n  geom_boxplot() +\n  facet_wrap(~ variable, scales = 'free_x') +\n  theme_penguins() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'none') \n\n\n\n\n\n\nFigure 12.1: Boxplots for the Penguin variables. For assessing homogeneity of variance, we should be looking for differences in width of the central 50% boxes in each panel, rather than difference in central tendency.\n\n\n\n\nInstead, you can see more directly what is tested by the Levene test by graphing the absolute deviations from the group means or medians. This is another example of the graphic idea that you can make visual comparisons easier by plotting quantities of direct interest. You can calculate the median deviation values as follows:\n\nvars &lt;- c(\"bill_length\", \"bill_depth\", \"flipper_length\", \"body_mass\")\npengDevs &lt;- colDevs(peng[, vars], peng$species, median) |&gt;\n  abs()\n\nFrom a boxplot of the absolute deviations in Figure 12.2 your eye can now focus on the central value, shown by the median ‘|’ line, because Levene’s method is testing whether these differ across groups.\n\nSee the code# calculate absolute differences from median\ndev_long &lt;- data.frame(species = peng$species, pengDevs) |&gt; \n  pivot_longer(bill_length:body_mass, \n               names_to = \"variable\", \n               values_to = \"value\") \n\ndev_long |&gt;\n  group_by(species) |&gt; \n  ggplot(aes(value, species, fill = species)) +\n  geom_boxplot() +\n  facet_wrap(~ variable, scales = 'free_x') +\n  xlab(\"absolute median deviation\") +\n  theme_penguins() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'none') \n\n\n\n\n\n\nFigure 12.2: Boxplots for absolute differences from group medians for the Penguin data. The visual test of equality of variance is whether the median lines in the boxplots align.\n\n\n\n\nIt is now easy to see that the medians largely align for all the variables except for body_mass.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#sec-homogeneity-MANOVA",
    "href": "12-eqcov.html#sec-homogeneity-MANOVA",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.3 Homogeneity of variance in MANOVA",
    "text": "12.3 Homogeneity of variance in MANOVA\nIn the MANOVA context, the main emphasis, of course, is on differences among mean vectors, testing \\[\n\\mathcal{H}_0 : \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2 = \\cdots = \\mathbf{\\mu}_g \\; .\n\\] However, the standard test statistics (Wilks’ Lambda, Hotelling-Lawley trace, Pillai-Bartlett trace, Roy’s maximum root) rely upon the analogous assumption that the within-group covariance matrices \\(\\mathbf{\\Sigma}_i\\) are equal for all groups, \\[\n\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\cdots = \\mathbf{\\Sigma}_g \\; .\n\\] This is much stronger than in the univariate case, because it also requires that all the correlations between pairs of variables are the same for all groups. For example, for two responses, there are three parameters (\\(\\rho, \\sigma_1^2, \\sigma_2^2\\)) assumed equal across all groups; for \\(p\\) responses, there are \\(p (p+1) / 2\\) assumed equal. The variances relate to size differences among data ellipses while the differences in the covariances appear as differences in shape.\n\nTo preview a main example, Figure 12.3 shows data ellipses for the main size variables in the palmerpenguins::penguins data. heplots::covEllipses() is specialized for viewing the relations among the data ellipsoids representing the sample covariance matrices, \\(\\mathbf{S}_1 = \\mathbf{S}_2 = \\cdots = \\mathbf{S}_g\\). It draws the data ellipse for each group, and also for the pooled within-group \\(\\mathbf{S}_p\\), as shown in Figure 12.3 for bill length and bill depth.\nYou can see that the sizes and shapes of the data ellipses are sort of similar in the left panel. The visual comparison becomes more precise when the data ellipses are all shifted to a common origin at the grand means (using center = TRUE). From this you can see that the Adelie group differs most from the others.\n\nSee the codeop &lt;- par(mar = c(4, 4, 1, 1) + .5,\n          mfrow = c(c(1,2)))\ncovEllipses(cbind(bill_length, bill_depth) ~ species, data=peng,\n  fill = TRUE,\n  fill.alpha = 0.1,\n  lwd = 3,\n  col = clr)\n\ncovEllipses(cbind(bill_length, bill_depth) ~ species, data=peng,\n  center = TRUE, \n  fill = c(rep(FALSE,3), TRUE), \n  fill.alpha = .1, \n  lwd = 3,\n  col = clr,\n  label.pos = c(1:3,0))\npar(op)\n\n\n\n\n\n\nFigure 12.3: Data ellipses for bill length and bill depth in the penguins data, also showing the pooled covariance. Left: As is; right: these are centered at the grand means for easier comparison.\n\n\n\n\nAll such pairwise plots in scatterplot matrix format are produced using the variables argument to covEllipses(), giving Figure 12.4\n\nCodeclr &lt;- c(peng.colors(), \"black\")\ncovEllipses(peng[,3:6], peng$species, \n  variables=1:4,\n  col = clr,\n  fill=TRUE, \n  fill.alpha=.1)\n\n\n\n\n\n\nFigure 12.4: All pairwise covariance ellipses for the penguins data. The covariance matrices are homogeneous when the ellipses for the groups all have the same size and shape.\n\n\n\n\nThe covariance ellipses in Figure 12.4 look pretty similar in size, shape and orientation. But what does Box’s M test (described below) say? As you can see, it concludes strongly against the null hypothesis, because the test is highly sensitive to small differences among the covariance matrices.\n\npeng.boxm &lt;- boxM(cbind(bill_length, bill_depth, flipper_length, body_mass) \n           ~ species, \n     data=peng) |&gt;\n  print()\n#&gt; \n#&gt;  Box's M-test for Homogeneity of Covariance Matrices\n#&gt; \n#&gt; data:  Y\n#&gt; Chi-Sq (approx.) = 75, df = 20, p-value = 3e-08\n\nIt will be useful to have another example as we proceed, so Figure 12.5 shows an analogous plot for the iris data we examined in Section 11.6. Even when these are shown uncentered, the differences in size, shape and orientation are much more apparent. Iris setosa stands out as having smaller variance on some of the variables, while the ellipses for virginica tend to be larger.\n\nCodeiris.colors &lt;- c(\"red\", \"darkgreen\", \"blue\")\ncovEllipses(iris[,1:4], iris$Species, \n  variables=1:4, \n  fill = TRUE,\n  fill.alpha=.1,\n  col = c(iris.colors, \"black\"),\n  label.pos=c(1:3,0))\n\n\n\n\n\n\nFigure 12.5: All pairwise covariance ellipses for the iris data.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#sec-boxM",
    "href": "12-eqcov.html#sec-boxM",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.4 Box’s M test",
    "text": "12.4 Box’s M test\nTake a moment and think, “How could we generalize a test of equality of variances, \\(s_1^2 = s_2^2 = \\cdots = s_g^2\\), to the multivariate case, where we have \\((p \\times p)\\) matrices, \\(\\mathbf{S}_1 = \\mathbf{S}_2 = \\cdots = \\mathbf{S}_g\\) for each group?”. Multivariate thinking suggests that that we calculate some measure of “size” of each \\(\\mathbf{S}_i\\), in a similar way to what is done in multivariate tests comparing \\(\\mathbf{H}\\) and \\(\\mathbf{E}\\) matrices.\nBox (1949) proposed the following likelihood-ratio test (LRT) statistic \\(\\mathcal{M}\\) for testing the hypothesis of equal covariance matrices, using the log of the determinant \\(\\vert \\mathbf{S}_i \\vert\\) as the measure of size. \\[\n\\mathcal{M} = (N -g) \\ln \\;|\\; \\mathbf{S}_p \\;|\\; - \\sum_{i=1}^g (n_i -1) \\ln \\;|\\; \\mathbf{S}_i \\;|\\; \\; ,\n\\tag{12.1}\\]\nwhere \\(N = \\sum n_i\\) is the total sample size and\n\\[\\mathbf{S}_p = (N-g)^{-1} \\sum_{i=1}^g (n_i - 1) \\mathbf{S}_i\\]\nis the pooled covariance matrix.\n\\(\\mathcal{M}\\) can thus be thought of as a ratio of the determinant of the pooled \\(\\mathbf{S}_p\\) to the geometric mean of the determinants of the separate \\(\\mathbf{S}_i\\).\nIn practice, there are various transformations of the value of \\(M\\) to yield a test statistic with an approximately known distribution (Timm, 1975). Roughly speaking, when each \\(n_i &gt; 20\\), a \\(\\chi^2\\) approximation is often used; otherwise an \\(F\\) approximation is known to be more accurate.\nAsymptotically, \\(-2 \\ln (\\mathcal{M})\\) has a \\(\\chi^2\\) distribution. The \\(\\chi^2\\) approximation due to Box (1949, 1950) is that \\[\nX^2 = -2 (1-c_1) \\ln (\\mathcal{M}) \\quad \\sim \\quad \\chi^2_{df}\n\\] with \\(df = (g-1) p (p+1)/2\\) degrees of freedom, and a bias correction constant: \\[\nc_1 = \\left(\n\\sum_i \\frac{1}{n_i -1}\n- \\frac{1}{N-g}\n\\right)\n\\frac{2p^2 +3p -1}{6 (p+1)(g-1)} \\; .\n\\]\nIn this form, Bartlett’s test for equality of variances in the univariate case is the special case of Box’s M when there is only one response variable, so Bartlett’s test is sometimes used as univariate follow-up to determine which response variables show heterogeneity of variance.\nYet, like its univariate counterparts, Box’s test is well-known to be highly sensitive to violation of (multivariate) normality and the presence of outliers2, as Box (1953) suggested in the opening chapter quote. Yet, it provides a nice framework for thinking about this problem more generally. …",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#visualizing-heterogeneity",
    "href": "12-eqcov.html#visualizing-heterogeneity",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.5 Visualizing heterogeneity",
    "text": "12.5 Visualizing heterogeneity\nA larger goal of this chapter is to use this background as another illustration of multivariate thinking, here, for visualizing and testing the heterogeneity of covariance matrices in multivariate designs. While researchers often rely on a single number to determine if their data have met a particular threshold, such compression will often obscure interesting information, particularly when a test concludes that differences exist, and one is left to wonder ``why?’’. It is within this context where, again, visualizations often reign supreme.\nWe have already seen one useful method in Section 12.3, which uses direct visualization of the information in the \\(\\mathbf{S}_i\\) and \\(\\mathbf{S}_p\\) using data ellipsoids to show size and shape as minimal schematic summaries; In what follows, I propose three additional visualization-based approaches to questions of heterogeneity of covariance in MANOVA designs:\n\na simple dotplot of the components of Box’s M test: the log determinants of the \\(\\mathbf{S}_i\\) together with that of the pooled \\(\\mathbf{S}_p\\). Extensions of these simple plots raise the question of whether measures of heterogeneity other than that captured in Box’s test might also be useful; and,\nPCA low-rank views to highlight features more easily seen there than in the full data space.\nthe connection between Levene-type tests and an ANOVA (of centered absolute differences) suggests a parallel with a multivariate extension of Levene-type tests and a MANOVA. We explore this with a version of Hypothesis-Error (HE) plots we have found useful for visualizing mean differences in MANOVA designs.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#sec-viz-boxM",
    "href": "12-eqcov.html#sec-viz-boxM",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.6 Visualizing Box’s \\(\\mathcal{M}\\)\n",
    "text": "12.6 Visualizing Box’s \\(\\mathcal{M}\\)\n\nBox’s test is based on a comparison of the log determinants of the \\(\\mathbf{S}_i\\) relative to that of the pooled \\(\\mathbf{S}_p\\), so the simplest thing to do is just plot them!\nboxM() produces a \"boxm\" object, for which there are summary() (details) and plot() methods. The plot() method gives a dot plot of the log determinants \\(\\ln \\vert \\mathbf{S}_i \\vert\\) together with that for the pooled covariance \\(\\ln \\vert \\mathbf{S}_p \\vert\\). Cai et al. (2015) provide the theory for the (asymptotic) confidence intervals shown.\n\nplot(peng.boxm, gplabel=\"species\", cex.lab = 1.5)\n\nplot(iris.boxm, gplabel=\"Species\", cex.lab = 1.5)\n\n\n\n\n\n\nFigure 12.6: Plots of the contributions to Box’s \\(\\mathcal{M}\\) statistic for the Penguin and iris data.\n\n\n\n\nIn these plots (Figure 12.6), the value for the pooled covariance appears within the range of the groups, because it is a weighted average. If you take a moment to look back at Figure 12.4, you’ll see that the data ellipses for Gentoo are slightly smaller in most pairwise views. From Figure 12.5, it is clear that setosa shows the smallest within-group variability. The scale values on the horizontal axis give a sense that the range across groups is considerably greater for the iris data than for the Penguins.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#sec-eqcov-low-rank-views",
    "href": "12-eqcov.html#sec-eqcov-low-rank-views",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.7 Low-rank views",
    "text": "12.7 Low-rank views\nWith \\(p&gt;3\\) response variables, a simple alternative to the pairwise 2D plots in data space shown in Figure 12.4 and Figure 12.5 is the projection into the principal component space accounting for the greatest amounts of total variance in the data. For the Iris data, a simple PCA of the covariance matrix shows that nearly 98% of total variance in the data is accounted for in the first two dimensions.\nFigure 12.7 shows the plots of the covariance ellipsoids for the first two principal component scores, uncentered (left panel) and centered (right panel). The dominant PC1 (92% of total variance) essentially orders the species by a measure of overall size of their sepals and petals. In the centered view, it can again be seen how Setosa differs in covariance from the other two species, and that while Virginca and Versicolor both have similar shapes to the pooled covariance matrix, Versicolor has somewhat greater variance on PC1.\n\niris.pca &lt;- prcomp(iris[,1:4])\n\ncovEllipses(iris.pca$x, iris$Species, \n  col = c(iris.colors, \"black\"),\n  fill=TRUE, fill.alpha=.1,\n  cex.lab = 1.5,\n  label.pos = c(1, 3, 3, 0), asp=1)\n\ncovEllipses(iris.pca$x, iris$Species,\n  center=TRUE,        \n  col = c(iris.colors, \"black\"),\n  fill=TRUE, fill.alpha=.1,\n  cex.lab = 1.5,\n  label.pos = c(1, 3, 3, 0), asp=1)\n\n\n\n\n\n\nFigure 12.7: Covariance ellipsoids for the first two principal components of the iris data. Left: uncentered, showing group means on the principal components; right: centered at the origin.\n\n\n\n\n\n12.7.1 Small dimensions can matter\nFor the Iris data, the first two principal components account for 98% of total variance, so we might think we are done here. Yet, as we’ve seen in other problems (outliers, collinearity), important information also exists in the space of the smallest principal component dimensions.\nThis is also true, as we will see for Box’s M test, because it is a (linear) function of all the eigenvalues of the between and within group covariance matrices, is therefore also subject to the influence of the smaller dimensions, where differences among \\(\\mathbf{S}_i\\) and of \\(\\mathbf{S}_p\\) can lurk.\n\ncovEllipses(iris.pca$x, iris$Species,\n  variables = 3:4,\n  center=TRUE,        \n  col = c(iris.colors, \"black\"),\n  fill=TRUE, fill.alpha=.1,\n  cex.lab = 1.5,\n  label.pos = c(1, 3, 3, 0), asp=1)\n\n\n\n\n\n\nFigure 12.8\n\n\n\n\nFigure 12.8 shows the covariance ellipsoids in (PC3, PC4) space. Even though these dimensions contribute little to total variance, there are more pronounced differences in the within-group shapes (correlations) relative to the pooled covariance, and these contribute to a rejection of homogeneity by Box’s M test. Here we see that the correlation for Virginca is of opposite sign from the other two groups.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#other-measures-of-heterogeneity",
    "href": "12-eqcov.html#other-measures-of-heterogeneity",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "\n12.8 Other measures of heterogeneity",
    "text": "12.8 Other measures of heterogeneity\nAs we saw above Section 12.3, the question of equality of covariance matrices can be expressed in terms of the similarity in size and shape of the data ellipses for the individual group \\(\\mathbf{S}_i\\) relative to that of \\(\\mathbf{S}_p\\). Box’s \\(\\mathcal{M}\\) test uses just one possible function to describe this size: the logs of their determinants.\nWhen \\(\\mathbf{\\Sigma}\\) is the covariance matrix of a multivariate vector \\(\\mathbf{y}\\) with eigenvalues \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\lambda_p\\), the properties shown in Table 12.1 represent methods of describing the size and shape of the ellipsoid in \\(\\mathbb{R}^{p}\\).3 Just as is the case for tests of the MLM itself where different functions of these give test statistics (Wilks’ \\(\\Lambda\\), Pillai trace, etc.), one could construct other test statistics for homogeneity of covariance matrices.\n\n\nTable 12.1: Statistical and geometrical properties of “size” of an ellipsoid\n\n\n\nSize\nConceptual formula\nGeometry\nFunction\n\n\n\n(a) Generalized variance:\n\\(\\det{\\mathbf{\\Sigma}} = \\prod_i \\lambda_i\\)\narea, (hyper)volume\ngeometric mean\n\n\n(b) Average variance:\n\\(\\text{tr}({\\mathbf{\\Sigma}}) = \\sum_i \\lambda_i\\)\nlinear sum\narithmetic mean\n\n\n(c) Average precision:\n\\(1/ \\text{tr}({\\mathbf{\\Sigma}^{-1}}) = 1/\\sum_i (1/\\lambda_i)\\)\n\nharmonic mean\n\n\n(d) Maximal variance:\n\\(\\lambda_1\\)\nmaximum dimension\nsupremum\n\n\n\n\n\n\nHence, for a sample covariance matrix \\(\\mathbf{S}\\), \\(\\vert \\mathbf{S} \\vert\\) is a measure of generalized variance and \\(\\ln \\vert \\mathbf{S} \\vert\\) is a measure of average variance across the \\(p\\) dimensions.\nThe \"boxM\" plot methods in r\"heplots\"` can compute and plot all of the functions of the eigenvalues in Table 12.1. The results are shown in Figure 12.9.\n\nplot(peng.boxm, which=\"product\", gplabel=\"Species\")\nplot(peng.boxm, which=\"sum\", gplabel=\"Species\")\nplot(peng.boxm, which=\"precision\", gplabel=\"Species\")\nplot(peng.boxm, which=\"max\", gplabel=\"Species\")\n\n\n\n\n\n\nFigure 12.9: Plot of eigenvalue statistics of the covariance matrices for the Penguin data\n\n\n\n\nExcept for the absence of error bars, the plot for log product in the upper left panel of Figure 12.9 is the same as that in Figure 12.6. In principle, it is possible to add such confidence intervals for all these measures through the use of bootstrapping, but this has not yet been implemented.\nFor this data set, the pattern of points in the plot for Box’s \\(\\mathcal{M}\\) is also more or less the same as that for the precision measure. The plots for the sum of and maximum eigenvalue are also similar to each other, but differ from those of the two measures in the left column of Figure 12.9. The main point is that these are not all the same, so different functions reflect different patterns of the eigenvalues, and could be used to define other statistical tests.\nPackages used here\n\n#&gt; Writing packages to  /home/mstruong/Documents/RStudio/collab/Vis-MLM-book/bib/pkgs.txt\n#&gt; 9  packages used here:\n#&gt;  broom, candisc, car, carData, dplyr, ggplot2, heplots, knitr, tidyr\n\n\n\n\n\n\nBartlett, M. S. (1937). Properties of sufficiency and statistical tests. Proceedings of the Royal Society of London. Series A, 160(901), 268–282. https://doi.org/10.2307/96803\n\n\nBox, G. E. P. (1949). A general distribution theory for a class of likelihood criteria. Biometrika, 36(3-4), 317–346. https://doi.org/10.1093/biomet/36.3-4.317\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and wear curves. Biometrics, 6, 362–389.\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances. Biometrika, 40(3/4), 318–335. https://doi.org/10.2307/2333350\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the American Statistical Association, 69(346), 364–367. https://doi.org/10.1080/01621459.1974.10482955\n\n\nCai, T. T., Liang, T., & Zhou, H. H. (2015). Law of log determinant of sample covariance matrix and optimal estimation of differential entropy for high-dimensional gaussian distributions. Journal of Multivariate Analysis, 137, 161–172. https://doi.org/https://doi.org/10.1016/j.jmva.2015.02.003\n\n\nCochran, W. G. (1941). The distribution of the largest of a set of estimated variances as a fraction of their total. Annals of Eugenics, 11(1), 47–52. https://doi.org/10.1111/j.1469-1809.1941.tb02271.x\n\n\nConover, W. J., Johnson, M. E., & Johnson, M. M. (1981). A comparative study of tests for homogeneity of variances, with applications to the outer continental shelf bidding data. Technometrics, 23(4), 351–361. https://doi.org/10.1080/00401706.1981.10487680\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights: Understanding statistical methods through elliptical geometry. Statistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nGastwirth, J. L., Gel, Y. R., & Miao, W. (2009). The impact of Levene’s test of equality of variances on statistical theory and practice. Statistical Science, 24(3), 343–360. https://doi.org/10.1214/09-STS301\n\n\nHartley, H. O. (1950). The use of range in analysis of variance. Biometrika, 37(3–4), 271–280. https://doi.org/10.1093/biomet/37.3-4.271\n\n\nHarwell, M. R., Rubinstein, E. N., Hayes, W. S., & Olds, C. C. (1992). Summarizing monte carlo results in methodological research: The one- and two-factor fixed effects ANOVA cases. Journal of Educational and Behavioral Statistics, 17(4), 315–339. https://doi.org/10.3102/10769986017004315\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. Olkin, S. G. Ghurye, W. Hoeffding, W. G. Madow, & H. B. Mann (Eds.), Contributions to probability and statistics: Essays in honor of Harold Hotelling (pp. 278–292). Stanford University Press.\n\n\nLix, J. M., L. M. Keselman, & Keselman, H. J. (1996). Consequences of assumption violations revisited: A quantitative review of alternatives to the one-way analysis of variance F test. Review of Educational Research, 66(4), 579–619. https://doi.org/10.3102/00346543066004579\n\n\nO’Brien, P. C. (1992). Robust procedures for testing equality of covariance matrices. Biometrics, 48(3), 819–827. http://www.jstor.org/stable/2532347\n\n\nRogan, J. C., & Keselman, H. J. (1977). Is the ANOVA f-test robust to variance heterogeneity when sample sizes are equal?: An investigation via a coefficient of variation. American Educational Research Journal, 14(4), 493–498. https://doi.org/10.3102/00028312014004493\n\n\nTiku, M. L., & Balakrishnan, N. (1984). Testing equality of population variances the robust way. Communications in Statistics - Theory and Methods, 13(17), 2143–2159. https://doi.org/10.1080/03610928408828818\n\n\nTimm, N. H. (1975). Multivariate analysis with applications in education and psychology. Wadsworth (Brooks/Cole).\n\n\nWelch, B. L. (1947). The generalization of \"student’s\" problem when several different population varlances are involved. Biometrika, 34(1–2), 28–35. https://doi.org/10.1093/biomet/34.1-2.28\n\n\nZhang, J., & Boos, D. D. (1992). Bootstrap critical values for testing homogeneity of covariance matrices. Journal of the American Statistical Association, 87(418), 425–429. http://www.jstor.org/stable/2290273",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "12-eqcov.html#footnotes",
    "href": "12-eqcov.html#footnotes",
    "title": "12  Visualizing Equality of Covariance Matrices",
    "section": "",
    "text": "If group sizes are greatly unequal and homogeneity of variance is violated, then the \\(F\\) statistic is too liberal (\\(p\\) values too large) when large sample variances are associated with small group sizes. Conversely, the \\(F\\) statistic is too conservative if large variances are associated with large group sizes.↩︎\nFor example, Tiku & Balakrishnan (1984) concluded from simulation studies that the normal-theory LRT provides poor control of Type I error under even modest departures from normality. O’Brien (1992) proposed some robust alternatives, and showed that Box’s normal theory approximation suffered both in controlling the null size of the test and in power. Zhang & Boos (1992) also carried out simulation studies with similar conclusions and used bootstrap methods to obtain corrected critical values.↩︎\nMore general theory and statistical applications of the geometry of ellispoids is given by Friendly et al. (2013).↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing Equality of Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "13-infl-robust.html",
    "href": "13-infl-robust.html",
    "title": "\n13  Multiviate Influence and Robust Estimation\n",
    "section": "",
    "text": "13.1 Multivariate influence\nIn the analysis of linear models, the identification and treatment of outliers and influential observations represents one of the most critical yet challenging aspects of statistical modeling. As you saw earlier (Section 6.6), even a single “bad” observation can completely alter the results of a linear model fit by ordinary least squares.\nUnivariate influence diagnostics have been well-established since the pioneering work of Cook (1977) and others (Belsley et al. (1980);Cook & Weisberg (1982)) and their wide implementation in R packages such as stats and car makes these readily accessible in statistical practice. If you seek statistical advice regarding a perplexing model, the consultant may well ask:\nHowever, the extension to multivariate response models introduces additional complexity that goes far beyond simply applying univariate methods to each response variable separately. The multivariate case requires consideration of the joint influence structure across all responses simultaneously, accounting for the correlation patterns among dependent variables and the potential for observations to be influential in some linear combinations of responses while appearing benign when examined multivariate. This multivariate perspective can reveal influence patterns that would otherwise remain hidden, as an observation might exert substantial leverage on the overall model fit through subtle but systematic effects across multiple responses.\nDetecting outliers and influential observations has now progressed to the point where the methods described below can usefully be applied to multivariate linear models. But having found some troublesome cases, the question arises, what to do about them?\nPackages\nIn this chapter we use the following packages. Load them now\nAn elegant extension of the ideas behind leverage, studentized residuals and measures of influence to the case of multivariate response data is due to Barrett & Ling (1992) (see also: Barrett (2003)). These methods have been implemented in the mvinfluence package (Friendly, 2025) which makes available several forms of influence plots to visualize the results.\nAs in the univariate case, the measures of multivariate influence stem from case-deletion idea of comparing some statistic calculated from the full sample to that statistic calculated when case \\(i\\) is deleted. The Barrett-Ling approach generalized this to the case of deleting a set \\(I\\) of \\(m \\ge 1\\) cases. This can be useful because some cases can “mask” the influence of others in the sense that when one is deleted, others become much more influential.\nThe following sections describe the notation and measures used in the calculations.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "13-infl-robust.html#sec-multivariate-influence",
    "href": "13-infl-robust.html#sec-multivariate-influence",
    "title": "\n13  Multiviate Influence and Robust Estimation\n",
    "section": "",
    "text": "13.1.1 Notation\nLet \\(\\mathbf{X}\\) be the model matrix in the multivariate linear model, \\(\\mathbf{Y}_{n \\times p} = \\mathbf{X}_{n \\times q} \\; \\mathbf{B}_{q \\times p} + \\mathbf{E}_{n \\times p}\\). The usual least squares estimate of \\(\\mathbf{B}\\) is given by \\(\\mathbf{B} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}  \\mathbf{X}^\\top \\mathbf{Y}\\).\nThen let\n\n\n\\(\\mathbf{X}_I\\) be the submatrix of \\(\\mathbf{X}\\) whose \\(m\\) rows are indexed by \\(I\\),\n\n\\(\\mathbf{X}_{(-I)}\\) is the complement, the submatrix of \\(\\mathbf{X}\\) with the \\(m\\) rows in \\(I\\) deleted,\n\nMatrices \\(\\mathbf{Y}_I\\), \\(\\mathbf{Y}_{(-I)}\\) are defined similarly, denoting the submatrix of \\(m\\) rows of \\(\\mathbf{Y}\\) and the submatrix with those rows deleted, respectively.\nIn the calculation of regression coefficients, \\(\\mathbf{B}_{(-I)} = (\\mathbf{X}_{(-I)}^\\top \\mathbf{X}_{(-I)})^{-1} \\mathbf{X}_{(-I)}^\\top \\mathbf{Y}_{I}\\) are the estimated coefficients when the cases indexed by \\(I\\) have been removed. The corresponding residuals are \\(\\mathbf{E}_{(-I)} = \\mathbf{Y}_{(-I)} - \\mathbf{X}_{(-I)} \\mathbf{B}_{(-I)}\\).\n\n13.1.2 Hat values and residuals\nThe influence measures defined by Barrett & Ling (1992) are functions of two matrices \\(\\mathbf{H}_I\\) and \\(\\mathbf{Q}_I\\) corresponding to hat values and residuals, defined as follows:\n\nFor the full data set, the “hat matrix”, \\(\\mathbf{H}\\), is given by \\(\\mathbf{H} = \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top\\),\n\n\\(\\mathbf{H}_I\\) is the \\(m \\times m\\) the submatrix of \\(\\mathbf{H}\\) corresponding to the index set \\(I\\), \\(\\mathbf{H}_I = \\mathbf{X} (\\mathbf{X}_I^\\top \\mathbf{X}_I)^{-1} \\mathbf{X}^\\top\\),\n\n\\(\\mathbf{Q}\\) is the analog of \\(\\mathbf{H}\\) defined for the residual matrix \\(\\mathbf{E}\\), that is, \\(\\mathbf{Q} = \\mathbf{E} (\\mathbf{E}^\\top \\mathbf{E})^{-1} \\mathbf{E}^\\top\\), with corresponding submatrix \\(\\mathbf{Q}_I = \\mathbf{E} \\, (\\mathbf{E}_I^\\top \\mathbf{E}_I)^{-1} \\, \\mathbf{E}^\\top\\),\n\n13.1.3 Cook’s distance\nIn these terms, Cook’s distance is defined for a univariate response by \\[\nD_I = (\\mathbf{b} - \\mathbf{b}_{(-I)})^T (\\mathbf{X}^T \\mathbf{X}) (\\mathbf{b} - \\mathbf{b}_{(-I)}) / p s^2 \\; ,\n\\] a measure of the squared distance between the coefficients \\(\\mathbf{b}\\) for the full data set and those \\(\\mathbf{b}_{(-I)}\\) obtained when the cases in \\(I\\) are deleted.\nIn the multivariate case, Cook’s distance is obtained by replacing the vector of coefficients \\(\\mathbf{b}\\) by \\(\\mathrm{vec} (\\mathbf{B})\\), the result of stringing out the coefficients for all responses in a single \\(n \\times p\\)-length vector.\n\\[\nD_I = \\frac{1}{p} [\\mathrm{vec} (\\mathbf{B} - \\mathbf{B}_{(-I)})]^T (S^{-1} \\otimes \\mathbf{X}^T \\mathbf{X}) \\mathrm{vec} (\\mathbf{B} - \\mathbf{B}_{(-I)})  \\; ,\n\\] where \\(\\otimes\\) is the Kronecker (direct) product and \\(\\mathbf{S} = \\mathbf{E}^T \\mathbf{E} / (n-p)\\) is the covariance matrix of the residuals.\n\n13.1.4 Leverage and residual components\nFor a univariate response, and when \\(m = 1\\), Cook’s distance can be re-written as a product of leverage and residual components as \\[\nD_i = \\left(\\frac{n-p}{p} \\right) \\frac{h_{ii} q_{ii}}{(1 - h_{ii})^2  } \\;.\n\\]\nThen we can define a leverage component \\(L_i\\) and residual component \\(R_i\\) as\n\\[\nL_i = \\frac{h_{ii}}{1 - h_{ii}} \\quad\\quad R_i = \\frac{q_{ii}}{1 - h_{ii}} \\;.\n\\]\n\\(R_i\\) is the studentized residual, and \\(D_i \\propto L_i \\times R_i\\).\nIn the general, multivariate case there are analogous matrix expressions for \\(\\mathbf{L}\\) and \\(\\mathbf{R}\\). When m &gt; 1, the quantities \\(\\mathbf{H}_I\\), \\(\\mathbf{Q}_I\\), \\(\\mathbf{L}_I\\), and \\(\\mathbf{R}_I\\) are \\(m \\times m\\) matrices. Where scalar quantities are needed, the mvinfluence functions apply a function, FUN, either det() or tr() to calculate a measure of “size”, as in\n  H &lt;- sapply(x$H, FUN)\n  Q &lt;- sapply(x$Q, FUN)\n  L &lt;- sapply(x$L, FUN)\n  R &lt;- sapply(x$R, FUN)",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "13-infl-robust.html#sec-robust-estimation",
    "href": "13-infl-robust.html#sec-robust-estimation",
    "title": "\n13  Multiviate Influence and Robust Estimation\n",
    "section": "\n13.2 Robust Estimation",
    "text": "13.2 Robust Estimation\n\n\n\n\nBarrett, B. E. (2003). Understanding influence in multivariate regression. Communications in Statistics - Theory and Methods, 32(3), 667–680. https://doi.org/10.1081/STA-120018557\n\n\nBarrett, B. E., & Ling, R. F. (1992). General classes of influence measures for multivariate regression. Journal of the American Statistical Association, 87(417), 184–191. https://www.jstor.org/stable/i314301\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley; Sons.\n\n\nCook, R. D. (1977). Detection of influential observation in linear regression. Technometrics, 19(1), 15–18. http://links.jstor.org/sici?sici=0040-1706%28197702%2919%3A1%3C15%3ADOIOIL%3E2.0.CO%3B2-8\n\n\nCook, R. D., & Weisberg, S. (1982). Residuals and influence in regression. Chapman; Hall.\n\n\nFriendly, M. (2025). Mvinfluence: Influence measures and diagnostic plots for multivariate linear models. https://github.com/friendly/mvinfluence",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiviate Influence and Robust Estimation</span>"
    ]
  },
  {
    "objectID": "14-case-studies.html",
    "href": "14-case-studies.html",
    "title": "\n14  Case studies\n",
    "section": "",
    "text": "14.1 Neuro- and Social-cognitive measures in psychiatric groups\nThis chapter presents some complete analyses of datasets that will be prominent in the book. Some of this material may later be moved to earlier chapters.\nPackages\nIn this chapter we use the following packages. Load them now\nA Ph.D. dissertation by Laura Hartman (2016) at York University was designed to evaluate whether and how clinical patients diagnosed (on the DSM-IV) as schizophrenic or with schizoaffective disorder could be distinguished from each other and from a normal, control sample on collections of standardized tests in the following domains:\nThe study is an important contribution to clinical research because the two diagnostic categories are subtly different and their symptoms often overlap. Yet, they’re very different and often require different treatments. A key difference between schizoaffective disorder and schizophrenia is the prominence of mood disorder involving bipolar, manic and depressive moods. With schizoaffective disorder, mood disorders are front and center. With schizophrenia, that is not a dominant part of the disorder, but psychotic ideation (hearing voices, seeing imaginary people) is.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "14-case-studies.html#neuro--and-social-cognitive-measures-in-psychiatric-groups",
    "href": "14-case-studies.html#neuro--and-social-cognitive-measures-in-psychiatric-groups",
    "title": "\n14  Case studies\n",
    "section": "",
    "text": "Neuro-cognitive: processing speed, attention, verbal learning, visual learning and problem solving;\nSocial-cognitive: managing emotions, theory of mind, externalizing, personalizing bias.\n\n\n\n14.1.1 Research questions\nThis example is concerned with the following substantitive questions:\n\nTo what extent can patients diagnosed as schizophrenic or with schizoaffective disorder be distinguished from each other and from a normal control sample using a well-validated, comprehensive neurocognitive battery specifically designed for individuals with psychosis (Heinrichs et al., 2015) ?\nIf the groups differ, do any of the cognitive domains show particularly larger or smaller differences among these groups?\nDo the neurocognitive measures discriminate among the in the same or different ways? If different, how many separate aspects or dimensions are distinguished?\n\nApart from the research interest, it could aid diagnosis and treatment if these similar mental disorders could be distinguished by tests in the cognitive domain.\n\n14.1.2 Data\nThe clinical sample comprised 116 male and female patients who met the following criteria: 1) a diagnosis of schizophrenia (\\(n\\) = 70) or schizoaffective disorder (\\(n\\) = 46) confirmed by the Structured Clinical Interview for DSM-IV-TR Axis I Disorders; 2) were outpatients; 3) a history free of developmental or learning disability; 4) age 18-65; 5) a history free of neurological or endocrine disorder; and 6) no concurrent diagnosis of substance use disorder. Non-psychiatric control participants (\\(n\\) = 146) were screened for medical and psychiatric illness and history of substance abuse and were recruited from three outpatient clinics.\n\ndata(NeuroCog, package=\"heplots\")\nglimpse(NeuroCog)\n#&gt; Rows: 242\n#&gt; Columns: 10\n#&gt; $ Dx        &lt;fct&gt; Schizophrenia, Schizophrenia, Schizophrenia, Sch…\n#&gt; $ Speed     &lt;int&gt; 19, 8, 14, 7, 21, 31, -1, 17, 7, 37, 30, 26, 32,…\n#&gt; $ Attention &lt;int&gt; 9, 25, 23, 18, 9, 10, 8, 20, 30, 15, 27, 20, 23,…\n#&gt; $ Memory    &lt;int&gt; 19, 15, 15, 14, 35, 26, 3, 27, 26, 17, 28, 22, 2…\n#&gt; $ Verbal    &lt;int&gt; 33, 28, 20, 34, 28, 29, 20, 30, 26, 33, 34, 33, …\n#&gt; $ Visual    &lt;int&gt; 24, 24, 13, 16, 29, 21, 12, 32, 27, 21, 19, 18, …\n#&gt; $ ProbSolv  &lt;int&gt; 39, 40, 32, 31, 45, 33, 29, 29, 30, 33, 30, 39, …\n#&gt; $ SocialCog &lt;int&gt; 28, 37, 24, 36, 28, 28, 28, 44, 39, 24, 32, 36, …\n#&gt; $ Age       &lt;int&gt; 44, 26, 55, 53, 51, 21, 53, 56, 48, 46, 48, 31, …\n#&gt; $ Sex       &lt;fct&gt; Female, Male, Female, Male, Male, Male, Male, Fe…\n\nThe diagnostic classification variable is called Dx in the dataset. To facilitate answering questions regarding group differences, the following contrasts were applied: the first column compares the control group to the average of the diagnosed groups, the second compares the schizophrenia group against the schizoaffective group.\n\ncontrasts(NeuroCog$Dx)\n#&gt;                 [,1] [,2]\n#&gt; Schizophrenia   -0.5    1\n#&gt; Schizoaffective -0.5   -1\n#&gt; Control          1.0    0\n\nIn this analysis, we ignore the SocialCog variable. The primary focus is on the variables Attention : ProbSolv.\n\n14.1.3 A first look\nAs always, plot the data first! We want an overview of the distributions of the variables to see the centers, spread, shape and possible outliers for each group on each variable.\nThe plot below combines the use of boxplots and violin plots to give an informative display. As we saw earlier (e.g., Section 10.5.1), doing this with ggplot2 requires reshaping the data to long format.\n\n# Reshape from wide to long\nNC_long &lt;- NeuroCog |&gt;\n  dplyr::select(-SocialCog, -Age, -Sex) |&gt;\n  tidyr::gather(key = response, value = \"value\", Speed:ProbSolv)\n# view 3 observations per group and measure\nNC_long |&gt;\n  group_by(Dx) |&gt;\n  sample_n(3) |&gt; ungroup()\n#&gt; # A tibble: 9 × 3\n#&gt;   Dx              response  value\n#&gt;   &lt;fct&gt;           &lt;chr&gt;     &lt;int&gt;\n#&gt; 1 Schizophrenia   Speed        39\n#&gt; 2 Schizophrenia   Visual       21\n#&gt; 3 Schizophrenia   Memory       40\n#&gt; 4 Schizoaffective ProbSolv     40\n#&gt; 5 Schizoaffective Speed        25\n#&gt; 6 Schizoaffective Verbal       48\n#&gt; 7 Control         Speed        33\n#&gt; 8 Control         ProbSolv     43\n#&gt; 9 Control         Attention    37\n\nIn the plot, we take care to adjust the transparency (alpha) values for the points, violin plots and boxplots so that all can be seen. Options for geom_boxplot() are used to give these greater visual prominence.\n\nCodeggplot(NC_long, aes(x=Dx, y=value, fill=Dx)) +\n  geom_jitter(shape=16, alpha=0.8, size=1, width=0.2) +\n  geom_violin(alpha = 0.1) +\n  geom_boxplot(width=0.5, alpha=0.4, \n               outlier.alpha=1, outlier.size = 3, outlier.color = \"red\") +\n  scale_x_discrete(labels = c(\"Schizo\", \"SchizAff\", \"Control\")) +\n  facet_wrap(~response, scales = \"free_y\", as.table = FALSE) +\n  theme_bw() +\n  theme(legend.position=\"bottom\",\n        axis.title = element_text(size = rel(1.2)),\n        axis.text  = element_text(face = \"bold\"),\n        strip.text = element_text(size = rel(1.2)))\n\n\n\n\n\n\nFigure 14.1: Boxplots and violin plots of the NeuroCog data.\n\n\n\n\nWe can see that the control participants score higher on all measures, but there is no consistent pattern of medians for the two patient groups. But these univariate summaries do not inform about the relations among variables.\n\n14.1.4 Bivariate views\nCorrgram\nA corrgram (Friendly, 2002) provides a useful reconnaisance plot of the bivariate correlations in the dataset. It suppresses details, and allows focus on the overall pattern. The corrgram::corrgram() function has the ability to enhance perception by permuting the variables in the order of their variable vectors in a biplot, so more highly correlated variables are adjacent in the plot, and example of effect ordering for data displays (Friendly & Kwan, 2003).\nThe plot below includes all variables except for Dx group. There are a number of panel.* functions for choosing how the correlation for each pair is rendered.\n\nNeuroCog |&gt;\n  select(-Dx) |&gt;\n  corrgram(order = TRUE,\n           diag.panel = panel.density,\n           upper.panel = panel.pie)\n\n\n\n\n\n\nFigure 14.2: corrgram of the NeuroCog data. The upper and lower triangles use two different ways of encoding the value of the correlation for each pair of variables.\n\n\n\n\nIn this plot you can see that adjacent variables are more highly correlated than those more widely separated. The diagonal panels show that most variables are reasonably symmetric in their distributions. Age, not included in this analysis is negatively correlated with the others: older participants tend to do less well on these tests.\nScatterplot matrix\nA scatterplot matrix gives a more detailed overview of all pairwise relations. The plot below suppresses the data points and summarizes the relation using data ellipses and regression lines. The model syntax, ~ Speed + ... |Dx, treats Dx as a conditioning variable (similar to the use of the color aestheic in ggplot2) giving a separate data ellipse and regression line for each group. (The legend is suppressed here. The groups are Schizophrenic, SchizoAffective, Normal.)\n\nscatterplotMatrix(~ Speed + Attention + Memory + Verbal + Visual + ProbSolv | Dx,\n  data=NeuroCog,\n  plot.points = FALSE,\n  smooth = FALSE,\n  legend = FALSE,\n  col = scales::hue_pal()(3),\n  ellipse=list(levels=0.68))\n\n\n\n\n\n\nFigure 14.3: Scatterplot matrix of the NeuroCog data. Points are suppressed here, focusing on the data ellipses and regression lines. Colors for the groups: Schizophrenic (red), SchizoAffective (green), Normal (blue)\n\n\n\n\nIn this figure, we can see that the regression lines have similar slopes and similar data ellipses for the groups, though with a few exceptions.\nTODO: Should we add biplot here?",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "14-case-studies.html#fitting-the-mlm",
    "href": "14-case-studies.html#fitting-the-mlm",
    "title": "\n14  Case studies\n",
    "section": "\n14.2 Fitting the MLM",
    "text": "14.2 Fitting the MLM\nWe proceed to fit the one-way MANOVA model.\n\nNC.mlm &lt;- lm(cbind(Speed, Attention, Memory, Verbal, Visual, ProbSolv) ~ Dx,\n             data=NeuroCog)\nAnova(NC.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Dx  2     0.299     6.89     12    470 1.6e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe first research question is captured by the contrasts for the Dx factor shown above. We can test these with car::linearHypothesis(). The contrast Dx1 for control vs. the diagnosed groups is highly significant,\n\n# control vs. patients\nprint(linearHypothesis(NC.mlm, \"Dx1\"), SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Pillai            1     0.289     15.9      6    234 2.8e-15 ***\n#&gt; Wilks             1     0.711     15.9      6    234 2.8e-15 ***\n#&gt; Hotelling-Lawley  1     0.407     15.9      6    234 2.8e-15 ***\n#&gt; Roy               1     0.407     15.9      6    234 2.8e-15 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nbut the second contrast, Dx2, comparing the schizophrenic and schizoaffective group, is not.\n\n# Schizo vs SchizAff\nprint(linearHypothesis(NC.mlm, \"Dx2\"), SSP=FALSE)\n#&gt; \n#&gt; Multivariate Tests: \n#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)\n#&gt; Pillai            1     0.006    0.249      6    234   0.96\n#&gt; Wilks             1     0.994    0.249      6    234   0.96\n#&gt; Hotelling-Lawley  1     0.006    0.249      6    234   0.96\n#&gt; Roy               1     0.006    0.249      6    234   0.96\n\nAs a quick check on the model, a \\(\\chi^2\\) QQ plot (Figure 14.4) reveals no problems with multivariate normality of residuals nor potentially harmful residuals\n\ncqplot(NC.mlm, id.n = 3)\n\n\n\n\n\n\nFigure 14.4\n\n\n\n\n\n14.2.1 HE plot\nSo the question becomes: how to understand these results.heplot() shows the visualization of the multivariate model in the space of two response variables (the first two by default). The result (Figure 14.5) tells a very simple story: The control group performs higher on higher measures than the diagnosed groups, which do not differ between themselves.\n(For technical reasons, to abbreviate the group labels in the plot, we need to update() the MLM model after the labels are reassigned.)\n\n# abbreviate levels for plots\nNeuroCog$Dx &lt;- factor(NeuroCog$Dx, \n                      labels = c(\"Schiz\", \"SchAff\", \"Contr\"))\nNC.mlm &lt;- update(NC.mlm)\n\nThen, feed the model to heplot() for a plot of the first two response variables, Speed and Attention. ::: {.cell layout-align=“center”}\nheplot(NC.mlm, \n       fill=TRUE, fill.alpha=0.1,\n       cex.lab=1.3, cex=1.25)\npar(op)\n\n\n\n\n\n\nFigure 14.5: HE plot of Speed and Attention in the MLM for the NeuroCog data. The labeled points show the means of the groups on the two variables. The blue H ellipse for groups indicates the strong positive correlation of the group means.\n\n\n\n:::\nThis pattern, of the control group higher than the others, is consistent across all of the response variables, as we see from a plot of pairs(NC.mlm):\n\npairs(NC.mlm, \n      fill=TRUE, fill.alpha=0.1,\n      var.cex=2)\n\n\n\n\n\n\nFigure 14.6: HE plot matrix of the MLM for NeuroCog data.\n\n\n\n\nIt signals that we are likely to see a simpler representation of the data in canonical space.\n\n14.2.2 Canonical space\nWe can gain further insight, and a simplified plot showing all the response variables by projecting the MANOVA into the canonical space, which is entirely 2-dimensional (because \\(df_h=2\\)). However, the output from candisc() shows that 98.5% of the mean differences among groups can be accounted for in just one canonical dimension. ::: {.cell layout-align=“center”}\nNC.can &lt;- candisc(NC.mlm)\nNC.can\n#&gt; \n#&gt; Canonical Discriminant Analysis for Dx:\n#&gt; \n#&gt;    CanRsq Eigenvalue Difference Percent Cumulative\n#&gt; 1 0.29295    0.41433      0.408    98.5       98.5\n#&gt; 2 0.00625    0.00629      0.408     1.5      100.0\n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;   LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1        0.703     7.53    12   468   9e-13 ***\n#&gt; 2        0.994     0.30     5   235    0.91    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n:::\nFigure 14.7 is the result of the plot() method for class \"candisc\" objects, that is, the result of calling plot(NC.can, ...). It plots the two canonical scores, \\(\\mathbf{Z}_{n \\times 2}\\) for the subjects, together with data ellipses for each of the three groups.\n\npos &lt;- c(4, 1, 4, 4, 1, 3)\ncol &lt;- c(\"red\", \"darkgreen\", \"blue\")\nplot(NC.can, \n     ellipse=TRUE, \n     rev.axes=c(TRUE,FALSE), \n     pch=c(7,9,10),\n     var.cex=1.2, cex.lab=1.5, var.lwd=2,  scale=4.5, \n     col=col,\n     var.col=\"black\", var.pos=pos,\n     prefix=\"Canonical dimension \")\n\n\n\n\n\n\nFigure 14.7: Canonical discriminant plot for the NeuroCog data MANOVA. Scores on the two canonical dimensions are plotted, together with 68% data ellipses for each group.\n\n\n\n\nThe interpretation of Figure 14.7 is again fairly straightforward. As noted earlier (Section 11.7), the projections of the variable vectors in this plot on the coordinate axes are proportional to the correlations of the responses with the canonical scores. From this, we see that the normal group differs from the two patient groups, having higher scores on all the neurocognitive variables, most of which are highyl correlated. The problem solving measure is slightly different, and this, compared to the cluster of memory, verbal and attention, is what distinguishes the schizophrenic group from the schizoaffectives.\nThe separation of the groups is essentially one-dimensional, with the control group higher on all measures. Moreover, the variables processing speed and visual memory are the purest measures of this dimension, but all variables contribute positively. The second canonical dimension accounts for only 1.5% of group mean differences and is non-significant (by a likelihood ratio test). Yet, if we were to interpret it, we would note that the schizophrenia group is slightly higher on this dimension, scoring better in problem solving and slightly worse on working memory, attention, and verbal learning tasks.\nSummary\nThis analysis gives a very simple description of the data, in relation to the research questions posed earlier:\n\nOn the basis of these neurocognitive tests, the schizophrenic and schizoaffective groups do not differ significantly overall, but these groups differ greatly from the normal controls.\nAll cognitive domains distinguish the groups in the same direction, with the greatest differences shown for the variables most closely aligned with the horizontal axis in Figure 14.7.",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "14-case-studies.html#social-cognitive-measures",
    "href": "14-case-studies.html#social-cognitive-measures",
    "title": "\n14  Case studies\n",
    "section": "\n14.3 Social cognitive measures",
    "text": "14.3 Social cognitive measures\nThe social cognitive measures were designed to tap various aspects of the perception and cognitive processing of emotions of others. Emotion perception was assessed using a Managing Emotions score from the MCCB. A “theory of mind” (ToM) score assessed ability to read the emotions of others from photographs of the eye region of male and female faces. Two other measures, externalizing bias (ExtBias) and personalizing bias (PersBias) were calculated from a scale measuring the degree to which individuals attribute internal, personal or situational causal attributions to positive and negative social events.\nThe analysis of the SocialCog data proceeds in a similar way: first we fit the MANOVA model, then test the overall differences among groups using Anova(). We find that the overall multivariate test is again significant,\n\ndata(SocialCog, package=\"heplots\")\nSC.mlm &lt;-  lm(cbind(MgeEmotions,ToM, ExtBias, PersBias) ~ Dx,\n               data=SocialCog)\nAnova(SC.mlm)\n#&gt; \n#&gt; Type II MANOVA Tests: Pillai test statistic\n#&gt;    Df test stat approx F num Df den Df  Pr(&gt;F)    \n#&gt; Dx  2     0.212     3.97      8    268 0.00018 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTesting the same two contrasts using linearHypothesis() (results not shown), w e find that the overall multivariate test is again significant, but now both contrasts are significant (Dx1: \\(F(4, 133)=5.21, p &lt; 0.001\\); Dx2: \\(F(4, 133)=2.49, p = 0.0461\\)), the test for Dx2 just barely.\n\n# control vs. patients\nprint(linearHypothesis(SC.mlm, \"Dx1\"), SSP=FALSE)\n# Schizo vs. SchizAff\nprint(linearHypothesis(SC.mlm, \"Dx2\"), SSP=FALSE)\n\nThese results are important, because, if they are reliable and make sense substantively, they imply that patients with schizophrenia and schizoaffective diagnoses can be distinguished by their performance on tasks assessing social perception and cognition. This was potentially a new finding in the literature on schizophrenia.\nAs we did above, it is useful to visualize the nature of these differences among groups with HE plots for the SC.mlm model. Each contrast has a corresponding \\(\\mathbf{H}\\) ellipse, which we can show in the plot using the hypotheses argument. With a single degree of freedom, these degenerate ellipses plot as lines.\n\nheplot(SC.mlm, \n       hypotheses=list(\"Dx1\"=\"Dx1\", \"Dx2\"=\"Dx2\"),\n       fill=TRUE, fill.alpha=.1,\n       cex.lab=1.5, cex=1.2)\n\n\n\n\n\n\nFigure 14.8: HE plot of Speed and Attention in the MLM for the SocialCog data. The labeled points show the means of the groups on the two variables. The lines for Dx1 and Dx2 show the tests of the contrasts among groups.\n\n\n\n\nIt can be seen that the three group means are approximately equally spaced on the ToM measure, whereas for MgeEmotions, the control and schizoaffective groups are quite similar, and both are higher than the schizophrenic group. This ordering of the three groups was somewhat similar for the other responses, as we could see in a pairs(SC.mlm) plot.\n\n14.3.1 Model checking\nNormally, we would continue this analysis, and consider other HE and canonical discriminant plots to further interpret the results, in particular the relations of the cognitive measures to group differences, or perhaps an analysis of the relationships between the neuro- and social-cognitive measures. We don’t pursue this here for reasons of length, but this example actually has a more important lesson to demonstrate.\nBefore beginning the MANOVA analyses, extensive data screening was done by the client using SPSS, in which all the response and predictor variables were checked for univariate normality and multivariate normality (MVN) for both sets. This traditional approach yielded a huge amount of tabular output and no graphs, and did not indicate any major violation of assumptions.1\nA simple visual test of MVN and the possible presence of multivariate outliers is related to the theory of the data ellipse: Under MVN, the squared Mahalanobis distances \\(D^2_M (\\mathbf{y}) = (\\mathbf{y} - \\bar{\\mathbf{y}})' \\, \\mathbf{S}^{-1} \\, (\\mathbf{y} - \\bar{\\mathbf{y}})\\) should follow a \\(\\chi^2_p\\) distribution. Thus, a quantile-quantile plot of the ordered \\(D^2_M\\) values vs. corresponding quantiles of the \\(\\chi^2\\) distribution should approximate a straight line (Cox, 1968; Healy, 1968). Note that this should be applied to the residuals from the model – residuals(SC.mlm) – and not to the response variables directly.\nheplots::cqplot() implements this for \"mlm\" objects Calling this function for the model SC.mlm produces Figure 14.9. It is immediately apparent that there is one extreme multivariate outlier; three other points are identified, but the remaining observations are nearly within the 95% confidence envelope (using a robust MVE estimate of \\(\\mathbf{S}\\)).\n\ncqplot(SC.mlm, method=\"mve\", \n       id.n=4, \n       main=\"\", \n       cex.lab=1.25)\n\n\n\n\n\n\nFigure 14.9: Chi-square quantile-quantile plot for residuals from the model SC.mlm. The confidence band gives a point-wise 95% envelope, providing information about uncertainty. One extreme multivariate outlier is highlighted.\n\n\n\n\nFurther checking revealed that this was a data entry error where one case (15) in the schizophrenia group had a score of -33 recorded on the ExtBias measure, whose valid range was (-10, +10). In R, it is very easy to re-fit a model to a subset of observations (rather than modifying the dataset itself) using update(). The result of the overall Anova and the test of Dx1 were unchanged; however, the multivariate test for the most interesting contrast Dx2 comparing the schizophrenia and schizoaffective groups became non-significant at the \\(\\alpha=0.05\\) level (\\(F(4, 133)=2.18, p = 0.0742\\)).\n\nSC.mlm1 &lt;- update(SC.mlm, \n                  subset=rownames(SocialCog)!=\"15\")\n\nAnova(SC.mlm1)\nprint(linearHypothesis(SC.mlm1, \"Dx1\"), SSP=FALSE)\nprint(linearHypothesis(SC.mlm1, \"Dx2\"), SSP=FALSE)\n\n\n14.3.2 Canonical HE plot\nThis outcome creates a bit of a quandry for further analysis (do univariate follow-up tests? try a robust model?) and reporting (what to claim about the Dx2 contrast?) that we don’t explore here. Rather, we proceed to attempt to interpret the MLM with the aid of canonical analysis and a canonical HE plot. The canonical analysis of the model SC.mlm1 now shows that both canonical dimensions are significant, and account for 83.9% and 16.1% of between group mean differences respectively.\n\nSC.can1 &lt;- candisc(SC.mlm1)\nSC.can1\n#&gt; \n#&gt; Canonical Discriminant Analysis for Dx:\n#&gt; \n#&gt;   CanRsq Eigenvalue Difference Percent Cumulative\n#&gt; 1 0.1645     0.1969      0.159    83.9       83.9\n#&gt; 2 0.0364     0.0378      0.159    16.1      100.0\n#&gt; \n#&gt; Test of H0: The canonical correlations in the \n#&gt; current row and all that follow are zero\n#&gt; \n#&gt;   LR test stat approx F numDF denDF Pr(&gt; F)    \n#&gt; 1        0.805     3.78     8   264 0.00032 ***\n#&gt; 2        0.964     1.68     3   133 0.17537    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nop &lt;- par(mar=c(5,4,1,1)+.1)\nheplot(SC.can1, \n  fill=TRUE, fill.alpha=.1,\n  hypotheses=list(\"Dx1\"=\"Dx1\", \"Dx2\"=\"Dx2\"),\n  lwd = c(1, 2, 3, 3),\n  col=c(\"red\", \"blue\", \"darkgreen\", \"darkgreen\"),\n  var.lwd=2, \n  var.col=\"black\", \n  label.pos=c(3,1), \n  var.cex=1.2, \n  cex=1.25, cex.lab=1.2, \n  scale=2.8,\n  prefix=\"Canonical dimension \")\npar(op)\n\n\n\n\n\n\nFigure 14.10: Canonical HE plot for the corrected SocialCog MANOVA. The variable vectors show the correlations of the responses with the canonical variables. The embedded green lines show the projections of the H ellipses for the contrasts Dx1 and Dx2 in canonical space.\n\n\n\n\nThe HE plot version of this canonical plot is shown in Figure 14.10. Because the heplot() method for a \"candisc\" object refits the original model to the \\(\\mathbf{Z}\\) canonical scores, it is easy to also project other linear hypotheses into this space. Note that in this view, both the Dx1 and Dx2 contrasts project outside \\(\\mathbf{E}\\) ellipse.2.\nThis canonical HE plot has a very simple description:\n\nDimension 1 orders the groups from control to schizoaffective to schizophrenia, while dimension 2 separates the schizoaffective group from the others;\nExternalizing bias and theory of mind contributes most to the first dimension, while personal bias and managing emotions are more aligned with the second; and,\nThe relations of the two contrasts to group differences and to the response variables can be easily read from this plot.\n\n\n#cat(\"Packages used here:\\n\")\nwrite_pkgs(file = .pkg_file)\n#&gt; 10  packages used here:\n#&gt;  broom, candisc, car, carData, corrgram, dplyr, ggplot2, heplots, knitr, tidyr\n\n\n\n\n\n\nCox, D. R. (1968). Notes on some aspects of regression analysis. Journal of the Royal Statistical Society Series A, 131, 265–279.\n\n\nFriendly, M. (2002). Corrgrams: Exploratory displays for correlation matrices. The American Statistician, 56(4), 316–324. https://doi.org/10.1198/000313002533\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays. Computational Statistics and Data Analysis, 43(4), 509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nHartman, L. I. (2016). Schizophrenia and schizoaffective disorder: One condition or two? [PhD dissertation]. York University.\n\n\nHealy, M. J. R. (1968). Multivariate normal plotting. Journal of the Royal Statistical Society Series C, 17(2), 157–161.\n\n\nHeinrichs, R. W., Pinnock, F., Muharib, E., Hartman, L., Goldberg, J., & McDermid Vaz, S. (2015). Neurocognitive normality in schizophrenia revisited. Schizophrenia Research: Cognition, 2(4), 227–232. https://doi.org/10.1016/j.scog.2015.09.001\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis with applications. Biometrika, 57(3), 519–530. https://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMardia, K. V. (1974). Applications of some measures of multivariate skewness and kurtosis in testing normality and robustness studies. Sankhya: The Indian Journal of Statistics, Series B, 36(2), 115–128. http://www.jstor.org/stable/25051892",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "14-case-studies.html#footnotes",
    "href": "14-case-studies.html#footnotes",
    "title": "\n14  Case studies\n",
    "section": "",
    "text": "Actually, multivariate normality of the predictors in \\(\\mathbf{X}\\) is not required in the MLM. This assumption applies only to the conditional values \\(\\mathbf{Y} \\;|\\; \\mathbf{X}\\), i.e., that the errors \\(\\mathbf{\\epsilon}_{i}' \\sim \\mathcal{N}_{p}(\\mathbf{0},\\boldsymbol{\\Sigma})\\) with constant covariance matrix. Moreover, the widely used MVN test statistics, such as Mardia’s (1970) test based on multivariate skewness and kurtosis are known to be quite sensitive to mild departures in kurtosis (Mardia, 1974) which do not threaten the validity of the multivariate tests.↩︎\nThe direct application of significance tests to canonical scores probably requires some adjustment because these are computed to have the optimal between-group discrimination.↩︎",
    "crumbs": [
      "Multivariate Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "91-colophon.html",
    "href": "91-colophon.html",
    "title": "Colophon",
    "section": "",
    "text": "Package versions\nThis book was produced using R version 4.4.1 (2024-06-14). Fundamental to this was the framework for reproducible documents provided by Yihui Xie’s knitr package.\nQuarto was used to compile and render the book in HTML and PDF formats. [** Don’t really need all this**]\nThe principal R package versions used in examples and illustrations are listed below. These were captured via sessioninfo:::package_info() from all library() commands in the text, and scripts which also updated the references to packages.\nAt the time of writing, most of these were current on CRAN repositories but some development versions are indicated as “local” in the source column.\npackage\nversion\ndate\nsource\n\n\n\nbayestestR\n0.15.0\n2024-10-17\nCRAN\n\n\nbroom\n1.0.7\n2024-09-26\nCRAN\n\n\ncandisc\n0.9.0\n2024-05-06\nCRAN\n\n\ncar\n3.1-3\n2024-09-27\nCRAN\n\n\ncarData\n3.0-5\n2022-01-06\nCRAN\n\n\ncorpcor\n1.6.10\n2021-09-16\nCRAN\n\n\ncorrelation\n0.8.5\n2024-06-16\nCRAN\n\n\ncorrgram\n1.14\n2021-04-29\nCRAN\n\n\ncorrplot\n0.95\n2024-10-14\nCRAN\n\n\ndatawizard\n0.13.0\n2024-10-05\nCRAN\n\n\ndplyr\n1.1.4\n2023-11-17\nCRAN\n\n\neasystats\n0.7.3\n2024-07-22\nCRAN\n\n\neffects\n4.2-2\n2022-07-13\nCRAN\n\n\neffectsize\n0.8.9\n2024-07-03\nCRAN\n\n\nfactoextra\n1.0.7\n2020-04-01\nCRAN\n\n\nFactoMineR\n2.11\n2024-04-20\nCRAN\n\n\nforcats\n1.0.0\n2023-01-29\nCRAN\n\n\ngenridge\n0.7.0\n2023-08-08\nCRAN\n\n\nGGally\n2.2.1\n2024-02-14\nCRAN\n\n\ngganimate\n1.0.9\n2024-02-27\nCRAN\n\n\nggbiplot\n0.6.2\n2024-01-08\nCRAN\n\n\nggdensity\n1.0.0\n2023-02-09\nCRAN\n\n\nggeffects\n1.7.2\n2024-10-13\nCRAN\n\n\nggpcp\n0.2.0\n2022-11-28\nCRAN\n\n\nggplot2\n3.5.1\n2024-04-23\nCRAN\n\n\nggpubr\n0.6.0\n2023-02-10\nCRAN\n\n\nggrepel\n0.9.6\n2024-09-07\nCRAN\n\n\nggstats\n0.7.0\n2024-09-22\nCRAN\n\n\nheplots\n1.7.0\n2024-05-02\nCRAN\n\n\nHotelling\n1.0-8\n2021-09-09\nCRAN\n\n\nimager\n1.0.2\n2024-05-13\nCRAN\n\n\ninsight\n0.20.5\n2024-10-02\nCRAN\n\n\nknitr\n1.48\n2024-07-07\nCRAN\n\n\nlubridate\n1.9.3\n2023-09-27\nCRAN\n\n\nmagrittr\n2.0.3\n2022-03-30\nCRAN\n\n\nmarginaleffects\n0.23.0\n2024-10-05\nCRAN\n\n\nMASS\n7.3-60.2\n2024-04-26\nCRAN\n\n\nmatlib\n1.0.1\n2024-10-24\nhttps://friendly.r-universe.dev\n\n\nmodelbased\n0.8.8\n2024-06-11\nCRAN\n\n\nmodelsummary\n2.2.0\n2024-09-02\nCRAN\n\n\nparameters\n0.23.0\n2024-10-18\nCRAN\n\n\npatchwork\n1.3.0\n2024-09-16\nCRAN\n\n\nperformance\n0.12.4\n2024-10-18\nCRAN\n\n\npurrr\n1.0.2\n2023-08-10\nCRAN\n\n\nreadr\n2.1.5\n2024-01-10\nCRAN\n\n\nreport\n0.5.9\n2024-07-10\nCRAN\n\n\nRtsne\n0.17\n2023-12-07\nCRAN\n\n\nsee\n0.9.0\n2024-09-06\nCRAN\n\n\nstringr\n1.5.1\n2023-11-14\nCRAN\n\n\ntibble\n3.2.1\n2023-03-20\nCRAN\n\n\ntidyr\n1.3.1\n2024-01-24\nCRAN\n\n\ntidyverse\n2.0.0\n2023-02-22\nCRAN\n\n\ntourr\n1.2.0\n2024-04-20\nCRAN\n\n\nvcd\n1.4-13\n2024-09-16\nCRAN\n\n\nVisCollin\n0.1.2\n2023-09-05\nCRAN",
    "crumbs": [
      "End matter",
      "Colophon"
    ]
  },
  {
    "objectID": "95-references.html",
    "href": "95-references.html",
    "title": "References",
    "section": "",
    "text": "Abbott, E. A. (1884). Flatland: A romance of many dimensions.\nBuccaneer Books.\n\n\nAdler, D., & Murdoch, D. (2023). Rgl: 3D visualization using\nOpenGL. https://CRAN.R-project.org/package=rgl\n\n\nAluja, T., Morineau, A., & Sanchez, G. (2018). Principal\ncomponent analysis for data science. https://pca4ds.github.io/\n\n\nAnderson, E. (1935). The irises of the Gaspé peninsula.\nBulletin of the American Iris Society, 35, 2–5.\n\n\nAndrews, D. F. (1972). Plots of high dimensional data.\nBiometrics, 28, 123–136.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. The American\nStatistician, 27, 17–21.\n\n\nArel-Bundock, V. (2025a). Marginaleffects: Predictions, comparisons,\nslopes, marginal means, and hypothesis tests. https://marginaleffects.com/\n\n\nArel-Bundock, V. (2025b). Modelsummary: Summary tables and plots for\nstatistical models and data: Beautiful, customizable, and\npublication-ready. https://modelsummary.com\n\n\nAsimov, D. (1985). Grand tour. SIAM Journal of Scientific and\nStatistical Computing, 6(1), 128–143.\n\n\nBarab’asi, A.-L. (2016). Network science. Cambridge University\nPress.\n\n\nBarrett, B. E. (2003). Understanding influence in multivariate\nregression. Communications in Statistics - Theory and Methods,\n32(3), 667–680. https://doi.org/10.1081/STA-120018557\n\n\nBarrett, B. E., & Ling, R. F. (1992). General classes of influence\nmeasures for multivariate regression. Journal of the American\nStatistical Association, 87(417), 184–191. https://www.jstor.org/stable/i314301\n\n\nBartlett, M. S. (1937). Properties of sufficiency and statistical tests.\nProceedings of the Royal Society of London. Series A,\n160(901), 268–282. https://doi.org/10.2307/96803\n\n\nBartlett, M. S. (1938). Further aspects of the theory of multiple\nregression. Mathematical Proceedings of the Cambridge Philosophical\nSociety, 34(1), 33–40. https://doi.org/10.1017/s0305004100019897\n\n\nBashaw, W. L., & Findley, W. G. (Eds.). (1967). Symposium on\ngeneral linear model approach to the analysis of experimental data in\neducational research, final report. https://files.eric.ed.gov/fulltext/ED026737.pdf\n\n\nBecker, R. A., Cleveland, W. S., & Shyu, M.-J. (1996). The visual\ndesign and control of trellis display. Journal of Computational and\nGraphical Statistics, 5(2), 123–155.\n\n\nBelsley, D. A. (1991). Conditioning diagnostics: Collinearity and\nweak data in regression. Wiley.\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression\ndiagnostics: Identifying influential data and sources of\ncollinearity. John Wiley; Sons.\n\n\nBiecek, P., Baniecki, H., Krzyzinski, M., & Cook, D. (2023).\nPerformance is not enough: A story of the rashomon’s quartet.\nhttps://arxiv.org/abs/2302.13356\n\n\nBlack, C., Southwell, C., Emmerson, L., Lunn, D., & Hart, T. (2018).\nTime-lapse imagery of adélie penguins reveals differential winter\nstrategies and breeding site occupation. PLOS ONE,\n13(3), e0193532. https://doi.org/10.1371/journal.pone.0193532\n\n\nBlishen, B., Carroll, W., & Moore, C. (1987). The 1981 socioeconomic\nindex for occupations in canada. Canadian Review of Sociology/Revue\nCanadienne de Sociologie, 24(4), 465–488. https://doi.org/10.1111/j.1755-618x.1987.tb00639.x\n\n\nBock, R. D. (1963). Programming univariate and multivariate analysis of\nvariance. Technometrics, 5(1), 95–117. https://doi.org/10.1080/00401706.1963.10490061\n\n\nBock, R. D. (1964). A computer program forunivariate and multivariate\nanalysis of variance. Proceedings of Scientific Symposium on\nStatistics.\n\n\nBodmer, W., Bailey, R. A., Charlesworth, B., Eyre-Walker, A., Farewell,\nV., Mead, A., & Senn, S. (2021). The outstanding scientist, r.a.\nFisher: His views on eugenics and race. Heredity,\n126(4), 565–576. https://doi.org/10.1038/s41437-020-00394-6\n\n\nBondy, J. A., & Murty, U. S. R. (2008). Graph theory.\nSpringer.\n\n\nBorg, I., & Groenen, P. J. F. (2005). Modern Multidimensional Scaling: Theory and\nApplications. Springer.\n\n\nBorg, I., Groenen, P. J. F., & Mair, P. (2018). Applied\nmultidimensional scaling and unfolding. In SpringerBriefs in\nStatistics. Springer International Publishing. https://doi.org/10.1007/978-3-319-73471-2\n\n\nBox, G. E. P. (1949). A general distribution theory for a class of\nlikelihood criteria. Biometrika, 36(3-4), 317–346. https://doi.org/10.1093/biomet/36.3-4.317\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and\nwear curves. Biometrics, 6, 362–389.\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances.\nBiometrika, 40(3/4), 318–335. https://doi.org/10.2307/2333350\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of\nvariances. Journal of the American Statistical Association,\n69(346), 364–367. https://doi.org/10.1080/01621459.1974.10482955\n\n\nBrown, P. J., & Zidek, J. V. (1980). Adaptive multivariate ridge\nregression. The Annals of Statistics, 8(1), 64–74. http://www.jstor.org/stable/2240743\n\n\nBuja, A., Cook, D., Asimov, D., & Hurley, C. (2005). Computational\nmethods for high-dimensional rotations in data visualization. In J. S.\nCR Rao EJ Wegman (Ed.), Handbook of statistics (pp. 391–413).\nElsevier. https://doi.org/10.1016/s0169-7161(04)24014-7\n\n\ncagne, M. (1885). Coordonnées parallèles\net axiales: Méthode de transformation\ngéométrique et\nprocédé nouveau de calcul graphique\ndéduits de la considération des\ncoordonnées parallèlles.\nGauthier-Villars. http://historical.library.cornell.edu/cgi-bin/cul.math/docviewer?did=00620001&seq=3\n\n\nCai, T. T., Liang, T., & Zhou, H. H. (2015). Law of log determinant\nof sample covariance matrix and optimal estimation of differential\nentropy for high-dimensional gaussian distributions. Journal of\nMultivariate Analysis, 137, 161–172. https://doi.org/https://doi.org/10.1016/j.jmva.2015.02.003\n\n\nCajori, F. (1926). Origins of fourth dimension concepts. The\nAmerican Mathematical Monthly, 33(8), 397–406. https://doi.org/10.1080/00029890.1926.11986607\n\n\nCattell, R. B. (1966). The scree test for the number of factors.\nMultivariate Behavioral Research, 1(2), 245–276. https://doi.org/10.1207/s15327906mbr0102_10\n\n\nChambers, J. M., Cleveland, W. S., Kleiner, B., & Tukey, P. A.\n(1983). Graphical methods for data analysis. Wadsworth.\n\n\nChambers, J. M., & Hastie, T. J. (1991). Statistical models in\ns (p. 624). Chapman & Hall/CRC.\n\n\nCharnes, A., Cooper, W. W., & Rhodes, E. (1981). Evaluating program\nand managerial efficiency: An application of data envelopment analysis\nto program follow through. Management Science, 27(6),\n668–697. http://www.jstor.org/stable/2631155\n\n\nCleveland, W. S. (1979). Robust locally weighted regression and\nsmoothing scatterplots. Journal of the American Statistical\nAssociation, 74, 829–836.\n\n\nCleveland, W. S. (1985). The elements of graphing data.\nWadsworth Advanced Books.\n\n\nCleveland, W. S., & Devlin, S. J. (1988). Locally weighted\nregression: An approach to regression analysis by local fitting.\nJournal of the American Statistical Association, 83,\n596–610.\n\n\nCleveland, W. S., & McGill, R. (1984). Graphical perception: Theory,\nexperimentation and application to the development of graphical methods.\nJournal of the American Statistical Association, 79,\n531–554.\n\n\nCleveland, W. S., & McGill, R. (1985). Graphical perception and\ngraphical methods for analyzing scientific data. Science,\n229, 828–833.\n\n\nClyde, D. J., Cramer, E. M., & Sherin, R. J. (1966).\nMultivariate statistical programs. Biometric\nLaboratory,University of Miami.\n\n\nCochran, W. G. (1941). The distribution of the largest of a set of\nestimated variances as a fraction of their total. Annals of\nEugenics, 11(1), 47–52. https://doi.org/10.1111/j.1469-1809.1941.tb02271.x\n\n\nConover, W. J., Johnson, M. E., & Johnson, M. M. (1981). A\ncomparative study of tests for homogeneity of variances, with\napplications to the outer continental shelf bidding data.\nTechnometrics, 23(4), 351–361. https://doi.org/10.1080/00401706.1981.10487680\n\n\nCook, D., Buja, A., Cabrera, J., & Hurley, C. (1995). Grand tour and\nprojection pursuit. Journal of Computational and Graphical\nStatistics, 4(3), 155. https://doi.org/10.2307/1390844\n\n\nCook, D., Buja, A., Lee, E.-K., & Wickham, H. (2008). Grand tours,\nprojection pursuit guided tours, and manual controls. In Handbook of\ndata visualization (pp. 295–314). Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-33037-0_13\n\n\nCook, D., & Laa, U. (2024). Interactively exploring\nhigh-dimensional data and models in R. Online. https://dicook.github.io/mulgar_book/\n\n\nCook, D., & Swayne, D. F. (2007). Interactive and dynamic\ngraphics for data analysis : With R and\nGGobi. Springer. http://www.ggobi.org/book/\n\n\nCook, R. D. (1977). Detection of influential observation in linear\nregression. Technometrics, 19(1), 15–18. http://links.jstor.org/sici?sici=0040-1706%28197702%2919%3A1%3C15%3ADOIOIL%3E2.0.CO%3B2-8\n\n\nCook, R. D. (1993). Exploring partial residual plots.\nTechnometrics, 35(4), 351–362.\n\n\nCook, R. D. (1996). Added-variable plots and curvature in linear\nregression. Technometrics, 38(3), 275–278. https://doi.org/10.1080/00401706.1996.10484507\n\n\nCook, R. D., & Weisberg, S. (1982). Residuals and influence in\nregression. Chapman; Hall.\n\n\nCook, R. D., & Weisberg, S. (1994). ARES plots for generalized\nlinear models. Computational Statistics & Data Analysis,\n17(3), 303–315. https://doi.org/10.1016/0167-9473(92)00075-3\n\n\nCostantini, G., Epskamp, S., Borsboom, D., Perugini, M., Mõttus, R.,\nWaldorp, L. J., & Cramer, A. O. J. (2015). State of the aRt personality research: A tutorial on network\nanalysis of personality data in R. Journal of Research\nin Personality, 54, 13–29. https://doi.org/10.1016/j.jrp.2014.07.003\n\n\nCotton, R. (2013). Learning R. O’Reilly Media.\n\n\nCox, D. R. (1968). Notes on some aspects of regression analysis.\nJournal of the Royal Statistical Society Series A,\n131, 265–279.\n\n\nCsárdi, G., Nepusz, T., Traag, V., Horvát, S., Zanini, F., Noom, D.,\n& Müller, K. (2024). igraph: Network\nanalysis and visualization in r. https://doi.org/10.5281/zenodo.7682609\n\n\nCurran, J., & Hersh, T. (2021). Hotelling: Hotelling’s t^2 test\nand variants. https://CRAN.R-project.org/package=Hotelling\n\n\nDavies, R., Locke, S., & D’Agostino McGowan, L. (2022).\ndatasauRus: Datasets from the datasaurus dozen. https://CRAN.R-project.org/package=datasauRus\n\n\nDavis, C. (1990). Body image and weight preoccupation: A comparison\nbetween exercising and non-exercising women. Appetite,\n16(1), 84. https://doi.org/10.1016/0195-6663(91)90115-9\n\n\nDempster, A. P. (1969). Elements of continuous multivariate\nanalysis. Addison-Wesley.\n\n\nDempster, A. P. (1972). Covariance selection. Biometrics,\n28(1), 157–175.\n\n\nDixon, W. J. (1965). BMD biomedical computer programs. Health\nSciences Computing Facility, School of Medicine, University of\nCalifornia; Health Sciences Computing Faculty.\n\n\nDray, S., & Siberchicot, A. (2025). Adegraphics: An S4\nlattice-based package for the representation of multivariate data.\nhttp://pbil.univ-lyon1.fr/ADE-4/\n\n\nDuncan, O. D. (1961). A socioeconomic index for all occupations. In Jr.\nA. J. Reiss, P. K. H. O. D. Duncan, & C. C. North (Eds.),\nOccupations and social status. The Free Press.\n\n\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least\nangle regression. The Annals of Statistics, 32(2),\n407–499.\n\n\nEmerson, J. W., Green, W. A., Schloerke, B., Crowley, J., Cook, D.,\nHofmann, H., & Wickham, H. (2013). The generalized pairs plot.\nJournal of Computational and Graphical Statistics,\n22(1), 79–91. http://www.tandfonline.com/doi/ref/10.1080/10618600.2012.694762\n\n\nEuler, L. (1758). Elementa doctrinae solidorum. Novi Commentarii\nAcademiae Scientiarum Petropolitanae, 4, 109–140. https://scholarlycommons.pacific.edu/euler-works/230/\n\n\nFarquhar, A. B., & Farquhar, H. (1891). Economic and industrial\ndelusions: A discourse of the case for protection. Putnam.\n\n\nFienberg, S. E. (1971). Randomization and social affairs: The 1970 draft\nlottery. Science, 171, 255–261.\n\n\nFinn, J. D. (1967). MULTIVARIANCE: Fortran program for\nunivariate and multivariate analysis of variance and covariance.\nSchool of Education, State University of New York at Buffalo.\n\n\nFisher, R. A. (1923). Studies in crop variation. II. The manurial\nresponse of different potato varieties. The Journal of Agricultural\nScience, 13(2), 311–320. https://hdl.handle.net/2440/15179\n\n\nFisher, R. A. (1925b). Statistical methods for research\nworkers. Oliver & Boyd.\n\n\nFisher, R. A. (1925a). Statistical methods for research workers\n(6th ed.). Oliver & Boyd.\n\n\nFisher, R. A. (1936). The use of multiple measurements in taxonomic\nproblems. Annals of Eugenics, 7(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x\n\n\nFishkeller, M. A., Friedman, J. H., & Tukey, J. W. (1974).\nPRIM-9, an interactive multidimensional data display and\nanalysis system. Proceedings of the Pacific ACM Regional\nConference.\n\n\nFlury, B., & Riedwyl, H. (1988). Multivariate statistics: A\npractical approach. Chapman & Hall.\n\n\nFox, J. (1987). Effect displays for generalized linear models. In C. C.\nClogg (Ed.), Sociological methodology, 1987 (pp. 347–361).\nJossey-Bass.\n\n\nFox, J. (2003). Effect displays in R for generalized linear\nmodels. Journal of Statistical Software, 8(15), 1–27.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear\nmodels (Third edition.). SAGE.\n\n\nFox, J. (2020). Regression diagnostics (2nd ed.).\nSAGE Publications, Inc. https://doi.org/10.4135/9781071878651\n\n\nFox, J. (2021). A mathematical primer for social statistics\n(2nd ed.). SAGE Publications, Inc. https://doi.org/10.4135/9781071878835\n\n\nFox, J., & Monette, G. (1992). Generalized collinearity diagnostics.\nJournal of the American Statistical Association,\n87(417), 178–183.\n\n\nFox, J., & Weisberg, S. (2018a). An R companion to\napplied regression (Third). SAGE Publications. https://books.google.ca/books?id=uPNrDwAAQBAJ\n\n\nFox, J., & Weisberg, S. (2018b). Visualizing fit and lack of fit in\ncomplex regression models with predictor effect plots and partial\nresiduals. Journal of Statistical Software, 87(9). https://doi.org/10.18637/jss.v087.i09\n\n\nFox, J., Weisberg, S., & Price, B. (2023). Car: Companion to\napplied regression. https://CRAN.R-project.org/package=car\n\n\nFox, J., Weisberg, S., Price, B., Friendly, M., & Hong, J. (2022).\nEffects: Effect displays for linear, generalized linear, and other\nmodels. https://www.r-project.org\n\n\nFriedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K.,\nSimon, N., & Yang, J. (2025). Glmnet: Lasso and elastic-net\nregularized generalized linear models. https://glmnet.stanford.edu\n\n\nFriendly, M. (1991). SAS System for statistical\ngraphics (1st ed.). SAS Institute. http://www.sas.\ncom/service/doc/pubcat/uspubcat/ind_files/56143.html\n\n\nFriendly, M. (1994). Mosaic displays for multi-way contingency tables.\nJournal of the American Statistical Association, 89,\n190–200. http://www.jstor.org/stable/2291215\n\n\nFriendly, M. (1999). Extending mosaic displays: Marginal, conditional,\nand partial views of categorical data. Journal of Computational and\nGraphical Statistics, 8(3), 373–395. http://datavis.ca/papers/drew/drew.pdf\n\n\nFriendly, M. (2002). Corrgrams: Exploratory displays for correlation\nmatrices. The American Statistician, 56(4), 316–324.\nhttps://doi.org/10.1198/000313002533\n\n\nFriendly, M. (2007). HE plots for multivariate general\nlinear models. Journal of Computational and Graphical\nStatistics, 16(2), 421–444. https://doi.org/10.1198/106186007X208407\n\n\nFriendly, M. (2008). The Golden Age of statistical\ngraphics. Statistical Science, 23(4), 502–535. https://doi.org/10.1214/08-STS268\n\n\nFriendly, M. (2011). Generalized ridge trace plots: Visualizing bias\nand precision with the genridge R package. SCS\nSeminar.\n\n\nFriendly, M. (2013). The generalized ridge trace plot: Visualizing bias\nand precision. Journal of Computational and Graphical\nStatistics, 22(1), 50–68. https://doi.org/10.1080/10618600.2012.681237\n\n\nFriendly, M. (2022). The life and works of andré-michel\nguerry, revisited. Sociological Spectrum, 42(4-6),\n233–259. https://doi.org/10.1080/02732173.2022.2078450\n\n\nFriendly, M. (2023). vcdExtra: Vcd extensions and additions. https://friendly.github.io/vcdExtra/\n\n\nFriendly, M. (2024). Genridge: Generalized ridge trace plots for\nridge regression. https://github.com/friendly/genridge\n\n\nFriendly, M. (2025). Mvinfluence: Influence measures and diagnostic\nplots for multivariate linear models. https://github.com/friendly/mvinfluence\n\n\nFriendly, M., & Fox, J. (2025). Candisc: Visualizing generalized\ncanonical discriminant and canonical correlation analysis. https://github.com/friendly/candisc/\n\n\nFriendly, M., Fox, J., & Chalmers, P. (2024). Matlib: Matrix\nfunctions for teaching and learning linear algebra and multivariate\nstatistics. https://github.com/friendly/matlib\n\n\nFriendly, M., & Kwan, E. (2003). Effect ordering for data displays.\nComputational Statistics and Data Analysis, 43(4),\n509–539. https://doi.org/10.1016/S0167-9473(02)00290-6\n\n\nFriendly, M., & Kwan, E. (2009). Where’s Waldo:\nVisualizing collinearity diagnostics. The American\nStatistician, 63(1), 56–65. https://doi.org/10.1198/tast.2009.0012\n\n\nFriendly, M., & Meyer, D. (2016). Discrete data analysis with\nR: Visualization and modeling techniques for categorical\nand count data. Chapman & Hall/CRC.\n\n\nFriendly, M., Monette, G., & Fox, J. (2013). Elliptical insights:\nUnderstanding statistical methods through elliptical geometry.\nStatistical Science, 28(1), 1–39. https://doi.org/10.1214/12-STS402\n\n\nFriendly, M., & Wainer, H. (2021). A history of data\nvisualization and graphic communication. Harvard University Press.\nhttps://doi.org/10.4159/9780674259034\n\n\nFuller, W. (2006). Measurement error models (2nd ed.). John\nWiley & Sons.\n\n\nFunkhouser, H. G. (1937). Historical development of the graphical\nrepresentation of statistical data. Osiris, 3(1),\n269–405. http://tinyurl.com/32ema9\n\n\nGabriel, K. R. (1971). The biplot graphic display of matrices with\napplication to principal components analysis. Biometrics,\n58(3), 453–467. https://doi.org/10.2307/2334381\n\n\nGabriel, K. R. (1981). Biplot display of multivariate matrices for\ninspection of data and diagnosis. In V. Barnett (Ed.), Interpreting\nmultivariate data (pp. 147–173). John Wiley; Sons.\n\n\nGalton, F. (1863). Meteorographica, or methods of mapping the\nweather. Macmillan. http://www.mugu.com/galton/books/meteorographica/index.htm\n\n\nGalton, F. (1886). Regression towards mediocrity in hereditary stature.\nJournal of the Anthropological Institute, 15, 246–263.\nhttp://www.jstor.org/cgi-bin/jstor/viewitem/09595295/dm995266/99p0374f/0\n\n\nGalton, F. (1889). Natural inheritance. Macmillan. http://galton.org/books/natural-inheritance/pdf/galton-nat-inh-1up-clean.pdf\n\n\nGannett, H. (1898). Statistical atlas of the united states, eleventh\n(1890) census. U.S. Government Printing Office.\n\n\nGastwirth, J. L., Gel, Y. R., & Miao, W. (2009). The impact of Levene’s test of equality of variances on\nstatistical theory and practice. Statistical Science,\n24(3), 343–360. https://doi.org/10.1214/09-STS301\n\n\nGayan De Silva. (2020). Exploring the world of artificial neural\nnetworks -a beginner’s overview. https://doi.org/10.13140/RG.2.2.14790.14406\n\n\nGelman, A., Hullman, J., & Kennedy, L. (2023). Causal quartets:\nDifferent ways to attain the same average treatment effect. http://www.stat.columbia.edu/~gelman/research/unpublished/causal_quartets.pdf\n\n\nGittins, R. (1985). Canonical analysis: A review with applications\nin ecology. Springer-Verlag.\n\n\nGoeman, J., Meijer, R., Chaturvedi, N., & Lueder, M. (2022).\nPenalized: L1 (lasso and fused lasso) and L2 (ridge) penalized\nestimation in GLMs and in the cox model. https://CRAN.R-project.org/package=penalized\n\n\nGorman, K. B., Williams, T. D., & Fraser, W. R. (2014). Ecological\nsexual dimorphism and environmental variability within a community of\nantarctic penguins (genus pygoscelis). PLoS\nONE, 9(3), e90081. https://doi.org/10.1371/journal.pone.0090081\n\n\nGower, J. C., & Hand, D. J. (1996). Biplots. Chapman &\nHall.\n\n\nGower, J. C., Lubbe, S. G., & Roux, N. J. L. (2011).\nUnderstanding biplots. Wiley. http://books.google.ca/books?id=66gQCi5JOKYC\n\n\nGrandjean, M. (2016). A social network analysis of Twitter:\nMapping the digital humanities community. Cogent Arts\n&Amp; Humanities, 3(1), 1171458. https://doi.org/10.1080/23311983.2016.1171458\n\n\nGraybill, F. A. (1961). An introduction to linear statistical\nmodels. McGraw-Hill.\n\n\nGreenacre, M. (1984). Theory and applications of correspondence\nanalysis. Academic Press.\n\n\nGreenacre, M. (2010). Biplots in practice.\nFundación BBVA. https://books.google.ca/books?id=dv4LrFP7U\\_EC\n\n\nGuerry, A.-M. (1833). Essai sur la statistique morale de la\nFrance. Crochard.\n\n\nHahsler, M., Buchta, C., & Hornik, K. (2024). Seriation:\nInfrastructure for ordering objects using seriation. https://github.com/mhahsler/seriation\n\n\nHaitovsky, Y. (1987). On multivariate ridge regression.\nBiometrika, 74(3), 563–570. https://doi.org/10.1093/biomet/74.3.563\n\n\nHarrell, F. E. (2015). Regression modeling strategies: With\napplications to linear models, logistic and ordinal regression, and\nsurvival analysis. Springer International Publishing. https://books.google.ca/books?id=sQ90rgEACAAJ\n\n\nHarrison, P. (2023). Langevitour: Smooth interactive touring of high\ndimensions, demonstrated with scRNA-seq data. The R Journal,\n15(2), 206–219. https://doi.org/10.32614/RJ-2023-046\n\n\nHarrison, P. (2025). Langevitour: Langevin tour. https://logarithmic.net/langevitour/\n\n\nHart, C., & Wang, E. (2022). Detourr: Portable and performant\ntour animations. https://CRAN.R-project.org/package=detourr\n\n\nHartigan, J. A. (1975a). Clustering algorithms. John Wiley;\nSons.\n\n\nHartigan, J. A. (1975b). Printer graphics for clustering. Journal of\nStatistical Computing and Simulation, 4, 187–213.\n\n\nHartley, H. O. (1950). The use of range in analysis of variance.\nBiometrika, 37(3–4), 271–280. https://doi.org/10.1093/biomet/37.3-4.271\n\n\nHartman, L. I. (2016). Schizophrenia and schizoaffective disorder:\nOne condition or two? [PhD dissertation]. York University.\n\n\nHarwell, M. R., Rubinstein, E. N., Hayes, W. S., & Olds, C. C.\n(1992). Summarizing monte carlo results in methodological research: The\none- and two-factor fixed effects ANOVA cases. Journal\nof Educational and Behavioral Statistics, 17(4), 315–339.\nhttps://doi.org/10.3102/10769986017004315\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning: Data mining, inference and prediction (2nd\ned.). Springer. http://www-stat.stanford.edu/~tibs/ElemStatLearn/\n\n\nHealy, M. J. R. (1968). Multivariate normal plotting. Journal of the\nRoyal Statistical Society Series C, 17(2), 157–161.\n\n\nHeinrichs, R. W., Pinnock, F., Muharib, E., Hartman, L., Goldberg, J.,\n& McDermid Vaz, S. (2015). Neurocognitive normality in schizophrenia\nrevisited. Schizophrenia Research: Cognition, 2(4),\n227–232. https://doi.org/10.1016/j.scog.2015.09.001\n\n\nHerschel, J. F. W. (1833). On the investigation of the orbits of\nrevolving double stars: Being a supplement to a paper entitled\n\"micrometrical measures of 364 double stars\". Memoirs of the Royal\nAstronomical Society, 5, 171–222.\n\n\nHoaglin, D. C., & Welsch, R. E. (1978). The hat matrix in regression\nand ANOVA. The American Statistician,\n32(1), 17–22. https://doi.org/10.1080/00031305.1978.10479237\n\n\nHocking, R. R. (2013). Methods and applications of linear models:\nRegression and the analysis of variance. Wiley. https://books.google.ca/books?id=iq2J-1iS6HcC\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression:\nBiased estimation for nonorthogonal problems.\nTechnometrics, 12, 55–67.\n\n\nHoerl, A. E., Kennard, R. W., & Baldwin, K. F. (1975). Ridge\nregression: Some simulations. Communications in Statistics,\n4(2), 105–123. https://doi.org/10.1080/03610927508827232\n\n\nHofmann, H., VanderPlas, S., & Ge, Y. (2022). Ggpcp: Parallel\ncoordinate plots in the ggplot2 framework. https://github.com/heike/ggpcp\n\n\nHofstadter, D. R. (1979). Gödel, escher, bach: An eternal golden\nbraid. Basic Books.\n\n\nHøjsgaard, S., Edwards, D., & Lauritzen, S. (2012). Graphical\nmodels with R. Springer Science & Business Media.\n\n\nHorst, A., Hill, A., & Gorman, K. (2022). Palmerpenguins: Palmer\narchipelago (antarctica) penguin data. https://allisonhorst.github.io/palmerpenguins/\n\n\nHotelling, H. (1931). The generalization of Student’s ratio. The Annals of Mathematical\nStatistics, 2(3), 360–378. https://doi.org/10.1214/aoms/1177732979\n\n\nHotelling, H. (1936). Relations between two sets of variates.\nBiometrika, 28(3/4), 321. https://doi.org/10.2307/2333955\n\n\nHuang, F. L. (2019). MANOVA: A procedure whose time has\npassed? Gifted Child Quarterly, 64(1), 56–60. https://doi.org/10.1177/0016986219887200\n\n\nHuberty, C. J., & Morris, J. D. (1989). Multivariate analysis versus\nmultiple univariate analyses. Psychological Bulletin,\n105(2), 302–308. https://doi.org/10.1037/0033-2909.105.2.302\n\n\nHusson, F., Josse, J., Le, S., & Mazet, J. (2024). FactoMineR:\nMultivariate exploratory data analysis and data mining. http://factominer.free.fr\n\n\nHusson, F., Le, S., & Pagès, J. (2017). Exploratory multivariate\nanalysis by example using r. Chapman & Hall. https://doi.org/10.1201/b21874\n\n\nIBM. (1965). Proceedings of the IBM scientific computing symposium\non statistics: Oct 21-23, 1963 (L. Robinson, Ed.). IBM. https://www.amazon.com/Proceedings-Scientific-Computing-Symposium-Statistics/dp/B000GL5RLU\n\n\nInselberg, A. (1985). The plane with parallel coordinates. The\nVisual Computer, 1, 69–91.\n\n\nIsvoranu, A.-M., Epskamp, S., Waldorp, L. J., & Borsboom, D. (2022).\nNetwork psychometrics with r: A guide for behavioral and social\nscientists. Routledge. https://doi.org/10.4324/9781003111238\n\n\nKassambara, A., & Mundt, F. (2020). Factoextra: Extract and\nvisualize the results of multivariate data analyses. http://www.sthda.com/english/rpkgs/factoextra\n\n\nKastellec, J. P., & Leoni, E. L. (2007). Using graphs instead of\ntables in political science. Perspectives on Politics,\n5(04), 755–771. https://doi.org/10.1017/S1537592707072209\n\n\nKorkmaz, S., Goksuluk, D., & Zararsiz, G. (2025). MVN:\nMultivariate normality tests. https://selcukorkmaz.github.io/mvn-tutorial/\n\n\nKrijthe, J. (2023). Rtsne: T-distributed stochastic neighbor\nembedding using a barnes-hut implementation. https://github.com/jkrijthe/Rtsne\n\n\nKruskal, J. B. (1964). Multidimensional scaling by optimizing goodness\nof fit to a nonmetric hypothesis. Psychometrika,\n29(1), 1–27. https://doi.org/10.1007/bf02289565\n\n\nKwan, E., Lu, I. R. R., & Friendly, M. (2009). Tableplot: A new tool\nfor assessing precise predictions. Zeitschrift für\nPsychologie / Journal of Psychology, 217(1), 38–48. https://doi.org/10.1027/0044-3409.217.1.38\n\n\nLarmarange, J. (2025). Ggstats: Extension to ggplot2 for plotting\nstats. https://larmarange.github.io/ggstats/\n\n\nLarsen, W. A., & McCleary, S. J. (1972). The use of partial residual\nplots in regression analysis. Technometrics, 14,\n781–790.\n\n\nLauritzen, S. L. (1996). Graphical models. Oxford University\nPress.\n\n\nLawless, J. F., & Wang, P. (1976). A simulation study of ridge and\nother regression estimators. Communications in Statistics,\n5, 307–323.\n\n\nLee, E.-K., & Cook, D. (2009). A projection pursuit index for large\np small n data. Statistics and Computing, 20(3),\n381–392. https://doi.org/10.1007/s11222-009-9131-1\n\n\nLee, S. (2021). Liminal: Multivariate data visualization with tours\nand embeddings. https://github.com/sa-lee/liminal/\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. Olkin,\nS. G. Ghurye, W. Hoeffding, W. G. Madow, & H. B. Mann (Eds.),\nContributions to probability and statistics: Essays in honor of\nHarold Hotelling (pp. 278–292). Stanford University\nPress.\n\n\nLix, J. M., L. M. Keselman, & Keselman, H. J. (1996). Consequences\nof assumption violations revisited: A quantitative review of\nalternatives to the one-way analysis of variance F test.\nReview of Educational Research, 66(4), 579–619. https://doi.org/10.3102/00346543066004579\n\n\nLongley, J. W. (1967). An appraisal of least squares programs for the\nelectronic computer from the point of view of the user. Journal of\nthe American Statistical Association, 62, 819–841.\nhttps://doi.org/https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500896\n\n\nLüdecke, D. (2025). Ggeffects: Create tidy data frames of marginal\neffects for ggplot from model outputs. https://strengejacke.github.io/ggeffects/\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., &\nMakowski, D. (2021). performance: An\nR package for assessment, comparison and testing of\nstatistical models. Journal of Open Source Software,\n6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., &\nMakowski, D. (2022). Easystats: Framework for easy statistical modeling,\nvisualization, and reporting. In CRAN. https://easystats.github.io/easystats/\n\n\nMaaten, L. van der, & Hinton, G. (2008). Visualizing data using\nt-SNE. Journal of Machine Learning\nResearch, 9, 2579–2605. http://www.jmlr.org/papers/v9/vandermaaten08a.html\n\n\nMardia, K. V. (1970). Measures of multivariate skewness and kurtosis\nwith applications. Biometrika, 57(3), 519–530.\nhttps://doi.org/http://dx.doi.org/10.2307/2334770\n\n\nMardia, K. V. (1974). Applications of some measures of multivariate\nskewness and kurtosis in testing normality and robustness studies.\nSankhya: The Indian Journal of Statistics, Series B,\n36(2), 115–128. http://www.jstor.org/stable/25051892\n\n\nMarquardt, D. W. (1970). Generalized inverses, ridge regression, biased\nlinear estimation, and nonlinear estimation. Technometrics,\n12, 591–612.\n\n\nMarquardt, D. W., & Snee, R. D. (1975). Ridge regression in\npractice. The American Statistician, 29(1), 3–20. https://doi.org/10.1080/00031305.1975.10479105\n\n\nMartí, R., & Laguna, M. (2003). Heuristics and meta-heuristics for\n2-layer straight line crossing minimization. Discrete Applied\nMathematics, 127(3), 665–678.\n\n\nMatejka, J., & Fitzmaurice, G. (2017, May). Same stats, different\ngraphs. Proceedings of the 2017 CHI Conference on Human\nFactors in Computing Systems. https://doi.org/10.1145/3025453.3025912\n\n\nMatloff, N. (2011). The art of R programming:\nA tour of statistical software design. No Starch\nPress.\n\n\nMcDonald, G. C. (2009). Ridge regression. Wiley Interdisciplinary\nReviews: Computational Statistics, 1(1), 93–100. https://doi.org/10.1002/wics.14\n\n\nMcGowan, L. D., Gerke, T., & Barrett, M. (2023). Causal inference is\nnot just a statistics problem. Journal of Statistics and Data\nScience Education, 1–9. https://doi.org/10.1080/26939169.2023.2276446\n\n\nMeyer, D., Zeileis, A., Hornik, K., & Friendly, M. (2024). Vcd:\nVisualizing categorical data. https://CRAN.R-project.org/package=vcd\n\n\nMeyers, L. S., Gamst, G., & Guarino, A. J. (2006). Applied\nmultivariate research: Design and interpretation. SAGE\nPublications.\n\n\nMonette, G. (1990). Geometry of multiple regression and interactive\n3-D graphics. In J. Fox & S. Long (Eds.), Modern\nmethods of data analysis (pp. 209–256). SAGE Publications.\n\n\nO’Brien, P. C. (1992). Robust procedures for testing equality of\ncovariance matrices. Biometrics, 48(3), 819–827. http://www.jstor.org/stable/2532347\n\n\nOksanen, J., Simpson, G. L., Blanchet, F. G., Kindt, R., Legendre, P.,\nMinchin, P. R., O’Hara, R. B., Solymos, P., Stevens, M. H. H., Szoecs,\nE., Wagner, H., Barbour, M., Bedward, M., Bolker, B., Borcard, D.,\nCarvalho, G., Chirico, M., De Caceres, M., Durand, S., … Borman, T.\n(2025). Vegan: Community ecology package. https://github.com/vegandevs/vegan\n\n\nOtto, J., & Kahle, D. (2023). Ggdensity: Interpretable bivariate\ndensity visualization with ggplot2. https://jamesotto852.github.io/ggdensity/\n\n\nPearson, K. (1896). Contributions to the mathematical theory of\nevolution—III, regression, heredity and panmixia.\nPhilosophical Transactions of the Royal Society of London,\n187, 253–318.\n\n\nPearson, K. (1901). On lines and planes of closest fit to systems of\npoints in space. Philosophical Magazine, 6(2),\n559–572.\n\n\nPearson, K. (1903). I. Mathematical contributions to the theory of\nevolution. —XI. On the influence of natural selection on the variability\nand correlation of organs. Philosophical Transactions of the Royal\nSociety of London, 200(321–330), 1–66. https://doi.org/10.1098/rsta.1903.0001\n\n\nPedersen, T. L., & Robinson, D. (2024). Gganimate: A grammar of\nanimated graphics. https://gganimate.com\n\n\nPineo, P. O., & Porter, J. (1967). Occupational prestige in canada*.\nCanadian Review of Sociology, 4(1), 24–40.\nhttps://doi.org/https://doi.org/10.1111/j.1755-618X.1967.tb00472.x\n\n\nPineo, P. O., & Porter, J. (2008). Occupational prestige in canada.\nCanadian Review of Sociology, 4(1), 24–40. https://doi.org/10.1111/j.1755-618x.1967.tb00472.x\n\n\nPlayfair, W. (1786). Commercial and political atlas: Representing,\nby copper-plate charts, the progress of the commerce, revenues,\nexpenditure, and debts of england, during the whole of the eighteenth\ncentury. Debrett; Robinson;; Sewell. http://ucpj.uchicago.edu/Isis/journal/demo/v000n000/000000/000000.fg4.html\n\n\nPlayfair, W. (1801). Statistical breviary; shewing, on a principle\nentirely new, the resources of every state and kingdom in\nEurope. Wallis.\n\n\nReaven, G. M., & Miller, R. G. (1968). Study of the relationship\nbetween glucose and insulin responses to an oral glucose load in man.\nDiabetes, 17(9), 560–569. https://doi.org/10.2337/diab.17.9.560\n\n\nReaven, G. M., & Miller, R. G. (1979). An attempt to define the\nnature of chemical diabetes using a multidimensional analysis.\nDiabetologia, 16, 17–24.\n\n\nRobinaugh, D. J., Hoekstra, R. H. A., Toner, E. R., & Borsboom, D.\n(2019). The network approach to psychopathology: A review of the\nliterature 2008–2018 and an agenda for future research.\nPsychological Medicine, 50(3), 353–366. https://doi.org/10.1017/s0033291719003404\n\n\nRogan, J. C., & Keselman, H. J. (1977). Is the ANOVA\nf-test robust to variance heterogeneity when sample sizes are equal?: An\ninvestigation via a coefficient of variation. American Educational\nResearch Journal, 14(4), 493–498. https://doi.org/10.3102/00028312014004493\n\n\nSarkar, D. (2025). Lattice: Trellis graphics for r. https://lattice.r-forge.r-project.org/\n\n\nScheffé, H. A. (1960). The analysis of variance. Wiley.\n\n\nSchloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M.,\nThoen, E., Elberg, A., & Crowley, J. (2024). GGally: Extension\nto ggplot2. https://ggobi.github.io/ggally/\n\n\nScott, D. W. (1992). Multivariate density estimation: Theory,\npractice, and visualization. Wiley.\n\n\nSearle, S. R., Speed, F. M., & Milliken, G. A. (1980). Population\nmarginal means in the linear model: An alternative to least squares\nmeans. The American Statistician, 34(4), 216–221.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\nfor normality (complete samples). Biometrika, 52(3–4),\n591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nShepard, R. N. (1962a). The analysis of proximities: Multidimensional\nscaling with an unknown distance function. i. Psychometrika,\n27(2), 125–140. https://doi.org/10.1007/bf02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities: Multidimensional\nscaling with an unknown distance function. II. Psychometrika,\n27(3), 219–246. https://doi.org/10.1007/bf02289621\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S.\n(1972a). Multidimensional scaling; theory and applications in the\nbehavioral sciences: Vols. II. Applications. Seminar Press. https://books.google.ca/books?id=PpFAAQAAIAAJ\n\n\nShepard, R. N., Romney, A. K., Nerlove, S. B., & Board, M. S. S.\n(1972b). Multidimensional scaling: Theory and applications in the\nbehavioral sciences: Vols. I. Theory. Seminar Press. https://books.google.ca/books?id=pJRAAQAAIAAJ\n\n\nShoben, E. J. (1983). Applications of multidimensional scaling in\ncognitive psychology. Applied Psychological Measurement,\n7(4), 473–490. https://doi.org/10.1177/014662168300700406\n\n\nSilverman, B. W. (1986). Density estimation for statistics and data\nanalysis. Chapman & Hall.\n\n\nSimpson, E. H. (1951). The interpretation of interaction in contingency\ntables. Journal of the Royal Statistical Society, Series B,\n30, 238–241.\n\n\nSwayne, D. F., Cook, D., & Buja, A. (1998). XGobi: Interactive\ndynamic data visualization in the x window system. Journal of\nComputational and Graphical Statistics, 7(1), 113–130. https://doi.org/10.1080/10618600.1998.10474764\n\n\nSwayne, D. F., Lang, D. T., Buja, A., & Cook, D. (2003).\nGGobi: Evolving from XGobi into an extensible\nframework for interactive data visualization. Computational\nStatistics &Amp; Data Analysis, 43(4), 423–444. https://doi.org/10.1016/s0167-9473(02)00286-4\n\n\nTeetor, P. (2011). R cookbook.\nO’Reilly Media.\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso.\nJournal of the Royal Statistical Society, Series B:\nMethodological, 58, 267–288.\n\n\nTiku, M. L., & Balakrishnan, N. (1984). Testing equality of\npopulation variances the robust way. Communications in Statistics -\nTheory and Methods, 13(17), 2143–2159. https://doi.org/10.1080/03610928408828818\n\n\nTimm, N. H. (1975). Multivariate analysis with applications in\neducation and psychology. Wadsworth (Brooks/Cole).\n\n\nTorgerson, W. S. (1952). Multidimensional scaling: I. Theory and method.\nPsychometrika, 17(4), 401–419. https://doi.org/10.1007/bf02288916\n\n\nVanderPlas, S., Ge, Y., Unwin, A., & Hofmann, H. (2023). Penguins go\nparallel: A grammar of graphics framework for generalized parallel\ncoordinate plots. Journal of Computational and Graphical\nStatistics, 1–16. https://doi.org/10.1080/10618600.2023.2195462\n\n\nVelleman, P. F., & Welsh, R. E. (1981). Efficient computing of\nregression diagnostics. The American Statistician,\n35(4), 234–242.\n\n\nVinod, H. D. (1978). A survey of ridge regression and related techniques\nfor improvements over ordinary least squares. The Review of\nEconomics and Statistics, 60(1), 121–131. http://www.jstor.org/stable/1924340\n\n\nVu, V. Q., & Friendly, M. (2024). Ggbiplot: A grammar of\ngraphics implementation of biplots. https://github.com/friendly/ggbiplot\n\n\nWaddell, A., & Oldford, R. W. (2023). Loon: Interactive\nstatistical data visualization. https://CRAN.R-project.org/package=loon\n\n\nWarne, F. T. (2014). A primer on multivariate analysis of\nvariance(MANOVA) for behavioral scientists. Practical Assessment,\nResearch & Evaluation, 19(1). https://scholarworks.umass.edu/pare/vol19/iss1/17/\n\n\nWegman, E. J. (1990). Hyperdimensional data analysis using parallel\ncoordinates. Journal of the American Statistical Association,\n85(411), 664–675.\n\n\nWei, T., & Simko, V. (2024). Corrplot: Visualization of a\ncorrelation matrix. https://github.com/taiyun/corrplot\n\n\nWelch, B. L. (1947). The generalization of \"student’s\" problem when\nseveral different population varlances are involved.\nBiometrika, 34(1–2), 28–35. https://doi.org/10.1093/biomet/34.1-2.28\n\n\nWest, D. B. (2001). Introduction to graph theory. Prentice\nhall.\n\n\nWhittaker, J. (1990). Graphical models in applied multivariate\nstatistics. John Wiley; Sons.\n\n\nWickham, H. (2014). Advanced R. Chapman and\nHall/CRC.\n\n\nWickham, H., & Cook, D. (2025). Tourr: Tour methods for\nmultivariate data visualisation. https://github.com/ggobi/tourr\n\n\nWickham, H., Cook, D., Hofmann, H., & Buja, A. (2011). Tourr: An\nR package for exploring multivariate data with projections.\nJournal of Statistical Software, 40(2). https://doi.org/10.18637/jss.v040.i02\n\n\nWilkinson, G. N., & Rogers, C. E. (1973). Symbolic description of\nfactorial models for analysis of variance. Applied Statistics,\n22(3), 392. https://doi.org/10.2307/2346786\n\n\nWiner, B. J. (1962). Statistical principles in experimental\ndesign. McGraw-Hill.\n\n\nWood, S. N. (2006). Generalized additive models: An introduction\nwith r. Chapman; Hall/CRC Press.\n\n\nWright, K. (2021). Corrgram: Plot a correlogram. https://kwstat.github.io/corrgram/\n\n\nXie, Y. (2021). Animation: A gallery of animations in statistics and\nutilities to create animations. https://yihui.org/animation/\n\n\nXu, Z., & Oldford, R. W. (2021). Loon.tour: Tour in ’loon’.\nhttps://cran.r-project.org/package=loon.tourr\n\n\nYee, T. W. (2015). Vector generalized linear and additive models:\nWith an implementation in r. Springer.\n\n\nYee, T. W. (2025). VGAM: Vector generalized linear and\nadditive models. https://CRAN.R-project.org/package=VGAM\n\n\nZhang, J., & Boos, D. D. (1992). Bootstrap critical values for\ntesting homogeneity of covariance matrices. Journal of the American\nStatistical Association, 87(418), 425–429. http://www.jstor.org/stable/2290273\n\n\nPackage used",
    "crumbs": [
      "End matter",
      "References"
    ]
  }
]