\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/r>>}
\aioptions{0|{vv }{ll}{, f.}{, jj}|5|5|pages}
\aifilename{index.ain}
\newlabel{preface}{{}{ix}{Preface}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{Preface}{ix}{chapter*.2}\protected@file@percent }
\newlabel{features}{{}{ix}{Features}{section*.3}{}}
\@writefile{toc}{\contentsline {section}{Features}{ix}{section*.3}\protected@file@percent }
\citation{ref-Fox2021}
\citation{ref-R-matlib}
\citation{ref-Cotton-2013}
\citation{ref-Matloff-2011}
\citation{ref-Wickham2019}
\citation{ref-LongTeetor2019}
\citation{ref-FoxWeisberg:2018}
\citation{ref-Fox:2016:ARA}
\citation{ref-R-car}
\newlabel{what-i-assume}{{}{x}{What I assume}{section*.4}{}}
\@writefile{toc}{\contentsline {section}{What I assume}{x}{section*.4}\protected@file@percent }
\newlabel{r-resources}{{}{x}{R Resources}{section*.5}{}}
\@writefile{toc}{\contentsline {section}{R Resources}{x}{section*.5}\protected@file@percent }
\citation{ref-Wilke2019}
\citation{ref-Healy2019}
\citation{ref-Unwin2024}
\citation{ref-Rennie2025}
\citation{ref-R-ggplot2}
\newlabel{r-graphics-resources}{{}{xi}{R graphics resources}{section*.6}{}}
\@writefile{toc}{\contentsline {section}{R graphics resources}{xi}{section*.6}\protected@file@percent }
\newlabel{conventions-used-in-this-book}{{}{xi}{Conventions used in this book}{section*.7}{}}
\@writefile{toc}{\contentsline {section}{Conventions used in this book}{xi}{section*.7}\protected@file@percent }
\newlabel{acknowledgements}{{}{xii}{Acknowledgements}{section*.8}{}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{xii}{section*.8}\protected@file@percent }
\citation{ref-Friendly:00:VCD}
\citation{ref-FriendlyMeyer:2016:DDAR}
\citation{ref-FriendlyWainer:2021:TOGS}
\HyPL@Entry{12<</S/D>>}
\newlabel{author}{{}{1}{Author}{chapter*.9}{}}
\@writefile{toc}{\contentsline {chapter}{Author}{1}{chapter*.9}\protected@file@percent }
\HyPL@Entry{13<</S/D>>}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Orienting Ideas}{1}{part.1}\protected@file@percent }
\citation{ref-Brinton1939}
\citation{ref-Brinton1939}
\citation{ref-Peddle:1910}
\citation{ref-Haskell:1919}
\citation{ref-Costelloe:1915}
\citation{ref-Friendly-etal:2015}
\citation{ref-FriendlyDenis:2001:valois}
\citation{ref-Tukey:1962}
\citation{ref-Tukey:77}
\newlabel{prelude}{{I}{3}{Warm-up Exercises}{chapter*.10}{}}
\@writefile{toc}{\contentsline {chapter}{Warm-up Exercises}{3}{chapter*.10}\protected@file@percent }
\newlabel{the-magic-of-graphs}{{I}{3}{The Magic of Graphs}{section*.11}{}}
\@writefile{toc}{\contentsline {section}{The Magic of Graphs}{3}{section*.11}\protected@file@percent }
\citation{ref-Maunder:1904}
\citation{ref-Hertzsprung:1911}
\citation{ref-Russell1914}
\citation{ref-SpenceGarrison:1993}
\citation{ref-Moseley:1913}
\newlabel{graphic-discoveries-19001950}{{I}{4}{Graphic discoveries, 1900--1950}{section*.12}{}}
\@writefile{toc}{\contentsline {subsection}{Graphic discoveries, 1900--1950}{4}{section*.12}\protected@file@percent }
\citation{ref-Abbott:1884}
\newlabel{one-two-many}{{I}{5}{ONE, TWO, MANY}{section*.13}{}}
\@writefile{toc}{\contentsline {section}{ONE, TWO, MANY}{5}{section*.13}\protected@file@percent }
\newlabel{flatland}{{I}{5}{Flatland}{section*.14}{}}
\@writefile{toc}{\contentsline {section}{Flatland}{5}{section*.14}\protected@file@percent }
\citation{ref-Abbott:1884}
\citation{ref-Cajori:1926}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A 2D Flatlander seeing a sphere as it passes through Flatland. The line, labeled `My Eye' indicates what the Flatlander would see. Source: Abbott (\citeproc {ref-Abbott:1884}{1884})}}{6}{figure.caption.15}\protected@file@percent }
\newlabel{fig-flatland-spheres}{{1}{6}{A 2D Flatlander seeing a sphere as it passes through Flatland. The line, labeled `My Eye' indicates what the Flatlander would see. Source: Abbott (\citeproc {ref-Abbott:1884}{1884})}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Geometrical objects in 1 to 4 dimensions. One more dimension can be thought of as the trace of movement over time.}}{7}{figure.caption.16}\protected@file@percent }
\newlabel{fig-1D-4D}{{2}{7}{Geometrical objects in 1 to 4 dimensions. One more dimension can be thought of as the trace of movement over time}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces }}{7}{figure.caption.17}\protected@file@percent }
\newlabel{fig-tesseract}{{3}{7}{}{figure.caption.17}{}}
\newlabel{eureka}{{I}{7}{EUREKA!}{section*.18}{}}
\@writefile{toc}{\contentsline {section}{EUREKA!}{7}{section*.18}\protected@file@percent }
\citation{ref-R-rgl}
\citation{ref-R-animation}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Four views of the \texttt  {pollen} data, zooming in, clockwise from the upper left to discover the word ``EUREKA''.}}{8}{figure.caption.19}\protected@file@percent }
\newlabel{fig-pollen-eureka}{{4}{8}{Four views of the \texttt {pollen} data, zooming in, clockwise from the upper left to discover the word ``EUREKA''}{figure.caption.19}{}}
\citation{ref-TukeyTukey:85}
\citation{ref-Wilkinson-etal:2005}
\citation{ref-Friedman:87}
\citation{ref-FriedmanTukey:74}
\citation{ref-Galton:1863}
\citation{ref-FriendlyWainer:2021:TOGS}
\citation{ref-ReavenMiller:79}
\citation{ref-Fishkeller-etal:1974b}
\citation{ref-ReavenMiller:68}
\citation{ref-ReavenMiller:68}
\citation{ref-ReavenMiller:79}
\citation{ref-ReavenMiller:79}
\newlabel{sec-discoveries}{{I}{9}{Multivariate scientific discoveries}{section*.20}{}}
\@writefile{toc}{\contentsline {section}{Multivariate scientific discoveries}{9}{section*.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Reproduction of a graph similar to that from Reaven \& Miller (\citeproc {ref-ReavenMiller:68}{1968}) on the relationship between glucose and insulin response to being given an oral dose of glucose.}}{10}{figure.caption.21}\protected@file@percent }
\newlabel{fig-diabetes1}{{5}{10}{Reproduction of a graph similar to that from Reaven \& Miller (\citeproc {ref-ReavenMiller:68}{1968}) on the relationship between glucose and insulin response to being given an oral dose of glucose}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Artist's rendition of data from Reaven \& Miller (\citeproc {ref-ReavenMiller:79}{1979}) as seen in three dimensions using the PRIM-9 system. Labels for the clusters have been added, identifying the three groups of patients. \emph  {Source}: Reaven \& Miller (\citeproc {ref-ReavenMiller:79}{1979}).}}{10}{figure.caption.22}\protected@file@percent }
\newlabel{fig-ReavenMiller-3d}{{6}{10}{Artist's rendition of data from Reaven \& Miller (\citeproc {ref-ReavenMiller:79}{1979}) as seen in three dimensions using the PRIM-9 system. Labels for the clusters have been added, identifying the three groups of patients. \emph {Source}: Reaven \& Miller (\citeproc {ref-ReavenMiller:79}{1979})}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{13}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-introduction}{{1}{13}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Why use a multivariate design?}{13}{section.1.1}\protected@file@percent }
\newlabel{why-use-a-multivariate-design}{{1.1}{13}{Why use a multivariate design?}{section.1.1}{}}
\citation{ref-Prokofieva2023}
\citation{ref-Brandmaier2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Multivariate vs.~multivariable methods}{14}{subsection.1.1.1}\protected@file@percent }
\newlabel{multivariate-vs.-multivariable-methods}{{1.1.1}{14}{Multivariate vs.~multivariable methods}{subsection.1.1.1}{}}
\citation{ref-py614bib}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Ubiquity of multivariate research designs}{15}{subsection.1.1.2}\protected@file@percent }
\newlabel{ubiquity-of-multivariate-research-designs}{{1.1.2}{15}{Ubiquity of multivariate research designs}{subsection.1.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Heatmap of frequencies of method keyword by decade for research papers submitted by students in a graduate level multivariate data analysis course.}}{15}{figure.caption.23}\protected@file@percent }
\newlabel{fig-heatmap}{{1.1}{15}{Heatmap of frequencies of method keyword by decade for research papers submitted by students in a graduate level multivariate data analysis course}{figure.caption.23}{}}
\citation{ref-Friendly-etal:ellipses:2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Wordcloud illustrating the topic content of the articles in the bibliographic database, showing the 50 most frequently-used, with font size proportional to frequency.}}{16}{figure.caption.24}\protected@file@percent }
\newlabel{fig-wordcloud}{{1.2}{16}{Wordcloud illustrating the topic content of the articles in the bibliographic database, showing the 50 most frequently-used, with font size proportional to frequency}{figure.caption.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Linear models: Univariate to multivariate}{16}{section.1.2}\protected@file@percent }
\newlabel{linear-models-univariate-to-multivariate}{{1.2}{16}{Linear models: Univariate to multivariate}{section.1.2}{}}
\citation{ref-Friendly-07-manova}
\citation{ref-FriendlyMeyer:2016:DDAR}
\citation{ref-Hofstadter1979}
\citation{ref-Hofstadter1979}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Visualization is harder}{17}{section.1.3}\protected@file@percent }
\newlabel{visualization-is-harder}{{1.3}{17}{Visualization is harder}{section.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Problems in understanding and communicating MLM results}{17}{section.1.4}\protected@file@percent }
\newlabel{sec-problems}{{1.4}{17}{Problems in understanding and communicating MLM results}{section.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf  {Projection}: The cover image from Douglas Hofstadter's \emph  {Gödel, Bach and Escher} (\citeproc {ref-Hofstadter1979}{1979}) illustrates projection of 3D solids onto each 2D plane. Each 2D view captures some salient aspect of the complete figure, but is incomplete.}}{18}{figure.caption.25}\protected@file@percent }
\newlabel{fig-cover-GBE}{{1.3}{18}{\textbf {Projection}: The cover image from Douglas Hofstadter's \emph {Gödel, Bach and Escher} (\citeproc {ref-Hofstadter1979}{1979}) illustrates projection of 3D solids onto each 2D plane. Each 2D view captures some salient aspect of the complete figure, but is incomplete}{figure.caption.25}{}}
\citation{ref-FarquharFarquhar:91}
\citation{ref-Tufte:83}
\citation{ref-Playfair:1786}
\citation{ref-Playfair:1801}
\citation{ref-Herschel:1833}
\citation{ref-Guerry:1833}
\citation{ref-Galton:1886}
\citation{ref-Pearson:1896}
\citation{ref-Friendly:2008:golden}
\citation{ref-Funkhouser:1937}
\citation{ref-Friendly:2008:golden}
\citation{ref-FriendlyWainer:2021:TOGS}
\citation{ref-Anscombe:73}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Getting Started}{19}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-getting_started}{{2}{19}{Getting Started}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Why plot your data?}{19}{section.2.1}\protected@file@percent }
\newlabel{sec-why_plot}{{2.1}{19}{Why plot your data?}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Anscombe's Quartet}{20}{subsection.2.1.1}\protected@file@percent }
\newlabel{sec-anscombe}{{2.1.1}{20}{Anscombe's Quartet}{subsection.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Scatterplots of Anscombe's Quartet. Each plot shows the fitted regression line and a 68\% data ellipse representing the correlation between \(x\) and \(y\).}}{21}{figure.caption.26}\protected@file@percent }
\newlabel{fig-ch02-anscombe1}{{2.1}{21}{Scatterplots of Anscombe's Quartet. Each plot shows the fitted regression line and a 68\% data ellipse representing the correlation between \(x\) and \(y\)}{figure.caption.26}{}}
\citation{ref-R-datasauRus}
\citation{ref-MatejkaFitzmaurice2017}
\citation{ref-Gelman-etal:2023}
\citation{ref-McGowan2023}
\citation{ref-Biecek-etal:2023}
\citation{ref-R-quartets}
\citation{ref-Davis:1990}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Plots of the Dinosaur Dozen datasets. Source: \href  {https://x.com/selcukorkmaz/status/1864583253253927156}{Selçuk Korkmaz on X}}}{23}{figure.caption.27}\protected@file@percent }
\newlabel{fig-datasaurus-pdf}{{2.2}{23}{Plots of the Dinosaur Dozen datasets. Source: \href {https://x.com/selcukorkmaz/status/1864583253253927156}{Selçuk Korkmaz on X}}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}One lousy point can ruin your day}{23}{subsection.2.1.2}\protected@file@percent }
\newlabel{sec-davis}{{2.1.2}{23}{One lousy point can ruin your day}{subsection.2.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Regression for Davis' data on reported weight and measures weight for men and women. Separate regression lines, predicting reported weight from measured weight are shown for males and females. One highly unusual point is highlighted.}}{25}{figure.caption.28}\protected@file@percent }
\newlabel{fig-ch02-davis-reg1}{{2.3}{25}{Regression for Davis' data on reported weight and measures weight for men and women. Separate regression lines, predicting reported weight from measured weight are shown for males and females. One highly unusual point is highlighted}{figure.caption.28}{}}
\citation{ref-Fienberg:71}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Regression for Davis' data on reported weight and measures weight for men and women. Separate regression lines, predicting measured weight from reported weight are shown for males and females. The highly unusual point no longer has an effect on the fitted lines.}}{26}{figure.caption.29}\protected@file@percent }
\newlabel{fig-ch02-davis-reg2}{{2.4}{26}{Regression for Davis' data on reported weight and measures weight for men and women. Separate regression lines, predicting measured weight from reported weight are shown for males and females. The highly unusual point no longer has an effect on the fitted lines}{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Shaken, not stirred: The 1970 Draft Lottery}{26}{subsection.2.1.3}\protected@file@percent }
\newlabel{sec-draft1970}{{2.1.3}{26}{Shaken, not stirred: The 1970 Draft Lottery}{subsection.2.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Congressman Alexander Pirnie (R-NY) drawing the first capsule for the Selective Service draft, Dec 1, 1969. Source: \url  {https://bit.ly/45c23sB}}}{27}{figure.caption.30}\protected@file@percent }
\newlabel{fig-draft-lottery-photo}{{2.5}{27}{Congressman Alexander Pirnie (R-NY) drawing the first capsule for the Selective Service draft, Dec 1, 1969. Source: \url {https://bit.ly/45c23sB}}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Basic scatterplot of 1970 Draft Lottery data plotting rank order of selection against birthdates in the year. Points are colored by month. The horizontal line is at the average rank.}}{28}{figure.caption.31}\protected@file@percent }
\newlabel{fig-draft-gg1}{{2.6}{28}{Basic scatterplot of 1970 Draft Lottery data plotting rank order of selection against birthdates in the year. Points are colored by month. The horizontal line is at the average rank}{figure.caption.31}{}}
\newlabel{visual-smoothers}{{2.1.3}{29}{Visual smoothers}{section*.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{Visual smoothers}{29}{section*.32}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Enhanced scatterplot of 1970 Draft Lottery data adding a linear regression line and loess smooth.}}{30}{figure.caption.33}\protected@file@percent }
\newlabel{fig-draft-gg2}{{2.7}{30}{Enhanced scatterplot of 1970 Draft Lottery data adding a linear regression line and loess smooth}{figure.caption.33}{}}
\newlabel{visual-summaries}{{2.1.3}{30}{Visual summaries}{section*.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{Visual summaries}{30}{section*.34}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Plot of the average rank per month with \(\pm 1\) standard error bars. The line shows the least squares regression line, treating months as equally spaced. The vertical axis has been truncated to highlight the decrease in lottery rank over the year.}}{31}{figure.caption.35}\protected@file@percent }
\newlabel{fig-draft-means}{{2.8}{31}{Plot of the average rank per month with \(\pm 1\) standard error bars. The line shows the least squares regression line, treating months as equally spaced. The vertical axis has been truncated to highlight the decrease in lottery rank over the year}{figure.caption.35}{}}
\citation{ref-Fienberg:71}
\newlabel{what-happened-here}{{2.1.3}{32}{What happened here?}{section*.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{What happened here?}{32}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Plots for data analysis}{33}{section.2.2}\protected@file@percent }
\newlabel{sec-plots-data-analysis}{{2.2}{33}{Plots for data analysis}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Diagnostic plots}{33}{subsection.2.2.1}\protected@file@percent }
\newlabel{diagnostic-plots}{{2.2.1}{33}{Diagnostic plots}{subsection.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Regression quartet: Four diagnostic plots for the linear model fit to the Davis data.}}{34}{figure.caption.37}\protected@file@percent }
\newlabel{fig-davis-diagnostic}{{2.9}{34}{Regression quartet: Four diagnostic plots for the linear model fit to the Davis data}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Principles of graphical display}{34}{section.2.3}\protected@file@percent }
\newlabel{principles-of-graphical-display}{{2.3}{34}{Principles of graphical display}{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}What have we learned?}{35}{section.2.4}\protected@file@percent }
\newlabel{what-have-we-learned}{{2.4}{35}{What have we learned?}{section.2.4}{}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Exploratory Methods}{37}{part.2}\protected@file@percent }
\citation{ref-Tukey:77}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Plots of Multivariate Data}{39}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-multivariate_plots}{{3}{39}{Plots of Multivariate Data}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Bivariate summaries}{40}{section.3.1}\protected@file@percent }
\newlabel{sec-bivariate_summaries}{{3.1}{40}{Bivariate summaries}{section.3.1}{}}
\newlabel{exm-salaries1}{{3.1}{40}{}{example.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Naive scatterplot of Salary vs.~years since PhD, ignoring other variables, and without graphical annotations.}}{41}{figure.caption.38}\protected@file@percent }
\newlabel{fig-Salaries-scat}{{3.1}{41}{Naive scatterplot of Salary vs.~years since PhD, ignoring other variables, and without graphical annotations}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Smoothers}{41}{subsection.3.1.1}\protected@file@percent }
\newlabel{smoothers}{{3.1.1}{41}{Smoothers}{subsection.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Scatterplot of Salary vs.~years since PhD, showing \textcolor {red}{linear} and \textcolor {darkgreen}{quadratic} smooths with 95\% confidence bands.}}{42}{figure.caption.39}\protected@file@percent }
\newlabel{fig-Salaries-lm}{{3.2}{42}{Scatterplot of Salary vs.~years since PhD, showing \textcolor {red}{linear} and \textcolor {darkgreen}{quadratic} smooths with 95\% confidence bands}{figure.caption.39}{}}
\citation{ref-Cleveland:79}
\citation{ref-ClevelandDevlin:88}
\citation{ref-Fox:2016:ARA}
\citation{ref-Wood:2006}
\newlabel{non-parametric-smoothers}{{3.1.1}{43}{Non-parametric smoothers}{section*.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Non-parametric smoothers}{43}{section*.40}\protected@file@percent }
\citation{ref-ChambersHastie1991}
\citation{ref-Becker:1996:VDC}
\citation{ref-Cleveland:85}
\citation{ref-R-lattice}
\citation{ref-R-car}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Scatterplot of Salary vs.~years since PhD, adding the loess smooth. The loess smooth curve and confidence band in \textcolor {green}{green} is nearly indistinguishable from a quadratic fit in \textcolor {blue}{blue}.}}{44}{figure.caption.41}\protected@file@percent }
\newlabel{fig-Salaries-loess}{{3.3}{44}{Scatterplot of Salary vs.~years since PhD, adding the loess smooth. The loess smooth curve and confidence band in \textcolor {green}{green} is nearly indistinguishable from a quadratic fit in \textcolor {blue}{blue}}{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Stratifiers}{44}{subsection.3.1.2}\protected@file@percent }
\newlabel{stratifiers}{{3.1.2}{44}{Stratifiers}{subsection.3.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Scatterplot of Salary vs.~years since PhD, grouped by rank.}}{46}{figure.caption.42}\protected@file@percent }
\newlabel{fig-Salaries-rank}{{3.4}{46}{Scatterplot of Salary vs.~years since PhD, grouped by rank}{figure.caption.42}{}}
\citation{ref-ClevelandMcGill:84b}
\citation{ref-ClevelandMcGill:85}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Scatterplot of Salary vs.~years since PhD, grouped by discipline.}}{47}{figure.caption.43}\protected@file@percent }
\newlabel{fig-Salaries-discipline}{{3.5}{47}{Scatterplot of Salary vs.~years since PhD, grouped by discipline}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Conditioning}{48}{subsection.3.1.3}\protected@file@percent }
\newlabel{conditioning}{{3.1.3}{48}{Conditioning}{subsection.3.1.3}{}}
\citation{ref-Monette:90}
\citation{ref-Dempster:69}
\citation{ref-Friendly-etal:ellipses:2013}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Scatterplot of Salary vs.~years since PhD, grouped by rank, with separate panels for discipline.}}{49}{figure.caption.44}\protected@file@percent }
\newlabel{fig-Salaries-faceted}{{3.6}{49}{Scatterplot of Salary vs.~years since PhD, grouped by rank, with separate panels for discipline}{figure.caption.44}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Data Ellipses}{49}{section.3.2}\protected@file@percent }
\newlabel{sec-data-ellipse}{{3.2}{49}{Data Ellipses}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Scatterplot of Salary vs.~years since PhD, grouped by sex, faceted by discipline and rank.}}{50}{figure.caption.45}\protected@file@percent }
\newlabel{fig-Salaries-facet-sex}{{3.7}{50}{Scatterplot of Salary vs.~years since PhD, grouped by sex, faceted by discipline and rank}{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces 2D data with curves of constant distance from the centroid. The blue solid ellipse shows a contour of constant Mahalanobis distance, taking the correlation into account; the dashed red circle is a contour of equal Euclidean distance. Given the data points, which of the points \textbf  {A} and \textbf  {B} is further from the mean (X)? \emph  {Source}: Re-drawn from \href  {https://ouzhang.rbind.io/2020/11/16/outliers-part4/}{Ou Zhang}}}{51}{figure.caption.46}\protected@file@percent }
\newlabel{fig-mahalanobis}{{3.8}{51}{2D data with curves of constant distance from the centroid. The blue solid ellipse shows a contour of constant Mahalanobis distance, taking the correlation into account; the dashed red circle is a contour of equal Euclidean distance. Given the data points, which of the points \textbf {A} and \textbf {B} is further from the mean (X)? \emph {Source}: Re-drawn from \href {https://ouzhang.rbind.io/2020/11/16/outliers-part4/}{Ou Zhang}}{figure.caption.46}{}}
\newlabel{eq-Dsq}{{3.1}{51}{Data Ellipses}{section*.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Drawing data ellipses}{51}{subsection.3.2.1}\protected@file@percent }
\newlabel{drawing-data-ellipses}{{3.2.1}{51}{Drawing data ellipses}{subsection.3.2.1}{}}
\newlabel{eq-ellE}{{3.2}{52}{Drawing data ellipses}{section*.48}{}}
\citation{ref-Galton:1886}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Data ellipses of 50\%, 68\% and 95\% coverage when the means are \(\bar {\mathbf {y}} = \mathbf {0}\) and the variance-covariance matrix is \(\mathbf {S}\).}}{53}{figure.caption.49}\protected@file@percent }
\newlabel{fig-ellipses-coverage}{{3.9}{53}{Data ellipses of 50\%, 68\% and 95\% coverage when the means are \(\bar {\mathbf {y}} = \mathbf {0}\) and the variance-covariance matrix is \(\mathbf {S}\)}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Ellipse properties}{54}{subsection.3.2.2}\protected@file@percent }
\newlabel{ellipse-properties}{{3.2.2}{54}{Ellipse properties}{subsection.3.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Galton's 1886 diagram, showing the relationship of height of children to the average of their parents' height. The diagram is essentially an overlay of a geometrical interpretation on a bivariate grouped frequency distribution, shown as numbers.}}{54}{figure.caption.50}\protected@file@percent }
\newlabel{fig-galton-corr}{{3.10}{54}{Galton's 1886 diagram, showing the relationship of height of children to the average of their parents' height. The diagram is essentially an overlay of a geometrical interpretation on a bivariate grouped frequency distribution, shown as numbers}{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Sunflower plot of Galton's data on heights of parents and their children (in.), with 40\%, 68\% and 95\% data ellipses and the regression lines of \(y\) on \(x\) (black) and \(x\) on \(y\) (grey).}}{55}{figure.caption.51}\protected@file@percent }
\newlabel{fig-galton-ellipse-r}{{3.11}{55}{Sunflower plot of Galton's data on heights of parents and their children (in.), with 40\%, 68\% and 95\% data ellipses and the regression lines of \(y\) on \(x\) (black) and \(x\) on \(y\) (grey)}{figure.caption.51}{}}
\citation{ref-Pearson:1901}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}R functions for data ellipses}{56}{subsection.3.2.3}\protected@file@percent }
\newlabel{r-functions-for-data-ellipses}{{3.2.3}{56}{R functions for data ellipses}{subsection.3.2.3}{}}
\citation{ref-Blishen-etal-1987}
\citation{ref-PineoPorter2008}
\newlabel{exm-prestige}{{3.2}{57}{}{example.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Scatterplot of prestige vs.~income, showing the linear regression line (\textcolor {red}{red}), the loess smooth with a confidence envelope (\textcolor {darkgreen}{darkgreen}) and a 68\% data ellipse. Points with the 4 largest Mahalanobis \(D^2\) values are labeled.}}{58}{figure.caption.52}\protected@file@percent }
\newlabel{fig-Prestige-scatterplot-income1}{{3.12}{58}{Scatterplot of prestige vs.~income, showing the linear regression line (\textcolor {red}{red}), the loess smooth with a confidence envelope (\textcolor {darkgreen}{darkgreen}) and a 68\% data ellipse. Points with the 4 largest Mahalanobis \(D^2\) values are labeled}{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Scatterplot of prestige vs.~education, showing the linear regression line (\textcolor {red}{red}), the loess smooth with a confidence envelope (\textcolor {darkgreen}{darkgreen}) and a 68\% data ellipse.}}{59}{figure.caption.53}\protected@file@percent }
\newlabel{fig-Prestige-scatterplot-educ1}{{3.13}{59}{Scatterplot of prestige vs.~education, showing the linear regression line (\textcolor {red}{red}), the loess smooth with a confidence envelope (\textcolor {darkgreen}{darkgreen}) and a 68\% data ellipse}{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Scatterplot of prestige vs.~education, labeling points whose absolute standardized residual is \textgreater {} 2.}}{60}{figure.caption.54}\protected@file@percent }
\newlabel{fig-Prestige-scatterplot-educ2}{{3.14}{60}{Scatterplot of prestige vs.~education, labeling points whose absolute standardized residual is \textgreater {} 2}{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Handling nonlinearity: Plotting on a log scale}{60}{subsection.3.2.4}\protected@file@percent }
\newlabel{sec-log-scale}{{3.2.4}{60}{Handling nonlinearity: Plotting on a log scale}{subsection.3.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Scatterplot of prestige vs.~log(income).}}{61}{figure.caption.55}\protected@file@percent }
\newlabel{fig-Prestige-scatterplot2}{{3.15}{61}{Scatterplot of prestige vs.~log(income)}{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Stratifying}{61}{subsection.3.2.5}\protected@file@percent }
\newlabel{sec-stratifying}{{3.2.5}{61}{Stratifying}{subsection.3.2.5}{}}
\citation{ref-R-palmerpenguins}
\citation{ref-Gorman2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Scatterplot of prestige vs.~income, stratified by occupational type. This implies a different interpretation, where occupation type is a moderator variable, so different regressions apply to each type. The linear regression line is dashed, while the loess smooth is solid.}}{62}{figure.caption.56}\protected@file@percent }
\newlabel{fig-Prestige-scatterplot3}{{3.16}{62}{Scatterplot of prestige vs.~income, stratified by occupational type. This implies a different interpretation, where occupation type is a moderator variable, so different regressions apply to each type. The linear regression line is dashed, while the loess smooth is solid}{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Example: Penguins data}{62}{subsection.3.2.6}\protected@file@percent }
\newlabel{sec-penguins}{{3.2.6}{62}{Example: Penguins data}{subsection.3.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Penguin species observed in the Palmer Archipelago. This is a cartoon, but it illustrates some features of penguin body size measurements, and the colors typically used for species. Image: Allison Horst}}{63}{figure.caption.57}\protected@file@percent }
\newlabel{fig-penguin-species}{{3.17}{63}{Penguin species observed in the Palmer Archipelago. This is a cartoon, but it illustrates some features of penguin body size measurements, and the colors typically used for species. Image: Allison Horst}{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Color palettes used for penguin species.}}{64}{figure.caption.58}\protected@file@percent }
\newlabel{fig-peng-colors}{{3.18}{64}{Color palettes used for penguin species}{figure.caption.58}{}}
\citation{ref-Rousseeuw-etal:2012}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Penguin bill length and bill depth according to species.}}{66}{figure.caption.59}\protected@file@percent }
\newlabel{fig-peng-ggplot1}{{3.19}{66}{Penguin bill length and bill depth according to species}{figure.caption.59}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.7}Visual thinning}{66}{subsection.3.2.7}\protected@file@percent }
\newlabel{sec-visual-thinning}{{3.2.7}{66}{Visual thinning}{subsection.3.2.7}{}}
\citation{ref-R-gggda}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces \textbf  {Visual thinning}: Suppressing the data points gives a visual summary of the relation between bill length and bill depth using the regression line and data ellipses.}}{67}{figure.caption.60}\protected@file@percent }
\newlabel{fig-peng-ggplot2}{{3.20}{67}{\textbf {Visual thinning}: Suppressing the data points gives a visual summary of the relation between bill length and bill depth using the regression line and data ellipses}{figure.caption.60}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Bagplots}{67}{section.3.3}\protected@file@percent }
\newlabel{sec-bagplots}{{3.3}{67}{Bagplots}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces \textbf  {Bagplot}: For each Penguin species the darker inner (bag) polygon reflects the innermost 50\% of the data points. The outer (fence) polygon corresponds to points enclosed by a multiple of the bag. Points outside the fence for each species are ploted individually.}}{68}{figure.caption.61}\protected@file@percent }
\newlabel{fig-peng-bagplot}{{3.21}{68}{\textbf {Bagplot}: For each Penguin species the darker inner (bag) polygon reflects the innermost 50\% of the data points. The outer (fence) polygon corresponds to points enclosed by a multiple of the bag. Points outside the fence for each species are ploted individually}{figure.caption.61}{}}
\citation{ref-Silverman:86}
\citation{ref-Scott1992}
\citation{ref-R-ggdensity}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Non-parametric bivariate density plots}{69}{section.3.4}\protected@file@percent }
\newlabel{sec-bivar-density}{{3.4}{69}{Non-parametric bivariate density plots}{section.3.4}{}}
\citation{ref-Simpson:51}
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces \textbf  {Bivariate densities} show the contours of the 3D surface representing the frequency in the joint distribution of bill length and bill depth.}}{70}{figure.caption.62}\protected@file@percent }
\newlabel{fig-peng-ggdensity}{{3.22}{70}{\textbf {Bivariate densities} show the contours of the 3D surface representing the frequency in the joint distribution of bill length and bill depth}{figure.caption.62}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Simpson's paradox: marginal and conditional relationships}{70}{section.3.5}\protected@file@percent }
\newlabel{sec-simpsons}{{3.5}{70}{Simpson's paradox: marginal and conditional relationships}{section.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces Marginal (a), conditional (b), and pooled within-sample (c) relationships of bill length and depth in the Penguins data. Each plot shows the 68\% data ellipse and regression line(s) with 95\% confidence bands.}}{71}{figure.caption.63}\protected@file@percent }
\newlabel{fig-peng-simpsons}{{3.23}{71}{Marginal (a), conditional (b), and pooled within-sample (c) relationships of bill length and depth in the Penguins data. Each plot shows the 68\% data ellipse and regression line(s) with 95\% confidence bands}{figure.caption.63}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Multivariate normality and outliers}{72}{section.3.6}\protected@file@percent }
\newlabel{sec-multivar-normality}{{3.6}{72}{Multivariate normality and outliers}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Galton data}{72}{subsection.3.6.1}\protected@file@percent }
\newlabel{galton-data}{{3.6.1}{72}{Galton data}{subsection.3.6.1}{}}
\citation{ref-Chambers-etal:83}
\@writefile{lof}{\contentsline {figure}{\numberline {3.24}{\ignorespaces Chi-square QQ plot of Galton's data on heights of parents and their offspring, with a 95\% pointwise confidence envelope}}{74}{figure.caption.64}\protected@file@percent }
\newlabel{fig-galton-cqplot}{{3.24}{74}{Chi-square QQ plot of Galton's data on heights of parents and their offspring, with a 95\% pointwise confidence envelope}{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.25}{\ignorespaces Scatterplot of Galton's data showing 68\% and 95\% data ellipses for the observations on parent and child height. The discrete points have been jittered to avoid overplotting. The three observations identified as having large \(D^2\) values are labeled with their observation number and given distinctive size, color and shape.}}{75}{figure.caption.65}\protected@file@percent }
\newlabel{fig-galton-outliers}{{3.25}{75}{Scatterplot of Galton's data showing 68\% and 95\% data ellipses for the observations on parent and child height. The discrete points have been jittered to avoid overplotting. The three observations identified as having large \(D^2\) values are labeled with their observation number and given distinctive size, color and shape}{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Penguin data}{75}{subsection.3.6.2}\protected@file@percent }
\newlabel{sec-multnorm-penguin}{{3.6.2}{75}{Penguin data}{subsection.3.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.26}{\ignorespaces Chi-square QQ plot of the Pengiun data. The three cases with the largest \(D^2\) values are identified with their case numbers.}}{76}{figure.caption.66}\protected@file@percent }
\newlabel{fig-peng-cqplot}{{3.26}{76}{Chi-square QQ plot of the Pengiun data. The three cases with the largest \(D^2\) values are identified with their case numbers}{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.27}{\ignorespaces Plot of bill length and bill depth, with the noteworthy points labeled. Only one (case 283) is a true multivariate outlier.}}{77}{figure.caption.67}\protected@file@percent }
\newlabel{fig-peng-ggplot-out}{{3.27}{77}{Plot of bill length and bill depth, with the noteworthy points labeled. Only one (case 283) is a true multivariate outlier}{figure.caption.67}{}}
\citation{ref-Hartigan:75b}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Scatterplot matrices}{78}{section.3.7}\protected@file@percent }
\newlabel{sec-scatmat}{{3.7}{78}{Scatterplot matrices}{section.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.28}{\ignorespaces Scatterplot matrix of the variables in the Prestige dataset produced by \texttt  {pairs()}}}{79}{figure.caption.68}\protected@file@percent }
\newlabel{fig-prestige-pairs}{{3.28}{79}{Scatterplot matrix of the variables in the Prestige dataset produced by \texttt {pairs()}}{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.29}{\ignorespaces Scatterplot matrix of the variables in the Prestige dataset from \texttt  {car::scatterplotMatrix()}.}}{80}{figure.caption.69}\protected@file@percent }
\newlabel{fig-prestige-spm1}{{3.29}{80}{Scatterplot matrix of the variables in the Prestige dataset from \texttt {car::scatterplotMatrix()}}{figure.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.30}{\ignorespaces Scatterplot matrix of the variables in the Prestige dataset from \texttt  {car::scatterplotMatrix()}, stratified by type of occupation.}}{81}{figure.caption.70}\protected@file@percent }
\newlabel{fig-prestige-spm2}{{3.30}{81}{Scatterplot matrix of the variables in the Prestige dataset from \texttt {car::scatterplotMatrix()}, stratified by type of occupation}{figure.caption.70}{}}
\citation{ref-Hartigan:75}
\citation{ref-Friendly:91}
\@writefile{lof}{\contentsline {figure}{\numberline {3.31}{\ignorespaces Scatterplot matrix of the quantitative variables in the penguins dataset, stratified by species.}}{83}{figure.caption.71}\protected@file@percent }
\newlabel{fig-peng-spm}{{3.31}{83}{Scatterplot matrix of the quantitative variables in the penguins dataset, stratified by species}{figure.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Visual thinning}{83}{subsection.3.7.1}\protected@file@percent }
\newlabel{visual-thinning}{{3.7.1}{83}{Visual thinning}{subsection.3.7.1}{}}
\citation{ref-Friendly:02:corrgram}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Corrgrams}{84}{section.3.8}\protected@file@percent }
\newlabel{sec-corrgram}{{3.8}{84}{Corrgrams}{section.3.8}{}}
\citation{ref-FriendlyKwan:03:effect}
\citation{ref-R-corrgram}
\citation{ref-R-corrplot}
\citation{ref-Friendly:02:corrgram}
\@writefile{lof}{\contentsline {figure}{\numberline {3.32}{\ignorespaces \textbf  {Visual thinning}: Scatterplot matrix of the crime data, showing only high-level summaries of the linear and nonlinear relations betgween each pair of variables.}}{85}{figure.caption.72}\protected@file@percent }
\newlabel{fig-crime-spm}{{3.32}{85}{\textbf {Visual thinning}: Scatterplot matrix of the crime data, showing only high-level summaries of the linear and nonlinear relations betgween each pair of variables}{figure.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.33}{\ignorespaces \textbf  {Corrgrams}: Some renderings for the value of a correlation in a corrgram display, conveying sign and magnitude in different ways.}}{86}{figure.caption.73}\protected@file@percent }
\newlabel{fig-corrgram-renderings}{{3.33}{86}{\textbf {Corrgrams}: Some renderings for the value of a correlation in a corrgram display, conveying sign and magnitude in different ways}{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.34}{\ignorespaces Mixed corrplot of the \texttt  {crime} data, showing the correlation between each pair of variables with an ellipse (lower) and a pie chart symbol (upper), all shaded in proportion to the correlation value, also shown numerically.}}{87}{figure.caption.74}\protected@file@percent }
\newlabel{fig-crime-corrplot}{{3.34}{87}{Mixed corrplot of the \texttt {crime} data, showing the correlation between each pair of variables with an ellipse (lower) and a pie chart symbol (upper), all shaded in proportion to the correlation value, also shown numerically}{figure.caption.74}{}}
\citation{ref-Friendly:94a}
\citation{ref-R-vcd}
\citation{ref-R-vcdExtra}
\citation{ref-Friendly:99:EMD}
\citation{ref-Friendly:99:EMD}
\citation{ref-FriendlyMeyer:2016:DDAR}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Generalized pairs plots}{88}{section.3.9}\protected@file@percent }
\newlabel{sec-ggpairs}{{3.9}{88}{Generalized pairs plots}{section.3.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.35}{\ignorespaces Corrplot of the \texttt  {crime} data with the variables reordered according to the angles of variable eigenvectors. Correlations are rendered with ellipses shaded in proportion to their magnitude.}}{89}{figure.caption.75}\protected@file@percent }
\newlabel{fig-crime-corrplot-AOE}{{3.35}{89}{Corrplot of the \texttt {crime} data with the variables reordered according to the angles of variable eigenvectors. Correlations are rendered with ellipses shaded in proportion to their magnitude}{figure.caption.75}{}}
\citation{ref-Emerson-etal:2013}
\citation{ref-R-GGally}
\@writefile{lof}{\contentsline {figure}{\numberline {3.36}{\ignorespaces Mosaic pairs plot for the combinations of species, sex and island. Diagnonal plots show the marginal frequency of each variable by the width of each rectangle. Off-diagonal mosaic plots subdivide by the conditional frequency of the second variable, shown numerically in the tiles. The tiles are shaded according to the departures from independence.}}{91}{figure.caption.76}\protected@file@percent }
\newlabel{fig-peng-mosaic}{{3.36}{91}{Mosaic pairs plot for the combinations of species, sex and island. Diagnonal plots show the marginal frequency of each variable by the width of each rectangle. Off-diagonal mosaic plots subdivide by the conditional frequency of the second variable, shown numerically in the tiles. The tiles are shaded according to the departures from independence}{figure.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.37}{\ignorespaces Basic \texttt  {ggpairs()} plot of penguin size variables and sex, stratified by species.}}{92}{figure.caption.77}\protected@file@percent }
\newlabel{fig-peng-ggpairs1}{{3.37}{92}{Basic \texttt {ggpairs()} plot of penguin size variables and sex, stratified by species}{figure.caption.77}{}}
\citation{ref-Ocagne:1885}
\citation{ref-Gannett:1898}
\citation{ref-Guerry:1833}
\citation{ref-Friendly2022}
\citation{ref-Inselberg:1985}
\citation{ref-Wegman:1990}
\citation{ref-Andrews:72}
\citation{ref-Friendly:91}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Parallel coordinate plots}{94}{section.3.10}\protected@file@percent }
\newlabel{sec-parcoord}{{3.10}{94}{Parallel coordinate plots}{section.3.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.38}{\ignorespaces Customized \texttt  {ggpairs()} plot of penguin size variables, together with species, island and sex.}}{95}{figure.caption.78}\protected@file@percent }
\newlabel{fig-peng-ggpairs7}{{3.38}{95}{Customized \texttt {ggpairs()} plot of penguin size variables, together with species, island and sex}{figure.caption.78}{}}
\citation{ref-R-ggpcp}
\citation{ref-VanderPlas2023}
\citation{ref-VanderPlas2023}
\@writefile{lof}{\contentsline {figure}{\numberline {3.39}{\ignorespaces Parallel coordinates plot of penguin size variables, together with sex and species.}}{97}{figure.caption.79}\protected@file@percent }
\newlabel{fig-peng-ggpcp1}{{3.39}{97}{Parallel coordinates plot of penguin size variables, together with sex and species}{figure.caption.79}{}}
\citation{ref-MartiLaguna2003}
\@writefile{lof}{\contentsline {figure}{\numberline {3.40}{\ignorespaces Parallel coordinates plot of penguin size variables, with the levels of species and island reordered.}}{98}{figure.caption.80}\protected@file@percent }
\newlabel{fig-peng-ggpcp2}{{3.40}{98}{Parallel coordinates plot of penguin size variables, with the levels of species and island reordered}{figure.caption.80}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.11}Animated tours}{98}{section.3.11}\protected@file@percent }
\newlabel{sec-animated-tours}{{3.11}{98}{Animated tours}{section.3.11}{}}
\citation{ref-Asimov:85}
\citation{ref-Hofstadter1979}
\citation{ref-Hofstadter1979}
\citation{ref-Cook-etal-2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.1}Projections}{99}{subsection.3.11.1}\protected@file@percent }
\newlabel{sec-projections}{{3.11.1}{99}{Projections}{subsection.3.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.41}{\ignorespaces The cover image from Hofstadter (\citeproc {ref-Hofstadter1979}{1979}) illustrates how projections are shadows of an object cast by a light from a given direction.}}{100}{figure.caption.81}\protected@file@percent }
\newlabel{fig-cover-GBE2}{{3.41}{100}{The cover image from Hofstadter (\citeproc {ref-Hofstadter1979}{1979}) illustrates how projections are shadows of an object cast by a light from a given direction}{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.42}{\ignorespaces Projection of a point \textbf  {x} onto a direction or axis \textbf  {p}.}}{100}{figure.caption.82}\protected@file@percent }
\newlabel{fig-projection}{{3.42}{100}{Projection of a point \textbf {x} onto a direction or axis \textbf {p}}{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.43}{\ignorespaces \textbf  {Projection example}: (a) The 8 points in \textbf  {X} form a cube of size 10; (b) the projection by \textbf  {P1} is the view ignoring \textbf  {x3} (two points coincide at each vertex); (c) the projection by \textbf  {P2} is an oblique view.}}{102}{figure.caption.83}\protected@file@percent }
\newlabel{fig-proj-combined}{{3.43}{102}{\textbf {Projection example}: (a) The 8 points in \textbf {X} form a cube of size 10; (b) the projection by \textbf {P1} is the view ignoring \textbf {x3} (two points coincide at each vertex); (c) the projection by \textbf {P2} is an oblique view}{figure.caption.83}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.44}{\ignorespaces \textbf  {Variable vectors}: Data variables viewed as vectors in the space of their projections. The angles of the \textbf  {x} vectors with respect to the \textbf  {y} coordinate axes show their relative contributions to each. The lengths of the \textbf  {x} vectors show the relative degree to which they are represented in the space of \textbf  {y}s. Left: the \textbf  {P1} projection; right: the \textbf  {P2} projection.}}{103}{figure.caption.85}\protected@file@percent }
\newlabel{fig-proj-vectors}{{3.44}{103}{\textbf {Variable vectors}: Data variables viewed as vectors in the space of their projections. The angles of the \textbf {x} vectors with respect to the \textbf {y} coordinate axes show their relative contributions to each. The lengths of the \textbf {x} vectors show the relative degree to which they are represented in the space of \textbf {y}s. Left: the \textbf {P1} projection; right: the \textbf {P2} projection}{figure.caption.85}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.11.1.1}Joint-views}{103}{subsubsection.3.11.1.1}\protected@file@percent }
\newlabel{joint-views}{{3.11.1.1}{103}{Joint-views}{subsubsection.3.11.1.1}{}}
\citation{ref-Asimov:85}
\citation{ref-Buja-etal-2005}
\citation{ref-Cook-etal-2008}
\citation{ref-Cook-etal-1995}
\citation{ref-Asimov:85}
\citation{ref-Swayne-etal-1998}
\citation{ref-CookSwayne:2007}
\citation{ref-Swayne-etal-2003}
\@writefile{lof}{\contentsline {figure}{\numberline {3.45}{\ignorespaces The \textbf  {P2} projection of the data showing vectors for the original variables in the space of \textbf  {Y}.}}{104}{figure.caption.86}\protected@file@percent }
\newlabel{fig-proj-P2-vec}{{3.45}{104}{The \textbf {P2} projection of the data showing vectors for the original variables in the space of \textbf {Y}}{figure.caption.86}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.2}Touring methods}{104}{subsection.3.11.2}\protected@file@percent }
\newlabel{touring-methods}{{3.11.2}{104}{Touring methods}{subsection.3.11.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.11.2.1}Guided tours}{104}{subsubsection.3.11.2.1}\protected@file@percent }
\newlabel{guided-tours}{{3.11.2.1}{104}{Guided tours}{subsubsection.3.11.2.1}{}}
\citation{ref-Wickham-etal-2011}
\citation{ref-R-tourr}
\@writefile{lof}{\contentsline {figure}{\numberline {3.46}{\ignorespaces \textbf  {Interpolations}: Illustration of a grand tour of interpolations of projection planes showing 2D scatterplots of the Penguin dataset. The seqeunce of views moves smoothly from an initial frame \textbf  {P(1)} to a final frame \textbf  {P(T)} where the penguin species are widely separated.}}{105}{figure.caption.87}\protected@file@percent }
\newlabel{fig-peng-tourr-diagram}{{3.46}{105}{\textbf {Interpolations}: Illustration of a grand tour of interpolations of projection planes showing 2D scatterplots of the Penguin dataset. The seqeunce of views moves smoothly from an initial frame \textbf {P(1)} to a final frame \textbf {P(T)} where the penguin species are widely separated}{figure.caption.87}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.11.2.2}\texttt  {tourr} package}{105}{subsubsection.3.11.2.2}\protected@file@percent }
\newlabel{tourr-package}{{3.11.2.2}{105}{\texorpdfstring {\texttt {tourr} package}{tourr package}}{subsubsection.3.11.2.2}{}}
\citation{ref-LeeCook-2009}
\citation{ref-Black-etal-2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.11.2.3}Penguin tours}{106}{subsubsection.3.11.2.3}\protected@file@percent }
\newlabel{penguin-tours}{{3.11.2.3}{106}{Penguin tours}{subsubsection.3.11.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.47}{\ignorespaces Grand tours of the penguin dataset in 2, 3, and 4 dimensions using different \texttt  {display\_*()} methods.}}{107}{figure.caption.88}\protected@file@percent }
\newlabel{fig-peng-tour-demo}{{3.47}{107}{Grand tours of the penguin dataset in 2, 3, and 4 dimensions using different \texttt {display\_*()} methods}{figure.caption.88}{}}
\citation{ref-CookLaa-mulgar}
\citation{ref-CookSwayne:2007}
\citation{ref-R-detourr}
\citation{ref-R-liminal}
\citation{ref-R-langevitour}
\citation{ref-Harrison2023}
\citation{ref-R-loon}
\newlabel{fig-peng-tour-grand-frames-1}{{3.48a}{108}{Initial frame}{figure.caption.89}{}}
\newlabel{fig-peng-tour-grand-frames-2}{{3.48b}{108}{Clusters}{figure.caption.89}{}}
\newlabel{fig-peng-tour-grand-frames-3}{{3.48c}{108}{Outliers}{figure.caption.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.48}{\ignorespaces Three frames from the grand tour of the Penguin data. (a) The initial frame is the projection showing only BD and BL, where bill length conveniently separates Adelie from the other two species. (b) A frame that shows the three species more widely separated. (c) A frame that shows two outliers with very large bills.}}{108}{figure.caption.89}\protected@file@percent }
\newlabel{fig-peng-tour-grand-frames}{{3.48}{108}{Three frames from the grand tour of the Penguin data. (a) The initial frame is the projection showing only BD and BL, where bill length conveniently separates Adelie from the other two species. (b) A frame that shows the three species more widely separated. (c) A frame that shows two outliers with very large bills}{figure.caption.89}{}}
\citation{ref-R-loon-tour}
\newlabel{fig-peng-tour-guided-1}{{3.49a}{109}{Optimizing \texttt {lda\_pp()}}{figure.caption.90}{}}
\newlabel{fig-peng-tour-guided-2}{{3.49b}{109}{Optimizing \texttt {anomaly\_index()}}{figure.caption.90}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.49}{\ignorespaces \textbf  {Guided tours}: These figures show the final frame in the animations of guided tours designed to find the projection that optimize an index. (a) The \texttt  {lda\_pp()} criterion optimizes the separation of the means for species relative to within-group variation. (b) The \texttt  {anomalies\_index()} optimizes the average Mahalanobis distance of points from the centroid}}{109}{figure.caption.90}\protected@file@percent }
\newlabel{fig-peng-tour-guided}{{3.49}{109}{\textbf {Guided tours}: These figures show the final frame in the animations of guided tours designed to find the projection that optimize an index. (a) The \texttt {lda\_pp()} criterion optimizes the separation of the means for species relative to within-group variation. (b) The \texttt {anomalies\_index()} optimizes the average Mahalanobis distance of points from the centroid}{figure.caption.90}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.12}Network diagrams}{109}{section.3.12}\protected@file@percent }
\newlabel{sec-network}{{3.12}{109}{Network diagrams}{section.3.12}{}}
\citation{ref-Costantini2015}
\@writefile{lof}{\contentsline {figure}{\numberline {3.50}{\ignorespaces Network diagram of the correlations among 25 items from a Big-Five personality scale, 5 items for each scale. The magnitude of a correlation is shown by the thickness and transparency of the edge between two item nodes. The sign of a correlation is shown by edge color and style: solid blue for positive and dashed red for negative. \emph  {Source}: \href  {https://bit.ly/3A6kvq5}{Rodrigues (2021)}}}{110}{figure.caption.91}\protected@file@percent }
\newlabel{fig-big5-qgraph-rodrigues}{{3.50}{110}{Network diagram of the correlations among 25 items from a Big-Five personality scale, 5 items for each scale. The magnitude of a correlation is shown by the thickness and transparency of the edge between two item nodes. The sign of a correlation is shown by edge color and style: solid blue for positive and dashed red for negative. \emph {Source}: \href {https://bit.ly/3A6kvq5}{Rodrigues (2021)}}{figure.caption.91}{}}
\citation{ref-BondyMurty2008}
\citation{ref-West2001}
\citation{ref-Grandjean2016}
\citation{ref-Barabasi2016network}
\citation{ref-IsvoranuEpskamp2022}
\citation{ref-Robinaugh2019}
\citation{ref-R-igraph}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12.1}Crime data}{111}{subsection.3.12.1}\protected@file@percent }
\newlabel{crime-data}{{3.12.1}{111}{Crime data}{subsection.3.12.1}{}}
\citation{ref-Hojsgaard2012graphical}
\citation{ref-Lauritzen1996}
\@writefile{lof}{\contentsline {figure}{\numberline {3.51}{\ignorespaces Network diagram depicting the correlations among the crime variables. Only edges for correlations that are significant at the \(\mitalpha = 0.01\) level are displayed.}}{112}{figure.caption.92}\protected@file@percent }
\newlabel{fig-crime-cor}{{3.51}{112}{Network diagram depicting the correlations among the crime variables. Only edges for correlations that are significant at the \(\alpha = 0.01\) level are displayed}{figure.caption.92}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12.2}Partial correlations}{112}{subsection.3.12.2}\protected@file@percent }
\newlabel{sec-partial-cor}{{3.12.2}{112}{Partial correlations}{subsection.3.12.2}{}}
\citation{ref-Whittaker1990}
\citation{ref-Dempster1972}
\newlabel{eq-parcor}{{3.3}{113}{Partial correlations}{section*.93}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.52}{\ignorespaces Network diagram of partial correlations among the crime variables, controlling for all others. Variable nodes have been positioned by a ``spring'' layout method \ldots  {}}}{114}{figure.caption.94}\protected@file@percent }
\newlabel{fig-crime-partial-spring}{{3.52}{114}{Network diagram of partial correlations among the crime variables, controlling for all others. Variable nodes have been positioned by a ``spring'' layout method \ldots {}}{figure.caption.94}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12.3}Visualizing partial correlations}{114}{subsection.3.12.3}\protected@file@percent }
\newlabel{sec-pvPlot}{{3.12.3}{114}{Visualizing partial correlations}{subsection.3.12.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.53}{\ignorespaces Partial variables plots for burglary and larceny (left) and for robbery and auto theft (right) in the network diagram for partial correlations of the crime variables.}}{115}{figure.caption.95}\protected@file@percent }
\newlabel{fig-crime-pvPlots}{{3.53}{115}{Partial variables plots for burglary and larceny (left) and for robbery and auto theft (right) in the network diagram for partial correlations of the crime variables}{figure.caption.95}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.13}What have we learned?}{115}{section.3.13}\protected@file@percent }
\newlabel{what-have-we-learned-1}{{3.13}{115}{What have we learned?}{section.3.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.14}Exercises}{117}{section.3.14}\protected@file@percent }
\newlabel{exercises}{{3.14}{117}{Exercises}{section.3.14}{}}
\newlabel{exr-salary-smooth}{{3.1}{117}{}{exercise.3.1}{}}
\newlabel{exr-salary-smooth2}{{3.2}{117}{}{exercise.3.2}{}}
\citation{ref-Euler:1758}
\citation{ref-Friendly-etal:ellipses:2013}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Dimension Reduction}{119}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-pca-biplot}{{4}{119}{Dimension Reduction}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}\emph  {Flatland} and \emph  {Spaceland}}{119}{section.4.1}\protected@file@percent }
\newlabel{sec-spaceland}{{4.1}{119}{\texorpdfstring {\emph {Flatland} and \emph {Spaceland}}{Flatland and Spaceland}}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Multivariate juicers}{120}{subsection.4.1.1}\protected@file@percent }
\newlabel{multivariate-juicers}{{4.1.1}{120}{Multivariate juicers}{subsection.4.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A multivariate juicer takes data from possibly high-dimensional data space and transforms it to a lower-dimenional space in which important effects can be more easily seen.}}{120}{figure.caption.96}\protected@file@percent }
\newlabel{fig-MV-juicer}{{4.1}{120}{A multivariate juicer takes data from possibly high-dimensional data space and transforms it to a lower-dimenional space in which important effects can be more easily seen}{figure.caption.96}{}}
\citation{ref-Galton:1886}
\citation{ref-Pearson:1896}
\citation{ref-Pearson:1901}
\citation{ref-FriendlyWainer:2021:TOGS}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Principal components analysis (PCA)}{121}{section.4.2}\protected@file@percent }
\newlabel{sec-pca}{{4.2}{121}{Principal components analysis (PCA)}{section.4.2}{}}
\citation{ref-Galton:1886}
\citation{ref-Pearson:1901}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Karl Pearson's (1901) geometric, visual argument for finding the line or plane of closest fit to a collection of points, P1, P2, P3, \ldots  {}}}{122}{figure.caption.97}\protected@file@percent }
\newlabel{fig-Pearson1901}{{4.2}{122}{Karl Pearson's (1901) geometric, visual argument for finding the line or plane of closest fit to a collection of points, P1, P2, P3, \ldots {}}{figure.caption.97}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Karl Pearson's diagram showing the elliptical geometry of regression and principal components analysis \ldots  {} \emph  {Source}: Pearson (1901), p.~566.}}{123}{figure.caption.98}\protected@file@percent }
\newlabel{fig-Pearson1901-2}{{4.3}{123}{Karl Pearson's diagram showing the elliptical geometry of regression and principal components analysis \ldots {} \emph {Source}: Pearson (1901), p.~566}{figure.caption.98}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}PCA by springs}{123}{subsection.4.2.1}\protected@file@percent }
\newlabel{pca-by-springs}{{4.2.1}{123}{PCA by springs}{subsection.4.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces First and last frames of an animation of PCA as a rotation in 3D space. The plot shows three variables for the \texttt  {iris} data, initially in data space and its' data ellipsoid, with points colored according to species of the iris flowers. This is rotated smoothly until the first two principal axes are aligned with the horizontal and vertical directions in the final frame.}}{124}{figure.caption.99}\protected@file@percent }
\newlabel{fig-pca-animation-pdf}{{4.4}{124}{First and last frames of an animation of PCA as a rotation in 3D space. The plot shows three variables for the \texttt {iris} data, initially in data space and its' data ellipsoid, with points colored according to species of the iris flowers. This is rotated smoothly until the first two principal axes are aligned with the horizontal and vertical directions in the final frame}{figure.caption.99}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Three frames from an animation of PCA fitted by springs. The blue data points are connected to their projections on the red line by springs perpendicular to that line. From an initial position, the springs pull that line in proportion to their squared distances, until the line finally settles down to the position where the forces are balanced and the minimum is achieved. \emph  {Source}: Amoeba, \url  {https://bit.ly/46tAicu}.}}{124}{figure.caption.100}\protected@file@percent }
\newlabel{fig-pca-springs}{{4.5}{124}{Three frames from an animation of PCA fitted by springs. The blue data points are connected to their projections on the red line by springs perpendicular to that line. From an initial position, the springs pull that line in proportion to their squared distances, until the line finally settles down to the position where the forces are balanced and the minimum is achieved. \emph {Source}: Amoeba, \url {https://bit.ly/46tAicu}}{figure.caption.100}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Mathematics and geometry of PCA}{125}{subsection.4.2.2}\protected@file@percent }
\newlabel{mathematics-and-geometry-of-pca}{{4.2.2}{125}{Mathematics and geometry of PCA}{subsection.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Geometry of PCA as a rotation from data space to principal component space, defined by the eigenvectors v1 and v2 of a covariance matrix}}{126}{figure.caption.101}\protected@file@percent }
\newlabel{fig-pca-rotation}{{4.6}{126}{Geometry of PCA as a rotation from data space to principal component space, defined by the eigenvectors v1 and v2 of a covariance matrix}{figure.caption.101}{}}
\newlabel{exm-workers-pca}{{4.1}{126}{}{example.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Scatterplot of Income vs.~Experience for the \texttt  {workers} data.}}{127}{figure.caption.102}\protected@file@percent }
\newlabel{fig-workers-scat}{{4.7}{127}{Scatterplot of Income vs.~Experience for the \texttt {workers} data}{figure.caption.102}{}}
\citation{ref-R-matlib}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Geometry of the PCA for the \texttt  {workers} data, showing the data ellipse, the eigenvectors of \(\mathbf {S}\), whose half-lengths are the square roots \(\sqrt  {\mitlambda _i}\) of the eigenvalues, and the bounding box of the ellipse.}}{129}{figure.caption.103}\protected@file@percent }
\newlabel{fig-workers-pca}{{4.8}{129}{Geometry of the PCA for the \texttt {workers} data, showing the data ellipse, the eigenvectors of \(\mathbf {S}\), whose half-lengths are the square roots \(\sqrt {\lambda _i}\) of the eigenvalues, and the bounding box of the ellipse}{figure.caption.103}{}}
\citation{ref-Husson-etal-2017}
\citation{ref-R-FactoMineR}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Finding principal components}{130}{subsection.4.2.3}\protected@file@percent }
\newlabel{finding-principal-components}{{4.2.3}{130}{Finding principal components}{subsection.4.2.3}{}}
\newlabel{example-crime-data}{{4.2.3}{131}{Example: Crime data}{section*.104}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Crime data}{131}{section*.104}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Visualizing variance proportions: screeplots}{132}{subsection.4.2.4}\protected@file@percent }
\newlabel{visualizing-variance-proportions-screeplots}{{4.2.4}{132}{Visualizing variance proportions: screeplots}{subsection.4.2.4}{}}
\citation{ref-Cattell1966}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Visualizing PCA scores and variable vectors}{133}{subsection.4.2.5}\protected@file@percent }
\newlabel{visualizing-pca-scores-and-variable-vectors}{{4.2.5}{133}{Visualizing PCA scores and variable vectors}{subsection.4.2.5}{}}
\newlabel{scores}{{4.2.5}{133}{Scores}{section*.106}{}}
\@writefile{toc}{\contentsline {subsubsection}{Scores}{133}{section*.106}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Screeplots for the PCA of the crime data. The left panel shows the traditional version, plotting variance proportions against component number, with linear guideline for the scree rule of thumb. The right panel plots cumulative proportions, showing cutoffs of 80\%, 90\%.}}{134}{figure.caption.105}\protected@file@percent }
\newlabel{fig-crime-ggscreeplot}{{4.9}{134}{Screeplots for the PCA of the crime data. The left panel shows the traditional version, plotting variance proportions against component number, with linear guideline for the scree rule of thumb. The right panel plots cumulative proportions, showing cutoffs of 80\%, 90\%}{figure.caption.105}{}}
\newlabel{variable-vectors}{{4.2.5}{135}{Variable vectors}{section*.108}{}}
\@writefile{toc}{\contentsline {subsubsection}{Variable vectors}{135}{section*.108}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Plot of component scores on the first two principal components for the \texttt  {crime} data. States are colored by \texttt  {region}.}}{136}{figure.caption.107}\protected@file@percent }
\newlabel{fig-crime-scores-plot12}{{4.10}{136}{Plot of component scores on the first two principal components for the \texttt {crime} data. States are colored by \texttt {region}}{figure.caption.107}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Plot of component loadings the first two principal components for the \texttt  {crime} data. These are interpreted as the contributions of the variables to the components.}}{138}{figure.caption.109}\protected@file@percent }
\newlabel{fig-crime-vectors}{{4.11}{138}{Plot of component loadings the first two principal components for the \texttt {crime} data. These are interpreted as the contributions of the variables to the components}{figure.caption.109}{}}
\citation{ref-Gabriel:71}
\citation{ref-Gabriel:81}
\citation{ref-GowerHand:96}
\citation{ref-Greenacre:2010:biplots}
\citation{ref-Gower-etal:2011}
\citation{ref-FriendlyMeyer:2016:DDAR}
\citation{ref-Greenacre:84}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Biplots}{139}{section.4.3}\protected@file@percent }
\newlabel{sec-biplot}{{4.3}{139}{Biplots}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Constructing a biplot}{139}{subsection.4.3.1}\protected@file@percent }
\newlabel{sec-biplot-svd}{{4.3.1}{139}{Constructing a biplot}{subsection.4.3.1}{}}
\newlabel{eq-svd1}{{4.4}{139}{Constructing a biplot}{section*.111}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces The singular value decomposition expresses a data matrix \textbf  {X} as the product of a matrix \textbf  {U} of observation scores, a diagonal matrix \(\mitLambda \) of singular values and a matrix \textbf  {V} of variable weights.}}{140}{figure.caption.110}\protected@file@percent }
\newlabel{fig-svd-diagram}{{4.12}{140}{The singular value decomposition expresses a data matrix \textbf {X} as the product of a matrix \textbf {U} of observation scores, a diagonal matrix \(\Lambda \) of singular values and a matrix \textbf {V} of variable weights}{figure.caption.110}{}}
\citation{ref-R-factoextra}
\citation{ref-R-FactoMineR}
\citation{ref-R-adegraphics}
\citation{ref-R-ggbiplot}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Biplots in R}{141}{subsection.4.3.2}\protected@file@percent }
\newlabel{biplots-in-r}{{4.3.2}{141}{Biplots in R}{subsection.4.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Example: Crime data}{141}{subsection.4.3.3}\protected@file@percent }
\newlabel{example-crime-data-1}{{4.3.3}{141}{Example: Crime data}{subsection.4.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Basic biplot of the crime data. State abbreviations are shown at their standardized scores on the first two dimensions. The variable vectors reflect the correlations of the variables with the biplot dimensions.}}{142}{figure.caption.112}\protected@file@percent }
\newlabel{fig-crime-biplot1}{{4.13}{142}{Basic biplot of the crime data. State abbreviations are shown at their standardized scores on the first two dimensions. The variable vectors reflect the correlations of the variables with the biplot dimensions}{figure.caption.112}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Enhanced biplot of the crime data, grouping the states by region and adding data ellipses.}}{143}{figure.caption.113}\protected@file@percent }
\newlabel{fig-crime-biplot2}{{4.14}{143}{Enhanced biplot of the crime data, grouping the states by region and adding data ellipses}{figure.caption.113}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Biplot of dimensions 1 \& 3 of the crime data, with data ellipses for the regions.}}{144}{figure.caption.114}\protected@file@percent }
\newlabel{fig-crime-biplot3}{{4.15}{144}{Biplot of dimensions 1 \& 3 of the crime data, with data ellipses for the regions}{figure.caption.114}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Biplot contributions and quality}{144}{subsection.4.3.4}\protected@file@percent }
\newlabel{biplot-contributions-and-quality}{{4.3.4}{144}{Biplot contributions and quality}{subsection.4.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Contributions of the crime variables to dimensions 1 (left) \& 2 (right) of the PCA solution}}{145}{figure.caption.115}\protected@file@percent }
\newlabel{fig-fviz-contrib}{{4.16}{145}{Contributions of the crime variables to dimensions 1 (left) \& 2 (right) of the PCA solution}{figure.caption.115}{}}
\citation{ref-Aluja-etal-2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Supplementary variables}{146}{subsection.4.3.5}\protected@file@percent }
\newlabel{sec-supp-vars}{{4.3.5}{146}{Supplementary variables}{subsection.4.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Fitting supplementary variables in a biplot is analogous (right) to regression on the principal component dimensions (left). \emph  {Source}: Aluja et al. (\citeproc {ref-Aluja-etal-2018}{2018}), Figure 2.11}}{147}{figure.caption.116}\protected@file@percent }
\newlabel{fig-supp-regession}{{4.17}{147}{Fitting supplementary variables in a biplot is analogous (right) to regression on the principal component dimensions (left). \emph {Source}: Aluja et al. (\citeproc {ref-Aluja-etal-2018}{2018}), Figure 2.11}{figure.caption.116}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces PCA plot of variables for the crime data, with vectors for the supplementary variables showing their association with the principal component dimensions.}}{149}{figure.caption.117}\protected@file@percent }
\newlabel{fig-crime-factominer}{{4.18}{149}{PCA plot of variables for the crime data, with vectors for the supplementary variables showing their association with the principal component dimensions}{figure.caption.117}{}}
\citation{ref-ReavenMiller:79}
\citation{ref-R-rgl}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Example: Diabetes data}{150}{subsection.4.3.6}\protected@file@percent }
\newlabel{example-diabetes-data}{{4.3.6}{150}{Example: Diabetes data}{subsection.4.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Two views of a 3D scatterplot of three main diagnostic variables in the \texttt  {Diabetes} dataset. The left panel shows an orientation similar to that of Figure~\ref {fig-ReavenMiller-3d}; the right panel shows a view from the back.}}{151}{figure.caption.118}\protected@file@percent }
\newlabel{fig-diabetes-3d}{{4.19}{151}{Two views of a 3D scatterplot of three main diagnostic variables in the \texttt {Diabetes} dataset. The left panel shows an orientation similar to that of Figure~\ref {fig-ReavenMiller-3d}; the right panel shows a view from the back}{figure.caption.118}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces 2D biplot of the Diabetes data}}{152}{figure.caption.119}\protected@file@percent }
\newlabel{fig-diabetes-ggbiplot}{{4.20}{152}{2D biplot of the Diabetes data}{figure.caption.119}{}}
\citation{ref-Torgerson1952}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Nonlinear dimension reduction}{153}{section.4.4}\protected@file@percent }
\newlabel{sec-nonlinear}{{4.4}{153}{Nonlinear dimension reduction}{section.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces *Nonlinear patterns**: Two representations of the same data are shown. In the plot at the left, the clusters are clear to the eye, but there is no linear relation that separates them. Transforming the data nonlinearly, to polar coordinates in the plot at the right, makes the two groups distinct.}}{153}{figure.caption.120}\protected@file@percent }
\newlabel{fig-nonlin-demo}{{4.21}{153}{*Nonlinear patterns**: Two representations of the same data are shown. In the plot at the left, the clusters are clear to the eye, but there is no linear relation that separates them. Transforming the data nonlinearly, to polar coordinates in the plot at the right, makes the two groups distinct}{figure.caption.120}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Multidimensional scaling}{153}{subsection.4.4.1}\protected@file@percent }
\newlabel{multidimensional-scaling}{{4.4.1}{153}{Multidimensional scaling}{subsection.4.4.1}{}}
\citation{ref-Kruskal1964}
\citation{ref-Shepard1962a}
\citation{ref-Shepard1962b}
\citation{ref-BorgGroenen2005}
\citation{ref-Borg2018}
\citation{ref-Shepard-etal-1972a}
\citation{ref-Shepard-etal-1972b}
\citation{ref-Shoben1983}
\citation{ref-R-vegan}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Badness of fit (Stress) of the MDS solution in relation to number of dimensions.}}{155}{figure.caption.121}\protected@file@percent }
\newlabel{fig-diabetes-stress}{{4.22}{155}{Badness of fit (Stress) of the MDS solution in relation to number of dimensions}{figure.caption.121}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Nonmetric MDS representation of the Diabetes data. The vectors reflect the correlations of the variables with the MDS dimensions.}}{156}{figure.caption.122}\protected@file@percent }
\newlabel{fig-diabetes-mds}{{4.23}{156}{Nonmetric MDS representation of the Diabetes data. The vectors reflect the correlations of the variables with the MDS dimensions}{figure.caption.122}{}}
\citation{ref-MaatenHinton2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}t-SNE}{157}{subsection.4.4.2}\protected@file@percent }
\newlabel{t-sne}{{4.4.2}{157}{t-SNE}{subsection.4.4.2}{}}
\citation{ref-R-Rtsne}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2.1}Comparing solutions}{158}{subsubsection.4.4.2.1}\protected@file@percent }
\newlabel{compare-solutions}{{4.4.2.1}{158}{Comparing solutions}{subsubsection.4.4.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces t-SNE representation of the Diabetes data.}}{159}{figure.caption.123}\protected@file@percent }
\newlabel{fig-diabetes-tsne}{{4.24}{159}{t-SNE representation of the Diabetes data}{figure.caption.123}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Comparison of the PCA and t-SNE 2D representations of the Diabetes data.}}{159}{figure.caption.124}\protected@file@percent }
\newlabel{fig-diabetes-pca-tsne}{{4.25}{159}{Comparison of the PCA and t-SNE 2D representations of the Diabetes data}{figure.caption.124}{}}
\citation{ref-R-gganimate}
\citation{ref-FriendlyKwan:03:effect}
\citation{ref-R-seriation}
\citation{ref-FriendlyKwan:03:effect}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Application: Variable ordering for data displays}{161}{section.4.5}\protected@file@percent }
\newlabel{sec-var-order}{{4.5}{161}{Application: Variable ordering for data displays}{section.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces Corrplot of \texttt  {mtcars} data, with the variables arranged in the order they appear in the dataset.}}{162}{figure.caption.125}\protected@file@percent }
\newlabel{fig-mtcars-corrplot-varorder}{{4.26}{162}{Corrplot of \texttt {mtcars} data, with the variables arranged in the order they appear in the dataset}{figure.caption.125}{}}
\newlabel{eq-angle-AOE}{{4.5}{162}{Application: Variable ordering for data displays}{section*.126}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces Biplot of the \texttt  {mtcars} data. The order of the variables around the circle, starting from ``gear'' (say) arranges them so that the most similar variables are adjacent in graphical displays.}}{163}{figure.caption.127}\protected@file@percent }
\newlabel{fig-mtcars-biplot}{{4.27}{163}{Biplot of the \texttt {mtcars} data. The order of the variables around the circle, starting from ``gear'' (say) arranges them so that the most similar variables are adjacent in graphical displays}{figure.caption.127}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces Corrplot of \texttt  {mtcars} data, with the variables ordered according to the variable vectors in the biplot.}}{164}{figure.caption.128}\protected@file@percent }
\newlabel{fig-mtcars-corrplot-pcaorder}{{4.28}{164}{Corrplot of \texttt {mtcars} data, with the variables ordered according to the variable vectors in the biplot}{figure.caption.128}{}}
\citation{ref-Turk1991}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Application: Eigenfaces}{165}{section.4.6}\protected@file@percent }
\newlabel{sec-eigenfaces}{{4.6}{165}{Application: Eigenfaces}{section.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces 640 x 954 black and white image of the \emph  {Mona Lisa}. Source: \href  {https://bit.ly/3Rgo41f}{Wikipedia}}}{166}{figure.caption.129}\protected@file@percent }
\newlabel{fig-MonaLisa}{{4.29}{166}{640 x 954 black and white image of the \emph {Mona Lisa}. Source: \href {https://bit.ly/3Rgo41f}{Wikipedia}}{figure.caption.129}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces Screeplot of the variance proportions in the Mona Lisa PCA.}}{168}{figure.caption.130}\protected@file@percent }
\newlabel{fig-mona-screeplot}{{4.30}{168}{Screeplot of the variance proportions in the Mona Lisa PCA}{figure.caption.130}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Elliptical insights: Outlier detection}{170}{section.4.7}\protected@file@percent }
\newlabel{sec-outlier-detection}{{4.7}{170}{Elliptical insights: Outlier detection}{section.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Example: Penguin data}{170}{subsection.4.7.1}\protected@file@percent }
\newlabel{sec-dsq-penguin}{{4.7.1}{170}{Example: Penguin data}{subsection.4.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}What have we learned?}{172}{section.4.8}\protected@file@percent }
\newlabel{what-have-we-learned-2}{{4.8}{172}{What have we learned?}{section.4.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.31}{\ignorespaces Re-construction of the Mona Lisa using 2, 3 , 4, 5, 10, 15, 20, 50, and 100 principal components.}}{173}{figure.caption.131}\protected@file@percent }
\newlabel{fig-mona-pca}{{4.31}{173}{Re-construction of the Mona Lisa using 2, 3 , 4, 5, 10, 15, 20, 50, and 100 principal components}{figure.caption.131}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.32}{\ignorespaces \textbf  {Outlier demonstration}: The left panel shows the original data and highlights the two discrepant points, which do not appear to be unusual on either x or y. The right panel shows the data rotated to principal components, where the labeled points stand out on the smallest PCA dimension.}}{174}{figure.caption.132}\protected@file@percent }
\newlabel{fig-outlier-demo}{{4.32}{174}{\textbf {Outlier demonstration}: The left panel shows the original data and highlights the two discrepant points, which do not appear to be unusual on either x or y. The right panel shows the data rotated to principal components, where the labeled points stand out on the smallest PCA dimension}{figure.caption.132}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.33}{\ignorespaces Biplot of the first two dimensions of the Penguin data. The points for the three noteworthy cases are labeled, but none of these appear to be unusual in this view.}}{175}{figure.caption.133}\protected@file@percent }
\newlabel{fig-peng-out-biplot1}{{4.33}{175}{Biplot of the first two dimensions of the Penguin data. The points for the three noteworthy cases are labeled, but none of these appear to be unusual in this view}{figure.caption.133}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.34}{\ignorespaces Biplot of dimensions 3-4 of the Penguin data. The three noteworthy birds stand out in this view.}}{176}{figure.caption.134}\protected@file@percent }
\newlabel{fig-peng-out-biplot2}{{4.34}{176}{Biplot of dimensions 3-4 of the Penguin data. The three noteworthy birds stand out in this view}{figure.caption.134}{}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Univariate Linear Models}{177}{part.3}\protected@file@percent }
\citation{ref-Galton:1886}
\citation{ref-Galton:1889}
\citation{ref-Pearson:1896}
\citation{ref-Fisher1923}
\citation{ref-Fisher:25}
\citation{ref-Scheffe1960}
\citation{ref-Graybill1961}
\citation{ref-Winer1962}
\citation{ref-IBM1965}
\citation{ref-BradshawFindley1967}
\citation{ref-Bock1963}
\citation{ref-Bock1964}
\citation{ref-Finn1967}
\citation{ref-Clyde-etal-1966}
\citation{ref-Dixon1965}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Overview of Linear models}{179}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-linear-models}{{5}{179}{Overview of Linear models}{chapter.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Techniques for linear models classified by number of predictors and number of response variables, and whether these are quantitative vs.~discrete}}{180}{figure.caption.135}\protected@file@percent }
\newlabel{fig-techniques}{{5.1}{180}{Techniques for linear models classified by number of predictors and number of response variables, and whether these are quantitative vs.~discrete}{figure.caption.135}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}The General Linear Model}{181}{section.5.1}\protected@file@percent }
\newlabel{sec-GLM}{{5.1}{181}{The General Linear Model}{section.5.1}{}}
\citation{ref-WilkinsonRogers1973}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Model formulas}{182}{subsection.5.1.1}\protected@file@percent }
\newlabel{sec-model-formulas}{{5.1.1}{182}{Model formulas}{subsection.5.1.1}{}}
\newlabel{exm-workers1}{{5.1}{183}{}{example.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Workers data with fitted linear and quadratic models for years of experience.}}{184}{figure.caption.136}\protected@file@percent }
\newlabel{fig-workers-fits}{{5.2}{184}{Workers data with fitted linear and quadratic models for years of experience}{figure.caption.136}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1.1}Factors}{185}{subsubsection.5.1.1.1}\protected@file@percent }
\newlabel{factors}{{5.1.1.1}{185}{Factors}{subsubsection.5.1.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1.2}Crossing}{185}{subsubsection.5.1.1.2}\protected@file@percent }
\newlabel{crossing}{{5.1.1.2}{185}{Crossing}{subsubsection.5.1.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1.3}Powers}{186}{subsubsection.5.1.1.3}\protected@file@percent }
\newlabel{powers}{{5.1.1.3}{186}{Powers}{subsubsection.5.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Model matrices}{187}{subsection.5.1.2}\protected@file@percent }
\newlabel{model-matrices}{{5.1.2}{187}{Model matrices}{subsection.5.1.2}{}}
\citation{ref-FoxWeisberg:2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Coding factors and contrasts}{188}{subsection.5.1.3}\protected@file@percent }
\newlabel{sec-contrasts}{{5.1.3}{188}{Coding factors and contrasts}{subsection.5.1.3}{}}
\newlabel{treatment-coding}{{5.1.3}{189}{Treatment coding}{section*.137}{}}
\@writefile{toc}{\contentsline {subsubsection}{Treatment coding}{189}{section*.137}\protected@file@percent }
\newlabel{deviation-coding}{{5.1.3}{190}{Deviation coding}{section*.138}{}}
\@writefile{toc}{\contentsline {subsubsection}{Deviation coding}{190}{section*.138}\protected@file@percent }
\newlabel{helmert-contrasts}{{5.1.3}{190}{Helmert contrasts}{section*.139}{}}
\@writefile{toc}{\contentsline {subsubsection}{Helmert contrasts}{190}{section*.139}\protected@file@percent }
\newlabel{polynomial-contrasts}{{5.1.3}{191}{Polynomial contrasts}{section*.140}{}}
\@writefile{toc}{\contentsline {subsubsection}{Polynomial contrasts}{191}{section*.140}\protected@file@percent }
\newlabel{custom-contrasts}{{5.1.3}{192}{Custom contrasts}{section*.142}{}}
\@writefile{toc}{\contentsline {subsubsection}{Custom contrasts}{192}{section*.142}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The coefficients for orthogonal polynomial contrasts for linear (1), quadratic (2) and cubic (3) terms for a numeric variable \(X\). The intercept or constant term is represented as 0. Orthogonality means that each pair of values is uncorrelated.}}{193}{figure.caption.141}\protected@file@percent }
\newlabel{fig-poly}{{5.3}{193}{The coefficients for orthogonal polynomial contrasts for linear (1), quadratic (2) and cubic (3) terms for a numeric variable \(X\). The intercept or constant term is represented as 0. Orthogonality means that each pair of values is uncorrelated}{figure.caption.141}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}What have we learned?}{194}{section.5.2}\protected@file@percent }
\newlabel{what-have-we-learned-3}{{5.2}{194}{What have we learned?}{section.5.2}{}}
\citation{ref-Belsley-etal:80}
\citation{ref-Fox2020}
\citation{ref-FoxWeisberg:2018}
\citation{ref-R-car}
\citation{ref-FoxWeisberg:2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Plots for Univariate Response Models}{197}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-linear-models-plots}{{6}{197}{Plots for Univariate Response Models}{chapter.6}{}}
\citation{ref-R-car}
\citation{ref-R-performance}
\citation{ref-R-easystats}
\citation{ref-Duncan:61}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}The ``regression quartet''}{198}{section.6.1}\protected@file@percent }
\newlabel{sec-regression-quartet}{{6.1}{198}{The ``regression quartet''}{section.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Example: Duncan's occupational prestige}{198}{subsection.6.1.1}\protected@file@percent }
\newlabel{sec-example-duncan}{{6.1.1}{198}{Example: Duncan's occupational prestige}{subsection.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Joint 95\% confidence ellipse for \((\mitbeta _{\text  {Inc}}, \mitbeta _{\text  {Educ}})\), together with their 1D shadows, which give 95\% confidence intervals for the separate coefficients and the linear hypothesis that the coefficients are equal. Projecting the confidence ellipse along the line with unit slope gives a confidence interval for the difference between coefficients, shown by the dark red line.}}{202}{figure.caption.143}\protected@file@percent }
\newlabel{fig-duncan-beta-diff}{{6.1}{202}{Joint 95\% confidence ellipse for \((\beta _{\text {Inc}}, \beta _{\text {Educ}})\), together with their 1D shadows, which give 95\% confidence intervals for the separate coefficients and the linear hypothesis that the coefficients are equal. Projecting the confidence ellipse along the line with unit slope gives a confidence interval for the difference between coefficients, shown by the dark red line}{figure.caption.143}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Diagnostic plots}{202}{subsection.6.1.2}\protected@file@percent }
\newlabel{sec-diagnostic-plots}{{6.1.2}{202}{Diagnostic plots}{subsection.6.1.2}{}}
\citation{ref-Duncan:61}
\citation{ref-Pineo-Porter-1967}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Regression quartet of diagnostic plots for the \texttt  {Duncan} data. Several possibly unusual observations are labeled.}}{203}{figure.caption.144}\protected@file@percent }
\newlabel{fig-duncan-plot-model}{{6.2}{203}{Regression quartet of diagnostic plots for the \texttt {Duncan} data. Several possibly unusual observations are labeled}{figure.caption.144}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Example: Canadian occupational prestige}{203}{subsection.6.1.3}\protected@file@percent }
\newlabel{sec-example-prestige}{{6.1.3}{203}{Example: Canadian occupational prestige}{subsection.6.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Diagnostic plots for the \texttt  {Duncan} data, using \texttt  {performance::check\_model()}.}}{204}{figure.caption.145}\protected@file@percent }
\newlabel{fig-duncan-check-model}{{6.3}{204}{Diagnostic plots for the \texttt {Duncan} data, using \texttt {performance::check\_model()}}{figure.caption.145}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Regression quartet of diagnostic plots for the \texttt  {Prestige} data. Several possibly unusual observations are labeled in each plot.}}{205}{figure.caption.146}\protected@file@percent }
\newlabel{fig-plot-prestige-mod}{{6.4}{205}{Regression quartet of diagnostic plots for the \texttt {Prestige} data. Several possibly unusual observations are labeled in each plot}{figure.caption.146}{}}
\citation{ref-KastellecLeoni:2007}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Other Model plots}{206}{section.6.2}\protected@file@percent }
\newlabel{other-model-plots}{{6.2}{206}{Other Model plots}{section.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Coefficient displays}{206}{section.6.3}\protected@file@percent }
\newlabel{sec-coefficient-displays}{{6.3}{206}{Coefficient displays}{section.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Displaying coefficients}{206}{subsection.6.3.1}\protected@file@percent }
\newlabel{displaying-coefficients}{{6.3.1}{206}{Displaying coefficients}{subsection.6.3.1}{}}
\citation{ref-R-modelsummary}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Table of coefficients for the main effects model.}}{207}{table.caption.147}\protected@file@percent }
\newlabel{tbl-modelsummary1}{{6.1}{207}{Table of coefficients for the main effects model}{table.caption.147}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Visualizing coefficients}{207}{subsection.6.3.2}\protected@file@percent }
\newlabel{visualizing-coefficients}{{6.3.2}{207}{Visualizing coefficients}{subsection.6.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Plot of coefficients and their standard error bars for the simple main effects model}}{208}{figure.caption.148}\protected@file@percent }
\newlabel{fig-modelplot1}{{6.5}{208}{Plot of coefficients and their standard error bars for the simple main effects model}{figure.caption.148}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Table of coefficients for three models.}}{209}{table.caption.149}\protected@file@percent }
\newlabel{tbl-modelsummary2}{{6.2}{209}{Table of coefficients for three models}{table.caption.149}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}More useful coefficient plots}{209}{subsection.6.3.3}\protected@file@percent }
\newlabel{more-useful-coefficient-plots}{{6.3.3}{209}{More useful coefficient plots}{subsection.6.3.3}{}}
\newlabel{standardized-coefficients}{{6.3.3}{209}{Standardized coefficients}{section*.151}{}}
\@writefile{toc}{\contentsline {subsubsection}{Standardized coefficients}{209}{section*.151}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Plot of raw coefficients and their confidence intervals for all three models}}{210}{figure.caption.150}\protected@file@percent }
\newlabel{fig-modelplot2}{{6.6}{210}{Plot of raw coefficients and their confidence intervals for all three models}{figure.caption.150}{}}
\citation{ref-R-ggstats}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Plot of standardized coefficients and their confidence intervals for all three models}}{211}{figure.caption.152}\protected@file@percent }
\newlabel{fig-modelplot3}{{6.7}{211}{Plot of standardized coefficients and their confidence intervals for all three models}{figure.caption.152}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Model comparison plot from \texttt  {ggcoef\_compare()}}}{212}{figure.caption.153}\protected@file@percent }
\newlabel{fig-ggcoef-compare}{{6.8}{212}{Model comparison plot from \texttt {ggcoef\_compare()}}{figure.caption.153}{}}
\newlabel{more-meaningful-units}{{6.3.3}{212}{More meaningful units}{section*.154}{}}
\@writefile{toc}{\contentsline {subsubsection}{More meaningful units}{212}{section*.154}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Plot of coefficients for prestige with scaled predictors for Model 1.}}{213}{figure.caption.155}\protected@file@percent }
\newlabel{fig-ggcoef-compare2}{{6.9}{213}{Plot of coefficients for prestige with scaled predictors for Model 1}{figure.caption.155}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Added-variable and related plots}{214}{section.6.4}\protected@file@percent }
\newlabel{sec-avplots}{{6.4}{214}{Added-variable and related plots}{section.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Scatterplot matrix showing pairwise relations among \texttt  {Heart} damage (\(y\)), \texttt  {Coffee} consumption (\(x_1\)) and \texttt  {Stress} (\(x_2\)), with linear regression lines and 68\% data ellipses for the bivariate relations}}{215}{figure.caption.156}\protected@file@percent }
\newlabel{fig-coffee-spm}{{6.10}{215}{Scatterplot matrix showing pairwise relations among \texttt {Heart} damage (\(y\)), \texttt {Coffee} consumption (\(x_1\)) and \texttt {Stress} (\(x_2\)), with linear regression lines and 68\% data ellipses for the bivariate relations}{figure.caption.156}{}}
\citation{ref-MostellerTukey-1977}
\citation{ref-VellemanWelsh:81}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Added-variable plots for Coffee and Stress in the multiple regression model}}{217}{figure.caption.157}\protected@file@percent }
\newlabel{fig-coffee-avPlots}{{6.11}{217}{Added-variable plots for Coffee and Stress in the multiple regression model}{figure.caption.157}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Properties of AV plots}{217}{subsection.6.4.1}\protected@file@percent }
\newlabel{properties-of-av-plots}{{6.4.1}{217}{Properties of AV plots}{subsection.6.4.1}{}}
\citation{ref-CookWeisberg-1994}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Marginal - conditional plots}{218}{subsection.6.4.2}\protected@file@percent }
\newlabel{sec-marginal-conditional}{{6.4.2}{218}{Marginal - conditional plots}{subsection.6.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Marginal \(+\) conditional (added-variable) plots for Coffee and Stress in the multiple regression predicting Heart disease. Each panel shows the 68\% conditional data ellipse for \(x_i^\star , y^\star \) residuals (shaded, blue) as well as the marginal 68\% data ellipse for the \((x_i, y)\) variables, shifted to the origin. Arrows connect the mean-centered marginal points (red) to the residual points (blue).}}{219}{figure.caption.158}\protected@file@percent }
\newlabel{fig-coffee-mcplot}{{6.12}{219}{Marginal \(+\) conditional (added-variable) plots for Coffee and Stress in the multiple regression predicting Heart disease. Each panel shows the 68\% conditional data ellipse for \(x_i^\star , y^\star \) residuals (shaded, blue) as well as the marginal 68\% data ellipse for the \((x_i, y)\) variables, shifted to the origin. Arrows connect the mean-centered marginal points (red) to the residual points (blue)}{figure.caption.158}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Prestige data}{219}{subsection.6.4.3}\protected@file@percent }
\newlabel{prestige-data}{{6.4.3}{219}{Prestige data}{subsection.6.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Added-variable plot for the quantitative predictors in the \texttt  {Prestige} data.}}{220}{figure.caption.159}\protected@file@percent }
\newlabel{fig-prestige-avplot}{{6.13}{220}{Added-variable plot for the quantitative predictors in the \texttt {Prestige} data}{figure.caption.159}{}}
\citation{ref-Cook-1996}
\citation{ref-LarsenMcCleary:72}
\citation{ref-Cook:93}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Added-variable plot for income, with a loess smooth.}}{221}{figure.caption.160}\protected@file@percent }
\newlabel{fig-prestige-av-income}{{6.14}{221}{Added-variable plot for income, with a loess smooth}{figure.caption.160}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Component + Residual plots}{221}{subsection.6.4.4}\protected@file@percent }
\newlabel{component-residual-plots}{{6.4.4}{221}{Component + Residual plots}{subsection.6.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Component + residual plot for income in the model for the quantitative predictors of prestige. The dashed \textcolor {blue}{blue} line is the partial linear fit for income. The solid \textcolor {red}{red} curve is the loess smooth.}}{222}{figure.caption.161}\protected@file@percent }
\newlabel{fig-prestige-crplot}{{6.15}{222}{Component + residual plot for income in the model for the quantitative predictors of prestige. The dashed \textcolor {blue}{blue} line is the partial linear fit for income. The solid \textcolor {red}{red} curve is the loess smooth}{figure.caption.161}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Component + residual plot for women in the model for the quantitative predictors of prestige.}}{223}{figure.caption.162}\protected@file@percent }
\newlabel{fig-prestige-crplot-women}{{6.16}{223}{Component + residual plot for women in the model for the quantitative predictors of prestige}{figure.caption.162}{}}
\citation{ref-Fox:87}
\citation{ref-Fox:03:effects}
\citation{ref-FoxWeisberg2018}
\citation{ref-Fisher-1936}
\citation{ref-Searle-etal:80}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Effect displays}{224}{section.6.5}\protected@file@percent }
\newlabel{sec-effect-displays}{{6.5}{224}{Effect displays}{section.6.5}{}}
\citation{ref-FoxWeisberg2018}
\citation{ref-R-ggeffects}
\citation{ref-R-marginaleffects}
\citation{ref-R-effects}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Prestige data}{226}{subsection.6.5.1}\protected@file@percent }
\newlabel{prestige-data-1}{{6.5.1}{226}{Prestige data}{subsection.6.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Predictor effect plot for all terms in the model with 95\% confidence bands.}}{227}{figure.caption.163}\protected@file@percent }
\newlabel{fig-prestige-allEffects}{{6.17}{227}{Predictor effect plot for all terms in the model with 95\% confidence bands}{figure.caption.163}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Predictor effect plot for education displaying partial residuals. The \textcolor {blue}{blue} line shows the slice of the fitted regression surface where other variables are held fixed. The \textcolor {red}{red} curve shows a loess smooth of the partial residuals.}}{228}{figure.caption.164}\protected@file@percent }
\newlabel{fig-prestige-effplot-educ}{{6.18}{228}{Predictor effect plot for education displaying partial residuals. The \textcolor {blue}{blue} line shows the slice of the fitted regression surface where other variables are held fixed. The \textcolor {red}{red} curve shows a loess smooth of the partial residuals}{figure.caption.164}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Predictor effect plot for women with partial residuals}}{229}{figure.caption.165}\protected@file@percent }
\newlabel{fig-prestige-effplot-women}{{6.19}{229}{Predictor effect plot for women with partial residuals}{figure.caption.165}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Predictor effect plot for income, plotted on a log scale.}}{230}{figure.caption.166}\protected@file@percent }
\newlabel{fig-prestige-effplot-inc-log}{{6.20}{230}{Predictor effect plot for income, plotted on a log scale}{figure.caption.166}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Outliers, leverage and influence}{230}{section.6.6}\protected@file@percent }
\newlabel{sec-leverage}{{6.6}{230}{Outliers, leverage and influence}{section.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}The leverage-influence quartet}{231}{subsection.6.6.1}\protected@file@percent }
\newlabel{sec-lev-inf-quartet}{{6.6.1}{231}{The leverage-influence quartet}{subsection.6.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces Leverage influence quartet with data 50\% ellipses. Case (1) original data; (2) adding one low-leverage outlier, ``O''; (3) adding one ``good'' leverage point, ``L''; (4) adding one ``bad'' leverage point, ``OL''. The dashed \textcolor {blue}{blue} line is the fitted line for the original data, while the solid \textcolor {red}{red} line reflects the additional point. The data ellipses show the effect of the additional point on precision.}}{233}{figure.caption.167}\protected@file@percent }
\newlabel{fig-levdemo}{{6.21}{233}{Leverage influence quartet with data 50\% ellipses. Case (1) original data; (2) adding one low-leverage outlier, ``O''; (3) adding one ``good'' leverage point, ``L''; (4) adding one ``bad'' leverage point, ``OL''. The dashed \textcolor {blue}{blue} line is the fitted line for the original data, while the solid \textcolor {red}{red} line reflects the additional point. The data ellipses show the effect of the additional point on precision}{figure.caption.167}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.1}Measuring leverage}{233}{subsubsection.6.6.1.1}\protected@file@percent }
\newlabel{measuring-leverage}{{6.6.1.1}{233}{Measuring leverage}{subsubsection.6.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces Data ellipses in the Leverage-influence quartet. This graph overlays the data ellipses and additional points from the four panels of Figure~\ref {fig-levdemo2}. It can be seen that only the OL point affects the slope, while the O and L points affect precision of the estimates in opposite directions.}}{234}{figure.caption.168}\protected@file@percent }
\newlabel{fig-levdemo2}{{6.22}{234}{Data ellipses in the Leverage-influence quartet. This graph overlays the data ellipses and additional points from the four panels of Figure~\ref {fig-levdemo2}. It can be seen that only the OL point affects the slope, while the O and L points affect precision of the estimates in opposite directions}{figure.caption.168}{}}
\newlabel{eq-hat-univar}{{6.1}{234}{Measuring leverage}{section*.169}{}}
\newlabel{eq-hat-multivar}{{6.2}{234}{Measuring leverage}{section*.170}{}}
\newlabel{exm-leverage}{{6.1}{234}{}{example.6.1}{}}
\citation{ref-CookWeisberg:82}
\citation{ref-HoaglinWelsch1978}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces Data ellipses for a bivariate normal sample with correlation 0.7, and two additional noteworthy points. The \textcolor {blue}{blue} point looks to be farther from the mean, but the \textcolor {red}{red} point is actually more than 5 times further by Mahalanobis squared distance, and thus has much greater leverage.}}{236}{figure.caption.171}\protected@file@percent }
\newlabel{fig-hatvalues-demo1}{{6.23}{236}{Data ellipses for a bivariate normal sample with correlation 0.7, and two additional noteworthy points. The \textcolor {blue}{blue} point looks to be farther from the mean, but the \textcolor {red}{red} point is actually more than 5 times further by Mahalanobis squared distance, and thus has much greater leverage}{figure.caption.171}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.2}Outliers: Measuring residuals}{236}{subsubsection.6.6.1.2}\protected@file@percent }
\newlabel{outliers-measuring-residuals}{{6.6.1.2}{236}{Outliers: Measuring residuals}{subsubsection.6.6.1.2}{}}
\citation{ref-VellemanWelsh:81}
\@writefile{lof}{\contentsline {figure}{\numberline {6.24}{\ignorespaces Hat values are proportional to squared Mahalanobis distances from the mean.}}{237}{figure.caption.172}\protected@file@percent }
\newlabel{fig-hatvalues-demo2}{{6.24}{237}{Hat values are proportional to squared Mahalanobis distances from the mean}{figure.caption.172}{}}
\newlabel{eq-studentized-residual}{{6.3}{237}{Outliers: Measuring residuals}{section*.173}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.3}Measuring influence}{237}{subsubsection.6.6.1.3}\protected@file@percent }
\newlabel{measuring-influence}{{6.6.1.3}{237}{Measuring influence}{subsubsection.6.6.1.3}{}}
\citation{ref-Belsley-etal:80}
\citation{ref-Cook:77}
\newlabel{eq-cooksd}{{6.4}{238}{Measuring influence}{section*.174}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Influence plots}{238}{subsection.6.6.2}\protected@file@percent }
\newlabel{influence-plots}{{6.6.2}{238}{Influence plots}{subsection.6.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.25}{\ignorespaces Influence plot for the demonstration data. The areas of the bubble symbols are proportional to Cook's \(D\). The impact of the three unusual points on Cook's \(D\) is clearly seen.}}{239}{figure.caption.175}\protected@file@percent }
\newlabel{fig-levdemo-infl}{{6.25}{239}{Influence plot for the demonstration data. The areas of the bubble symbols are proportional to Cook's \(D\). The impact of the three unusual points on Cook's \(D\) is clearly seen}{figure.caption.175}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.3}Duncan data}{239}{subsection.6.6.3}\protected@file@percent }
\newlabel{sec-duncan-influence}{{6.6.3}{239}{Duncan data}{subsection.6.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.26}{\ignorespaces Influence plot for the model predicting occupational prestige in Duncan's data. Cases with large studentized residuals, hat-values or Cook's distances are labeled.}}{240}{figure.caption.176}\protected@file@percent }
\newlabel{fig-duncan-infl}{{6.26}{240}{Influence plot for the model predicting occupational prestige in Duncan's data. Cases with large studentized residuals, hat-values or Cook's distances are labeled}{figure.caption.176}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.4}Influence in added-variable plots}{241}{subsection.6.6.4}\protected@file@percent }
\newlabel{influence-in-added-variable-plots}{{6.6.4}{241}{Influence in added-variable plots}{subsection.6.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.27}{\ignorespaces Added variable plots for the Duncan model, highlighting the impact of the observations for minister and conductor in each plot. The \textcolor {green}{green} lines show the residuals for these observations. The \textcolor {red}{red} line in each panel shows the regression line omitting these observations.}}{241}{figure.caption.177}\protected@file@percent }
\newlabel{fig-duncan-av-influence}{{6.27}{241}{Added variable plots for the Duncan model, highlighting the impact of the observations for minister and conductor in each plot. The \textcolor {green}{green} lines show the residuals for these observations. The \textcolor {red}{red} line in each panel shows the regression line omitting these observations}{figure.caption.177}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}What have we learned?}{242}{section.6.7}\protected@file@percent }
\newlabel{what-have-we-learned-4}{{6.7}{242}{What have we learned?}{section.6.7}{}}
\citation{ref-Friendly-etal:ellipses:2013}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Topics in Linear Models}{245}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{topics-in-linear-models}{{7}{245}{Topics in Linear Models}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Ellipsoids in data space and \(\boldsymbol  {\mitbeta }\) space}{245}{section.7.1}\protected@file@percent }
\newlabel{sec-betaspace}{{7.1}{245}{\texorpdfstring {Ellipsoids in data space and \(\boldsymbol {\beta }\) space}{Ellipsoids in data space and \textbackslash boldsymbol\{\textbackslash beta\} space}}{section.7.1}{}}
\citation{ref-Dempster:69}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Dual and inverse spaces}{246}{subsection.7.1.1}\protected@file@percent }
\newlabel{dual-and-inverse-spaces}{{7.1.1}{246}{Dual and inverse spaces}{subsection.7.1.1}{}}
\newlabel{exm-dual-points-lines}{{7.1}{246}{}{example.7.1}{}}
\newlabel{exm-dual-inverses}{{7.2}{246}{}{example.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Duality of \((x, y)\) lines in data space (left) and points in \(\mitbeta \)-space (right). Each line in data space corresponds to a point, whose intercept and slope are shown in \(\mitbeta \)-space. Labels in both plots identify the points and lines.}}{247}{figure.caption.178}\protected@file@percent }
\newlabel{fig-dual-points-lines}{{7.1}{247}{Duality of \((x, y)\) lines in data space (left) and points in \(\beta \)-space (right). Each line in data space corresponds to a point, whose intercept and slope are shown in \(\beta \)-space. Labels in both plots identify the points and lines}{figure.caption.178}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Data ellipse and confidence ellipse}{247}{subsection.7.1.2}\protected@file@percent }
\newlabel{sec-data-beta}{{7.1.2}{247}{Data ellipse and confidence ellipse}{subsection.7.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Geometric properties of an ellipse \(\mathbf {S}\) and its inverse, \(\mathbf {S}^{-1}\). The principal axes (dotted lines) are given by the eigenvectors, which are the same for \(\mathbf {S}\) and \(\mathbf {S}^{-1}\). Multiplying \(\mathbf {S}\) by 2 makes it's ellipse larger by \(\sqrt  {2}\), while the same factor makes the ellipse for \((2 \mathbf {S})^{-1}\) smaller by the same factor.}}{248}{figure.caption.179}\protected@file@percent }
\newlabel{fig-inverse}{{7.2}{248}{Geometric properties of an ellipse \(\mathbf {S}\) and its inverse, \(\mathbf {S}^{-1}\). The principal axes (dotted lines) are given by the eigenvectors, which are the same for \(\mathbf {S}\) and \(\mathbf {S}^{-1}\). Multiplying \(\mathbf {S}\) by 2 makes it's ellipse larger by \(\sqrt {2}\), while the same factor makes the ellipse for \((2 \mathbf {S})^{-1}\) smaller by the same factor}{figure.caption.179}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Data space and \(\boldsymbol  {\mitbeta }\) space representations of Coffee and Stress. Left: 40\% and 68\% data ellipses. Right: Joint 95\% confidence ellipse (blue) for (\(\mitbeta _{\text  {Coffee}}, \mitbeta _{\text  {Stress}}\)), confidence interval generating ellipse (red) with 95\% univariate shadows. \(H_0\) marks the joint hypothesis that both coefficients equal zero.}}{249}{figure.caption.180}\protected@file@percent }
\newlabel{fig-coffee-data-beta-both}{{7.3}{249}{Data space and \(\boldsymbol {\beta }\) space representations of Coffee and Stress. Left: 40\% and 68\% data ellipses. Right: Joint 95\% confidence ellipse (blue) for (\(\beta _{\text {Coffee}}, \beta _{\text {Stress}}\)), confidence interval generating ellipse (red) with 95\% univariate shadows. \(H_0\) marks the joint hypothesis that both coefficients equal zero}{figure.caption.180}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Measurement error}{250}{section.7.2}\protected@file@percent }
\newlabel{sec-meas-error}{{7.2}{250}{Measurement error}{section.7.2}{}}
\citation{ref-Fuller2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Errors in predictors}{251}{subsection.7.2.1}\protected@file@percent }
\newlabel{errors-in-predictors}{{7.2.1}{251}{Errors in predictors}{subsection.7.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Example: Measurement Error Quartet}{251}{subsection.7.2.2}\protected@file@percent }
\newlabel{example-measurement-error-quartet}{{7.2.2}{251}{Example: Measurement Error Quartet}{subsection.7.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces \textbf  {The measurement error quartet}: Each plot shows the linear regression of \emph  {y} on \emph  {x}, but where additional error variance has been added to \emph  {y} or \emph  {x} or both. The widths of the confidence bands and the vertical extent of the data ellipses show the effect on precision.}}{253}{figure.caption.181}\protected@file@percent }
\newlabel{fig-measerr-demo}{{7.4}{253}{\textbf {The measurement error quartet}: Each plot shows the linear regression of \emph {y} on \emph {x}, but where additional error variance has been added to \emph {y} or \emph {x} or both. The widths of the confidence bands and the vertical extent of the data ellipses show the effect on precision}{figure.caption.181}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Model statistics for the combinations of additional error variance in x or y or both. Left: model R; right: Residual standard error.}}{255}{figure.caption.182}\protected@file@percent }
\newlabel{fig-measerr-stats}{{7.5}{255}{Model statistics for the combinations of additional error variance in x or y or both. Left: model R; right: Residual standard error}{figure.caption.182}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Coffee data: Bias and precision}{255}{subsection.7.2.3}\protected@file@percent }
\newlabel{coffee-data-bias-and-precision}{{7.2.3}{255}{Coffee data: Bias and precision}{subsection.7.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Effects of measurement error in Stress on the marginal relationship between Heart disease and Stress. Each panel starts with the observed data (\(\mitdelta = 0\)), then adds random normal error, \(\symcal {N}(0, \mitdelta \times \text  {SD}_\text  {Stress})\) with standard deviations multiplied by \(\mitdelta \) = 0.75, 1.0, 1.5, to the value of Stress. Increasing measurement error biases the slope for Stress toward 0. Left: 50\% data ellipses; right: 50\% confidence ellipses.}}{256}{figure.caption.183}\protected@file@percent }
\newlabel{fig-coffee-measerr-data-beta}{{7.6}{256}{Effects of measurement error in Stress on the marginal relationship between Heart disease and Stress. Each panel starts with the observed data (\(\delta = 0\)), then adds random normal error, \(\mathcal {N}(0, \delta \times \text {SD}_\text {Stress})\) with standard deviations multiplied by \(\delta \) = 0.75, 1.0, 1.5, to the value of Stress. Increasing measurement error biases the slope for Stress toward 0. Left: 50\% data ellipses; right: 50\% confidence ellipses}{figure.caption.183}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Biasing effect of measurement error in one variable (Stress) on on the coefficient of another variable (Coffee) in a multiple regression. The coefficient for Coffee is driven towards its value in the marginal model using Coffee alone, as measurement error in Stress makes it less informative in the joint model.}}{256}{figure.caption.184}\protected@file@percent }
\newlabel{fig-coffee-measerr}{{7.7}{256}{Biasing effect of measurement error in one variable (Stress) on on the coefficient of another variable (Coffee) in a multiple regression. The coefficient for Coffee is driven towards its value in the marginal model using Coffee alone, as measurement error in Stress makes it less informative in the joint model}{figure.caption.184}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}What have we learned?}{257}{section.7.3}\protected@file@percent }
\newlabel{what-have-we-learned-5}{{7.3}{257}{What have we learned?}{section.7.3}{}}
\citation{ref-Graybill1961}
\citation{ref-Hocking2013}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Collinearity \& Ridge Regression}{259}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-collin}{{8}{259}{Collinearity \& Ridge Regression}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}What is collinearity?}{259}{section.8.1}\protected@file@percent }
\newlabel{sec-what-is-collin}{{8.1}{259}{What is collinearity?}{section.8.1}{}}
\citation{ref-FriendlyKwan:2009}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Collinearity diagnostics for a multiple regression model from SPSS. \emph  {Source}: Arndt Regorz, How to interpret a Collinearity Diagnostics table in SPSS, https://bit.ly/3YRB82b}}{260}{figure.caption.185}\protected@file@percent }
\newlabel{fig-collinearity-diagnostics-SPSS}{{8.1}{260}{Collinearity diagnostics for a multiple regression model from SPSS. \emph {Source}: Arndt Regorz, How to interpret a Collinearity Diagnostics table in SPSS, https://bit.ly/3YRB82b}{figure.caption.185}{}}
\citation{ref-Longley:1967}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces A scene from one of the \emph  {Where's Waldo} books. Waldo wears a red-striped shirt, but far too many of the other figures in the scene have horizontal red stripes, making it very difficult to find him among all the distractors. This is often the problem with collinearity diagnostics. \emph  {Source}: Modified from https://bit.ly/48KPcOo}}{261}{figure.caption.186}\protected@file@percent }
\newlabel{fig-wheres-waldo}{{8.2}{261}{A scene from one of the \emph {Where's Waldo} books. Waldo wears a red-striped shirt, but far too many of the other figures in the scene have horizontal red stripes, making it very difficult to find him among all the distractors. This is often the problem with collinearity diagnostics. \emph {Source}: Modified from https://bit.ly/48KPcOo}{figure.caption.186}{}}
\citation{ref-Fox:2016:ARA}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Visualizing collinearity}{262}{subsection.8.1.1}\protected@file@percent }
\newlabel{sec-vis-collin}{{8.1.1}{262}{Visualizing collinearity}{subsection.8.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Effect of collinearity on the least squares regression plane. (a) Small correlation between predictors; (b) Perfect correlation ; (c) Very strong correlation. The black points show the data Y values, white points are the fitted values in the regression plane, and + signs represent the values of X1 and X2. \emph  {Source}: Adapted from Fox (\citeproc {ref-Fox:2016:ARA}{2016}), Fig. 13.2}}{262}{figure.caption.187}\protected@file@percent }
\newlabel{fig-collin-demo}{{8.3}{262}{Effect of collinearity on the least squares regression plane. (a) Small correlation between predictors; (b) Perfect correlation ; (c) Very strong correlation. The black points show the data Y values, white points are the fitted values in the regression plane, and + signs represent the values of X1 and X2. \emph {Source}: Adapted from Fox (\citeproc {ref-Fox:2016:ARA}{2016}), Fig. 13.2}{figure.caption.187}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Data space and \(\mitbeta \) space}{262}{subsection.8.1.2}\protected@file@percent }
\newlabel{sec-data-beta-space}{{8.1.2}{262}{\texorpdfstring {Data space and \(\beta \) space}{Data space and \textbackslash beta space}}{subsection.8.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Measuring collinearity}{264}{section.8.2}\protected@file@percent }
\newlabel{sec-measure-collin}{{8.2}{264}{Measuring collinearity}{section.8.2}{}}
\citation{ref-Marquardt:1970}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces 95\% \textcolor {darkgreen}{Data ellipses} for x1, x2 and the corresponding 95\% \textcolor {red}{confidence ellipses} for their coefficients in the model predicting y. In the confidence ellipse plots, reference lines show the value (0,0) for the null hypothesis and ``+'' marks the true values for the coefficients. This figure adapts an example by John Fox (2022).}}{265}{figure.caption.188}\protected@file@percent }
\newlabel{fig-collin-data-beta}{{8.4}{265}{95\% \textcolor {darkgreen}{Data ellipses} for x1, x2 and the corresponding 95\% \textcolor {red}{confidence ellipses} for their coefficients in the model predicting y. In the confidence ellipse plots, reference lines show the value (0,0) for the null hypothesis and ``+'' marks the true values for the coefficients. This figure adapts an example by John Fox (2022)}{figure.caption.188}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Variance inflation factors}{265}{subsection.8.2.1}\protected@file@percent }
\newlabel{sec-vif}{{8.2.1}{265}{Variance inflation factors}{subsection.8.2.1}{}}
\citation{ref-FoxMonette:92}
\newlabel{generalized-vif}{{8.2.1}{266}{Generalized VIF}{section*.189}{}}
\@writefile{toc}{\contentsline {subsubsection}{Generalized VIF}{266}{section*.189}\protected@file@percent }
\newlabel{exm-cars-collin}{{8.1}{266}{}{example.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}VIF displays}{268}{subsection.8.2.2}\protected@file@percent }
\newlabel{vif-displays}{{8.2.2}{268}{VIF displays}{subsection.8.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Collinearity diagnostics}{268}{subsection.8.2.3}\protected@file@percent }
\newlabel{sec-colldiag}{{8.2.3}{268}{Collinearity diagnostics}{subsection.8.2.3}{}}
\citation{ref-Belsley-etal:80}
\citation{ref-Belsley:91a}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Variance inflation plot. VIF is plotted on a log scale. Colored bands show regions of \textcolor {darkgreen}{low}, \textcolor {blue}{medium} and \textcolor {red}{high} variance inflation.}}{269}{figure.caption.190}\protected@file@percent }
\newlabel{fig-cars-check-collin}{{8.5}{269}{Variance inflation plot. VIF is plotted on a log scale. Colored bands show regions of \textcolor {darkgreen}{low}, \textcolor {blue}{medium} and \textcolor {red}{high} variance inflation}{figure.caption.190}{}}
\newlabel{eq-rxinv-eigen}{{8.1}{269}{Collinearity diagnostics}{section*.191}{}}
\newlabel{eq-VIF-sum}{{8.2}{269}{Collinearity diagnostics}{section*.192}{}}
\citation{ref-Belsley:91a}
\citation{ref-Belsley-etal:80}
\citation{ref-Belsley:91a}
\citation{ref-Belsley:91a}
\citation{ref-Kwan-etal:2009}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Tableplots}{271}{section.8.3}\protected@file@percent }
\newlabel{sec-tableplot}{{8.3}{271}{Tableplots}{section.8.3}{}}
\citation{ref-Gabriel:71}
\citation{ref-GowerHand:96}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces \textbf  {Tableplot} of condition indices and variance proportions for the \texttt  {cars} data. In column 1, the square symbols are scaled relative to a maximum condition index of 30. In the remaining columns, variance proportions (times 100) are shown as circles scaled relative to a maximum of 100.}}{272}{figure.caption.193}\protected@file@percent }
\newlabel{fig-cars-tableplot}{{8.6}{272}{\textbf {Tableplot} of condition indices and variance proportions for the \texttt {cars} data. In column 1, the square symbols are scaled relative to a maximum condition index of 30. In the remaining columns, variance proportions (times 100) are shown as circles scaled relative to a maximum of 100}{figure.caption.193}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Collinearity biplots}{272}{section.8.4}\protected@file@percent }
\newlabel{sec-collin-biplots}{{8.4}{272}{Collinearity biplots}{section.8.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces \textbf  {Collinearity biplot} of the Cars data, showing the last two dimensions. The projections of the variable vectors on the coordinate axes are proportional to their variance proportions. To reduce graphic clutter, only the most outlying observations in predictor space are identified by case labels. An extreme outlier (case 20) appears in the lower right corner.}}{274}{figure.caption.194}\protected@file@percent }
\newlabel{fig-cars-collin-biplot}{{8.7}{274}{\textbf {Collinearity biplot} of the Cars data, showing the last two dimensions. The projections of the variable vectors on the coordinate axes are proportional to their variance proportions. To reduce graphic clutter, only the most outlying observations in predictor space are identified by case labels. An extreme outlier (case 20) appears in the lower right corner}{figure.caption.194}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Remedies for collinearity: What can I do?}{275}{section.8.5}\protected@file@percent }
\newlabel{sec-remedies}{{8.5}{275}{Remedies for collinearity: What can I do?}{section.8.5}{}}
\citation{ref-Pesaran2019}
\newlabel{exm-centering}{{8.2}{276}{}{example.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Centering a predictor removes the nessessary correlation in a quadratic regression. Left: linear relatioship fitting \(y\) to the uncentered \(x\). Right: fitting to the centered \((x - \bar {x}\).}}{277}{figure.caption.195}\protected@file@percent }
\newlabel{fig-collin-centering}{{8.8}{277}{Centering a predictor removes the nessessary correlation in a quadratic regression. Left: linear relatioship fitting \(y\) to the uncentered \(x\). Right: fitting to the centered \((x - \bar {x}\)}{figure.caption.195}{}}
\newlabel{exm-interactions}{{8.3}{278}{}{example.8.3}{}}
\citation{ref-HoerlKennard:1970a}
\citation{ref-Tibshriani:regr:1996}
\citation{ref-Efron-etal:leas:2004}
\citation{ref-Vinod:1978}
\citation{ref-McDonald:2009}
\citation{ref-Brown-Zidek-1980}
\citation{ref-Haitovsky1987}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Ridge regression}{279}{section.8.6}\protected@file@percent }
\newlabel{sec-ridge}{{8.6}{279}{Ridge regression}{section.8.6}{}}
\citation{ref-Friendly-2011-gentalk}
\citation{ref-Friendly:genridge:2013}
\citation{ref-HoerlKennard:1970a}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.1}Properties of ridge regression}{280}{subsection.8.6.1}\protected@file@percent }
\newlabel{properties-of-ridge-regression}{{8.6.1}{280}{Properties of ridge regression}{subsection.8.6.1}{}}
\citation{ref-Friendly-etal:ellipses:2013}
\citation{ref-Friendly-etal:ellipses:2013}
\newlabel{eq-ridgeRSS}{{8.3}{281}{Properties of ridge regression}{section*.196}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces Geometric interpretation of ridge regression, using elliptical contours of the \(\text  {RSS}(k)\) function. The blue circles at the origin show the constraint that the sum of squares of coefficients, \(\boldsymbol  {\mitbeta }^\mathsf {T}\boldsymbol  {\mitbeta }\) be less than \(k\). The red ellipses show the covariance ellipse of two coefficients \(\boldsymbol  {\mitbeta }\). Ridge regression finds the point \(\widehat {\boldsymbol  {\mitbeta }}^{\mathrm {RR}}_k\) where the OLS contours just kiss the constraint region. \emph  {Source}: Friendly et al. (\citeproc {ref-Friendly-etal:ellipses:2013}{2013}).}}{281}{figure.caption.197}\protected@file@percent }
\newlabel{fig-ridge-demo}{{8.9}{281}{Geometric interpretation of ridge regression, using elliptical contours of the \(\text {RSS}(k)\) function. The blue circles at the origin show the constraint that the sum of squares of coefficients, \(\boldsymbol {\beta }^\mathsf {T}\boldsymbol {\beta }\) be less than \(k\). The red ellipses show the covariance ellipse of two coefficients \(\boldsymbol {\beta }\). Ridge regression finds the point \(\widehat {\boldsymbol {\beta }}^{\mathrm {RR}}_k\) where the OLS contours just kiss the constraint region. \emph {Source}: Friendly et al. (\citeproc {ref-Friendly-etal:ellipses:2013}{2013})}{figure.caption.197}{}}
\citation{ref-HoerlKennard:1970a}
\citation{ref-Hastie-etal-2009}
\citation{ref-R-glmnet}
\citation{ref-R-penalized}
\citation{ref-R-genridge}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.2}The \texttt  {genridge} package}{282}{subsection.8.6.2}\protected@file@percent }
\newlabel{the-genridge-package}{{8.6.2}{282}{\texorpdfstring {The \texttt {genridge} package}{The genridge package}}{subsection.8.6.2}{}}
\citation{ref-Longley:1967}
\@writefile{toc}{\contentsline {section}{\numberline {8.7}Univariate ridge trace plots}{283}{section.8.7}\protected@file@percent }
\newlabel{sec-ridge-univar}{{8.7}{283}{Univariate ridge trace plots}{section.8.7}{}}
\newlabel{exm-longley}{{8.4}{283}{}{example.8.4}{}}
\citation{ref-Hoerl-etal-1975}
\citation{ref-LawlessWang:1976}
\citation{ref-Tibshriani:regr:1996}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Univariate ridge trace plot for the coefficients of predictors of Employment in Longley's data via ridge regression, with ridge constants k = (0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08). The dotted lines show optimal values for shrinkage by two criteria (HKB, LW).}}{285}{figure.caption.198}\protected@file@percent }
\newlabel{fig-longley-traceplot1}{{8.10}{285}{Univariate ridge trace plot for the coefficients of predictors of Employment in Longley's data via ridge regression, with ridge constants k = (0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08). The dotted lines show optimal values for shrinkage by two criteria (HKB, LW)}{figure.caption.198}{}}
\citation{ref-Friendly:genridge:2013}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces Univariate ridge trace plot using equivalent degrees of freedom, \(\text  {df}_k\) to specify shrinkage. This scale is easier to understand and makes the traces of prarameters more nearly linear.}}{286}{figure.caption.199}\protected@file@percent }
\newlabel{fig-longley-traceplot2}{{8.11}{286}{Univariate ridge trace plot using equivalent degrees of freedom, \(\text {df}_k\) to specify shrinkage. This scale is easier to understand and makes the traces of prarameters more nearly linear}{figure.caption.199}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7.1}What's not to like?}{286}{subsection.8.7.1}\protected@file@percent }
\newlabel{whats-not-to-like}{{8.7.1}{286}{What's not to like?}{subsection.8.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.8}Bivariate ridge trace plots}{286}{section.8.8}\protected@file@percent }
\newlabel{sec-ridge-bivar}{{8.8}{286}{Bivariate ridge trace plots}{section.8.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.1}Visualizing the bias-variance tradeoff}{287}{subsection.8.8.1}\protected@file@percent }
\newlabel{visualizing-the-bias-variance-tradeoff}{{8.8.1}{287}{Visualizing the bias-variance tradeoff}{subsection.8.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces \textbf  {Bivariate ridge trace} plots for the coefficients of four predictors against the coefficient for GNP in Longley's data, with k = 0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08. In most cases, the coefficients are driven toward zero, but the bivariate plot also makes clear the reduction in variance, as well as the bivariate path of shrinkage.}}{288}{figure.caption.200}\protected@file@percent }
\newlabel{fig-longley-plot-ridge}{{8.12}{288}{\textbf {Bivariate ridge trace} plots for the coefficients of four predictors against the coefficient for GNP in Longley's data, with k = 0, 0.002, 0.005, 0.01, 0.02, 0.04, 0.08. In most cases, the coefficients are driven toward zero, but the bivariate plot also makes clear the reduction in variance, as well as the bivariate path of shrinkage}{figure.caption.200}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces Scatterplot matrix of bivariate ridge trace plots. Each panel shows the effect of shrinkage on the covariance ellipse for a given pair of predictors.}}{289}{figure.caption.201}\protected@file@percent }
\newlabel{fig-longley-pairs}{{8.13}{289}{Scatterplot matrix of bivariate ridge trace plots. Each panel shows the effect of shrinkage on the covariance ellipse for a given pair of predictors}{figure.caption.201}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.9}Low-rank views}{290}{section.8.9}\protected@file@percent }
\newlabel{sec-ridge-low-rank}{{8.9}{290}{Low-rank views}{section.8.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces \textbf  {Precision plot} showing the tradeoff between bias and precision. Bias increases as we move away from the OLS solution, but precision increases.}}{291}{figure.caption.202}\protected@file@percent }
\newlabel{fig-longley-precision-plot}{{8.14}{291}{\textbf {Precision plot} showing the tradeoff between bias and precision. Bias increases as we move away from the OLS solution, but precision increases}{figure.caption.202}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Ridge traceplot for the longley regression viewed in PCA space. The dimensions are the linear combinations of the predictors which account for greatest variance.}}{292}{figure.caption.203}\protected@file@percent }
\newlabel{fig-longley-pca-traceplot}{{8.15}{292}{Ridge traceplot for the longley regression viewed in PCA space. The dimensions are the linear combinations of the predictors which account for greatest variance}{figure.caption.203}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.1}Biplot view}{292}{subsection.8.9.1}\protected@file@percent }
\newlabel{biplot-view}{{8.9.1}{292}{Biplot view}{subsection.8.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces \textbf  {pairs method}: All pairwise bivariate ridge plots shown in PCA space.}}{293}{figure.caption.204}\protected@file@percent }
\newlabel{fig-longley-pca-pairs}{{8.16}{293}{\textbf {pairs method}: All pairwise bivariate ridge plots shown in PCA space}{figure.caption.204}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces Bivariate ridge trace plot for the smallest two dimensions. The coefficients for these two dimensions head smoothly toward zero and their variance also shrinks.}}{294}{figure.caption.205}\protected@file@percent }
\newlabel{fig-longley-pca-dim56}{{8.17}{294}{Bivariate ridge trace plot for the smallest two dimensions. The coefficients for these two dimensions head smoothly toward zero and their variance also shrinks}{figure.caption.205}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces Biplot view of the ridge trace plot for the smallest two dimensions, where the effects of shrinkage are most apparent.}}{295}{figure.caption.206}\protected@file@percent }
\newlabel{fig-longley-pca-biplot}{{8.18}{295}{Biplot view of the ridge trace plot for the smallest two dimensions, where the effects of shrinkage are most apparent}{figure.caption.206}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.10}What have we learned?}{295}{section.8.10}\protected@file@percent }
\newlabel{what-have-we-learned-6}{{8.10}{295}{What have we learned?}{section.8.10}{}}
\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Multivariate Linear Models}{297}{part.4}\protected@file@percent }
\citation{ref-Hotelling:1931}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Hotelling's \(T^2\)}{299}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-Hotelling}{{9}{299}{\texorpdfstring {Hotelling's \(T^2\)}{Hotelling's T\^{}2}}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}\(T^2\) as a generalized \(t\)-test}{299}{section.9.1}\protected@file@percent }
\newlabel{sec-T2}{{9.1}{299}{\texorpdfstring {\(T^2\) as a generalized \(t\)-test}{T\^{}2 as a generalized t-test}}{section.9.1}{}}
\newlabel{eq-t2}{{9.1}{300}{\texorpdfstring {\(T^2\) as a generalized \(t\)-test}{T\^{}2 as a generalized t-test}}{section*.207}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}\(T^2\) properties}{300}{section.9.2}\protected@file@percent }
\newlabel{sec-t2-properties}{{9.2}{300}{\texorpdfstring {\(T^2\) properties}{T\^{}2 properties}}{section.9.2}{}}
\newlabel{eq-eigen}{{9.2}{300}{\texorpdfstring {\(T^2\) properties}{T\^{}2 properties}}{section*.209}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Hotelling's T\^{}2 statistic as the squared distance between the sample means and hypothesized means relative to the variance-covariance matrix. \emph  {Source}: Author}}{301}{figure.caption.208}\protected@file@percent }
\newlabel{fig-T2-diagram}{{9.1}{301}{Hotelling's T\^{}2 statistic as the squared distance between the sample means and hypothesized means relative to the variance-covariance matrix. \emph {Source}: Author}{figure.caption.208}{}}
\newlabel{eq-Fstat}{{9.3}{301}{\texorpdfstring {\(T^2\) properties}{T\^{}2 properties}}{section*.210}{}}
\citation{ref-R-Hotelling}
\newlabel{eq-T2-two-sample}{{9.4}{302}{\texorpdfstring {\(T^2\) properties}{T\^{}2 properties}}{section*.211}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Example: Mathscore data}{302}{subsection.9.2.1}\protected@file@percent }
\newlabel{sec-mathscore-example}{{9.2.1}{302}{Example: Mathscore data}{subsection.9.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Data ellipses for the \texttt  {mathscore} data, enclosing approximately 68\% of the observations in each group.}}{304}{figure.caption.212}\protected@file@percent }
\newlabel{fig-mathscore-cov1}{{9.2}{304}{Data ellipses for the \texttt {mathscore} data, enclosing approximately 68\% of the observations in each group}{figure.caption.212}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Data ellipses and the pooled covariance matrix \texttt  {mathscore} data.}}{305}{figure.caption.213}\protected@file@percent }
\newlabel{fig-mathscore-cov2}{{9.3}{305}{Data ellipses and the pooled covariance matrix \texttt {mathscore} data}{figure.caption.213}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}HE plot and discriminant axis}{305}{section.9.3}\protected@file@percent }
\newlabel{sec-t2-heplot}{{9.3}{305}{HE plot and discriminant axis}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}The \texttt  {heplot()} function}{306}{subsection.9.3.1}\protected@file@percent }
\newlabel{the-heplot-function}{{9.3.1}{306}{\texorpdfstring {The \texttt {heplot()} function}{The heplot() function}}{subsection.9.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Hypothesis error plot of the \texttt  {mathscore} data. The line through the group means is the H ellipse, which plots as a line here. The \textcolor {red}{red} ellipse labeled `Error' represents the pooled within-group covariance matrix.}}{307}{figure.caption.214}\protected@file@percent }
\newlabel{fig-mathscore-HE}{{9.4}{307}{Hypothesis error plot of the \texttt {mathscore} data. The line through the group means is the H ellipse, which plots as a line here. The \textcolor {red}{red} ellipse labeled `Error' represents the pooled within-group covariance matrix}{figure.caption.214}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}HE plot, data ellipses and discriminant axis}{307}{subsection.9.3.2}\protected@file@percent }
\newlabel{he-plot-data-ellipses-and-discriminant-axis}{{9.3.2}{307}{HE plot, data ellipses and discriminant axis}{subsection.9.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces HE plot overlaid on top of the within-group data ellipses, with lines showing the projection of each point on the line joining the means, which is the discriminant axis.}}{308}{figure.caption.215}\protected@file@percent }
\newlabel{fig-mathscore-HE-overlay}{{9.5}{308}{HE plot overlaid on top of the within-group data ellipses, with lines showing the projection of each point on the line joining the means, which is the discriminant axis}{figure.caption.215}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Discriminant analysis}{308}{section.9.4}\protected@file@percent }
\newlabel{sec-t2-discrim}{{9.4}{308}{Discriminant analysis}{section.9.4}{}}
\citation{ref-FluryReidwyl-1988}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}More variables}{310}{section.9.5}\protected@file@percent }
\newlabel{sec-t2-more-variables}{{9.5}{310}{More variables}{section.9.5}{}}
\newlabel{exm-banknote}{{9.1}{310}{}{example.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Violin plots comparing group 1 and 2 for the two observed measures and the linear discriminant score.}}{311}{figure.caption.216}\protected@file@percent }
\newlabel{fig-mathscore-violins}{{9.6}{311}{Violin plots comparing group 1 and 2 for the two observed measures and the linear discriminant score}{figure.caption.216}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces Overlaid violin and boxplots of the banknote variables. The violin plots give a sense of the shapes of the distributions, while the boxplots highlight the center and spread.}}{312}{figure.caption.217}\protected@file@percent }
\newlabel{fig-banknote-violin}{{9.7}{312}{Overlaid violin and boxplots of the banknote variables. The violin plots give a sense of the shapes of the distributions, while the boxplots highlight the center and spread}{figure.caption.217}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Biplots}{313}{subsection.9.5.1}\protected@file@percent }
\newlabel{sec-t2-biplot}{{9.5.1}{313}{Biplots}{subsection.9.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces Biplot of the banknote variables, showing how the size measurements are related to each other. The points and data ellipses for the component scores are colored by Status, showing how the counterfeit and genuine bills are distinguished by these measures.}}{314}{figure.caption.218}\protected@file@percent }
\newlabel{fig-banknote-biplot}{{9.8}{314}{Biplot of the banknote variables, showing how the size measurements are related to each other. The points and data ellipses for the component scores are colored by Status, showing how the counterfeit and genuine bills are distinguished by these measures}{figure.caption.218}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}Testing mean differences}{314}{subsection.9.5.2}\protected@file@percent }
\newlabel{testing-mean-differences}{{9.5.2}{314}{Testing mean differences}{subsection.9.5.2}{}}
\citation{ref-Pearson-1903}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Variance accounted for: \(\miteta ^2\)}{316}{section.9.6}\protected@file@percent }
\newlabel{variance-accounted-for-eta2}{{9.6}{316}{\texorpdfstring {Variance accounted for: \(\eta ^2\)}{Variance accounted for: \textbackslash eta\^{}2}}{section.9.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}The Grand Scheme}{316}{section.9.7}\protected@file@percent }
\newlabel{the-grand-scheme}{{9.7}{316}{The Grand Scheme}{section.9.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces The Hypothesis Error plot framework for a two-group design. Above: Data ellipses can be summarized in an HE plot showing the pooled within-group error (\(\mathbf {E}\)) ellipse and the \(\mathbf {H}\) `ellipse' for the group means. Below: Observations projected on the line joining the means give discriminant scores which correpond to a one-dimensional canonical space, represented by a boxplot of their scores and arrows reflecting the variable weights.}}{317}{figure.caption.219}\protected@file@percent }
\newlabel{fig-HE-framework}{{9.9}{317}{The Hypothesis Error plot framework for a two-group design. Above: Data ellipses can be summarized in an HE plot showing the pooled within-group error (\(\mathbf {E}\)) ellipse and the \(\mathbf {H}\) `ellipse' for the group means. Below: Observations projected on the line joining the means give discriminant scores which correpond to a one-dimensional canonical space, represented by a boxplot of their scores and arrows reflecting the variable weights}{figure.caption.219}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.8}What have we learned?}{317}{section.9.8}\protected@file@percent }
\newlabel{what-have-we-learned-7}{{9.8}{317}{What have we learned?}{section.9.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.9}Exercises}{318}{section.9.9}\protected@file@percent }
\newlabel{exercises-1}{{9.9}{318}{Exercises}{section.9.9}{}}
\newlabel{exr-hotel1}{{9.1}{318}{}{exercise.9.1}{}}
\citation{ref-Yee2015}
\citation{ref-R-VGAM}
\citation{ref-FriendlyMeyer:2016:DDAR}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Multivariate Linear Models}{319}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-mlm-review}{{10}{319}{Multivariate Linear Models}{chapter.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Structure of the MLM}{320}{section.10.1}\protected@file@percent }
\newlabel{sec-mlm-structure}{{10.1}{320}{Structure of the MLM}{section.10.1}{}}
\newlabel{eq-mlm}{{10.1}{321}{Structure of the MLM}{section*.220}{}}
\citation{ref-ShapiroWilk1965}
\citation{ref-Mardia:1970:MMS}
\citation{ref-Mardia:1974}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}Assumptions}{322}{subsection.10.1.1}\protected@file@percent }
\newlabel{sec-mlm-assumptions}{{10.1.1}{322}{Assumptions}{subsection.10.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Fitting the model}{323}{section.10.2}\protected@file@percent }
\newlabel{sec-mlm-fitting}{{10.2}{323}{Fitting the model}{section.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Example: Dog food data}{323}{subsection.10.2.1}\protected@file@percent }
\newlabel{sec-dogfood-data}{{10.2.1}{323}{Example: Dog food data}{subsection.10.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Boxplots for time to start eating and amount eaten by dogs given one of four dogfood formulas.}}{324}{figure.caption.221}\protected@file@percent }
\newlabel{fig-dogfood-boxplot}{{10.1}{324}{Boxplots for time to start eating and amount eaten by dogs given one of four dogfood formulas}{figure.caption.221}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Sums of squares}{325}{subsection.10.2.2}\protected@file@percent }
\newlabel{sec-sums-of-squares}{{10.2.2}{325}{Sums of squares}{subsection.10.2.2}{}}
\newlabel{eq-THE-dev}{{10.2}{326}{Sums of squares}{section*.222}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Breakdown of the total \(\mathbf {SSP}_{T}\) into sums of squares and products for between-group hypothesis variance (\(\mathbf {SSP}_{H}\)) and within-group, error variance (\(\mathbf {SSP}_{E}\)).}}{326}{figure.caption.223}\protected@file@percent }
\newlabel{fig-visualizing-SSP}{{10.2}{326}{Breakdown of the total \(\mathbf {SSP}_{T}\) into sums of squares and products for between-group hypothesis variance (\(\mathbf {SSP}_{H}\)) and within-group, error variance (\(\mathbf {SSP}_{E}\))}{figure.caption.223}{}}
\newlabel{eq-SSP-T}{{10.3}{326}{Sums of squares}{section*.224}{}}
\newlabel{eq-SSP-H}{{10.4}{326}{Sums of squares}{section*.225}{}}
\newlabel{eq-SSP-E}{{10.5}{326}{Sums of squares}{section*.226}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}How big is \(SS_H\) compared to \(SS_E\)?}{327}{subsection.10.2.3}\protected@file@percent }
\newlabel{sec-H-vs-E}{{10.2.3}{327}{\texorpdfstring {How big is \(SS_H\) compared to \(SS_E\)?}{How big is SS\_H compared to SS\_E?}}{subsection.10.2.3}{}}
\newlabel{eq-F-statistic}{{10.6}{328}{\texorpdfstring {How big is \(SS_H\) compared to \(SS_E\)?}{How big is SS\_H compared to SS\_E?}}{section*.227}{}}
\newlabel{eq-he-eigen}{{10.7}{328}{\texorpdfstring {How big is \(SS_H\) compared to \(SS_E\)?}{How big is SS\_H compared to SS\_E?}}{section*.228}{}}
\newlabel{eq-he-eigen2}{{10.8}{328}{\texorpdfstring {How big is \(SS_H\) compared to \(SS_E\)?}{How big is SS\_H compared to SS\_E?}}{section*.229}{}}
\citation{ref-Friendly-etal:ellipses:2013}
\citation{ref-Schatzoff1966}
\citation{ref-Olson:1974}
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces How test statistics for multivariate tests combine the size of dimensions of \(\mathbf {H}\mathbf {E}^{-1}\) into a single measure.}}{329}{table.caption.230}\protected@file@percent }
\newlabel{tbl-mstats}{{10.1}{329}{How test statistics for multivariate tests combine the size of dimensions of \(\mathbf {H}\mathbf {E}^{-1}\) into a single measure}{table.caption.230}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Multivariate test statistics}{329}{section.10.3}\protected@file@percent }
\newlabel{sec-multivar-tests}{{10.3}{329}{Multivariate test statistics}{section.10.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}Testing contrasts and linear hypotheses}{330}{subsection.10.3.1}\protected@file@percent }
\newlabel{sec-contrasts2}{{10.3.1}{330}{Testing contrasts and linear hypotheses}{subsection.10.3.1}{}}
\newlabel{eq-hmat}{{10.9}{330}{Testing contrasts and linear hypotheses}{section*.231}{}}
\newlabel{eq-H-contrasts}{{10.10}{332}{Testing contrasts and linear hypotheses}{section*.232}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}ANOVA \(\rightarrow \) MANOVA}{333}{section.10.4}\protected@file@percent }
\newlabel{anova-rightarrow-manova}{{10.4}{333}{\texorpdfstring {ANOVA \(\rightarrow \) MANOVA}{ANOVA \textbackslash rightarrow MANOVA}}{section.10.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Diagram of data from simple MANOVA design involving three groups and two response measures, \(Y_1\) and \(Y_2\), summarized by their data ellipses.}}{334}{figure.caption.233}\protected@file@percent }
\newlabel{fig-manova-diagram}{{10.3}{334}{Diagram of data from simple MANOVA design involving three groups and two response measures, \(Y_1\) and \(Y_2\), summarized by their data ellipses}{figure.caption.233}{}}
\citation{ref-Meyers-etal:2006}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces A simple MANOVA design involving three groups and two response measures, \(Y_1\) and \(Y_2\), but with different patterns of the differences among the group means. The red arrows suggest interpretations in terms of dimensions or aspects of the response variables.}}{335}{figure.caption.234}\protected@file@percent }
\newlabel{fig-manova-response-dimensions}{{10.4}{335}{A simple MANOVA design involving three groups and two response measures, \(Y_1\) and \(Y_2\), but with different patterns of the differences among the group means. The red arrows suggest interpretations in terms of dimensions or aspects of the response variables}{figure.caption.234}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.1}Example: Father parenting data}{335}{subsection.10.4.1}\protected@file@percent }
\newlabel{example-father-parenting-data}{{10.4.1}{335}{Example: Father parenting data}{subsection.10.4.1}{}}
\newlabel{exploratory-plots}{{10.4.1}{336}{Exploratory plots}{section*.235}{}}
\@writefile{toc}{\contentsline {subsubsection}{Exploratory plots}{336}{section*.235}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Faceted boxplots of scores on the three parenting scales, showing also the mean for each.}}{337}{figure.caption.236}\protected@file@percent }
\newlabel{fig-parenting-boxpl}{{10.5}{337}{Faceted boxplots of scores on the three parenting scales, showing also the mean for each}{figure.caption.236}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Bivariate data ellipses for pairs of the three responses, showing the means, correlations and variances for the three groups.}}{338}{figure.caption.237}\protected@file@percent }
\newlabel{fig-parenting-covEllipses}{{10.6}{338}{Bivariate data ellipses for pairs of the three responses, showing the means, correlations and variances for the three groups}{figure.caption.237}{}}
\newlabel{testing-the-model}{{10.4.1}{338}{Testing the model}{section*.238}{}}
\@writefile{toc}{\contentsline {subsubsection}{Testing the model}{338}{section*.238}\protected@file@percent }
\newlabel{linear-hypotheses-contrasts}{{10.4.1}{340}{Linear hypotheses \& contrasts}{section*.239}{}}
\@writefile{toc}{\contentsline {subsubsection}{Linear hypotheses \& contrasts}{340}{section*.239}\protected@file@percent }
\citation{ref-Warne2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.2}Ordered factors}{342}{subsection.10.4.2}\protected@file@percent }
\newlabel{ordered-factors}{{10.4.2}{342}{Ordered factors}{subsection.10.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.3}Example: Adolescent mental health}{342}{subsection.10.4.3}\protected@file@percent }
\newlabel{example-adolescent-mental-health}{{10.4.3}{342}{Example: Adolescent mental health}{subsection.10.4.3}{}}
\newlabel{eq-AH-mod}{{10.11}{342}{Example: Adolescent mental health}{section*.240}{}}
\newlabel{exploratory-plots-1}{{10.4.3}{343}{Exploratory plots}{section*.241}{}}
\@writefile{toc}{\contentsline {subsubsection}{Exploratory plots}{343}{section*.241}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Means of anxiety and depression by grade, with \(\pm 1\) standard error bars.}}{344}{figure.caption.242}\protected@file@percent }
\newlabel{fig-addhealth-means-each}{{10.7}{344}{Means of anxiety and depression by grade, with \(\pm 1\) standard error bars}{figure.caption.242}{}}
\newlabel{fit-the-mlm}{{10.4.3}{344}{Fit the MLM}{section*.244}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fit the MLM}{344}{section*.244}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces Within-group covariance ellipses for the \texttt  {grade} groups.}}{345}{figure.caption.243}\protected@file@percent }
\newlabel{fig-addhealth-covellipse}{{10.8}{345}{Within-group covariance ellipses for the \texttt {grade} groups}{figure.caption.243}{}}
\citation{ref-JohnsonWichern1998}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}Factorial MANOVA}{347}{section.10.5}\protected@file@percent }
\newlabel{sec-factorial-manova}{{10.5}{347}{Factorial MANOVA}{section.10.5}{}}
\newlabel{exm-plastic1}{{10.1}{347}{}{example.10.1}{}}
\citation{ref-R-ggdist}
\citation{ref-Plaster:89}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Line plots of means and their standard errors for the response \texttt  {tear} (left) and \texttt  {gloss} (right) in the plastic film data.}}{349}{figure.caption.245}\protected@file@percent }
\newlabel{fig-plastic-ggline}{{10.9}{349}{Line plots of means and their standard errors for the response \texttt {tear} (left) and \texttt {gloss} (right) in the plastic film data}{figure.caption.245}{}}
\newlabel{exm-MockJury1}{{10.2}{349}{}{example.10.2}{}}
\citation{ref-Tibshriani:regr:1996}
\citation{ref-Harrell2015RMS}
\@writefile{toc}{\contentsline {section}{\numberline {10.6}MRA \(\rightarrow \) MMRA}{351}{section.10.6}\protected@file@percent }
\newlabel{sec-MRA-to-MMRA}{{10.6}{351}{\texorpdfstring {MRA \(\rightarrow \) MMRA}{MRA \textbackslash rightarrow MMRA}}{section.10.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces Line plots of means and their standard errors for the response \texttt  {Years} (left) and \texttt  {Serious} (right) in the Mock Jury data.}}{352}{figure.caption.246}\protected@file@percent }
\newlabel{fig-jury-ggline}{{10.10}{352}{Line plots of means and their standard errors for the response \texttt {Years} (left) and \texttt {Serious} (right) in the Mock Jury data}{figure.caption.246}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.1}Example: NLSY data}{352}{subsection.10.6.1}\protected@file@percent }
\newlabel{sec-NLSY-mmra}{{10.6.1}{352}{Example: NLSY data}{subsection.10.6.1}{}}
\newlabel{exploratory-plots-2}{{10.6.1}{353}{Exploratory plots}{section*.247}{}}
\@writefile{toc}{\contentsline {subsubsection}{Exploratory plots}{353}{section*.247}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces Density plots for the variables in the \texttt  {NLSY} dataset.}}{354}{figure.caption.248}\protected@file@percent }
\newlabel{fig-NLSY-density}{{10.11}{354}{Density plots for the variables in the \texttt {NLSY} dataset}{figure.caption.248}{}}
\newlabel{fitting-models}{{10.6.1}{354}{Fitting models}{section*.251}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fitting models}{354}{section*.251}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.12}{\ignorespaces Scatterplot of mathematics score against reading score in the NLSY data}}{355}{figure.caption.249}\protected@file@percent }
\newlabel{fig-NLSY-scat1}{{10.12}{355}{Scatterplot of mathematics score against reading score in the NLSY data}{figure.caption.249}{}}
\newlabel{overall-test}{{10.6.1}{355}{Overall test}{section*.252}{}}
\@writefile{toc}{\contentsline {subsubsection}{Overall test}{355}{section*.252}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.13}{\ignorespaces Scatterplot of mathematics score against reading score in the NLSY data, highlighting noteworthy points}}{356}{figure.caption.250}\protected@file@percent }
\newlabel{fig-NLSY-scat2}{{10.13}{356}{Scatterplot of mathematics score against reading score in the NLSY data, highlighting noteworthy points}{figure.caption.250}{}}
\newlabel{coefficient-plots}{{10.6.1}{356}{Coefficient plots}{section*.253}{}}
\@writefile{toc}{\contentsline {subsubsection}{Coefficient plots}{356}{section*.253}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.6.1.1}Behavioral measures}{357}{subsubsection.10.6.1.1}\protected@file@percent }
\newlabel{behavioral-measures}{{10.6.1.1}{357}{Behavioral measures}{subsubsection.10.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.14}{\ignorespaces Bivariate coefficient plot for reading and math with 95\% confidence ellipses. The variables have been standardized to make their units comparable.}}{358}{figure.caption.254}\protected@file@percent }
\newlabel{fig-NLSY-coefplot1}{{10.14}{358}{Bivariate coefficient plot for reading and math with 95\% confidence ellipses. The variables have been standardized to make their units comparable}{figure.caption.254}{}}
\citation{ref-Charnes-etal-1981}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.2}Example: School data}{359}{subsection.10.6.2}\protected@file@percent }
\newlabel{sec-schooldata-mmra}{{10.6.2}{359}{Example: School data}{subsection.10.6.2}{}}
\newlabel{exploratory-plots-3}{{10.6.2}{359}{Exploratory plots}{section*.255}{}}
\@writefile{toc}{\contentsline {subsubsection}{Exploratory plots}{359}{section*.255}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.15}{\ignorespaces \(\mitchi ^2\) QQ plot of the \texttt  {schooldata} variables.}}{360}{figure.caption.256}\protected@file@percent }
\newlabel{fig-schooldata-cqplot}{{10.15}{360}{\(\chi ^2\) QQ plot of the \texttt {schooldata} variables}{figure.caption.256}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.16}{\ignorespaces Scatterplots of each of the three response variables against each of the five predictors in the \texttt  {schooldata} dataset. Three of the points identified as possible multivariate outliers are labeled.}}{362}{figure.caption.257}\protected@file@percent }
\newlabel{fig-schooldata-scats}{{10.16}{362}{Scatterplots of each of the three response variables against each of the five predictors in the \texttt {schooldata} dataset. Three of the points identified as possible multivariate outliers are labeled}{figure.caption.257}{}}
\newlabel{fitting-models-1}{{10.6.2}{362}{Fitting models}{section*.258}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fitting models}{362}{section*.258}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.7}Model diagnostics for MLMs}{363}{section.10.7}\protected@file@percent }
\newlabel{sec-model-diagnostics-MLM}{{10.7}{363}{Model diagnostics for MLMs}{section.10.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.1}Multivariate normality of residuals}{363}{subsection.10.7.1}\protected@file@percent }
\newlabel{multivariate-normality-of-residuals}{{10.7.1}{363}{Multivariate normality of residuals}{subsection.10.7.1}{}}
\citation{ref-R-MVN}
\citation{ref-Mardia:1974}
\@writefile{lof}{\contentsline {figure}{\numberline {10.17}{\ignorespaces \(\mitchi ^2\) QQ plot of the residuals in the \texttt  {schooldat} multivariate regression model.}}{364}{figure.caption.259}\protected@file@percent }
\newlabel{fig-schoolmod-cqplot}{{10.17}{364}{\(\chi ^2\) QQ plot of the residuals in the \texttt {schooldat} multivariate regression model}{figure.caption.259}{}}
\citation{ref-Rousseeuw2004}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.2}Distance plot}{365}{subsection.10.7.2}\protected@file@percent }
\newlabel{distance-plot}{{10.7.2}{365}{Distance plot}{subsection.10.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.18}{\ignorespaces Plot of Mahalanobis distances of the least squares residuals vs.~Mahalanobis distances of the predictors in the model}}{365}{figure.caption.260}\protected@file@percent }
\newlabel{fig-school-distplot}{{10.18}{365}{Plot of Mahalanobis distances of the least squares residuals vs.~Mahalanobis distances of the predictors in the model}{figure.caption.260}{}}
\citation{ref-Rousseeuw2004}
\citation{ref-R-mvinfluence}
\citation{ref-BarrettLing:92}
\citation{ref-Barrett:2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.3}Multivariate influence}{366}{subsection.10.7.3}\protected@file@percent }
\newlabel{sec-multivar-infl}{{10.7.3}{366}{Multivariate influence}{subsection.10.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.19}{\ignorespaces Influence plot for the \texttt  {schooldat} multivariate regression model. Five cases are labeled as ``noteworthy'' on either axis.}}{367}{figure.caption.261}\protected@file@percent }
\newlabel{fig-schoolmod-infl}{{10.19}{367}{Influence plot for the \texttt {schooldat} multivariate regression model. Five cases are labeled as ``noteworthy'' on either axis}{figure.caption.261}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.8}ANCOVA \(\rightarrow \) MANCOVA}{368}{section.10.8}\protected@file@percent }
\newlabel{sec-ANCOVA-MANCOVA}{{10.8}{368}{\texorpdfstring {ANCOVA \(\rightarrow \) MANCOVA}{ANCOVA \textbackslash rightarrow MANCOVA}}{section.10.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.20}{\ignorespaces Two possible outcome patterns for a two-group design assessing the effect of a treatment on weight, measured pre- and post-treatment. (a) Additive effects of Group and \(x\); (b) Different slopes for the two groups. Plus signs show the means \((\bar {x}_i, \bar {y}_i)\) for the two groups.}}{369}{figure.caption.262}\protected@file@percent }
\newlabel{fig-ANCOVA-ex}{{10.20}{369}{Two possible outcome patterns for a two-group design assessing the effect of a treatment on weight, measured pre- and post-treatment. (a) Additive effects of Group and \(x\); (b) Different slopes for the two groups. Plus signs show the means \((\bar {x}_i, \bar {y}_i)\) for the two groups}{figure.caption.262}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.8.1}Example: Paired-associate tasks and academic performance}{369}{subsection.10.8.1}\protected@file@percent }
\newlabel{sec-PA-tasks}{{10.8.1}{369}{Example: Paired-associate tasks and academic performance}{subsection.10.8.1}{}}
\citation{ref-Timm:75}
\@writefile{lof}{\contentsline {figure}{\numberline {10.21}{\ignorespaces Scatterplots of each of the three response variables against each of the five predictors in the \texttt  {Rohwer} dataset.}}{371}{figure.caption.263}\protected@file@percent }
\newlabel{fig-Rohwer-scats}{{10.21}{371}{Scatterplots of each of the three response variables against each of the five predictors in the \texttt {Rohwer} dataset}{figure.caption.263}{}}
\newlabel{mancova-model}{{10.8.1}{371}{MANCOVA model}{section*.264}{}}
\@writefile{toc}{\contentsline {subsubsection}{MANCOVA model}{371}{section*.264}\protected@file@percent }
\newlabel{adjusted-means}{{10.8.1}{372}{Adjusted means}{section*.266}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adjusted means}{372}{section*.266}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.22}{\ignorespaces Bivariate coefficient plots for the MANCOVA model with confidence ellipses of 68\% coverage.}}{373}{figure.caption.265}\protected@file@percent }
\newlabel{fig-rohwer-mod1-coef}{{10.22}{373}{Bivariate coefficient plots for the MANCOVA model with confidence ellipses of 68\% coverage}{figure.caption.265}{}}
\newlabel{homogeneity-of-regression}{{10.8.1}{373}{Homogeneity of regression}{section*.267}{}}
\@writefile{toc}{\contentsline {subsubsection}{Homogeneity of regression}{373}{section*.267}\protected@file@percent }
\newlabel{separate-models}{{10.8.1}{374}{Separate models}{section*.268}{}}
\@writefile{toc}{\contentsline {subsubsection}{Separate models}{374}{section*.268}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.9}What have we learned?}{375}{section.10.9}\protected@file@percent }
\newlabel{what-have-we-learned-8}{{10.9}{375}{What have we learned?}{section.10.9}{}}
\citation{ref-Huang2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Visualizing Multivariate Models}{377}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-vis-mlm}{{11}{377}{Visualizing Multivariate Models}{chapter.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}HE plot framework}{378}{section.11.1}\protected@file@percent }
\newlabel{sec-he-framework}{{11.1}{378}{HE plot framework}{section.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces \textbf  {Dogfood quartet}: Illustration of the conceptual ideas of the HE plot framework for the dogfood data. (a) Scatterplot of the data; (b) Summary using data ellipses; (c) HE plot shows the variation in the means in relation to pooled within group variance; (d) Transformation from data space to canonical space}}{379}{figure.caption.269}\protected@file@percent }
\newlabel{fig-dogfood-quartet}{{11.1}{379}{\textbf {Dogfood quartet}: Illustration of the conceptual ideas of the HE plot framework for the dogfood data. (a) Scatterplot of the data; (b) Summary using data ellipses; (c) HE plot shows the variation in the means in relation to pooled within group variance; (d) Transformation from data space to canonical space}{figure.caption.269}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}HE plot construction}{380}{section.11.2}\protected@file@percent }
\newlabel{sec-he-plot-construct}{{11.2}{380}{HE plot construction}{section.11.2}{}}
\newlabel{eq-Ec}{{11.1}{380}{HE plot construction}{section*.270}{}}
\citation{ref-Anderson:35}
\citation{ref-Fisher:1936}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces HE plot for the \texttt  {dogfood} data, showing the means of the four groups, which generates the \(\mathbf {H}\) ellipse for the effect of \texttt  {formula}. The \(\mathbf {E}\) ellipse labeled `Error' shows the within-group variances and covariance.}}{381}{figure.caption.271}\protected@file@percent }
\newlabel{fig-dogfood-HE}{{11.2}{381}{HE plot for the \texttt {dogfood} data, showing the means of the four groups, which generates the \(\mathbf {H}\) ellipse for the effect of \texttt {formula}. The \(\mathbf {E}\) ellipse labeled `Error' shows the within-group variances and covariance}{figure.caption.271}{}}
\newlabel{exm-iris-data}{{11.1}{381}{}{example.11.1}{}}
\citation{ref-Bodmer-etal-2021}
\citation{ref-DeSilva2020}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Diagram of an iris flower showing the measurements of petal and sepal size. Each flower has three sepals and three alternating petals. The sepals have brightly colored central sections. \emph  {Source}: Gayan De Silva (\citeproc {ref-DeSilva2020}{2020})}}{382}{figure.caption.272}\protected@file@percent }
\newlabel{fig-iris-diagram}{{11.3}{382}{Diagram of an iris flower showing the measurements of petal and sepal size. Each flower has three sepals and three alternating petals. The sepals have brightly colored central sections. \emph {Source}: Gayan De Silva (\citeproc {ref-DeSilva2020}{2020})}{figure.caption.272}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}MANOVA model}{383}{subsection.11.2.1}\protected@file@percent }
\newlabel{sec-iris-mod}{{11.2.1}{383}{MANOVA model}{subsection.11.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Scatterplot matrix of the \texttt  {iris} dataset. The species are summarized by 68\% data ellipses and linear regression lines in each pairwise plot.}}{384}{figure.caption.273}\protected@file@percent }
\newlabel{fig-iris-spm}{{11.4}{384}{Scatterplot matrix of the \texttt {iris} dataset. The species are summarized by 68\% data ellipses and linear regression lines in each pairwise plot}{figure.caption.273}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}HE plots}{385}{section.11.3}\protected@file@percent }
\newlabel{sec-he-plots}{{11.3}{385}{HE plots}{section.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces HE plots for the multivariate model \texttt  {iris.mod}. The left panel shows the plot for the Sepal variables; the right panel plots the Petal variables.}}{386}{figure.caption.274}\protected@file@percent }
\newlabel{fig-iris-HE1}{{11.5}{386}{HE plots for the multivariate model \texttt {iris.mod}. The left panel shows the plot for the Sepal variables; the right panel plots the Petal variables}{figure.caption.274}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Significance scaling}{386}{section.11.4}\protected@file@percent }
\newlabel{sec-signif-scaling}{{11.4}{386}{Significance scaling}{section.11.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces HE plots for sepal width and sepal length in the iris dataset. Left: \emph  {effect} scaling of the \(\mathbf {H}\) matrix; right: \emph  {significance} scaling, where protrusion of \(\mathbf {H}\) outside \(\mathbf {E}\) indicates a significant effect by Roy's test.}}{387}{figure.caption.275}\protected@file@percent }
\newlabel{fig-iris-HE2}{{11.6}{387}{HE plots for sepal width and sepal length in the iris dataset. Left: \emph {effect} scaling of the \(\mathbf {H}\) matrix; right: \emph {significance} scaling, where protrusion of \(\mathbf {H}\) outside \(\mathbf {E}\) indicates a significant effect by Roy's test}{figure.caption.275}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.5}Visualizing contrasts and linear hypotheses}{387}{section.11.5}\protected@file@percent }
\newlabel{sec-he-vis-contrasts}{{11.5}{387}{Visualizing contrasts and linear hypotheses}{section.11.5}{}}
\newlabel{eq-H-Species}{{11.2}{387}{Visualizing contrasts and linear hypotheses}{section*.276}{}}
\citation{ref-Friendly-etal:ellipses:2013}
\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces HE plot for sepal length and width in the \texttt  {iris} data showing the tests of the two contrasts, using significance scaling.}}{389}{figure.caption.277}\protected@file@percent }
\newlabel{fig-iris-contrasts}{{11.7}{389}{HE plot for sepal length and width in the \texttt {iris} data showing the tests of the two contrasts, using significance scaling}{figure.caption.277}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}HE plot matrices}{389}{section.11.6}\protected@file@percent }
\newlabel{sec-HEplot-matrices}{{11.6}{389}{HE plot matrices}{section.11.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.8}{\ignorespaces All pairwise HE plots for the iris data.}}{390}{figure.caption.278}\protected@file@percent }
\newlabel{fig-iris-pairs}{{11.8}{390}{All pairwise HE plots for the iris data}{figure.caption.278}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.7}Low-D views: Canonical analysis}{390}{section.11.7}\protected@file@percent }
\newlabel{sec-candisc}{{11.7}{390}{Low-D views: Canonical analysis}{section.11.7}{}}
\citation{ref-Gittins:85}
\citation{ref-R-candisc}
\citation{ref-Bartlett1938}
\newlabel{eq-canZ}{{11.3}{391}{Low-D views: Canonical analysis}{section*.279}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.1}Coeficients}{392}{subsection.11.7.1}\protected@file@percent }
\newlabel{coeficients}{{11.7.1}{392}{Coeficients}{subsection.11.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.2}Canonical scores plot}{393}{subsection.11.7.2}\protected@file@percent }
\newlabel{canonical-scores-plot}{{11.7.2}{393}{Canonical scores plot}{subsection.11.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.3}Canonical HE plot}{393}{subsection.11.7.3}\protected@file@percent }
\newlabel{canonical-he-plot}{{11.7.3}{393}{Canonical HE plot}{subsection.11.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.9}{\ignorespaces Plot of canonical scores for the iris data. Ellipses give 68\% coverage data ellipses for the canonical scores. Variable vectors make angles with the Can1 and Can2 axes indicating their correlations.}}{394}{figure.caption.280}\protected@file@percent }
\newlabel{fig-iris-candisc}{{11.9}{394}{Plot of canonical scores for the iris data. Ellipses give 68\% coverage data ellipses for the canonical scores. Variable vectors make angles with the Can1 and Can2 axes indicating their correlations}{figure.caption.280}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.10}{\ignorespaces Canonical HE plot for the iris data. Compared with Figure~\ref {fig-iris-candisc}, it substutes canonical \(\mathbf {H}\) and \(\mathbf {E}\) ellipses for the canonical scores shown there.}}{395}{figure.caption.281}\protected@file@percent }
\newlabel{fig-iris-HEcan}{{11.10}{395}{Canonical HE plot for the iris data. Compared with Figure~\ref {fig-iris-candisc}, it substutes canonical \(\mathbf {H}\) and \(\mathbf {E}\) ellipses for the canonical scores shown there}{figure.caption.281}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.8}Factorial MANOVA}{395}{section.11.8}\protected@file@percent }
\newlabel{sec-HE-factorial}{{11.8}{395}{Factorial MANOVA}{section.11.8}{}}
\newlabel{exm-plastic2}{{11.2}{395}{}{example.11.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.11}{\ignorespaces HE plot for effects on \texttt  {tear} and \texttt  {gloss} according to the factors \texttt  {rate}, \texttt  {additive} and their interaction, \texttt  {rate:additive}. The thicker lines show effect size scaling; thinner lines show significance scaling.}}{396}{figure.caption.282}\protected@file@percent }
\newlabel{fig-plastic-HE1}{{11.11}{396}{HE plot for effects on \texttt {tear} and \texttt {gloss} according to the factors \texttt {rate}, \texttt {additive} and their interaction, \texttt {rate:additive}. The thicker lines show effect size scaling; thinner lines show significance scaling}{figure.caption.282}{}}
\newlabel{exm-MockJury2}{{11.3}{397}{}{example.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.12}{\ignorespaces HE plot for effects on \texttt  {tear} and \texttt  {gloss} using significance scaling. To this is added points showing the means for the combinations of rate and additive.}}{398}{figure.caption.283}\protected@file@percent }
\newlabel{fig-plastic-HE2}{{11.12}{398}{HE plot for effects on \texttt {tear} and \texttt {gloss} using significance scaling. To this is added points showing the means for the combinations of rate and additive}{figure.caption.283}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.13}{\ignorespaces Two pairwise HE plots showing the effect of the classified attractivenes of the photo and ratings of those photos. Left: \texttt  {exciting} vs.~\texttt  {phyattr} ratings; right: \texttt  {independent} vs.~\texttt  {phyattr} ratings.}}{399}{figure.caption.284}\protected@file@percent }
\newlabel{fig-jury-HE}{{11.13}{399}{Two pairwise HE plots showing the effect of the classified attractivenes of the photo and ratings of those photos. Left: \texttt {exciting} vs.~\texttt {phyattr} ratings; right: \texttt {independent} vs.~\texttt {phyattr} ratings}{figure.caption.284}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.9}Quantitative predictors: MMRA}{400}{section.11.9}\protected@file@percent }
\newlabel{sec-he-mmra}{{11.9}{400}{Quantitative predictors: MMRA}{section.11.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.14}{\ignorespaces Canonical discriminant plot for the factor \texttt  {Attr} in the model \texttt  {jury.mod1} for the ratings of the photos classified as \textcolor {blue}{Beautiful}, \textcolor {darkgreen}{Average} or \textcolor {red}{Unattractive}. Ellipses have 50\% coverage for the canonical scores. Variable vectors reflect the correlations of the rating scales with the canonical dimensions.}}{401}{figure.caption.285}\protected@file@percent }
\newlabel{fig-jury-can}{{11.14}{401}{Canonical discriminant plot for the factor \texttt {Attr} in the model \texttt {jury.mod1} for the ratings of the photos classified as \textcolor {blue}{Beautiful}, \textcolor {darkgreen}{Average} or \textcolor {red}{Unattractive}. Ellipses have 50\% coverage for the canonical scores. Variable vectors reflect the correlations of the rating scales with the canonical dimensions}{figure.caption.285}{}}
\newlabel{exm-NLSY-HE}{{11.4}{401}{}{example.11.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.15}{\ignorespaces HE plot for the simple model for the NLSY data fitting reading and math scores from income and education.}}{402}{figure.caption.286}\protected@file@percent }
\newlabel{fig-NLSY-heplot1}{{11.15}{402}{HE plot for the simple model for the NLSY data fitting reading and math scores from income and education}{figure.caption.286}{}}
\newlabel{exm-schooldata-HE}{{11.5}{402}{}{example.11.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.16}{\ignorespaces HE plot adding the \(\mathbf {H}\) ellipse for the overall test that both predictors have no effect on the outcome scores.}}{403}{figure.caption.287}\protected@file@percent }
\newlabel{fig-NLSY-heplot2}{{11.16}{403}{HE plot adding the \(\mathbf {H}\) ellipse for the overall test that both predictors have no effect on the outcome scores}{figure.caption.287}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.17}{\ignorespaces HE plot for reading and mathematics scores in the multivariate regression model for the school dataset. Predictor effects appear as lines whose lenght indicates the magnitude of the relationship and whose orientation reflects their correlations with the outcome variables shown in the plot.}}{404}{figure.caption.288}\protected@file@percent }
\newlabel{fig-school-heplot1}{{11.17}{404}{HE plot for reading and mathematics scores in the multivariate regression model for the school dataset. Predictor effects appear as lines whose lenght indicates the magnitude of the relationship and whose orientation reflects their correlations with the outcome variables shown in the plot}{figure.caption.288}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.18}{\ignorespaces Pairwise HE plots for the three outcome variables in the multivariate regression model for the school dataset.}}{405}{figure.caption.289}\protected@file@percent }
\newlabel{fig-school-heplot2}{{11.18}{405}{Pairwise HE plots for the three outcome variables in the multivariate regression model for the school dataset}{figure.caption.289}{}}
\citation{ref-Gittins:85}
\@writefile{toc}{\contentsline {section}{\numberline {11.10}Canonical correlation analysis}{406}{section.11.10}\protected@file@percent }
\newlabel{sec-cancor}{{11.10}{406}{Canonical correlation analysis}{section.11.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.19}{\ignorespaces Diagram illustrating canonical correlation. For two \(y\) variables, all linear combinations are vectors in their plane, and similarly for the \(x\) variables. Maximizing the correlation between linear combinations of each is equivalent to making the angle \(\mitphi \) between them as small as possible, or maximizing \(\cos ({\mittheta })\), shown in the diagram at the right. The thick grey arrow indignates that the two planes should be overlaid at a common origin. \emph  {Source}: Re-drawn by Udi Alter following a Cross-Validated discussion by user `ttnphns', https://bit.ly/4dgq2cp}}{406}{figure.caption.290}\protected@file@percent }
\newlabel{fig-cancor-diagram}{{11.19}{406}{Diagram illustrating canonical correlation. For two \(y\) variables, all linear combinations are vectors in their plane, and similarly for the \(x\) variables. Maximizing the correlation between linear combinations of each is equivalent to making the angle \(\phi \) between them as small as possible, or maximizing \(\cos ({\theta })\), shown in the diagram at the right. The thick grey arrow indignates that the two planes should be overlaid at a common origin. \emph {Source}: Re-drawn by Udi Alter following a Cross-Validated discussion by user `ttnphns', https://bit.ly/4dgq2cp}{figure.caption.290}{}}
\citation{ref-Hotelling1936}
\newlabel{exm-schooldata-cancor}{{11.6}{407}{}{example.11.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.20}{\ignorespaces Plots of canonical scores for the first two canonical dimensions of the \texttt  {schooldata} dataset, omitting the two highly influential cases.}}{409}{figure.caption.291}\protected@file@percent }
\newlabel{fig-school-can}{{11.20}{409}{Plots of canonical scores for the first two canonical dimensions of the \texttt {schooldata} dataset, omitting the two highly influential cases}{figure.caption.291}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.21}{\ignorespaces Plots of canonical scores on the first canonical dimension for the \texttt  {schooldata}, including the influential cases, which stand out as so far frome the rest of the observations.}}{410}{figure.caption.292}\protected@file@percent }
\newlabel{fig-school-can0}{{11.21}{410}{Plots of canonical scores on the first canonical dimension for the \texttt {schooldata}, including the influential cases, which stand out as so far frome the rest of the observations}{figure.caption.292}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.11}MANCOVA models}{410}{section.11.11}\protected@file@percent }
\newlabel{sec-he-mancova}{{11.11}{410}{MANCOVA models}{section.11.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.22}{\ignorespaces HE plot for the canonical correlation analysis of the schooldata. Vectors for the variables indicate their correlations with the canonical dimensions.}}{411}{figure.caption.293}\protected@file@percent }
\newlabel{fig-school-hecan}{{11.22}{411}{HE plot for the canonical correlation analysis of the schooldata. Vectors for the variables indicate their correlations with the canonical dimensions}{figure.caption.293}{}}
\newlabel{exm-rowher-mancova}{{11.7}{411}{}{example.11.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.23}{\ignorespaces All-pairs HE plot for SAT, PPVT and Raven using the MANCOVA model. The ellipses labeled `Regr' show the test of the overall effect of the quantitative predictors.}}{412}{figure.caption.294}\protected@file@percent }
\newlabel{fig-rohwer-HE-mod1-pairs}{{11.23}{412}{All-pairs HE plot for SAT, PPVT and Raven using the MANCOVA model. The ellipses labeled `Regr' show the test of the overall effect of the quantitative predictors}{figure.caption.294}{}}
\newlabel{homogeneity-of-regression-1}{{11.7}{413}{Homogeneity of regression}{section*.295}{}}
\@writefile{toc}{\contentsline {subsubsection}{Homogeneity of regression}{413}{section*.295}\protected@file@percent }
\newlabel{separate-models-1}{{11.7}{413}{Separate models}{section*.297}{}}
\@writefile{toc}{\contentsline {subsubsection}{Separate models}{413}{section*.297}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.24}{\ignorespaces HE plot for SAT and PPVT using the heterogeneous regression model. The ellipse labeled `Regr' shows the test of the covariates combined, and the ellipse labeled `slopes' shows the combined difference in slopes between the two groups.}}{414}{figure.caption.296}\protected@file@percent }
\newlabel{fig-rohwer-HE-mod2}{{11.24}{414}{HE plot for SAT and PPVT using the heterogeneous regression model. The ellipse labeled `Regr' shows the test of the covariates combined, and the ellipse labeled `slopes' shows the combined difference in slopes between the two groups}{figure.caption.296}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.12}What have we learned?}{414}{section.11.12}\protected@file@percent }
\newlabel{what-have-we-learned-9}{{11.12}{414}{What have we learned?}{section.11.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.25}{\ignorespaces Overlaid HE plots for SAT and PPVT, for the low and high SES groups, when each group is fit separately.}}{415}{figure.caption.298}\protected@file@percent }
\newlabel{fig-rohwer-HE-lohi}{{11.25}{415}{Overlaid HE plots for SAT and PPVT, for the low and high SES groups, when each group is fit separately}{figure.caption.298}{}}
\citation{ref-Box:1953}
\citation{ref-Lix:1996}
\citation{ref-Gastwirth-etal:2009}
\citation{ref-Welch:1947}
\citation{ref-Box:1949}
\citation{ref-FriendlySigal:2018:eqcov}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Visualizing Equality of Covariance Matrices}{417}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-eqcov}{{12}{417}{Visualizing Equality of Covariance Matrices}{chapter.12}{}}
\citation{ref-Harwell:1992}
\citation{ref-Hartley:1950}
\citation{ref-Cochran:1941}
\citation{ref-Bartlett:1937}
\citation{ref-Rogan:1977}
\citation{ref-Levene:1960}
\citation{ref-BrownForsythe:1974}
\citation{ref-Conover-etal:1981}
\citation{ref-Gastwirth-etal:2009}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Homogeneity of Variance in Univariate ANOVA}{418}{section.12.1}\protected@file@percent }
\newlabel{sec-homogeneity-ANOVA}{{12.1}{418}{Homogeneity of Variance in Univariate ANOVA}{section.12.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Visualizing Levene's test}{420}{section.12.2}\protected@file@percent }
\newlabel{sec-mlevene}{{12.2}{420}{Visualizing Levene's test}{section.12.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces Boxplots for the Penguin variables. For assessing homogeneity of variance, we should be looking for differences in \textbf  {width} of the central 50\% boxes in each panel, rather than difference in central tendency.}}{421}{figure.caption.299}\protected@file@percent }
\newlabel{fig-peng-boxplots}{{12.1}{421}{Boxplots for the Penguin variables. For assessing homogeneity of variance, we should be looking for differences in \textbf {width} of the central 50\% boxes in each panel, rather than difference in central tendency}{figure.caption.299}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Homogeneity of variance in MANOVA}{421}{section.12.3}\protected@file@percent }
\newlabel{sec-homogeneity-MANOVA}{{12.3}{421}{Homogeneity of variance in MANOVA}{section.12.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces Boxplots for absolute differences from group medians for the Penguin data. The visual test of equality of variance is whether the median lines in the boxplots align.}}{422}{figure.caption.300}\protected@file@percent }
\newlabel{fig-peng-devplots}{{12.2}{422}{Boxplots for absolute differences from group medians for the Penguin data. The visual test of equality of variance is whether the median lines in the boxplots align}{figure.caption.300}{}}
\newlabel{exm-peng-covellipses}{{12.1}{422}{}{example.12.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces Data ellipses for bill length and bill depth in the penguins data, also showing the pooled covariance. Left: As is; right: these are centered at the grand means for easier comparison.}}{423}{figure.caption.301}\protected@file@percent }
\newlabel{fig-peng-covEllipse0}{{12.3}{423}{Data ellipses for bill length and bill depth in the penguins data, also showing the pooled covariance. Left: As is; right: these are centered at the grand means for easier comparison}{figure.caption.301}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces All pairwise covariance ellipses for the penguins data. The covariance matrices are homogeneous when the ellipses for the groups all have the same size and shape as that for the mean-centered pooled data (shown in black).}}{424}{figure.caption.302}\protected@file@percent }
\newlabel{fig-peng-covEllipse-pairs}{{12.4}{424}{All pairwise covariance ellipses for the penguins data. The covariance matrices are homogeneous when the ellipses for the groups all have the same size and shape as that for the mean-centered pooled data (shown in black)}{figure.caption.302}{}}
\newlabel{exm-iris-covellipses}{{12.2}{424}{}{example.12.2}{}}
\citation{ref-Box:1949}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Box's \(\symcal {M}\) test}{425}{section.12.4}\protected@file@percent }
\newlabel{sec-boxM}{{12.4}{425}{\texorpdfstring {Box's \(\mathcal {M}\) test}{Box's \textbackslash mathcal\{M\} test}}{section.12.4}{}}
\newlabel{eq-boxm}{{12.1}{425}{\texorpdfstring {Box's \(\mathcal {M}\) test}{Box's \textbackslash mathcal\{M\} test}}{section*.304}{}}
\citation{ref-Timm:75}
\citation{ref-Box:1949}
\citation{ref-Box:1950}
\@writefile{lof}{\contentsline {figure}{\numberline {12.5}{\ignorespaces All pairwise covariance ellipses for the iris data. The large differences in size and shape indicate substantial heterogeneity in variances and covariances.}}{426}{figure.caption.303}\protected@file@percent }
\newlabel{fig-iris-covEllipse-pairs}{{12.5}{426}{All pairwise covariance ellipses for the iris data. The large differences in size and shape indicate substantial heterogeneity in variances and covariances}{figure.caption.303}{}}
\citation{ref-TikuBalakrishnan:1984}
\citation{ref-OBrien:1992}
\citation{ref-ZhangBoos:1992:BCV}
\citation{ref-Box:1953}
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Visualizing heterogeneity}{427}{section.12.5}\protected@file@percent }
\newlabel{visualizing-heterogeneity}{{12.5}{427}{Visualizing heterogeneity}{section.12.5}{}}
\citation{ref-FriendlySigal:2018:eqcov}
\citation{ref-Cai-etal:2015}
\@writefile{toc}{\contentsline {section}{\numberline {12.6}Visualizing Box's \(\symcal {M}\)}{428}{section.12.6}\protected@file@percent }
\newlabel{sec-viz-boxM}{{12.6}{428}{\texorpdfstring {Visualizing Box's \(\mathcal {M}\)}{Visualizing Box's \textbackslash mathcal\{M\}}}{section.12.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.6}{\ignorespaces Plots of the contributions to Box's \(\symcal {M}\) statistic for the Penguin and iris data.}}{428}{figure.caption.305}\protected@file@percent }
\newlabel{fig-peng-iris-boxm-plots}{{12.6}{428}{Plots of the contributions to Box's \(\mathcal {M}\) statistic for the Penguin and iris data}{figure.caption.305}{}}
\citation{ref-FriendlySigal:2018:eqcov}
\@writefile{toc}{\contentsline {section}{\numberline {12.7}Low-rank views}{429}{section.12.7}\protected@file@percent }
\newlabel{sec-eqcov-low-rank-views}{{12.7}{429}{Low-rank views}{section.12.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.7.1}Small dimensions can matter}{429}{subsection.12.7.1}\protected@file@percent }
\newlabel{small-dimensions-can-matter}{{12.7.1}{429}{Small dimensions can matter}{subsection.12.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.7}{\ignorespaces Covariance ellipsoids for the first two principal components of the iris data. Left (a): uncentered, showing group means on the principal components; right (b): centered at the origin, making size and shape comparison easier.}}{430}{figure.caption.306}\protected@file@percent }
\newlabel{fig-iris-pca-covellipses}{{12.7}{430}{Covariance ellipsoids for the first two principal components of the iris data. Left (a): uncentered, showing group means on the principal components; right (b): centered at the origin, making size and shape comparison easier}{figure.caption.306}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.8}Other measures of heterogeneity}{430}{section.12.8}\protected@file@percent }
\newlabel{sec-other-measures}{{12.8}{430}{Other measures of heterogeneity}{section.12.8}{}}
\citation{ref-Friendly-etal:ellipses:2013}
\@writefile{lof}{\contentsline {figure}{\numberline {12.8}{\ignorespaces Covariance ellipses for the smallest principal components of the iris data.}}{431}{figure.caption.307}\protected@file@percent }
\newlabel{fig-iris-pca-covellipses-dim34}{{12.8}{431}{Covariance ellipses for the smallest principal components of the iris data}{figure.caption.307}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12.1}{\ignorespaces Statistical and geometrical properties of ``size'' of an ellipsoid}}{431}{table.caption.308}\protected@file@percent }
\newlabel{tbl-eigval-ellipse}{{12.1}{431}{Statistical and geometrical properties of ``size'' of an ellipsoid}{table.caption.308}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.9}{\ignorespaces Plot of eigenvalue statistics of the covariance matrices for the Penguin data.}}{432}{figure.caption.309}\protected@file@percent }
\newlabel{fig-peng-boxm-plots}{{12.9}{432}{Plot of eigenvalue statistics of the covariance matrices for the Penguin data}{figure.caption.309}{}}
\citation{ref-Anderson2006}
\@writefile{toc}{\contentsline {section}{\numberline {12.9}Multivariate analog of Levene's test}{433}{section.12.9}\protected@file@percent }
\newlabel{sec-multivar-levene}{{12.9}{433}{Multivariate analog of Levene's test}{section.12.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.9.1}Canonical discriminant analysis}{433}{subsection.12.9.1}\protected@file@percent }
\newlabel{canonical-discriminant-analysis}{{12.9.1}{433}{Canonical discriminant analysis}{subsection.12.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.10}{\ignorespaces HE plots for the multivariate generalization of Levene's test based on MANOVA of absolute deviations from group medians.}}{434}{figure.caption.310}\protected@file@percent }
\newlabel{fig-iris-dev-pairs}{{12.10}{434}{HE plots for the multivariate generalization of Levene's test based on MANOVA of absolute deviations from group medians}{figure.caption.310}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.11}{\ignorespaces Canonical discriminant plot for the multivariate generalization of Levene's test based on MANOVA of absolute deviations from group medians.}}{435}{figure.caption.311}\protected@file@percent }
\newlabel{fig-iris-dev-can}{{12.11}{435}{Canonical discriminant plot for the multivariate generalization of Levene's test based on MANOVA of absolute deviations from group medians}{figure.caption.311}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.10}What Have We Learned?}{435}{section.12.10}\protected@file@percent }
\newlabel{what-have-we-learned-10}{{12.10}{435}{What Have We Learned?}{section.12.10}{}}
\citation{ref-Cook:77}
\citation{ref-Belsley-etal:80}
\citation{ref-CookWeisberg:82}
\citation{ref-R-olsrr}
\citation{ref-R-HLMdiag}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Multiviate Influence and Robust Estimation}{437}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec-influence-robust}{{13}{437}{Multiviate Influence and Robust Estimation}{chapter.13}{}}
\citation{ref-BarrettLing:92}
\citation{ref-Barrett:2003}
\citation{ref-R-mvinfluence}
\citation{ref-BarrettLing:92}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Multivariate influence}{438}{section.13.1}\protected@file@percent }
\newlabel{sec-multivariate-influence}{{13.1}{438}{Multivariate influence}{section.13.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.1}Notation}{438}{subsection.13.1.1}\protected@file@percent }
\newlabel{notation}{{13.1.1}{438}{Notation}{subsection.13.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.2}Hat values and residuals}{438}{subsection.13.1.2}\protected@file@percent }
\newlabel{hat-values-and-residuals}{{13.1.2}{438}{Hat values and residuals}{subsection.13.1.2}{}}
\citation{ref-BarrettLing:92}
\citation{ref-Barrett:2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.3}Cook's distance}{439}{subsection.13.1.3}\protected@file@percent }
\newlabel{cooks-distance}{{13.1.3}{439}{Cook's distance}{subsection.13.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.4}Leverage and residual components}{439}{subsection.13.1.4}\protected@file@percent }
\newlabel{leverage-and-residual-components}{{13.1.4}{439}{Leverage and residual components}{subsection.13.1.4}{}}
\citation{ref-Barrett:2003}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}The Mysterious Case 9}{440}{section.13.2}\protected@file@percent }
\newlabel{the-mysterious-case-9}{{13.2}{440}{The Mysterious Case 9}{section.13.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}Cook's D}{440}{subsection.13.2.1}\protected@file@percent }
\newlabel{sec-cookd-multivar}{{13.2.1}{440}{Cook's D}{subsection.13.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces Scatterplot matrix for the toy example.}}{441}{figure.caption.312}\protected@file@percent }
\newlabel{fig-toy-scatmat}{{13.1}{441}{Scatterplot matrix for the toy example}{figure.caption.312}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.2}{\ignorespaces Influence plots for the univariate models for \texttt  {y1} and \texttt  {y2}}}{442}{figure.caption.313}\protected@file@percent }
\newlabel{fig-toy-inflplots}{{13.2}{442}{Influence plots for the univariate models for \texttt {y1} and \texttt {y2}}{figure.caption.313}{}}
\citation{ref-Tukey:59}
\citation{ref-Barrett:2003}
\@writefile{lof}{\contentsline {figure}{\numberline {13.3}{\ignorespaces Studentized residual influence plot for the multivariate model \texttt  {(y1,\ y2)\ \textasciitilde {}\ x}. Dotted vertical lines mark large hat values, \(H > {2, 3} p/n\). The dotted horizontal line marks large values of the squared studentized residual.}}{443}{figure.caption.314}\protected@file@percent }
\newlabel{fig-toy-inflplot-mlm-stres}{{13.3}{443}{Studentized residual influence plot for the multivariate model \texttt {(y1,\ y2)\ \textasciitilde {}\ x}. Dotted vertical lines mark large hat values, \(H > {2, 3} p/n\). The dotted horizontal line marks large values of the squared studentized residual}{figure.caption.314}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.4}{\ignorespaces LR plot of \(\log (L)\) versus \(\log (R)\) for the multivariate model \texttt  {(y1,\ y2)\ \textasciitilde {}\ x}. Dotted lines show contours of constant Cook's distance.}}{444}{figure.caption.315}\protected@file@percent }
\newlabel{fig-toy-inflplot-mlm-LR}{{13.4}{444}{LR plot of \(\log (L)\) versus \(\log (R)\) for the multivariate model \texttt {(y1,\ y2)\ \textasciitilde {}\ x}. Dotted lines show contours of constant Cook's distance}{figure.caption.315}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}DFBETAS}{444}{subsection.13.2.2}\protected@file@percent }
\newlabel{dfbetas}{{13.2.2}{444}{DFBETAS}{subsection.13.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.5}{\ignorespaces }}{445}{figure.caption.316}\protected@file@percent }
\newlabel{fig-toy-dfbetas}{{13.5}{445}{}{figure.caption.316}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.3}Example: NLSY data}{445}{section.13.3}\protected@file@percent }
\newlabel{example-nlsy-data}{{13.3}{445}{Example: NLSY data}{section.13.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.4}Example: Penguin data}{446}{section.13.4}\protected@file@percent }
\newlabel{sec-peng-mvinfluence}{{13.4}{446}{Example: Penguin data}{section.13.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.6}{\ignorespaces Influence plot for the NLSY data\ldots  {}}}{447}{figure.caption.317}\protected@file@percent }
\newlabel{fig-NLSY-inflplot1}{{13.6}{447}{Influence plot for the NLSY data\ldots {}}{figure.caption.317}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.7}{\ignorespaces Influence plot for the NLSY data\ldots  {}}}{448}{figure.caption.318}\protected@file@percent }
\newlabel{fig-NLSY-inflplot2}{{13.7}{448}{Influence plot for the NLSY data\ldots {}}{figure.caption.318}{}}
\citation{ref-Rousseeuw2004}
\@writefile{lof}{\contentsline {figure}{\numberline {13.8}{\ignorespaces Influence plot for the Penguin data, showing the squared studentized residual vs.~hat value. Unusual points on either variable or on the Cook's D statistic are identified with their case number.}}{449}{figure.caption.319}\protected@file@percent }
\newlabel{fig-peng-inflplot1}{{13.8}{449}{Influence plot for the Penguin data, showing the squared studentized residual vs.~hat value. Unusual points on either variable or on the Cook's D statistic are identified with their case number}{figure.caption.319}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.5}Robust Estimation}{449}{section.13.5}\protected@file@percent }
\newlabel{sec-robust-estimation}{{13.5}{449}{Robust Estimation}{section.13.5}{}}
\citation{ref-Rousseeuw1984}
\citation{ref-Yohai1987}
\@writefile{toc}{\contentsline {section}{\numberline {13.6}Example: Penguin data}{450}{section.13.6}\protected@file@percent }
\newlabel{sec-peng-robust}{{13.6}{450}{Example: Penguin data}{section.13.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.9}{\ignorespaces Diagram ploting the function \(\mitrho (e_i)\) of the contributions of the residuals \(e_i\) to what is minimized in various fitting methods.}}{451}{figure.caption.320}\protected@file@percent }
\newlabel{fig-weight-fns}{{13.9}{451}{Diagram ploting the function \(\rho (e_i)\) of the contributions of the residuals \(e_i\) to what is minimized in various fitting methods}{figure.caption.320}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.10}{\ignorespaces Index plot of the observation weights for the robust model, \texttt  {peng.rlm}. Observations with weights \textless {} 0.5 are labeled with their case number.}}{452}{figure.caption.321}\protected@file@percent }
\newlabel{fig-peng-robmlm-plot}{{13.10}{452}{Index plot of the observation weights for the robust model, \texttt {peng.rlm}. Observations with weights \textless {} 0.5 are labeled with their case number}{figure.caption.321}{}}
\citation{ref-Hartman:2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Case Studies}{455}{chapter.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{case-studies}{{14}{455}{Case Studies}{chapter.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Neuro- and Social-cognitive measures in psychiatric groups}{455}{section.14.1}\protected@file@percent }
\newlabel{neuro--and-social-cognitive-measures-in-psychiatric-groups}{{14.1}{455}{Neuro- and Social-cognitive measures in psychiatric groups}{section.14.1}{}}
\citation{ref-Heinrichs-etal:2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.1}Research questions}{456}{subsection.14.1.1}\protected@file@percent }
\newlabel{research-questions}{{14.1.1}{456}{Research questions}{subsection.14.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.2}Data}{456}{subsection.14.1.2}\protected@file@percent }
\newlabel{data}{{14.1.2}{456}{Data}{subsection.14.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.3}A first look}{457}{subsection.14.1.3}\protected@file@percent }
\newlabel{a-first-look}{{14.1.3}{457}{A first look}{subsection.14.1.3}{}}
\citation{ref-FriendlyKwan:03:effect}
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces Boxplots combined with violin plots of the \texttt  {NeuroCog} data.}}{458}{figure.caption.322}\protected@file@percent }
\newlabel{fig-NC-boxplot}{{14.1}{458}{Boxplots combined with violin plots of the \texttt {NeuroCog} data}{figure.caption.322}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Bivariate views}{458}{section.14.2}\protected@file@percent }
\newlabel{bivariate-views}{{14.2}{458}{Bivariate views}{section.14.2}{}}
\newlabel{corrgram}{{14.2}{459}{Corrgram}{section*.323}{}}
\@writefile{toc}{\contentsline {subsection}{Corrgram}{459}{section*.323}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces Corrgram of the \texttt  {NeuroCog} data. The upper and lower triangles use two different ways of encoding the value of the correlation for each pair of variables.}}{459}{figure.caption.324}\protected@file@percent }
\newlabel{fig-NC-corrgram}{{14.2}{459}{Corrgram of the \texttt {NeuroCog} data. The upper and lower triangles use two different ways of encoding the value of the correlation for each pair of variables}{figure.caption.324}{}}
\newlabel{scatterplot-matrix}{{14.2}{460}{Scatterplot matrix}{section*.325}{}}
\@writefile{toc}{\contentsline {subsection}{Scatterplot matrix}{460}{section*.325}\protected@file@percent }
\newlabel{biplot}{{14.2}{460}{Biplot}{section*.327}{}}
\@writefile{toc}{\contentsline {subsection}{Biplot}{460}{section*.327}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces Scatterplot matrix of the \texttt  {NeuroCog} data. Points are suppressed here, focusing on the data ellipses and regression lines. Colors for the groups: \textcolor {red}{Schizophrenic} \textcolor {green}{SchizoAffective} \textcolor {blue}{Control}}}{461}{figure.caption.326}\protected@file@percent }
\newlabel{fig-NC-scatmat}{{14.3}{461}{Scatterplot matrix of the \texttt {NeuroCog} data. Points are suppressed here, focusing on the data ellipses and regression lines. Colors for the groups: \textcolor {red}{Schizophrenic} \textcolor {green}{SchizoAffective} \textcolor {blue}{Control}}{figure.caption.326}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.4}{\ignorespaces Biplot of the NeuroCog data. The three groups differ are ordered along the first PCA dimension. Variable vectors show the correlations of the response variables with the two components.}}{462}{figure.caption.328}\protected@file@percent }
\newlabel{fig-neuro-biplot}{{14.4}{462}{Biplot of the NeuroCog data. The three groups differ are ordered along the first PCA dimension. Variable vectors show the correlations of the response variables with the two components}{figure.caption.328}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Fitting the MLM}{462}{section.14.3}\protected@file@percent }
\newlabel{fitting-the-mlm}{{14.3}{462}{Fitting the MLM}{section.14.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.5}{\ignorespaces Chisquare QQ plot of the MANOVA model}}{463}{figure.caption.329}\protected@file@percent }
\newlabel{fig-NC-cqplot}{{14.5}{463}{Chisquare QQ plot of the MANOVA model}{figure.caption.329}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.1}HE plot}{464}{subsection.14.3.1}\protected@file@percent }
\newlabel{he-plot}{{14.3.1}{464}{HE plot}{subsection.14.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.2}Canonical space}{464}{subsection.14.3.2}\protected@file@percent }
\newlabel{canonical-space}{{14.3.2}{464}{Canonical space}{subsection.14.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.6}{\ignorespaces HE plot of Speed and Attention in the MLM for the \texttt  {NeuroCog} data. The labeled points show the means of the groups on the two variables. The blue \(\mathbf {H}\) ellipse for groups indicates the strong positive correlation of the group means.}}{465}{figure.caption.330}\protected@file@percent }
\newlabel{fig-NC-HEplot}{{14.6}{465}{HE plot of Speed and Attention in the MLM for the \texttt {NeuroCog} data. The labeled points show the means of the groups on the two variables. The blue \(\mathbf {H}\) ellipse for groups indicates the strong positive correlation of the group means}{figure.caption.330}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.7}{\ignorespaces HE plot matrix of the MLM for \texttt  {NeuroCog} data.}}{466}{figure.caption.331}\protected@file@percent }
\newlabel{fig-NC-HE-pairs}{{14.7}{466}{HE plot matrix of the MLM for \texttt {NeuroCog} data}{figure.caption.331}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.8}{\ignorespaces Canonical discriminant plot for the \texttt  {NeuroCog} data MANOVA. Scores on the two canonical dimensions are plotted, together with 68\% data ellipses for each group.}}{467}{figure.caption.332}\protected@file@percent }
\newlabel{fig-NC-candisc}{{14.8}{467}{Canonical discriminant plot for the \texttt {NeuroCog} data MANOVA. Scores on the two canonical dimensions are plotted, together with 68\% data ellipses for each group}{figure.caption.332}{}}
\citation{ref-Hartman:2016}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}Social cognitive measures}{468}{section.14.4}\protected@file@percent }
\newlabel{sec-social-cog}{{14.4}{468}{Social cognitive measures}{section.14.4}{}}
\citation{ref-Mardia:1970:MMS}
\citation{ref-Mardia:1974}
\citation{ref-Cox:1968}
\citation{ref-Healy:1968:MNP}
\@writefile{lof}{\contentsline {figure}{\numberline {14.9}{\ignorespaces HE plot of Speed and Attention in the MLM for the \texttt  {SocialCog} data. The labeled points show the means of the groups on the two variables. The lines for Dx1 and Dx2 show the tests of the contrasts among groups.}}{469}{figure.caption.333}\protected@file@percent }
\newlabel{fig-SC-HEplot}{{14.9}{469}{HE plot of Speed and Attention in the MLM for the \texttt {SocialCog} data. The labeled points show the means of the groups on the two variables. The lines for Dx1 and Dx2 show the tests of the contrasts among groups}{figure.caption.333}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.1}Model checking}{469}{subsection.14.4.1}\protected@file@percent }
\newlabel{model-checking}{{14.4.1}{469}{Model checking}{subsection.14.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.10}{\ignorespaces Chi-square quantile-quantile plot for residuals from the model \texttt  {SC.mlm}. The confidence band gives a point-wise 95\% envelope, providing information about uncertainty. One extreme multivariate outlier is highlighted.}}{470}{figure.caption.334}\protected@file@percent }
\newlabel{fig-SC-cqplot}{{14.10}{470}{Chi-square quantile-quantile plot for residuals from the model \texttt {SC.mlm}. The confidence band gives a point-wise 95\% envelope, providing information about uncertainty. One extreme multivariate outlier is highlighted}{figure.caption.334}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.2}Canonical HE plot}{470}{subsection.14.4.2}\protected@file@percent }
\newlabel{canonical-he-plot-1}{{14.4.2}{470}{Canonical HE plot}{subsection.14.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.11}{\ignorespaces Canonical HE plot for the corrected \texttt  {SocialCog} MANOVA. The variable vectors show the correlations of the responses with the canonical variables. The embedded green lines show the projections of the \textbf  {H} ellipses for the contrasts \texttt  {Dx1} and \texttt  {Dx2} in canonical space.}}{472}{figure.caption.335}\protected@file@percent }
\newlabel{fig-SC1-hecan}{{14.11}{472}{Canonical HE plot for the corrected \texttt {SocialCog} MANOVA. The variable vectors show the correlations of the responses with the canonical variables. The embedded green lines show the projections of the \textbf {H} ellipses for the contrasts \texttt {Dx1} and \texttt {Dx2} in canonical space}{figure.caption.335}{}}
\@writefile{toc}{\contentsline {part}{V\hspace  {1em}End Matter}{473}{part.5}\protected@file@percent }
\newlabel{colophon}{{V}{475}{Colophon}{chapter*.336}{}}
\@writefile{toc}{\contentsline {chapter}{Colophon}{475}{chapter*.336}\protected@file@percent }
\gdef \LT@i {\LT@entry 
    {1}{63.09583pt}\LT@entry 
    {1}{42.69566pt}\LT@entry 
    {1}{49.94423pt}\LT@entry 
    {1}{313.71124pt}}
\newlabel{package-versions}{{V}{476}{Package versions}{section*.337}{}}
\@writefile{toc}{\contentsline {section}{Package versions}{476}{section*.337}\protected@file@percent }
\bibcite{ref-Abbott:1884}{\citeproctext }
\bibcite{ref-Aluja-etal-2018}{\citeproctext }
\bibcite{ref-Anderson:35}{\citeproctext }
\bibcite{ref-Anderson2006}{\citeproctext }
\bibcite{ref-Andrews:72}{\citeproctext }
\bibcite{ref-Anscombe:73}{\citeproctext }
\bibcite{ref-R-modelsummary}{\citeproctext }
\bibcite{ref-R-marginaleffects}{\citeproctext }
\bibcite{ref-Asimov:85}{\citeproctext }
\bibcite{ref-Barabasi2016network}{\citeproctext }
\bibcite{ref-Barrett:2003}{\citeproctext }
\bibcite{ref-BarrettLing:92}{\citeproctext }
\bibcite{ref-Bartlett:1937}{\citeproctext }
\bibcite{ref-Bartlett1938}{\citeproctext }
\bibcite{ref-BradshawFindley1967}{\citeproctext }
\bibcite{ref-Becker:1996:VDC}{\citeproctext }
\bibcite{ref-Belsley:91a}{\citeproctext }
\bibcite{ref-Belsley-etal:80}{\citeproctext }
\bibcite{ref-Biecek-etal:2023}{\citeproctext }
\bibcite{ref-Black-etal-2018}{\citeproctext }
\bibcite{ref-Blishen-etal-1987}{\citeproctext }
\newlabel{references}{{V}{479}{References}{chapter*.338}{}}
\@writefile{toc}{\contentsline {chapter}{References}{479}{chapter*.338}\protected@file@percent }
\newlabel{refs}{{V}{479}{References}{section*.339}{}}
\bibcite{ref-Bock1963}{\citeproctext }
\bibcite{ref-Bock1964}{\citeproctext }
\bibcite{ref-Bodmer-etal-2021}{\citeproctext }
\bibcite{ref-BondyMurty2008}{\citeproctext }
\bibcite{ref-BorgGroenen2005}{\citeproctext }
\bibcite{ref-Borg2018}{\citeproctext }
\bibcite{ref-Box:1949}{\citeproctext }
\bibcite{ref-Box:1950}{\citeproctext }
\bibcite{ref-Box:1953}{\citeproctext }
\bibcite{ref-Brandmaier2014}{\citeproctext }
\bibcite{ref-Brinton1939}{\citeproctext }
\bibcite{ref-BrownForsythe:1974}{\citeproctext }
\bibcite{ref-Brown-Zidek-1980}{\citeproctext }
\bibcite{ref-R-gggda}{\citeproctext }
\bibcite{ref-Buja-etal-2005}{\citeproctext }
\bibcite{ref-Ocagne:1885}{\citeproctext }
\bibcite{ref-Cai-etal:2015}{\citeproctext }
\bibcite{ref-Cajori:1926}{\citeproctext }
\bibcite{ref-Cattell1966}{\citeproctext }
\bibcite{ref-Chambers-etal:83}{\citeproctext }
\bibcite{ref-ChambersHastie1991}{\citeproctext }
\bibcite{ref-Charnes-etal-1981}{\citeproctext }
\bibcite{ref-Cleveland:79}{\citeproctext }
\bibcite{ref-Cleveland:85}{\citeproctext }
\bibcite{ref-ClevelandDevlin:88}{\citeproctext }
\bibcite{ref-ClevelandMcGill:84b}{\citeproctext }
\bibcite{ref-ClevelandMcGill:85}{\citeproctext }
\bibcite{ref-Clyde-etal-1966}{\citeproctext }
\bibcite{ref-Cochran:1941}{\citeproctext }
\bibcite{ref-Conover-etal:1981}{\citeproctext }
\bibcite{ref-Cook-etal-1995}{\citeproctext }
\bibcite{ref-Cook-etal-2008}{\citeproctext }
\bibcite{ref-CookLaa-mulgar}{\citeproctext }
\bibcite{ref-CookSwayne:2007}{\citeproctext }
\bibcite{ref-Cook:77}{\citeproctext }
\bibcite{ref-Cook:93}{\citeproctext }
\bibcite{ref-Cook-1996}{\citeproctext }
\bibcite{ref-CookWeisberg:82}{\citeproctext }
\bibcite{ref-CookWeisberg-1994}{\citeproctext }
\bibcite{ref-Costantini2015}{\citeproctext }
\bibcite{ref-Costelloe:1915}{\citeproctext }
\bibcite{ref-Cotton-2013}{\citeproctext }
\bibcite{ref-Cox:1968}{\citeproctext }
\bibcite{ref-R-igraph}{\citeproctext }
\bibcite{ref-R-Hotelling}{\citeproctext }
\bibcite{ref-R-quartets}{\citeproctext }
\bibcite{ref-Davis:1990}{\citeproctext }
\bibcite{ref-Dempster:69}{\citeproctext }
\bibcite{ref-Dempster1972}{\citeproctext }
\bibcite{ref-Dixon1965}{\citeproctext }
\bibcite{ref-R-adegraphics}{\citeproctext }
\bibcite{ref-Duncan:61}{\citeproctext }
\bibcite{ref-Efron-etal:leas:2004}{\citeproctext }
\bibcite{ref-Emerson-etal:2013}{\citeproctext }
\bibcite{ref-Euler:1758}{\citeproctext }
\bibcite{ref-FarquharFarquhar:91}{\citeproctext }
\bibcite{ref-Fienberg:71}{\citeproctext }
\bibcite{ref-Finn1967}{\citeproctext }
\bibcite{ref-Fisher1923}{\citeproctext }
\bibcite{ref-Fisher:25}{\citeproctext }
\bibcite{ref-Fisher-1936}{\citeproctext }
\bibcite{ref-Fisher:1936}{\citeproctext }
\bibcite{ref-Fishkeller-etal:1974b}{\citeproctext }
\bibcite{ref-FluryReidwyl-1988}{\citeproctext }
\bibcite{ref-Fox:87}{\citeproctext }
\bibcite{ref-Fox:03:effects}{\citeproctext }
\bibcite{ref-Fox:2016:ARA}{\citeproctext }
\bibcite{ref-Fox2020}{\citeproctext }
\bibcite{ref-Fox2021}{\citeproctext }
\bibcite{ref-FoxMonette:92}{\citeproctext }
\bibcite{ref-FoxWeisberg:2018}{\citeproctext }
\bibcite{ref-FoxWeisberg2018}{\citeproctext }
\bibcite{ref-R-car}{\citeproctext }
\bibcite{ref-R-effects}{\citeproctext }
\bibcite{ref-Friedman:87}{\citeproctext }
\bibcite{ref-FriedmanTukey:74}{\citeproctext }
\bibcite{ref-R-glmnet}{\citeproctext }
\bibcite{ref-Friendly:91}{\citeproctext }
\bibcite{ref-Friendly:94a}{\citeproctext }
\bibcite{ref-Friendly:99:EMD}{\citeproctext }
\bibcite{ref-Friendly:00:VCD}{\citeproctext }
\bibcite{ref-Friendly:02:corrgram}{\citeproctext }
\bibcite{ref-Friendly-07-manova}{\citeproctext }
\bibcite{ref-Friendly:2008:golden}{\citeproctext }
\bibcite{ref-Friendly-2011-gentalk}{\citeproctext }
\bibcite{ref-Friendly:genridge:2013}{\citeproctext }
\bibcite{ref-Friendly2022}{\citeproctext }
\bibcite{ref-R-genridge}{\citeproctext }
\bibcite{ref-R-mvinfluence}{\citeproctext }
\bibcite{ref-R-vcdExtra}{\citeproctext }
\bibcite{ref-py614bib}{\citeproctext }
\bibcite{ref-FriendlyDenis:2001:valois}{\citeproctext }
\bibcite{ref-R-candisc}{\citeproctext }
\bibcite{ref-R-matlib}{\citeproctext }
\bibcite{ref-FriendlyKwan:03:effect}{\citeproctext }
\bibcite{ref-FriendlyKwan:2009}{\citeproctext }
\bibcite{ref-FriendlyMeyer:2016:DDAR}{\citeproctext }
\bibcite{ref-Friendly-etal:ellipses:2013}{\citeproctext }
\bibcite{ref-FriendlySigal:2018:eqcov}{\citeproctext }
\bibcite{ref-Friendly-etal:2015}{\citeproctext }
\bibcite{ref-FriendlyWainer:2021:TOGS}{\citeproctext }
\bibcite{ref-Fuller2006}{\citeproctext }
\bibcite{ref-Funkhouser:1937}{\citeproctext }
\bibcite{ref-Gabriel:71}{\citeproctext }
\bibcite{ref-Gabriel:81}{\citeproctext }
\bibcite{ref-Galton:1863}{\citeproctext }
\bibcite{ref-Galton:1886}{\citeproctext }
\bibcite{ref-Galton:1889}{\citeproctext }
\bibcite{ref-Gannett:1898}{\citeproctext }
\bibcite{ref-Gastwirth-etal:2009}{\citeproctext }
\bibcite{ref-DeSilva2020}{\citeproctext }
\bibcite{ref-Gelman-etal:2023}{\citeproctext }
\bibcite{ref-R-datasauRus}{\citeproctext }
\bibcite{ref-Gittins:85}{\citeproctext }
\bibcite{ref-R-penalized}{\citeproctext }
\bibcite{ref-Gorman2014}{\citeproctext }
\bibcite{ref-GowerHand:96}{\citeproctext }
\bibcite{ref-Gower-etal:2011}{\citeproctext }
\bibcite{ref-Grandjean2016}{\citeproctext }
\bibcite{ref-Graybill1961}{\citeproctext }
\bibcite{ref-Greenacre:84}{\citeproctext }
\bibcite{ref-Greenacre:2010:biplots}{\citeproctext }
\bibcite{ref-Guerry:1833}{\citeproctext }
\bibcite{ref-R-seriation}{\citeproctext }
\bibcite{ref-Haitovsky1987}{\citeproctext }
\bibcite{ref-Harrell2015RMS}{\citeproctext }
\bibcite{ref-Harrison2023}{\citeproctext }
\bibcite{ref-R-langevitour}{\citeproctext }
\bibcite{ref-R-detourr}{\citeproctext }
\bibcite{ref-Hartigan:75}{\citeproctext }
\bibcite{ref-Hartigan:75b}{\citeproctext }
\bibcite{ref-Hartley:1950}{\citeproctext }
\bibcite{ref-Hartman:2016}{\citeproctext }
\bibcite{ref-Harwell:1992}{\citeproctext }
\bibcite{ref-Haskell:1919}{\citeproctext }
\bibcite{ref-Hastie-etal-2009}{\citeproctext }
\bibcite{ref-Healy2019}{\citeproctext }
\bibcite{ref-Healy:1968:MNP}{\citeproctext }
\bibcite{ref-R-olsrr}{\citeproctext }
\bibcite{ref-Heinrichs-etal:2015}{\citeproctext }
\bibcite{ref-Herschel:1833}{\citeproctext }
\bibcite{ref-Hertzsprung:1911}{\citeproctext }
\bibcite{ref-HoaglinWelsch1978}{\citeproctext }
\bibcite{ref-Hocking2013}{\citeproctext }
\bibcite{ref-HoerlKennard:1970a}{\citeproctext }
\bibcite{ref-Hoerl-etal-1975}{\citeproctext }
\bibcite{ref-R-ggpcp}{\citeproctext }
\bibcite{ref-Hofstadter1979}{\citeproctext }
\bibcite{ref-Hojsgaard2012graphical}{\citeproctext }
\bibcite{ref-R-palmerpenguins}{\citeproctext }
\bibcite{ref-Hotelling:1931}{\citeproctext }
\bibcite{ref-Hotelling1936}{\citeproctext }
\bibcite{ref-Huang2019}{\citeproctext }
\bibcite{ref-Husson-etal-2017}{\citeproctext }
\bibcite{ref-IBM1965}{\citeproctext }
\bibcite{ref-Inselberg:1985}{\citeproctext }
\bibcite{ref-IsvoranuEpskamp2022}{\citeproctext }
\bibcite{ref-JohnsonWichern1998}{\citeproctext }
\bibcite{ref-R-factoextra}{\citeproctext }
\bibcite{ref-KastellecLeoni:2007}{\citeproctext }
\bibcite{ref-R-ggdist}{\citeproctext }
\bibcite{ref-R-MVN}{\citeproctext }
\bibcite{ref-R-Rtsne}{\citeproctext }
\bibcite{ref-Kruskal1964}{\citeproctext }
\bibcite{ref-Kwan-etal:2009}{\citeproctext }
\bibcite{ref-R-ggstats}{\citeproctext }
\bibcite{ref-LarsenMcCleary:72}{\citeproctext }
\bibcite{ref-Lauritzen1996}{\citeproctext }
\bibcite{ref-LawlessWang:1976}{\citeproctext }
\bibcite{ref-R-FactoMineR}{\citeproctext }
\bibcite{ref-LeeCook-2009}{\citeproctext }
\bibcite{ref-R-liminal}{\citeproctext }
\bibcite{ref-Levene:1960}{\citeproctext }
\bibcite{ref-Lix:1996}{\citeproctext }
\bibcite{ref-LongTeetor2019}{\citeproctext }
\bibcite{ref-Longley:1967}{\citeproctext }
\bibcite{ref-R-HLMdiag}{\citeproctext }
\bibcite{ref-R-ggeffects}{\citeproctext }
\bibcite{ref-R-performance}{\citeproctext }
\bibcite{ref-R-easystats}{\citeproctext }
\bibcite{ref-MaatenHinton2008}{\citeproctext }
\bibcite{ref-Mardia:1970:MMS}{\citeproctext }
\bibcite{ref-Mardia:1974}{\citeproctext }
\bibcite{ref-Marquardt:1970}{\citeproctext }
\bibcite{ref-MartiLaguna2003}{\citeproctext }
\bibcite{ref-MatejkaFitzmaurice2017}{\citeproctext }
\bibcite{ref-Matloff-2011}{\citeproctext }
\bibcite{ref-Maunder:1904}{\citeproctext }
\bibcite{ref-McDonald:2009}{\citeproctext }
\bibcite{ref-McGowan2023}{\citeproctext }
\bibcite{ref-R-vcd}{\citeproctext }
\bibcite{ref-Meyers-etal:2006}{\citeproctext }
\bibcite{ref-Monette:90}{\citeproctext }
\bibcite{ref-Moseley:1913}{\citeproctext }
\bibcite{ref-MostellerTukey-1977}{\citeproctext }
\bibcite{ref-R-rgl}{\citeproctext }
\bibcite{ref-OBrien:1992}{\citeproctext }
\bibcite{ref-R-vegan}{\citeproctext }
\bibcite{ref-Olson:1974}{\citeproctext }
\bibcite{ref-R-ggdensity}{\citeproctext }
\bibcite{ref-Pearson:1896}{\citeproctext }
\bibcite{ref-Pearson:1901}{\citeproctext }
\bibcite{ref-Pearson-1903}{\citeproctext }
\bibcite{ref-Peddle:1910}{\citeproctext }
\bibcite{ref-R-gganimate}{\citeproctext }
\bibcite{ref-Pesaran2019}{\citeproctext }
\bibcite{ref-Pineo-Porter-1967}{\citeproctext }
\bibcite{ref-PineoPorter2008}{\citeproctext }
\bibcite{ref-Plaster:89}{\citeproctext }
\bibcite{ref-Playfair:1786}{\citeproctext }
\bibcite{ref-Playfair:1801}{\citeproctext }
\bibcite{ref-Prokofieva2023}{\citeproctext }
\bibcite{ref-ReavenMiller:68}{\citeproctext }
\bibcite{ref-ReavenMiller:79}{\citeproctext }
\bibcite{ref-Rennie2025}{\citeproctext }
\bibcite{ref-Robinaugh2019}{\citeproctext }
\bibcite{ref-Rogan:1977}{\citeproctext }
\bibcite{ref-Rousseeuw-etal:2012}{\citeproctext }
\bibcite{ref-Rousseeuw2004}{\citeproctext }
\bibcite{ref-Rousseeuw1984}{\citeproctext }
\bibcite{ref-Russell1914}{\citeproctext }
\bibcite{ref-R-lattice}{\citeproctext }
\bibcite{ref-Schatzoff1966}{\citeproctext }
\bibcite{ref-Scheffe1960}{\citeproctext }
\bibcite{ref-R-GGally}{\citeproctext }
\bibcite{ref-Scott1992}{\citeproctext }
\bibcite{ref-Searle-etal:80}{\citeproctext }
\bibcite{ref-ShapiroWilk1965}{\citeproctext }
\bibcite{ref-Shepard1962a}{\citeproctext }
\bibcite{ref-Shepard1962b}{\citeproctext }
\bibcite{ref-Shepard-etal-1972b}{\citeproctext }
\bibcite{ref-Shepard-etal-1972a}{\citeproctext }
\bibcite{ref-Shoben1983}{\citeproctext }
\bibcite{ref-Silverman:86}{\citeproctext }
\bibcite{ref-Simpson:51}{\citeproctext }
\bibcite{ref-SpenceGarrison:1993}{\citeproctext }
\bibcite{ref-Swayne-etal-1998}{\citeproctext }
\bibcite{ref-Swayne-etal-2003}{\citeproctext }
\bibcite{ref-Tibshriani:regr:1996}{\citeproctext }
\bibcite{ref-TikuBalakrishnan:1984}{\citeproctext }
\bibcite{ref-Timm:75}{\citeproctext }
\bibcite{ref-Torgerson1952}{\citeproctext }
\bibcite{ref-Tufte:83}{\citeproctext }
\bibcite{ref-Tukey:59}{\citeproctext }
\bibcite{ref-Tukey:1962}{\citeproctext }
\bibcite{ref-Tukey:77}{\citeproctext }
\bibcite{ref-TukeyTukey:85}{\citeproctext }
\bibcite{ref-Turk1991}{\citeproctext }
\bibcite{ref-Unwin2024}{\citeproctext }
\bibcite{ref-VanderPlas2023}{\citeproctext }
\bibcite{ref-VellemanWelsh:81}{\citeproctext }
\bibcite{ref-Vinod:1978}{\citeproctext }
\bibcite{ref-R-ggbiplot}{\citeproctext }
\bibcite{ref-R-loon}{\citeproctext }
\bibcite{ref-Warne2014}{\citeproctext }
\bibcite{ref-Wegman:1990}{\citeproctext }
\bibcite{ref-R-corrplot}{\citeproctext }
\bibcite{ref-Welch:1947}{\citeproctext }
\bibcite{ref-West2001}{\citeproctext }
\bibcite{ref-Whittaker1990}{\citeproctext }
\bibcite{ref-R-ggplot2}{\citeproctext }
\bibcite{ref-Wickham2019}{\citeproctext }
\bibcite{ref-R-tourr}{\citeproctext }
\bibcite{ref-Wickham-etal-2011}{\citeproctext }
\bibcite{ref-Wilke2019}{\citeproctext }
\bibcite{ref-WilkinsonRogers1973}{\citeproctext }
\bibcite{ref-Wilkinson-etal:2005}{\citeproctext }
\bibcite{ref-Winer1962}{\citeproctext }
\bibcite{ref-Wood:2006}{\citeproctext }
\bibcite{ref-R-corrgram}{\citeproctext }
\bibcite{ref-R-animation}{\citeproctext }
\bibcite{ref-R-loon-tour}{\citeproctext }
\bibcite{ref-Yee2015}{\citeproctext }
\bibcite{ref-R-VGAM}{\citeproctext }
\bibcite{ref-Yohai1987}{\citeproctext }
\bibcite{ref-ZhangBoos:1992:BCV}{\citeproctext }
\@writefile{toc}{\contentsline {chapter}{Index}{493}{section*.339}\protected@file@percent }
\gdef \@abspage@last{509}
