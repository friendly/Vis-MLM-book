
```{r include=FALSE}
source("R/common.R")
knitr::opts_chunk$set(fig.path = "figs/ch07/")
```

{{< include latex/latex-commands.qmd >}}


# Topics in Linear Models

The geometric and graphical approach of earlier chapters has already introduced some new ideas
for thinking about multivariate data, models for explaining them, and graphical methods for 
understanding their results. These can be applied to better understand common problems that arise
in data analysis.

**Packages**

In this chapter I use the following packages. Load them now:

```{r load-pkgs}
library(ggplot2)
library(dplyr)
library(tidyr)
library(patchwork)
library(car)
```


## Ellipsoids in data space and $\mathbf{\beta}$ space {#sec:betaspace}

It is most common to look at data and fitted models in "data space," where axes correspond to
variables, points represent observations, and fitted models are plotted as lines (or planes) in this space.
As we've suggested, data ellipsoids provide informative summaries of relationships in data space.
For linear models, particularly regression models with quantitative predictors, there is another space---"$\mathbf{\beta}$ space"---that provides deeper views of models and the relationships among them.

In $\mathbf{\beta}$ space, the axes pertain to coefficients, for example $(\beta_0, \beta_1)$
in a simple linear regression. Points in this space are models (true, hypothesized, fitted) whose coordinates represent values of these parameters. For example, one point 
$\widehat{\mathbf{\beta}}_{\text{OLS}} = (\hat{\beta}_0, \hat{\beta}_1)$ represents the least squares estimate, points $\widehat{\mathbf{\beta}}_{\text{WLS}}$ and $\widehat{\mathbf{\beta}}_{\text{ML}}$
would give weighted least squares or maximum likelihood estimates, and the line
$\beta_1 = 0$ represents the null hypothesis that the slope is zero. 

In the sense described below, data space and $\vec{\beta}$ space are \emph{dual} to each other.
In simple linear regression, for example:  

* each line like $\mathbf{y} = a + b \mathbf{x}$
in data space corresponds to a point $(a,b)$  in $\mathbf{\beta}$ space, 
* the set of points on any line $\beta_1 = \alpha + \gamma \beta_0$
in $\mathbf{\beta}$ space corresponds to a set of lines through a given point 
$(\alpha, \gamma)$ in data space, and 
* the geometric proposition that every pair of points defines a line in one space corresponds to
the proposition that every two lines intersect in a point in the other space.

Moreover, ellipsoids in these spaces are dual and inversely related to each other.
In data space, joint confidence intervals for the mean vector or joint prediction
regions for the data are given by the ellipsoids $(\bar{x}_1, \bar{x}_2)^\textsf{T} \oplus c \sqrt{\mathbf{S}}$, where the covariance matrix $\mathbf{S}$ depends on $\mathbf{X}\trans \mathbf{X}$.
In the dual $\vec{\beta}$ space, joint confidence regions for the coefficients of 
a response variable $y$ on $(x_1, x_2)$
are given by ellipsoids of the form $\widehat{\mathbf{\beta}} \oplus c \sqrt{\mathbf{S}^{-1}}$,
and depend on $\inv{(\mathbf{X}\trans \mathbf{X})}$.
We illustrate these relationships in the example below.

### Coffee, stress and heart disease

Consider the dataset `coffee`, giving measures of `Heart` ($y$), an index of cardiac damage, 
`Coffee` ($x_1$), a measure of daily coffee consumption, and 
`Stress` ($x_2$), a measure of occupational stress, in a contrived sample of $n=20$ university people.[^coffee]
For the sake of the example we assume that the main goal is
to determine whether or not coffee is good or bad for your heart, and stress
represents one potential confounding variable among others (age, smoking, etc.)
that might be useful to control statistically.

[^coffee]: This example was developed by Georges Monette.

```{r}
load(here::here("data", "coffee.RData"))
coffee |> dplyr::sample_n(6)
```

@fig-coffee-scatmat shows the scatterplot matrix, giving the marginal relations between all pairs 
of variables. The marginal message seems to be that coffee is
bad for your heart, stress is bad for your heart and coffee consumption is
also related to occupational stress.
```{r}
#| label: fig-coffee-scatmat
#| echo: false
#| fig-align: center
#| out-width: "100%"
op <- par(mar=c(4,3,3,1)+0.1)
scatterplotMatrix(~ Heart + Coffee + Stress, data=coffee,
                  smooth = FALSE,
                  pch = 16, col = "brown",
                  cex.axis = 1.3, cex.labels = 3,
                  ellipse = list(levels = 0.68, fill.alpha = 0.1))
```

Yet, when we fit both variables together, we obtain the following results,
suggesting that coffee is good for you---the coefficient for coffee is now
negative, though non-significant.  How can this be?

```{r coffee-mod}
coffee.mod <- lm(Heart ~ Coffee + Stress, data=coffee)
broom::tidy(coffee.mod)
```

## Measurement error


### OLS is BLUE
In classical linear models, the predictors are often considered to be fixed
variables, or, if random, to be measured without error and independent of the regression errors.
Either condition, along with the assumption of linearity, guarantees
that the standard OLS estimators are _unbiased_. That is, in a simple linear regression,
$y = \beta_0 + \beta_1 x + \epsilon$, the estimated slope $\hat{\beta}_1$
wiil have an average, expected value $\mathcal{E} (\hat{\beta}_1)$ equal to the true
population value $\beta_1$ over repeated samples.

Not only this, but the Gauss-Markov theorem guarantees that the OLS estimator is
also the most _efficient_ because it has the least variance among all linear
and unbiased estimators. The classical OLS estimator is said to be BLUE:
**B**est (lowest variance), **L**inear (among linear estimators), **U**nbiased, **E**stimator.

### Errors in predictors
Errors in the response $y$ are accounted for in the model and measured by 
the mean squared error, $\text{MSE} = \hat{\sigma}_\epsilon^2$.
But in practice, of course, predictor variables are often also observed
indicators, subject to their own error. Indeed, in the behavioral sciences
it is rare that predictors are perfectly reliable and measured exactly.
This fact that is recognized in errors-in-variables
regression models [@Fuller2006]
and in more general structural equation models,
but often ignored otherwise.  Ellipsoids in data space and $\beta$ space
are well suited to showing the effect of measurement error in predictors on OLS estimates.


The statistical facts are well known, though perhaps counter-intuitive in certain details:
measurement error in a predictor biases regression coefficients (towards 0), while
error in the measurement in  $y$
increases the MSE and thus standard errors of the regression coefficients but does not introduce
bias in the coefficients.

#### Example
An illuminating example can be constructed by starting with the simple linear regression
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i \; ,
$$
where $x_i$ is the _true_, fully reliable predictor and $y$ is the response, with error variance $\sigma_\epsilon^2$.
Now consider that we don't measure $x_i$ exactly, but instead observe $x^\star_i$.
$$
x^\star_i = x_i + \eta_i \; ,
$$
where the measurement error $\eta_i$ is independent of the true $x_i$ with variance $\sigma^2_\eta$.
We can extend this example to also consider the effect
of adding additional, independent error variance to $y$, so that instead of  $y_i$ we observe

$$
y^\star_i = y_i + \nu_i
$$
with variance $\sigma^2_\nu$. 

Let's simulate an example where the true relation is $y = 0.2 + 0.3 x$ with error standard deviation $\sigma = 0.5$.
I'll take $x$ to be uniformly distributed in [0, 10] and calculate $y$ as normally distributed around that linear relation.

```{r measerr-demo1}
library(dplyr)
library(ggplot2)
library(forcats)
library(patchwork)

set.seed(123)
n <- 300

a <- 0.2    # true intercept
b <- 0.3    # true slope
sigma <- 0.5 # baseline error standard deviation

x <- runif(n, 0, 10)
y <- rnorm(n, a + b*x, sigma)
demo <- data.frame(x,y)
```

Then, generate alternative values $x^\star$ and $y^\star$ with additional error standard deviations around $x$ given by $\sigma_\eta = 4$ and around $y$ given by $\sigma_\nu = 1$.
```{r measerr-demo2}
err_y <- 1   # additional error stdev for y
err_x <- 4   # additional error stdev for x
demo  <- demo |>
  mutate(y_star = rnorm(n, y, err_y),
         x_star = rnorm(n, x, err_x))
```

There are four possible models we could fit and compare, using the combinations of $(x, x^\star)$ and $(y, y^\star)$

```{r measerr-demo3}
fit_1 <- lm(y ~ x,           data = demo)   # no additional error
fit_2 <- lm(y_star ~ x,      data = demo)   # error in y
fit_3 <- lm(y ~ x_star,      data = demo)   # error in x
fit_4 <- lm(y_star ~ x_star, data = demo)   # error in x and y
```

However, to show the differences visually, we can simply plot the data for each pair and show the regression lines
(with confidence bands) and the data ellipses. To do this efficiently with `ggplot2`, it is necessary to transform the 
`demo` data to long format with columns `x` and `y`, distinguished by `name` for the four combinations.

```{r}
# make the demo dataset long, with names for the four conditions
df <- bind_rows(
  data.frame(x=demo$x,      y=demo$y,      name="No measurement error"),
  data.frame(x=demo$x,      y=demo$y_star, name="Measurement error on y"),
  data.frame(x=demo$x_star, y=demo$y,      name="Measurement error on x"),
  data.frame(x=demo$x_star, y=demo$y_star, name="Measurement error on x and y")) |>
  mutate(name = fct_inorder(name)) 
```

Then, we can plot the data in `df` with points, regression lines and a data ellipse, faceting by
`name` to give the **measurement error quartet**.
```{r}
#| label: fig-measerr-demo
#| out-width: "100%"
#| fig-cap: "**The measurement error quartet**: Each plot shows the linear regression of _y_ on _x_, but where additional error variance has been added to _y_ or _x_ or both. The widths of the confidence bands and the vertical extent of the data ellipses show the effect on precision."
ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.3) +
  stat_ellipse(geom = "polygon", 
               color = "blue",fill= "blue", 
               alpha=0.1, linewidth = 1.1) +
  geom_smooth(method="lm", formula = y~x, fullrange=TRUE, level=0.995,
              color = "red", fill = "red", alpha = 0.2) +
  facet_wrap(~name) +
  theme_bw(base_size = 14)
```

Comparing the plots in the first row, you can see that when additional error is added to $y$, the regression slope remains essentially unchanged, illustrating that the estimate is unbiased. However, the confidence bounds on the regression line become wider, and the data ellipse becomes fatter in the $y$ direction, illustrating the loss of precision.

The effect of error in $x$ is less kind. Comparing the first row of plots with the second row, you can see that the estimated slope decreases when errors are added to $x$. This is called _attenuation bias_, and it can be shown that
$$
\widehat{\beta}_{x^\star} \longrightarrow \frac{\beta}{1+\sigma^2_\eta /\sigma^2_x} \; ,
$$
where $\beta$ here refers to the regression slope and $\longrightarrow$ means "converges to", as the sample size gets large.
Thus, as $\sigma^2_\eta$ increases, $\widehat{\beta}_{x^\star}$ becomes less than \beta$.

Beyond plots like @fig-measerr-demo, we can see the effects of error in $x$ or $y$ on the model summary
statistics such as the correlation $r_{xy}$ or MSE by extracting these from the fitted models.
This is easily done using `dplyr::nest_by(name)` and fitting the regression model to each subset, from which we can obtain the model statistics using `sigma()`, `coef()` and so forth.

```{r measerr-stats}
model_stats <- df |>
  dplyr::nest_by(name) |>
  mutate(model = list(lm(y ~ x, data = data)),
         sigma = sigma(model),
         intercept = coef(model)[1],
         slope = coef(model)[2],
         r = sqrt(summary(model)$r.squared)) |>
  mutate(errX = stringr::str_detect(name, " x"),
         errY = stringr::str_detect(name, " y")) |>
  relocate(errX, errY, r, .after = name) |>
  select(-data) |>
  print()
```

We plot the model $R = r_{xy}$ and the estimated residual standard error in @fig-measerr-stats below.
The lines connecting the points are approximately parallel, indicating that errors of measurement
in $x$ and $y$ have nearly additive effects on model summaries.
```{r}
#| label: fig-measerr-stats
#| out-width: "80%"
#| code-fold: false
#| fig-cap: "Model statistics for the combinations of additional error variance in x or y or both. Left: model R; right: Residual standard error."
p1 <- ggplot(data=model_stats, 
             aes(x = errX, y = r, 
                 group = errY, color = errY, shape = errY)) +
  geom_point(size = 4) +
  geom_line(linewidth = 1.2) +
  labs(x = "Error on X?",
       y = "Model R ",
       color = "Error on Y?",
       shape = "Error on Y?") +
  theme_bw(base_size = 14) +
  theme(legend.position = "inside",
        legend.position.inside = c(.75, .7))

p2 <- ggplot(data=model_stats, 
             aes(x = errX, y = sigma, 
                 group = errY, color = errY, shape = errY)) +
  geom_point(size = 4) +
  geom_line(linewidth = 1.2) +
  labs(x = "Error on X?",
     y = "Model residual standard error",
     color = "Error on Y?",
     shape = "Error on Y?") +
  theme_bw(base_size = 14) +
  theme(legend.position = "inside",
        legend.position.inside = c(.75, .7))

p1 + p2
```

#### $\beta$ space

In multiple regression the effects of measurement error in a predictor become more complex,
because error variance in one predictor, $x_1$, say, can affect the coefficients of other
terms in the model.

Consider the marginal relation between Heart disease and Stress in the `coffee` data.
@fig-coffee-measerr-data-beta shows this with data ellipses in data space and the corresponding
confidence ellipses in $\beta$ space.
Each panel starts with the observed data (the darkest ellipse, marked $0$), then adds random normal error,
$\mathcal{N}(0, \delta \times \mathrm{SD}_{Stress})$, with $\delta = \{0.75, 1.0, 1.5\}$, to the value of Stress, while keeping the mean of Stress the same.
All of the data ellipses have the same vertical shadows ($\text{SD}_{\textrm{Heart}}$), while the horizontal shadows increase with $\delta$, driving the slope for Stress toward 0.

In $\beta$ space, it can be seen that the estimated coefficients, $(\beta_0, \beta_{\textrm{Stress}})$
vary along a line and approach $\beta_{\textrm{Stress}}=0$ as $\delta$ gets sufficiently large.
The shadows of
ellipses for $(\beta_0, \beta_{\textrm{Stress}})$ along the $\beta_{\textrm{Stress}}$ axis
also demonstrate the effects of measurement error
on the standard error of $\beta_{\textrm{Stress}}$.



```{r}
#| label: fig-coffee-measerr-data-beta.png
#| out-width: "100%"
#| echo: false
#| fig-cap: "Effects of measurement error in Stress on the marginal relationship between Heart disease and Stress. Each panel starts with the observed data ($\\delta = 0$), then adds random normal error, 
#| $\\mathcal{N}(0, \\delta \\times \\text{SD}_\\text{Stress})$ with standard deviations multiplied by
#| $\\delta$ = 0.75, 1.0, 1.5, to the value of Stress. Increasing measurement error biases the slope for Stress toward 0. Left: 50% data ellipses; right: 50% confidence ellipses."
knitr::include_graphics("images/coffee-measerr-data-beta.png")
```

Perhaps less well-known, but both more surprising and interesting, is the effect that measurement error in one variable, $x_1$, 
has on the estimate of the coefficient for an \emph{other} variable, $x_2$, in a multiple regression model.
@fig-coffee-measerr shows the confidence ellipses for $(\beta_{\textrm{Coffee}}, \beta_{\textrm{Stress}})$ in the multiple regression predicting Heart disease, adding random normal error
$\mathcal{N}(0, \delta \times \mathrm{SD}_{Stress})$, with $\delta = \{0, 0.2, 0.4, 0.8\}$, to the value of Stress alone.  
As can be plainly seen, while this measurement error in Stress attenuates its coefficient,
it also has the effect of biasing the coefficient for Coffee toward that in the \emph{marginal}
regression of Heart disease on Coffee alone.

```{r}
#| label: fig-coffee-measerr
#| out-width: "50%"
#| echo: false
#| fig-cap: "Biasing effect of measurement error in one variable (Stress) on on the coefficient of another variable (Coffee) in a multiple regression.  The coefficient for Coffee is driven towards its value in the marginal model using Coffee alone, as measurement error in Stress makes it less informative in the joint model."
knitr::include_graphics("images/coffee-measerr.png")
```

