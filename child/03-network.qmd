
## Network diagrams {#sec-network}

A major theme throughout this chapter has been to understand how to extend data visualization from
simple bivariate scatterplots to increasingly more complex situations with larger datasets.
With a moderate number of variables, techniques such as smoothing, summarizing with
data ellipses and fitted curves, and visual thinning can be used to tame "big $N$" datasets
with thousands of cases. 

However "big $p$" datasets, with more than a moderate number ($p$) of variables
still remain a challenge. It is hard to see how the more advanced methods
(corrgrams, parallel coordinate) described earlier could cope with $p = 20, 50, 100, 500, \dots$ variables.
At some point, each of these begins to break down for the purpose of visualizing associations
among many variables. We are forced to thin the information presented in graphs
more and more as the number of variables increases.

It turns out that there is a way to increase the number of variables displayed dramatically,
if we are mainly interested in the pairwise correlations for reasonably
normally distributed data. A graphical **network diagram** portrays variables by _nodes_ (vertices),
connected by (weighted) _edges_ whose properties reflect the strength of connections between
pairs, such as a correlation. Such diagrams can reveal properties not readily seen by other means.

As an example consider @fig-big5-qgraph-rodrigues, which portrays the correlations
among 25 self-report items reflecting 5 factors (the "Big Five")
considered in personality psychology to represent the dominant aspects of
all of personality. These factors are easily remembered by the acronum
**OCEAN**: **O**penness, **C**onscientiousness, **E**xtraversion, **A**greeableness and **N**euroticism.
The dataset, `psych::bfi`, contains data from an online sample of $n=2800$ with 5 items for each scale.

In this figure (taken from [Rodrigues (2021)](https://bit.ly/3A6kvq5)),
the item nodes are labeled according to the OCEAN factor they are assumed to measure.
For 25 items, there are $25 \times 24 / 2 = 300$ correlations, way too much to see.
A clearer picture arises when we reduce the number of edges shown according to some
criterion. Here,
edges are drawn _only_ between nodes where the correlation is considered important
by a method ("glasso" = graphical LASSO) designed to make the graph optimally sparse.

<!-- Actually, the psych::bfi data is:
25 personality self report items taken from the International Personality Item Pool (ipip.ori.org) were included as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project. The data from 2800 subjects are included here as a demonstration set for scale construction, factor analysis, and Item Response Theory analysis. Three additional demographic variables (sex, education, and age) are also included.
-->


```{r}
#| label: fig-big5-qgraph-rodrigues
#| echo: false
#| out-width: "100%"
#| fig-cap: "Network diagram of the correlations among 25 items from a Big-Five personality scale, 5 items for each scale. The magnitude of a correlation is shown by the thickness and transparency of the edge between two item nodes. The sign of a correlation is shown by edge color and style: solid blue for positive and dashed red for negative. _Source_: [Rodrigues (2021)](https://bit.ly/3A6kvq5)"
knitr::include_graphics("images/big5-qgraph-rodrigues.png")
```

The edges shown in @fig-big5-qgraph-rodrigues reflect the Pearson correlation between a given pair of items by the visual attributes of color and line style: magnitude is shown by both the thickness and transparency of the edge; the sign of the correlation is shown by color and line type:
solid `r blue` for positive correlations and dashed `r red` for negative ones.

According to some theories, the five personality factors should be largely non-overlapping,
so there should not be many edges connecting items of one factor with those of another.
Yet, there are quite a few cross-factor connections in @fig-big5-qgraph-rodrigues,
so perhaps the theory is wrong, or, more likely, the 25 items are not good representatives of
these underlying dimensions. The network diagram shown here is a visual tool for thought
and refinement. See @Costantini2015 for a tutorial on network analysis of personality data in R.

Network diagrams stem from mathematical graph theory [@BondyMurty2008; @West2001] of the abstract properties of nodes and edges used to represent pairwise relationships.
These can be used to model many types of relations and processes in physical, biological, social and other sciences, where such properties as connectedness, centrality, cliques of connected nodes and so forth
provide a vocabulary used to understand and explain complex systems.

For one example, @Grandjean2016 used network analysis to study the connections among 2500 Twitter
users (nodes) who identified as belonging to a "digital humanities" community
from the relations (edges) of who follows whom.
Grandjean also used these methods to study the relationships among [characters in Shakespeare's tragedies](http://www.martingrandjean.ch/network-visualization-shakespeare/) in terms of 
the characters (nodes) and edges representing how often they appeared in the same scene.

The wide applicability of these ideas has led to what is now called _network science_ [@Barabasi2016network]
encompassing computer networks, biological networks, cognitive and semantic networks, and social networks.
Recent developments in psychology led to a framework of _network psychometrics_ [@IsvoranuEpskamp2022],
where, for example, symptoms of psychopathology (phobias, anxiety, substance abuse) can be
conceptualized as an interconnected network of clusters and studied for possible causal relations [@Robinaugh2019].

Because a network diagram can potentially reflect hundreds of variables,
various [graph layout algorithms](https://en.wikipedia.org/wiki/Graph_drawing) have been developed to automatically
position the nodes so as to generate aesthetically pleasing network visualizations that emphasize important structural properties, like clusters and central nodes, while minimizing visual clutter (many crossing lines) to promote understandability and usability.


There are quite a few R packages for constructing network diagrams, both static and dynamic / interactive,
and these differ considerably in how the information required for a graph is structured as
R objects, and the flexibility to produce attractive graphs.
Among these, `r pkg("igraph", cite=TRUE)` structures the data as a dataset of vertices and edges with properties

-> packages: qgraph, ...

### Crime data

For the present purposes, let's see what network diagrams can tell us about the crime data
analyzed earlier. Here, I first reorder the variables as in @fig-crime-corrplot-AOE.
In the call to `qgraph()`, the argument `minimum = "sig"` says to show only the edges
for significant correlations (at $\alpha = 0.01$ here).
In @fig-crime-cor, the variable nodes are positioned around a circle
(`layout = "circle"`), which is the default.

<!-- fig.code: R/crime/crime-network.R -->

```{r}
#| label: fig-crime-cor
#| out-width: "80%"
#| fig-width: 7
#| fig-height: 7
#| fig-cap: "Network diagram depicting the correlations among the crime variables. Only edges for correlations that are significant at the $\\alpha = 0.01$ level are displayed."
library(qgraph)
ord <- corrMatOrder(crime.cor, order = "AOE")
rownames(crime.cor)[ord]
crime.cor <- crime.cor[ord, ord]

# "association graph": network of correlations
qgraph(crime.cor, 
  title = "Crime data:\ncorrelations", title.cex = 1.5,
  graph = "cor",
  layout = "circle",
  minimum = "sig", sampleSize = nrow(crime), alpha = 0.01,
  color = grey(.9), vsize = 12,
  labels = rownames(crime.cor),
  posCol = "blue")
```

<!--
```{r}
#| label: fig-crime-cor-image
#| out-width: "80%"
#| fig-cap: "Network diagram depicting the correlations among the crime variables."
knitr::include_graphics("images/crime-cor.png")
```
-->

In this figure, you can see the group of property crimes (auto theft, larceny, burglary) at the left
separated from the violent crimes against persons at the right.

### Partial correlations {#sec-partial-cor}

Among the more important statistical applications of network graph theory is the idea that you can also use them to study the the _partial_ (conditional)
associations among variables with the contributions of all other variables
removed in what are called Graphical Gaussian Models (GGMs)
[@Lauritzen1996; @Hojsgaard2012graphical]. In a network diagram of these
partial associations,

* The edges between nodes represent the _partial correlations_ between those variables.

* The absence of an edge between two nodes indicates their variables are _conditionally independent_, given the other variables.

So, whereas a network diagram of correlations shows _marginal associations_
ignoring other variables, one of partial correlations allows you to visualize the _direct_ relationship between each pair of variables, removing the indirect effects that might be mediated through all other variables.

For a set of variables $X = \{x_1, x_2, \dots, x_p \}$, the partial correlation
between $x_i$ and $x_i$, controlling for all other variables
$Z = X \setminus \{x_i, x_j\} = x_\text{others}$ is equivalent to the correlation between the _residuals_ of the linear regressions of $x_i$ on all other $\mathbf{Z}$ and $x_j$ on $\mathbf{Z}$.
(The notation $X \setminus \{x_i, x_j\}$ is read as "$X$ without the set $\{x_i, x_j\}$").

Mathematically, let $\hat{x}_i$ and $\hat{x}_j$ be the predicted values from the linear regressions of $x_i$ on $\mathbf{Z}$ and of $x_j$ on $\mathbf{Z}$, respectively. The partial correlation $p_{ij}$ between $x_i$ and $x_j$ controlling for $\mathbf{Z}$ is given by:
$$
p_{x_i,x_j|\mathbf{Z}} = r( x_i, x_j \mid \text{others}) = \text{cor}[ (x_i - \hat{x}_i),\; (x_j - \hat{x}_j)]
$$ {#eq-parcor}

But, rather than running all these linear regressions, they can all be computed
from the inverse of the correlation matrix [@Whittaker1990, Ch. 5],
a relation first noted by @Dempster1972.
Let $\mathbf{R}$ be the correlation matrix of the variables.
Then, the matrix $\mathbf{P}$ of partial correlations
can be obtained from the negative inverse, $-\mathbf{R}^{-1}$,
standardized to a correlation matrix by dividing by the square root of
product of its diagonal elements,
$$
P_{ij} = - \frac{R^{-1}_{ij}}{\sqrt{(R^{-1}_{ii} \cdot R^{-1}_{jj})}} \period
$$


<!-- Detrius from earlier descriptions ...
$$
\mathbf{P} = -\mathbf{R}^{-1} \comma
$$
where $\mathbf{R}^{-1}$ is the inverse of the correlation matrix $\mathbf{R}$.

Let $\mathbf{S}$ be the covariance matrix of the variable, and $\mathbf{R}$ be the corresponding
correlation matrix.
Then the matrix of partial variances and covariances, $\mathbf{T}$ is

$$
\mathbf{T} = -\mathbf{S}^{-1} \period
$$
The diagonal elements 
$R_{ij} = \frac{S_{ij}}{\sqrt{S_{ii} S_{jj}}}$
-->

The practical implications of this are:

* If a partial correlation is close to zero, it suggests the relationship between two variables is primarily mediated through other variables.

* Non-zero partial correlations indicate a direct relationship that persists after controlling for other variables.

@fig-crime-partial-spring shows the partial correlation network for the
crime data, using the `qgraph()` argument `graph = "pcor"`
To provide a more interpretable result, the argument `layout = "spring"`
positions the nodes using a force-embedded algorithm where
edges act like springs, pulling connected nodes together
and unconnected nodes repel each other, pushing them apart.

```{r}
#| label: fig-crime-partial-spring
#| out-width: "80%"
#| fig-width: 7
#| fig-height: 7
#| fig-cap: "Network diagram of partial correlations among the crime variables, controlling for all others. Variable nodes have been positioned by a \"spring\" layout method ... "
qgraph(crime.cor, 
       title = "Crime data:\npartial correlations", title.cex = 1.5,
       graph = "pcor",
       layout = "spring", repulsion = 1.2,
       minimum = "sig", sampleSize = nrow(crime), alpha = 0.05,
       color = grey(.9), vsize = 14,
       labels = rownames(crime.cor),
       edge.labels = TRUE, edge.label.cex = 1.7,
       posCol = "blue")
```

@fig-crime-partial-spring shows that, once all other crime variables are controlled
for each pair, there remain only a few partial correlations at the $\alpha = 0.05$ level.
Of these, only the largest three in absolute value
are significant at $\alpha = 0.01$.

Thus, once all other variables are taken into account, what remains is mainly
a strong positive association between burglary and larceny and a moderate one
between auto theft and robbery.
There also remains a moderate negative correlation between murder and larceny.
The spring layout makes it clear that, with suppression of 
weak edges, auto theft and robbery form a cluster separated from the other variables.


### Visualizing partial correlations {#sec-pvPlot}

Just as you can visualize _marginal_ association between variables in a scatterplot,
you can also visualize _conditional_ association.
A **partial variables plot** is simply a scatterplot of the partial residuals
$e_i = (x_i - \hat{x}_i)$ from a regression of $x_i$ on the other variables $Z$
against those $e_j = (x_j - \hat{x}_j)$ for another variable $x_j$.

In this, you can use all the bells and whistles of standard scatterplots
(regression lines, smooths, data ellipses, ...) to listen more attentively to the 
story partial association has to tell.
The function `pvPlot()` calculates the partial residuals and then calls
`car::dataEllipse()` for display. The five most "unusual"
observations by Mahalanobis $D^2$ are identified with their abbreviated state labels.
@fig-crime-pvPlots shows these plots for the variable pairs with the two 
largest partial correlations.

<!-- **TODO**: I'd like to put these two figures side-by-side and integrate the code chunks, but `car::scatterplot` doesn't seem to allow this. -->

```{r}
#| label: fig-crime-pvPlots-code
#| eval: false
#| fig-height: 6
#| fig-width: 6
#| out-width: "80%"
#| fig-cap: "Partial variables plot for burglary and larceny in the network diagram for partial correlations of the crime variables."
source("R/pvPlot.R")
# select numeric, make `st` into rownames
crime.num <- crime |>
  tibble::column_to_rownames("st") |>
  dplyr::select(where(is.numeric))

pvPlot(crime.num, vars = c("burglary", "larceny"), 
       id = list(n=5),
       cex.lab = 1.5)
pvPlot(crime.num, vars = c("robbery", "auto"),
       id = list(n=5),
       cex.lab = 1.5)
```

```{r}
#| label: fig-crime-pvPlots
#| echo: false
#| out-width: "100%"
#| fig-cap: "Partial variables plots for burglary and larceny (left) and for robbery and auto theft (right) in the network diagram for partial correlations of the crime variables."
knitr::include_graphics("images/crime-pvPlot-1-2.png")
```

In the pvPlot for burglary and larceny, you can see that the high partial correlation is largely
driven by the extreme points at the left and and right sides. Once all other variables are
taken into account, Arizona (AZ) and Hawaii (HI) have larger incidence of both crimes, while
Arkansas (AK) are smaller on both. 

In the pvPlot for robbery and auto theft, New York stands out as an
influential, high-leverage point (see @sec-leverage); Massachusetts (MA) is noteworthy because
auto theft in that state is considerably higher than what would be predicted from all other variables.

