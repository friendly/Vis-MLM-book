
## Network diagrams

A major theme throughout this chapter has been to understand how to extend data visualization from
simple bivariate scatterplots to increasingly more complex situations with larger datasets.
With a moderate number of variables, techniques such as smoothing, summarizing with
data ellipses and fitted curves, and visual thinning can be used to tame "big $N$" datasets
with thousands of cases. 

However "big $p$" datasets, with more than a moderate number ($p$) of variables
still remain a challenge. It is hard to see how the more advanced methods
(corrgrams, parallel coordinate) described earlier could cope with $p = 20, 50, 100, 500, \dots$ variables.
At some point, each of these begins to break down for the purpose of visualizing associations
among many variables. We are forced to thin the information presented in graphs
more and more as the number of variables increases.

It turns out that there is a way to increase the number of variables displayed dramatically,
if we are mainly interested in the pairwise correlations for reasonably
normally distributed data. A graphical **network diagram** portrays variables by _nodes_ (vertices),
connected by (weighted) _edges_ whose properties reflect the strength of connections between
pairs, such as a correlation. Such diagrams can reveal properties not readily seen by other means.

As an example consider @fig-big5-qgraph-rodrigues, which portrays the correlations
among 25 self-report items reflecting 5 factors (the "Big Five")
considered in personality psychology to represent the dominant aspects of
all of personality. These factors are easily remembered by the acronum
**OCEAN**: **O**penness, **C**onscientiousness, **E**xtraversion, **A**greeableness and **N**euroticism.
The dataset, `psych::bfi`, contains data from an online sample of $n=2800$ with 5 items for each scale.

In this figure (taken from [Rodrigues (2021)](https://bit.ly/3A6kvq5)),
the item nodes are labeled according to the OCEAN factor they are assumed to measure.
For 25 items, there are $25 \times 24 / 2 = 300$ correlations, way too much to see.
A clearer picture arises when we reduce the number of edges shown according to some
criterion. Here,
edges are drawn _only_ between nodes where the correlation is considered significant
by a method ("glasso") designed to make the graph optimally sparse.

<!-- Actually, the psych::bfi data is:
25 personality self report items taken from the International Personality Item Pool (ipip.ori.org) were included as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project. The data from 2800 subjects are included here as a demonstration set for scale construction, factor analysis, and Item Response Theory analysis. Three additional demographic variables (sex, education, and age) are also included.
-->


```{r}
#| label: fig-big5-qgraph-rodrigues
#| echo: false
#| out-width: "100%"
#| fig-cap: "Network diagram of the correlations among 25 items from a Big-Five personality scale, 5 items for each scale. The magnitude of a correlation is shown by the thickness and transparency of the edge between two item nodes. The sign of a correlation is shown by edge color and style: solid blue for positive and dashed red for negative. _Source_: [Rodrigues (2021)](https://bit.ly/3A6kvq5)"
knitr::include_graphics("images/big5-qgraph-rodrigues.png")
```

The edges shown in @fig-big5-qgraph-rodrigues reflect the Pearson correlation between a given pair of items by the visual attributes of color and line style: magnitude is shown by both the thickness and transparency of the edge; the sign of the correlation is shown by color and line type:
solid `r blue` for positive correlations and dashed `r red` for negative ones.

According to some theories, the five personality factors should be largely non-overlapping,
so there should not be many edges connecting items of one factor with those of another.
Yet, there are quite a few cross-factor connections in @fig-big5-qgraph-rodrigues,
so perhaps the theory is wrong, or, more likely, the 25 items are not good representatives of
these underlying dimensions. The network diagram shown here is a visual tool for thought
and refinement 

Network diagrams stem from mathematical graph theory [@BondyMurty2008; @West2001] of the properties of nodes and edges used to represent pairwise relationships, ...

-> network science [@Barabasi2016network], network psychometrics, ...

-> Graph layout: Because a network diagram can potentially reflect hundreds of variables,
various layout algorithms have been developed to automatically
position the nodes so as to generate aesthetically pleasing network visualizations that emphasize important structural properties, like clusters and central nodes.

-> packages: qgraph, ...

### Crime data

For the present purposes, let's see what network diagrams can tell us about the crime data
analyzed earlier. Here, I first reorder the variables as in @fig-crime-corrplot-AOE.
In the call to `qgraph()`, the argument `minimum = "sig"` says to show only the edges
for significant correlations (at $\alpha = 0.01$ here).
In @fig-crime-cor, the variable nodes are positioned around a circle
(`layout = "circle"`), which is the default.

<!-- fig.code: R/crime/crime-network.R -->

```{r}
#| label: fig-crime-cor
#| out-width: "80%"
#| fig-width: 7
#| fig-height: 7
#| fig-cap: "Network diagram depicting the correlations among the crime variables. Only edges for correlations that are significant at the $\\alpha = 0.01$ level are displayed."
library(qgraph)
ord <- corrMatOrder(crime.cor, order = "AOE")
rownames(crime.cor)[ord]
crime.cor <- crime.cor[ord, ord]

# "association graph": network of correlations
qgraph(crime.cor, 
  title = "Crime data:\ncorrelations", title.cex = 1.5,
  graph = "cor",
  layout = "circle",
  minimum = "sig", sampleSize = nrow(crime), alpha = 0.01,
  color = grey(.9), vsize = 12,
  labels = rownames(crime.cor),
  posCol = "blue")
```

<!--
```{r}
#| label: fig-crime-cor-image
#| out-width: "80%"
#| fig-cap: "Network diagram depicting the correlations among the crime variables."
knitr::include_graphics("images/crime-cor.png")
```
-->

### Partial correlations

Among the more important statistical applications of network graph theory is the idea that you can also use them to study the the _partial_ (conditional)
associations among variables with the contributions of all other variables
removed in what are called Graphical Gaussian Models (GGMs)
[@Lauritzen1996; @Hojsgaard2012graphical]. In a network diagram of these
partial associations,

* The edges between nodes represent the _partial correlations_ between those variables.

* The absence of an edge between two nodes indicates their variables are _conditionally independent_, given the other variables.

So, whereas a network diagram of correlations shows _marginal associations_
ignoring other variables, one of partial correlations allows you to visualize the _direct_ relationship between each pair of variables, removing the indirect effects that might be mediated through all other variables.

For a set of variables $X = \{x_1, x_2, \dots, x_p \}$, the partial correlation
between $x_i$ and $x_i$, controlling for all other variables
$Z = X \setminus \{x_i, x_j\} = x_\text{others}$ is equivalent to the correlation between the _residuals_ of the linear regressions of $x_i$ on all other $\mathbf{Z}$ and $x_j$ on $\mathbf{Z}$.

Mathematically, let $\hat{x}_i$ and $\hat{x}_j$ be the predicted values from the linear regressions of $x_i$ on $\mathbf{Z}$ and of $x_j$ on $\mathbf{Z}$, respectively. The partial correlation $p_{ij}$ between $x_i$ and $x_j$ controlling for $\mathbf{Z}$ is given by:
$$
p_{x_i,x_j|\mathbf{Z}} = r( x_i, x_j \mid \text{others}) = \text{cor}[ (x_i - \hat{x}_i),\; (x_j - \hat{x}_j)]
$$ {#eq-parcor}

But, rather than running all these linear regressions, they can all be computed
from the inverse of the correlation matrix [@Whittaker1990, Ch. 5],
a relation first noted by @Dempster1972.
Let $\mathbf{R}$ be the correlation matrix, and $\mathbf{P}$ be the matrix of partial correlations. Then:
$$
\mathbf{P} = -\mathbf{R}^{-1} \comma
$$
where $\mathbf{R}^{-1}$ is the inverse of the correlation matrix $\mathbf{R}$.

@fig-crime-partial-spring shows the partial correlation network for the
crime data, using the `qgraph()` argument `graph = "pcor"`
To provide a more interpretable result, the argument `layout = "spring"`
positions the nodes using a force-embedded algorithm where
edges act like springs, pulling connected nodes together
and unconnected nodes repel each other, pushing them apart.

```{r}
#| label: fig-crime-partial-spring
#| out-width: "80%"
#| fig-width: 7
#| fig-height: 7
#| fig-cap: "Network diagram of partial correlations among the crime variables, controlling for all others. Variable nodes have been positioned by a \"spring\" layout method ... "
qgraph(crime.cor, 
       title = "Crime data:\npartial correlations", title.cex = 1.5,
       graph = "pcor",
       layout = "spring", repulsion = 1.2,
       minimum = "sig", sampleSize = nrow(crime), alpha = 0.05,
       color = grey(.9), vsize = 14,
       labels = rownames(crime.cor),
       edge.labels = TRUE, edge.label.cex = 1.7,
       posCol = "blue")
```

@fig-crime-partial-spring shows that, once all other crime variables are controlled
for each pair, there remain only a few partial correlations at the $\alpha = 0.05$ level.
Of these, only the largest three in absolute value
are significant at $\alpha = 0.01$.

Thus, once all other variables are taken into account, what remains is mainly
a strong positive association between burglary and larceny and a moderate one
between auto theft and robbery.
There also remains a moderate negative correlation between murder and larceny.
The spring layout makes it clear that, with suppression of 
weak edges, auto theft and robbery form a cluster separated from the other variables.


### Visualizing partial correlations

Just as you can visualize _marginal_ association between variables in a scatterplot,
you can also visualize _conditional_ association.
A **partial variables plot** is simply a scatterplot of the partial residuals
$e_i = (x_i - \hat{x}_i)$ from a regression of $x_i$ on the other variables $Z$
against those $e_j = (x_j - \hat{x}_j)$ for another variable $x_j$.

In this, you can use all the bells and whistles of standard scatterplots
(regression lines, smooths, data ellipses, ...) to listen more attentively to the 
story partial association has to tell.
The function `pvPlot()` calculates the partial residuals and then calls
`car::scatterplot()` for display. As written, the five most "unusual"
observations by Mahalanobis $D^2$ are identified with their state labels.

**TODO**: I'd like to put these two figures side-by-side and integrate the code chunks, but `car::scatterplot` doesn't seem to allow this.

```{r echo = -1}
#| label: fig-crime-pvPlot1
#| fig-height: 6
#| fig-width: 6
#| out-width: "80%"
#| fig-cap: "Partial variables plot for burglary and larceny in the network diagram for partial correlations of the crime variables."
op <- par(mar = c(5, 5, 1, 1)+.5)
source("R/pvPlot.R")
# select numeric, make `st` into rownames
crime.num <- crime |>
  tibble::column_to_rownames("st") |>
  dplyr::select(where(is.numeric))

pvPlot(crime.num, vars = c("burglary", "larceny"), 
       ellipse = list(levels = 0.68, fill.alpha = 0.1, robust = FALSE),
       cex.lab = 1.5)
```

```{r echo = -1}
#| label: fig-crime-pvPlot2
#| fig-height: 6
#| fig-width: 6
#| out-width: "80%"
#| fig-cap: "Partial variables plot for robbery and auto theft in the network diagram for partial correlations of the crime variables."
op <- par(mar = c(5, 5, 1, 1)+.5)
pvPlot(crime.num, vars = c("robbery", "auto"),
       ellipse = list(levels = 0.68, fill.alpha = 0.1, robust = FALSE),
       cex.lab = 1.5)
```


