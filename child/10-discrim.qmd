## Discriminant analysis {#sec-discrim-MLM}

```{r setup, include=FALSE}
library(MASS)
library(ggplot2)
library(patchwork)
```


As described earlier (@sec-t2-discrim), Linear Discriminant Analysis (LDA) is similar to a one-way MANOVA, but with its emphasis
on the problem of _classifying observations_ into groups rather than _testing_ whether there are significant differences
of the means for the response variables. You would use LDA rather than MANOVA when your goal is to predict group membership and identify which variables best distinguish between pre-defined groups, rather than just testing for group differences.

Thus, LDA can be seen as a **flipped MANOVA**, where the role of dependent and independent variables are reversed.
You can see this difference in the model formulas used in `lm()` compared with `MASS::lda()`, which fits a linear discriminant analysis (LDA) model: The outcomes on the right-hand side are `cbind(y1, y2, y3)` for the MANOVA but is simply `group` for a discriminant analysis.

```{r eval=FALSE}
manova.mod  <- lm(cbind(y1, y2, y3) ~ group)
discrim.mod <- MASS::lda(group ~ y1 + y2 + y3)
```

One consequence of this flipped emphasis is that predicted values from `predict()` for an LDA is the predicted _group membership_ for an observation
with values $y_1, y_2, y_3$ rather than the predicted _response values_ $\hat{y}_1, \hat{y}_2, \hat{y}_3$ in MANOVA.
This is useful for classifying new observations from an LDA model, such as determining whether new Swiss banknotes are real or fake (@exm-banknote) or classifying a new penguin.

As we have seen, the multivariate linear model fit by `lm()` applies equally well when there are two or more grouping factors
and or quantitative predictors, whereas discriminant analysis is mostly restricted to the case of a single `group` factor.[^candisc]

[^candisc]: The function `candisc::candisc()` carries out a _generalized_ discriminant analysis, for one term in a multivariate
linear model. `candisc::candiscList()` does this for all terms in a model.

In both cases, the analysis and significance tests are based on the familiar breakdown (@eq-SSP) of total variability, 
$\mathbf{T} \equiv \mathbf{SSP}_{T}$, of the observations around the grand means into portions attributable to differences _between_ the group means $\mathbf{H} \equiv \mathbf{SSP}_{H}$
and the portions attributable to differences _within_ the groups around their means, $\mathbf{E} \equiv \mathbf{SSP}_{E}$ as described in
@sec-sum-of-squares,

$$
\mathbf{T} = \mathbf{H} + \mathbf{E} \; .
$$
As we saw earlier (@sec-H-vs-E), significance tests are based on the $s = \min{(p, \text{df}_h)}$ non-zero
eigenvalues $\lambda_i$ of $\mathbf{H}\mathbf{E}^{-1}$ which are combined into a single test statistic such
as Wilks' $\Lambda = \Pi_i^s (1+\lambda_i)^{-1}$ or Hotelling-Lawley trace, $\Sigma_i^s \lambda_i$.

The corresponding eigenvectors, $\mathbf{V}$ of $\mathbf{H}\mathbf{E}^{-1}$ are the weights for
the linear combinations of the quantitative variables on which the groups are most widely separated.
The transformation of the $\mathbf{Y}$ variables in data space to the space of these linear combinations
is given by $\mathbf{Z}_{n \times s} = \mathbf{Y} \; \mathbf{E}^{-1/2} \; \mathbf{V}$ as we saw earlier (sec REF?).

But here is where MANOVA and discriminant analysis diverge again. In discriminant analysis, 
you an replace the observed $\mathbf{Y}$ variables with the uncorrelated $s$ discriminant variates defined by $\mathbf{V}$
and obtain _exactly_ the same classifications of the observations. 
The first, $\mathbf{v}_1$, associated with the largest eigenvalue $\lambda_1$, accounts for the greatest
proportion of between-group separation; the second, $\mathbf{v}_2$ is next in discriminating power, and so forth.
But maybe we can classify nearly as well with a small subset of $k < s$ discriminants without great loss.
Hence, just as in PCA, discriminant analysis can be thought of as a **dimension reduction** technique,
squeezing the most discrininant juice out of the data in a few dimensions.


<!-- method notes
https://en.wikipedia.org/wiki/Linear_discriminant_analysis
https://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/

https://stats.stackexchange.com/questions/82959/how-is-manova-related-to-lda
https://stats.stackexchange.com/questions/48786/algebra-of-lda-fisher-discrimination-power-of-a-variable-and-linear-discriminan#48859
https://stats.stackexchange.com/questions/22884/how-does-linear-discriminant-analysis-reduce-the-dimensions/22889#22889
https://stats.stackexchange.com/questions/169436/how-lda-a-classification-technique-also-serves-as-dimensionality-reduction-tec

Takes up LDA, QDA, mixture MDA models, ...
https://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/
-->

The discussion here is limited to what this altered focus adds to _visualizing differences_ among groups on a collection
of response variables. See @Kleka1980, @Lachenbruch1975 for a basic, but more general introduction to discriminant analysis methods.
There is a useful discussion in this Cross Validated question 
[How is MANOVA related to LDA?](https://bit.ly/4gpMykN) <!-- {.content-visible when-format="pdf"} (https://bit.ly/4gpMykN). -->

As in MANOVA, **linear** discriminant analysis assumes equal variance covariance matrices across the groups. In this case, the boundaries separating
predicted group memberships are _hyper-planes_ in the data space of the quantitative variables. In 2D plots, these
appear as lines, and a goal of this section is to show how you can plot these. When the variance covariance matrices differ
substantially, the boundaries become curved, and the method is called **quadratic** discriminant analysis,
implemented in `MASS::qda()`.[^other-discrim]

[^other-discrim]: There are a number of other methods of discriminant analysis. For example, in
**mixture discriminant analysis** (MDA), each class is assumed to be a Gaussian mixture of subclasses ...
**flexible discriminant analysis** (FDA) is anextension of LDA that uses non-linear combinations of predictors such as splines. FDA is useful to model multivariate non-normality or non-linear relationships among variables within each group, allowing for a more accurate classification. These are implemented in the 
<!-- `r package("mda")`  -->
package `mda`
as `mda()` and `fda()`, which have the same syntax and similar
methods to `lda()`.

Another way that LDA differs from MANOVA is that for classification purposes, the relative proportions of the groups
in your sample or in the population has a role in determining the classification rules. These are called
_prior probabilities_, which adjust the boundaries used to classify new observations. A higher prior probability for a group increases its assigned likelihood, effectively "pulling" the classification boundary in its favor. 



::: {#exm-penguin-new}
**Penguins on Island Z**

<!-- fig.code: R/peng-lda-pred-new.R -->

For an example, to illustrate how discriminant analysis works, and how to use it to classify observations,
imagine you are a researcher on an expedition to Antarctica to survey the penguin population. 
You stop at a small, as yet unnamed island "Z", and find five penguins you want to study. You call them
Abe, Betsy, Chloe, Dave and Emma. How can you determine their species based on what you know of the penguins studied before?

```{r peng-new}
peng_new <- data.frame(
  species = rep(NA, 5),
  island = rep("Z", 5),
  bill_length = c(35, 52, 52, 50, 40),
  bill_depth= c(18, 20, 15, 16, 15),
  flipper_length = c(220, 190, 210, 190, 195),
  body_mass = c(5000, 3900, 4000, 3500, 5500),
  sex = c("m", "f", "f", "m", "f"),
  row.names = c("Abe", "Betsy", "Chloe", "Dave", "Emma")
  )
peng_new
```

So, you can run a discriminant analysis using the existing data and then use that to classify the new penguins on island Z.
By default, `MASS::lda()` uses the proportions of the three species as the prior probabilities, which are given in the printed output.
With $g = 3$ groups, and $p = 4$ variables, there are only $s = 2$ discriminant dimensions, of which the first, `LD1` accounts for
86.6% of the total ...

```{r peng-lda}
data(peng, package = "heplots")
peng.lda <- lda(species ~ bill_length + bill_depth + flipper_length + body_mass, 
                data = peng)
print(peng.lda, digits = 3)
```

The output from this, `peng.lda`, can be used to classify our new penguins using the `MASS::predict.lda()` method for an `"lda"`
object. The printed result isn't pretty, because the function simply returns a list, with elements:

* `class`: The predicted class for each observation
* `posterior`: The posterior probabilities for the observations being assigned to each `class`. The predicted class is that of the maximum probability.
* `x`: The scores of the test cases on the discriminant variables. 

```{r peng-lda-pred}
peng_pred <- predict(peng.lda, newdata = peng_new) |>
  print(digits = 4)
```

A little manipulation of the result from `predict()` makes what has happened here more apparent.

```{r peng-class-data}
class <- peng_pred$class 
posterior <- peng_pred$posterior
maxp <- apply(posterior, 1, max)
data.frame(class, round(posterior, 4), maxp)
```

This says that Abe has the highest probability in the Adelie class, Betty and Dave are almost certainly of the Chinstrap species,
while Chloe and Emma are almost certainly Gentoos.
:::

### Visualizing classification in data space

A simple way to understand what happens in discriminant analysis is to plot the original data in data space and add labeled points
for the new observations. To make this easier, I created a function `predict_discrim()`, which joins the measures in the
test dataset with the predicted class for each new observation.

```{r}
source("R/predict_discrim.R")
pred <- predict_discrim(peng.lda, newdata = peng_new[, 3:6]) |>
  print()
```

```{r peng-plots-setup}
#| echo: false
ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))
source("R/penguin/penguin-colors.R")

theme_penguins <- list(
  scale_color_penguins(shade="dark"),
  scale_fill_penguins(shade="dark"))
legend_inside <- function(position) {
  theme(legend.position = "inside",
        legend.position.inside = position)
  }
```

Using this you can simply plot the original Penguin data for two variables and use `geom_label()` to plot the observations of the
new penguins in this space. In @fig-peng-new-data, I do this in two separate plots, one for the bill length and depth variables, and another
for `flipper_length` vs. `body_mass`. The key thing is that the call to `geom_label()` uses the predicted `species` and
coordinates for the new penguins from the `pred` dataset obtained from `predict_discrim()`.

```{r}
#| label: fig-peng-new-data
#| fig-height: 5
#| fig-width: 10
#| out-width: "100%"
#| fig-cap: "Plots of the Penguin data, showing the locations of the newly classified observations from island Z."
p1 <- ggplot(peng, 
       aes(x = bill_length, y = bill_depth,
           color = species, shape = species, fill=species)) +
  geom_point(size=2) +
  stat_ellipse(geom = "polygon", level = 0.95, alpha = 0.4) +
  geom_label(data = pred, label=row.names(pred), 
             fill="white", size = 5, fontface="bold") +
  theme_penguins +
  legend_inside(c(0.87, 0.15))

p2 <- ggplot(peng, 
       aes(x = flipper_length, y = body_mass,
           color = species, shape = species, fill=species)) +
  geom_point(size=2) +
  stat_ellipse(geom = "polygon", level = 0.95, alpha = 0.4) +
  geom_label(data = pred, label=row.names(pred), 
             fill="white", size = 5, fontface="bold") +
  theme_penguins +
  legend_inside(c(0.87, 0.15))

p1 + p2
```

In @fig-peng-new-data, Betsy is well within the data ellipses for their species in both plots.
Abe looks very much like an Adelie in the panel for the bill variables, but more like a Gentoo
in terms of flipper length and body mass.
Chloe, classed as a Gentoo is at the margin of the 95% ellipses. Dave, classified as a Chinstrap, looks more like
a Gentoo in terms of bill length and depth, but is in the region occupied by Adelie and Chinstraps in the
right panel. Emma is outside the data ellipses for all three species in both plots.

These plots are just two of the $4 \times 3 / 2 = 6$ possible pairwise plots that would appear in a scatterplot matrix.

### Visualizing classification in discriminant space

One main virtue of discriminant analysis is that it allows us to see the data in the reduced-rank space that shows the
greatest differences among the groups. If you add points for the newly  classified observations, you can see
better why they are classified as they were.
The `r package("MASS")` has a `plot.lda()` method, but the result is far too
ugly to be useful. Here's what you get with the default settings:

```{r echo=-1}
#| label: fig-peng-new-discrim
#| fig-height: 5
#| fig-width: 6
#| out-width: "60%"
#| fig-cap: "Plot of the Penguin observations in the space of the two discriminant dimensions. The default plot method uses the classes of the observations as labels."
op <- par(mar = c(4, 4, 1, 1)+.5)
plot(peng.lda)
```

My goal here is to illustrate plotting in discriminant space and show how the basic plot from `plot.lda()`
can be made to be informative. First, `plot_discrim()` can also return the discriminant scores for the new observations
via the argument `scores = TRUE`. I'll use this dataset to add labels to the plot.

```{r}
pred <- predict_discrim(peng.lda, 
                        newdata = peng_new[, 3:6],
                        scores = TRUE)
print(pred[, -(1:4)])
```

Then, I want to use more informative axis labels, showing the percent of between-group variance associated with each dimension,
which come from the `svd` component of the `peng.lda` object.

```{r}
svd <- peng.lda$svd
var <- 100 * round(svd^2/sum(svd^2), 3)
labs <- glue::glue("Discriminant dimension {1:2} ({var}%)") |>
  print()
```
 
Next, you can override how `plot.lda()` plots the observations by creating a panel function that simply plots the points
rather than their observation classes,

```{r}
panel.pts <- function(x, y, ...) points(x, y, ...)
```

Finally, ...

```{r echo = -1}
#| label: fig-peng-new-discrim2
#| fig-height: 5
#| fig-width: 6
#| out-width: "60%"
#| fig-cap: "A customized plot of the Penguin observations in the space of the two discriminant dimensions...."
op <- par(mar = c(4, 4, 1, 1)+.5)
panel.pts <- function(x, y, ...) points(x, y, ...)
col <- peng.colors()
plot(peng.lda, 
     panel = panel.pts,
     col = col[peng$species],
     pch = (15:17)[peng$species],
     xlab = labs[1],
     ylab = labs[2],
     ylim = c(-6, 6),
     cex.lab = 1.3
)


# plot the new observations and label them with their row.names in the predicted data
with(pred,{
     points(LD1, LD2, pch = 16, col = "black", cex = 2)
     text(LD1, LD2, label=row.names(pred),
          col = col[species], cex = 1.3, font = 2,
          pos = c(3, 1, 1, 1, 3),
          xpd = TRUE)
  })
```





### Visualizing prediction regions

