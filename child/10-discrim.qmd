## Discriminant analysis {#sec-discrim-MLM}

```{r load-em, include=FALSE}
library(MASS)             # lda() and qda()
library(ggplot2)
library(patchwork)
library(klaR)             # for partimat()
library(marginaleffects)  # for datagrid()
```

```{r include=FALSE}
# only for testing by itself
source(here::here("R/common.R"))
library(dplyr)
```

As described earlier (@sec-t2-discrim), Linear Discriminant Analysis (LDA) is similar to a one-way MANOVA, but with its emphasis
on the problem of _classifying observations_ into groups rather than _testing_ whether there are significant differences
of the means for the response variables. You would use LDA rather than MANOVA when your goal is to predict group membership and identify which variables best distinguish between pre-defined groups, rather than just testing for group differences.

Thus, LDA can be seen as a **flipped MANOVA**, where the role of dependent and independent variables are reversed.
You can see this difference in the model formulas used in `lm()` compared with `MASS::lda()`, which fits a linear discriminant analysis (LDA) model: The outcomes on the right-hand side are `cbind(y1, y2, y3)` for the MANOVA but is simply `group` for a discriminant analysis.

```{r eval=FALSE}
manova.mod  <- lm(cbind(y1, y2, y3) ~ group)
discrim.mod <- MASS::lda(group ~ y1 + y2 + y3)
```

One consequence of this flipped emphasis is that predicted values from `predict()` for an LDA is the predicted _group membership_ for an observation
with values $y_1, y_2, y_3$ rather than the predicted _response values_ $\hat{y}_1, \hat{y}_2, \hat{y}_3$ in MANOVA.
This is useful for classifying new observations from an LDA model, such as determining whether new Swiss banknotes are real or fake (@exm-banknote) or classifying a new penguin.

As we have seen, the multivariate linear model fit by `lm()` applies equally well when there are two or more grouping factors
and or quantitative predictors, whereas discriminant analysis is mostly restricted to the case of a single `group` factor.[^candisc]

[^candisc]: The function `candisc::candisc()` carries out a _generalized_ discriminant analysis, for one term in a multivariate
linear model. `candisc::candiscList()` does this for all terms in a model.

In both cases, the analysis and significance tests are based on the familiar breakdown (@eq-SSP) of total variability, 
$\mathbf{T} \equiv \mathbf{SSP}_{T}$, of the observations around the grand means into portions attributable to differences _between_ the group means $\mathbf{H} \equiv \mathbf{SSP}_{H}$
and the portions attributable to differences _within_ the groups around their means, $\mathbf{E} \equiv \mathbf{SSP}_{E}$ as described in
@sec-sum-of-squares,

$$
\mathbf{T} = \mathbf{H} + \mathbf{E} \; .
$$
As we saw earlier (@sec-H-vs-E), significance tests are based on the $s = \min{(p, \text{df}_h)}$ non-zero
eigenvalues $\lambda_i$ of $\mathbf{H}\mathbf{E}^{-1}$ which are combined into a single test statistic such
as Wilks' $\Lambda = \Pi_i^s (1+\lambda_i)^{-1}$ or Hotelling-Lawley trace, $\Sigma_i^s \lambda_i$.

The corresponding eigenvectors, $\mathbf{V}$ of $\mathbf{H}\mathbf{E}^{-1}$ are the weights for
the linear combinations of the quantitative variables on which the groups are most widely separated.
The transformation of the $\mathbf{Y}$ variables in data space to the space of these linear combinations
is given by $\mathbf{Z}_{n \times s} = \mathbf{Y} \; \mathbf{E}^{-1/2} \; \mathbf{V}$ as we saw earlier (sec REF?).

But here is where MANOVA and discriminant analysis diverge again. In discriminant analysis, 
you an replace the observed $\mathbf{Y}$ variables with the uncorrelated $s$ discriminant variates defined by $\mathbf{V}$
and obtain _exactly_ the same classifications of the observations. 
The first, $\mathbf{v}_1$, associated with the largest eigenvalue $\lambda_1$, accounts for the greatest
proportion of between-group separation; the second, $\mathbf{v}_2$ is next in discriminating power, and so forth.
But maybe we can classify nearly as well with a small subset of $k < s$ discriminants without great loss.
Hence, just as in PCA, discriminant analysis can be thought of as a **dimension reduction** technique,
squeezing the most discrininant juice out of the data in a few dimensions.


<!-- method notes
https://en.wikipedia.org/wiki/Linear_discriminant_analysis
https://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/

https://stats.stackexchange.com/questions/82959/how-is-manova-related-to-lda
https://stats.stackexchange.com/questions/48786/algebra-of-lda-fisher-discrimination-power-of-a-variable-and-linear-discriminan#48859
https://stats.stackexchange.com/questions/22884/how-does-linear-discriminant-analysis-reduce-the-dimensions/22889#22889
https://stats.stackexchange.com/questions/169436/how-lda-a-classification-technique-also-serves-as-dimensionality-reduction-tec

Takes up LDA, QDA, mixture MDA models, ...
https://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/
-->

The discussion here is limited to what this altered focus adds to _visualizing differences_ among groups on a collection
of response variables. See @Klecka1980, @Lachenbruch1975 for a basic, but more general introduction to discriminant analysis methods.
There is a useful discussion in this Cross Validated question 
[How is MANOVA related to LDA?](https://bit.ly/4gpMykN) <!-- {.content-visible when-format="pdf"} (https://bit.ly/4gpMykN). -->

As in MANOVA, **linear** discriminant analysis assumes equal variance covariance matrices across the groups. In this case, the boundaries separating
predicted group memberships are _hyper-planes_ in the data space of the quantitative variables. In 2D plots, these
appear as lines, and a goal of this section is to show how you can plot these. When the variance covariance matrices differ
substantially, the boundaries become curved, and the method is called **quadratic** discriminant analysis,
implemented in `MASS::qda()`.[^other-discrim]

[^other-discrim]: There are a number of other methods of discriminant analysis. For example, in
**mixture discriminant analysis** (MDA), each class is assumed to be a mixture of several Gaussian distributions, rather than a single one.
**flexible discriminant analysis** (FDA) is an extension of LDA that uses non-linear combinations of predictors such as splines. FDA is useful to model multivariate non-normality or non-linear relationships among variables within each group, allowing for a more accurate classification. These are implemented in the 
package `mda`
as `mda()` and `fda()`, which have the same syntax and similar
methods to `lda()`.

Another way that LDA differs from MANOVA is that for classification purposes, the relative proportions of the groups
in your sample or in the population has a role in determining the classification rules. These are called
**prior probabilities**, which adjust the boundaries used to classify new observations. A higher prior probability for a group increases its assigned likelihood, effectively "pulling" the classification boundary in its favor.[^prior-probs]

[^prior-probs]: You can choose prior probabilities for discriminant analysis by (a)
using equal probabilities for the groups, (b) calculating them based on the observed sample sizes in your dataset, or (c) manually specifying known prior probabilities if you have external information about the population. The choice depends on whether you want the model to be influenced by group sizes or if you have pre-existing knowledge about the likelihood of group membership. The choice is also influenced by the cost of misclassification. ...

::: {#exm-penguin-new}
**Penguins on Island Z**

<!-- fig.code: R/peng-lda-pred-new.R -->

For an example, to illustrate how discriminant analysis works and how to use it to classify observations,
imagine you are a researcher on an expedition to Antarctica to survey the penguin population. 
You stop at a small, as yet unnamed island "Z", and find five penguins you want to study. You call them
Abe, Betsy, Chloe, Dave and Emma. How can you determine their species based on what you know of the penguins studied before?

```{r peng-new}
peng_new <- data.frame(
  species = rep(NA, 5),
  island = rep("Z", 5),
  bill_length = c(35, 52, 52, 50, 40),
  bill_depth= c(18, 20, 15, 16, 15),
  flipper_length = c(220, 190, 210, 190, 195),
  body_mass = c(5000, 3900, 4000, 3500, 5500),
  sex = c("m", "f", "f", "m", "f"),
  row.names = c("Abe", "Betsy", "Chloe", "Dave", "Emma")
  ) |>
  print()
```

So, you can run a discriminant analysis using the existing data and then use that to classify the new penguins on island Z.
By default, `MASS::lda()` uses the proportions of the three species as the prior probabilities, which are given in the printed output.
With $g = 3$ groups, and $p = 4$ variables, there are only $s = 2$ discriminant dimensions, of which the first, `LD1` accounts for
86.6% of the total ...

```{r peng-lda}
data(peng, package = "heplots")
peng.lda <- lda(species ~ bill_length + bill_depth + flipper_length + body_mass, 
                data = peng)
print(peng.lda, digits = 3)
```

### Classification accuracy

How well did LDA succeed in classifying the penguins in the _existing_ `peng` dataset?
You can answer this by making a table of the frequencies of the species in the data against 
their predicted `class`, obtained from the `MASS::predict.lda()` method for an `"lda"`
object:
```{r class-table}
class_table <- table(peng$species,
                     predict(peng.lda)$class,
                     dnn = c("actual", "predicted")) |>
  print()
# overall rates
accuracy <- sum(diag(class_table))/sum(class_table) * 100
error <- 100 - accuracy
c(accuracy, error)
```

That's pretty good! Only 4 penguins are misclassified, giving an error rate of 1.2%. You can identify the
errors by joining the data with the predicted class and filtering those that don't match.

```{r}
data.frame(id = row.names(peng),
           peng[, c(1, 3:6)],
           predicted = predict(peng.lda)$class) |>
  filter(species != predicted) |>
  relocate(predicted, .after = species)
```

The only confusions are between Adelie and Chinstraps. We'll see why this is the case in plots developed below.

### Classifying new penguins

The fitted model, `peng.lda`, can also be used to classify our new penguins by supplying
using the `predict.lda()` with  `newdata = peng_new`. 
The printed result isn't pretty, because the function simply returns a list, with elements:

* `class`: The predicted class for each observation
* `posterior`: The posterior probabilities for the observations being assigned to each `class`. The predicted class is that of the maximum probability.
* `x`: The scores of the test cases on the discriminant variables. 

```{r peng-lda-pred}
peng_pred <- predict(peng.lda, newdata = peng_new) |>
  print(digits = 4)
```

A little manipulation of the result from `predict()` makes what has happened here more apparent.

```{r peng-class-data}
class <- peng_pred$class 
posterior <- peng_pred$posterior
maxp <- apply(posterior, 1, max)
data.frame(class, round(posterior, 4), maxp)
```

This says that Abe has the highest probability in the Adelie class, Betty and Dave are almost certainly of the Chinstrap species,
while Chloe and Emma are almost certainly Gentoos.
:::

### Visualizing classification in data space

A simple way to understand what happens in discriminant analysis is to plot the original data in data space and add labeled points
for the new observations. To make this easier, I created a function `predict_discrim()`, which joins the measures in the
test dataset with the predicted class for each new observation. In the result, `species` is the predicted class.

```{r peng-pred}
source(here::here("R/predict_discrim.R"))
pred <- predict_discrim(peng.lda, newdata = peng_new[, 3:6]) |>
  print()
```

```{r peng-plots-setup}
#| echo: false
ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))
source(here::here("R/penguin/penguin-colors.R"))

# theme_penguins <- list(
#   scale_color_penguins(shade="dark"),
#   scale_fill_penguins(shade="dark"))
legend_inside <- function(position) {
  theme(legend.position = "inside",
        legend.position.inside = position)
  }
```

Using this you can simply plot the original Penguin data for two variables and use `geom_label()` to identify the observations of the
new penguins in this space. In @fig-peng-new-data, I do this in two separate plots, one for the bill length and depth variables, and another
for `flipper_length` vs. `body_mass`. The key thing is that the call to `geom_label()` uses the predicted `species` and
coordinates for the new penguins from the `pred` dataset obtained from `predict_discrim()`.

```{r}
#| label: fig-peng-new-data
#| fig-height: 5
#| fig-width: 10
#| out-width: "100%"
#| fig-cap: "Plots of the Penguin data, showing the locations of the newly classified observations from island Z."
p1 <- ggplot(peng, 
       aes(x = bill_length, y = bill_depth,
           color = species, shape = species, fill=species)) +
  geom_point(size=2) +
  stat_ellipse(geom = "polygon", level = 0.95, alpha = 0.4) +
  geom_label(data = pred, label=row.names(pred), 
             fill="white", size = 5, fontface="bold") +
  theme_penguins("dark") +
  legend_inside(c(0.87, 0.15))

p2 <- ggplot(peng, 
       aes(x = flipper_length, y = body_mass,
           color = species, shape = species, fill=species)) +
  geom_point(size=2) +
  stat_ellipse(geom = "polygon", level = 0.95, alpha = 0.4) +
  geom_label(data = pred, label=row.names(pred), 
             fill="white", size = 5, fontface="bold") +
  theme_penguins("dark") +
  legend_inside(c(0.87, 0.15))

p1 + p2
```

In @fig-peng-new-data, Betsy is well within the data ellipses for their species in both plots.
Abe looks very much like an Adelie in the panel for the bill variables, but more like a Gentoo
in terms of flipper length and body mass.
Chloe, classed as a Gentoo is at the margin of the 95% ellipses. Dave, classified as a Chinstrap, looks more like
a Gentoo in terms of bill length and depth, but is in the region occupied by Adelie and Chinstraps in the
right panel. Emma is outside the data ellipses for all three species in both plots.

These plots are just two of the $4 \times 3 / 2 = 6$ possible pairwise plots that would appear in a scatterplot matrix.

### Visualizing classification in discriminant space

One main virtue of discriminant analysis is that it allows us to see the data in the reduced-rank space that shows the
greatest differences among the groups. The 4D data space of the observed $\mathbf{y}$ variables is replaced by 2D discriminant space of the $\mathbf{z}$s, which contains the same information for classification.

If you add points for the newly  classified observations, you can see
better why they are classified as they were in a single plot.
The `r package("MASS")` has a `plot.lda()` method, but the usual result is far too
ugly to be useful. Here's what you get with the default settings:

```{r echo=-1}
#| label: fig-peng-new-discrim
#| fig-height: 5
#| fig-width: 6
#| out-width: "60%"
#| fig-cap: "Plot of the Penguin observations in the space of the two discriminant dimensions. The default plot method uses the classes of the observations as labels."
op <- par(mar = c(4, 4, 1, 1)+.5)
plot(peng.lda)
```

But, you can turn this sow's ear into a silk purse (with a little graph-craft)!
My goal here is to illustrate plotting in discriminant space and show how the basic plot from `plot.lda()`
can be made more informative. First, `predict_discrim()` can also return the discriminant scores (`LD1`, `LD2`) for the new observations
via the argument `scores = TRUE`. I'll use this dataset to add labels to the plot.

```{r}
pred <- predict_discrim(peng.lda, 
                        newdata = peng_new[, 3:6],
                        scores = TRUE)
print(pred[, -(1:4)])
```

Then, I want to use more informative axis labels, showing the percent of between-group variance associated with each dimension.
These come from the (poorly named) `svd` component of the `peng.lda` object.

```{r}
svd <- peng.lda$svd
var <- 100 * round(svd^2/sum(svd^2), 3)
labs <- glue::glue("Discriminant dimension {1:2} ({var}%)") |>
  print()
```
 
Fortunately, `plot.lda()` allows a `panel =` argument, to control how the observations are are represented.
You can override the default by creating a panel function that simply plots the points
rather than their observation classes,

```{r panel-pts}
panel.pts <- function(x, y, ...) points(x, y, ...)
```

Finally, call `plot.lda()` with this panel function, set the color and shape of the point symbols to match our
Penguin theme and use the axis labels set above in `labs`. You can then add points and labels for the newly classified penguins.

```{r echo = -1}
#| label: fig-peng-new-discrim2
#| fig-height: 6
#| fig-width: 6
#| out-width: "60%"
#| fig-cap: "A customized plot of the Penguin observations in the space of the two discriminant dimensions. Predicted values for the new observations are shown with labeled points."
op <- par(mar = c(4, 4, 1, 1)+.5)
panel.pts <- function(x, y, ...) points(x, y, ...)
col <- peng.colors()
plot(peng.lda, 
     panel = panel.pts,
     col = col[peng$species],
     pch = (15:17)[peng$species],
     xlab = labs[1],
     ylab = labs[2],
     ylim = c(-6, 6),
     cex.lab = 1.3
)

# plot the new observations and label them with their row.names in the predicted data
with(pred,{
     points(LD1, LD2, pch = 16, col = "black", cex = 2)
     text(LD1, LD2, label=row.names(pred),
          col = col[species], cex = 1.3, font = 2,
          pos = c(3, 1, 1, 1, 3),
          xpd = TRUE)
  })
```

Compared with the separate 2D views in data space (@fig-peng-new-data),
this view accounts for 100% of the variance due to separation of the species groups.
You can see that Betsy and Dave are rather close to the other Chinstraps.
Abe, Chloe and Emma are rather far from the centers of the species they are
classified as, yet they are closer to their groups than to any of the two others.

While @fig-peng-new-discrim2 is a substantial improvement over @fig-peng-new-discrim, it is harder to go much
further, because `plot.lda()` method conceals what it does to make the plot using the `"lda"` object.
You have to dive into that object to get the information needed to add additional graphical information.

For example, if you also wanted to show data ellipses or other bivariate summaries in this plot, it would
be necessary to get the predicted classes for each observation in `peng` dataset, together with the
discriminant scores, `LD1` and `LD2`, obtained using `scores=TRUE` in the call to `predict_discrim()`.
Data ellipses could then be added added using `car::dataEllipse()` as follows (not shown)

```{r peng-discrim-ellipses}
#| eval: false
pred_all <- predict_discrim(peng.lda, scores=TRUE)

# add data ellipses
dataEllipse(LD2 ~ LD1 | species, data = pred_all,
            levels = 0.68, fill=TRUE, fill.alpha = 0.1,
            group.labels = NULL,
            add = TRUE, plot.points = FALSE,
            col = col)
```



### Visualizing prediction regions

An even better way to understand and interpret discriminant analysis results is to visualize the boundaries that separate the prediction
into one group rather than another. Our penguins live in a 4-dimensional data space, and the boundaries
of the predicted classification regions are of one less dimension---3D hyperplanes here.

Any point in this data space can be classified into one of the group using a `predict()` method. Therefore,
to visualize the prediction regions, you can calculate the prediction over a grid of values of the
$y$ variables, and then plot those using colored tiles. The steps are:

* Construct a $k \times k$ grid of values of the two focal variables $y_1, y_2$ to be plotted:
  * For each focal $y$ variable, divide it's range into $k$ intervals, as in `seq(min(y)), max(y), k)`
  * Set each non-focal variable to its's mean.
  * Use a `predict()` method to get the predicted classification group for each point. 

* For base R graphs, you can use `image(x, y, z, ...)` to plot this grid of the `(x, y)`, where `z` is a $k \times k$-length vector of the
predicted class of an observation at this point. In `ggplot2` this is easier, using `geom_tile().

* Boundaries of the regions are the contours of the maximum of the posterior probabilities. You can plot these using `contour(x, y, z, ...)`
in base R or `geom_contour()` in `ggplot2`.

* On top of this, you can plot the data observations, data ellipses for the groups or anything else.

This idea is essentially the same as used in _effect plots_ (@sec-effect-displays)---you
calculate predicted values over a range of the variables shown, while controlling those not shown at a typical value.

**Partition plots with `partimat()`**

The function `klar::partimat()` produces such "partition" plots for `lda()`, `qda()` as well as a number of other classification methods, including recursive partioning trees (`rpart::rpart()`), naive Bayes (`e1071::naiveBayes()`) and other methods. The graphs produced are
generally UGLY; but they are easy to produce and convey a sense of the method. Once we have the idea in mind, we can try to make
prettier, more useful versions.

`partimat()` takes a model formula for the classification, here `species ~ .` for the penguin data. It produces a classification plot
for every combination of two $y$ variables in the data, and can show them either in a scatterplot matrix format (`plot.matrix = TRUE`), or a rectangular display of only the unique pairs. The latter takes less space, and so allows higher resolution in the individual plots.
The `method` argument in the call uses `lda()` to determine the result shown in @fig-peng-partimat1.

<!-- fig.code: R/peng.lda.R -->

```{r}
#| label: fig-peng-partimat1
#| fig-height: 5
#| fig-width: 7.5
#| out-width: "90%"
#| fig-cap: "Partition plots for the penguin data. Each pairwise scatterplot shades the plot background according to how an observation there would be classified."
peng |>
  dplyr::select(species, bill_length:body_mass) |>
  partimat(species ~ ., data = _, 
         method = "lda",
         plot.matrix = FALSE,
         image.colors = scales::alpha(col, alpha = 0.4),
         main = "Penguin Partition Plot"
  )
```

In each pairwise plot of the penguin variables shown in @fig-peng-partimat1, the background is colored according to the species that
any observation there (holding others constant) would be classified as. This fails in software design because there is little
flexibility in what else is shown to represent the observations. The penguin data values are shown using the first letter of
their species, colored black if that bird is classified correctly, `r red` otherwise.[^partimat-encoding]

[^partimat-encoding]: In fairness, `klaR::partimat()` was designed to emphasize the classification accuracy shown in each pairwise plot,
also printed as an error rate for each plot. You can see, for example that the plot of `body_mass` against `bill_depth` in row 2, column 2
of @fig-peng-partimat1 shows only two distinct regions, for Adelie and Gentoo. All the Chinstraps appear mixed in with the Adelies
and, giving an error rate of 0.204 in this plot. Two other panels also show high error rates.

**Using `ggplot()`**

To construct similar (but better) plots using `r pkg("ggplot2")`, I follow the steps outlined above to get predicted
classes over a grid, in this case for the penguin bill variables. 
The function `marginaleffects::datagrid()` makes this easy, automatically marginalizing the other variables.
You can supply a function, here `range80()`, to specify a sequence of a variable over its range.

```{r pred-grid}
# make a grid of values for prediction
range80 = \(x) seq(min(x), max(x), length.out = 80)
grid <- datagrid(bill_length = range80, 
                 bill_depth = range80, newdata = peng)
# get predicted species for the grid points
pred_grid <- predict_discrim(peng.lda, newdata = grid) 
head(pred_grid)
```

Then, in the plotting steps, `geom_tile()` is used to display the predictions in the `pred_grid` dataset, using the penguin colors for the species. Points and other layers use the full `peng` dataset. I also calculate the 
means for each species and use these as direct labels for the groups, to avoid a legend. [**TODO** Should probably use `asp = 1` (`coord_equal()`) here to equate axes.]

```{r peng-ggplot-code}
#| echo: true
#| results: hide
means <- peng |>
  group_by(species) |>
  summarise(across(c(bill_length, bill_depth), \(x) mean(x, na.rm = TRUE) ))

p1 <- ggplot(data = peng, aes(x = bill_length, y = bill_depth)) +
  # Plot decision regions
  geom_tile(data = pred_grid, aes(fill = species), alpha = 0.2) +
  stat_ellipse(aes(color=species), level = 0.68, linewidth = 1.2) +
  # Plot original data points
  geom_point(aes(color = species, shape=species),
             size =2) +
  geom_label(data=means, aes(label = species, color = species),
             size =5) +
  theme_penguins("dark") +
  theme_minimal(base_size = 16) +
  theme(legend.position = "none")
```

Doing the same steps for `flipper_length` and `body_mass`, and then showing the two plots together 
(as in @fig-peng-new-data) gives @fig-peng-regions.
```{r}
#| label: fig-peng-regions
#| echo: false
#| fig-height: 5
#| fig-width: 10
#| out-width: "100%"
#| fig-cap: "Plots of the Penguin data showing the linear discriminant prediction regions for the three species."
grid <- marginaleffects::datagrid(flipper_length = range80, 
                                  body_mass = range80, newdata = peng)
pred_grid <- predict_discrim(peng.lda, newdata = grid) 

means <- peng |>
  group_by(species) |>
  summarise(across(c(flipper_length, body_mass), \(x) mean(x, na.rm = TRUE) ))
p2 <- ggplot(data = peng, aes(x = flipper_length, y = body_mass)) +
  # Plot decision regions
  geom_tile(data = pred_grid, aes(fill = species), alpha = 0.2) +
  stat_ellipse(aes(color=species), level = 0.68, linewidth = 1.2) +
  # Plot original data points
  geom_point(aes(color = species, shape=species),
             size =2) +
  geom_label(data=means, aes(label = species, color = species),
             size =5) +
  theme_penguins("dark") +
  theme_minimal(base_size = 16) +
  theme(legend.position = "none")

p1 + p2
```

The groups are nicely separated in the left panel for the bill variables. For the other two variables
in the right panel, Adelie and Chinstrap overlap considerably, but are well distinguished from the Gentoos.
The predictions for these groups favor Adelie for penguins of greater body mass.
These plots are essentially the same as those in the (1, 1) and (2, 3) panels in @fig-peng-partimat1,
but with axes reversed.

### Prediction regions in discriminant space

The 2D plot of the data in discriminant space (@fig-peng-new-discrim2) is much simpler to understand because it captures
all the information regarding penguin classification in a single plot, rather than the six shown in 
@fig-peng-partimat1. Can we make a similar plot, showing the prediction regions in discriminant space?

If you recall that dimension reduction interpretation of discriminant analysis allows you to replace the 
observed $\mathbf{y}$s with their discriminant scores $\mathbf{z}$ with the same accuracy when there are
only $s=2$ possible dimensions, you can do this first finding the discriminant scores, `LD1` and `LD2`
and using those as predictors in a new call to `lda()`:

```{r peng-lda2}
peng_scored <- predict_discrim(peng.lda, scores=TRUE, posterior = FALSE)

peng.lda2 <- lda(species ~ LD1 + LD2, data=peng_scored)
peng.lda2
```

As you can see from the results, the proportions of between-class variance accounted for are nearly the same
as in the original `peng.lda`. Note that the coefficients for the old vs. new discriminants are nearly
the identity matrix, except that the sign of the coefficients for `LD` have been flipped.
[**TODO**: Can I fix this, ie., flip LD1 in the plot?]

Then you can follow the earlier examples in data space: Set up a grid of values for `LD1` and `LD2`
over the discriminant space of `peng_scored` and obtain the predicted classifications for each point.

```{r grid-scored}
grid <- datagrid(LD1 = range80, 
                 LD2 = range80, newdata = peng_scored) 
pred_grid <- predict_discrim(peng.lda2, newdata = grid, posterior = FALSE) 
```

Finally, the plotting steps are identical to that used in @fig-peng-regions.
The resulting plot in @fig-peng-LD-predict gives a single, complete picture of the results of discriminant
analysis for this dataset.

```{r}
#| label: fig-peng-LD-predict
#| code-fold: true
#| fig-height: 6
#| fig-width: 6
#| out-width: "60%"
#| fig-cap: "Plot of the Penguin data in discriminant space showing the prediction regions for the three species."
means <- peng_scored |>
  group_by(species) |>
  summarise(across(LD1:LD2, \(x) mean(x, na.rm = TRUE) ))
means

ggplot(data = peng_scored, aes(x = LD1, y = LD2)) +
  # Plot decision regions
  geom_tile(data = pred_grid, aes(fill = species), alpha = 0.2) +
  stat_ellipse(aes(color=species), level = 0.68, linewidth = 1.2) +
  # Plot original data points
  geom_point(aes(color = species, shape=species),
             size =2) +
  geom_label(data=means, aes(label = species, color = species),
             size =5) +
  theme_penguins() +
  theme_minimal(base_size = 16) +
  theme(legend.position = "none")
```

