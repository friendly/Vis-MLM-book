## Discriminant analysis {#sec-discrim-MLM}

```{r setup, include=FALSE}
library(MASS)
library(ggplot2)
library(patchwork)
library(klaR)
```


As described earlier (@sec-t2-discrim), Linear Discriminant Analysis (LDA) is similar to a one-way MANOVA, but with its emphasis
on the problem of _classifying observations_ into groups rather than _testing_ whether there are significant differences
of the means for the response variables. You would use LDA rather than MANOVA when your goal is to predict group membership and identify which variables best distinguish between pre-defined groups, rather than just testing for group differences.

Thus, LDA can be seen as a **flipped MANOVA**, where the role of dependent and independent variables are reversed.
You can see this difference in the model formulas used in `lm()` compared with `MASS::lda()`, which fits a linear discriminant analysis (LDA) model: The outcomes on the right-hand side are `cbind(y1, y2, y3)` for the MANOVA but is simply `group` for a discriminant analysis.

```{r eval=FALSE}
manova.mod  <- lm(cbind(y1, y2, y3) ~ group)
discrim.mod <- MASS::lda(group ~ y1 + y2 + y3)
```

One consequence of this flipped emphasis is that predicted values from `predict()` for an LDA is the predicted _group membership_ for an observation
with values $y_1, y_2, y_3$ rather than the predicted _response values_ $\hat{y}_1, \hat{y}_2, \hat{y}_3$ in MANOVA.
This is useful for classifying new observations from an LDA model, such as determining whether new Swiss banknotes are real or fake (@exm-banknote) or classifying a new penguin.

As we have seen, the multivariate linear model fit by `lm()` applies equally well when there are two or more grouping factors
and or quantitative predictors, whereas discriminant analysis is mostly restricted to the case of a single `group` factor.[^candisc]

[^candisc]: The function `candisc::candisc()` carries out a _generalized_ discriminant analysis, for one term in a multivariate
linear model. `candisc::candiscList()` does this for all terms in a model.

In both cases, the analysis and significance tests are based on the familiar breakdown (@eq-SSP) of total variability, 
$\mathbf{T} \equiv \mathbf{SSP}_{T}$, of the observations around the grand means into portions attributable to differences _between_ the group means $\mathbf{H} \equiv \mathbf{SSP}_{H}$
and the portions attributable to differences _within_ the groups around their means, $\mathbf{E} \equiv \mathbf{SSP}_{E}$ as described in
@sec-sum-of-squares,

$$
\mathbf{T} = \mathbf{H} + \mathbf{E} \; .
$$
As we saw earlier (@sec-H-vs-E), significance tests are based on the $s = \min{(p, \text{df}_h)}$ non-zero
eigenvalues $\lambda_i$ of $\mathbf{H}\mathbf{E}^{-1}$ which are combined into a single test statistic such
as Wilks' $\Lambda = \Pi_i^s (1+\lambda_i)^{-1}$ or Hotelling-Lawley trace, $\Sigma_i^s \lambda_i$.

The corresponding eigenvectors, $\mathbf{V}$ of $\mathbf{H}\mathbf{E}^{-1}$ are the weights for
the linear combinations of the quantitative variables on which the groups are most widely separated.
The transformation of the $\mathbf{Y}$ variables in data space to the space of these linear combinations
is given by $\mathbf{Z}_{n \times s} = \mathbf{Y} \; \mathbf{E}^{-1/2} \; \mathbf{V}$ as we saw earlier (sec REF?).

But here is where MANOVA and discriminant analysis diverge again. In discriminant analysis, 
you an replace the observed $\mathbf{Y}$ variables with the uncorrelated $s$ discriminant variates defined by $\mathbf{V}$
and obtain _exactly_ the same classifications of the observations. 
The first, $\mathbf{v}_1$, associated with the largest eigenvalue $\lambda_1$, accounts for the greatest
proportion of between-group separation; the second, $\mathbf{v}_2$ is next in discriminating power, and so forth.
But maybe we can classify nearly as well with a small subset of $k < s$ discriminants without great loss.
Hence, just as in PCA, discriminant analysis can be thought of as a **dimension reduction** technique,
squeezing the most discrininant juice out of the data in a few dimensions.


<!-- method notes
https://en.wikipedia.org/wiki/Linear_discriminant_analysis
https://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/

https://stats.stackexchange.com/questions/82959/how-is-manova-related-to-lda
https://stats.stackexchange.com/questions/48786/algebra-of-lda-fisher-discrimination-power-of-a-variable-and-linear-discriminan#48859
https://stats.stackexchange.com/questions/22884/how-does-linear-discriminant-analysis-reduce-the-dimensions/22889#22889
https://stats.stackexchange.com/questions/169436/how-lda-a-classification-technique-also-serves-as-dimensionality-reduction-tec

Takes up LDA, QDA, mixture MDA models, ...
https://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/
-->

The discussion here is limited to what this altered focus adds to _visualizing differences_ among groups on a collection
of response variables. See @Klecka1980, @Lachenbruch1975 for a basic, but more general introduction to discriminant analysis methods.
There is a useful discussion in this Cross Validated question 
[How is MANOVA related to LDA?](https://bit.ly/4gpMykN) <!-- {.content-visible when-format="pdf"} (https://bit.ly/4gpMykN). -->

As in MANOVA, **linear** discriminant analysis assumes equal variance covariance matrices across the groups. In this case, the boundaries separating
predicted group memberships are _hyper-planes_ in the data space of the quantitative variables. In 2D plots, these
appear as lines, and a goal of this section is to show how you can plot these. When the variance covariance matrices differ
substantially, the boundaries become curved, and the method is called **quadratic** discriminant analysis,
implemented in `MASS::qda()`.[^other-discrim]

[^other-discrim]: There are a number of other methods of discriminant analysis. For example, in
**mixture discriminant analysis** (MDA), each class is assumed to be a Gaussian mixture of subclasses ...
**flexible discriminant analysis** (FDA) is anextension of LDA that uses non-linear combinations of predictors such as splines. FDA is useful to model multivariate non-normality or non-linear relationships among variables within each group, allowing for a more accurate classification. These are implemented in the 
package `mda`
as `mda()` and `fda()`, which have the same syntax and similar
methods to `lda()`.

Another way that LDA differs from MANOVA is that for classification purposes, the relative proportions of the groups
in your sample or in the population has a role in determining the classification rules. These are called
_prior probabilities_, which adjust the boundaries used to classify new observations. A higher prior probability for a group increases its assigned likelihood, effectively "pulling" the classification boundary in its favor. 



::: {#exm-penguin-new}
**Penguins on Island Z**

<!-- fig.code: R/peng-lda-pred-new.R -->

For an example, to illustrate how discriminant analysis works, and how to use it to classify observations,
imagine you are a researcher on an expedition to Antarctica to survey the penguin population. 
You stop at a small, as yet unnamed island "Z", and find five penguins you want to study. You call them
Abe, Betsy, Chloe, Dave and Emma. How can you determine their species based on what you know of the penguins studied before?

```{r peng-new}
peng_new <- data.frame(
  species = rep(NA, 5),
  island = rep("Z", 5),
  bill_length = c(35, 52, 52, 50, 40),
  bill_depth= c(18, 20, 15, 16, 15),
  flipper_length = c(220, 190, 210, 190, 195),
  body_mass = c(5000, 3900, 4000, 3500, 5500),
  sex = c("m", "f", "f", "m", "f"),
  row.names = c("Abe", "Betsy", "Chloe", "Dave", "Emma")
  )
peng_new
```

So, you can run a discriminant analysis using the existing data and then use that to classify the new penguins on island Z.
By default, `MASS::lda()` uses the proportions of the three species as the prior probabilities, which are given in the printed output.
With $g = 3$ groups, and $p = 4$ variables, there are only $s = 2$ discriminant dimensions, of which the first, `LD1` accounts for
86.6% of the total ...

```{r peng-lda}
data(peng, package = "heplots")
peng.lda <- lda(species ~ bill_length + bill_depth + flipper_length + body_mass, 
                data = peng)
print(peng.lda, digits = 3)
```

The output from this, `peng.lda`, can be used to classify our new penguins using the `MASS::predict.lda()` method for an `"lda"`
object. The printed result isn't pretty, because the function simply returns a list, with elements:

* `class`: The predicted class for each observation
* `posterior`: The posterior probabilities for the observations being assigned to each `class`. The predicted class is that of the maximum probability.
* `x`: The scores of the test cases on the discriminant variables. 

```{r peng-lda-pred}
peng_pred <- predict(peng.lda, newdata = peng_new) |>
  print(digits = 4)
```

A little manipulation of the result from `predict()` makes what has happened here more apparent.

```{r peng-class-data}
class <- peng_pred$class 
posterior <- peng_pred$posterior
maxp <- apply(posterior, 1, max)
data.frame(class, round(posterior, 4), maxp)
```

This says that Abe has the highest probability in the Adelie class, Betty and Dave are almost certainly of the Chinstrap species,
while Chloe and Emma are almost certainly Gentoos.
:::

### Visualizing classification in data space

A simple way to understand what happens in discriminant analysis is to plot the original data in data space and add labeled points
for the new observations. To make this easier, I created a function `predict_discrim()`, which joins the measures in the
test dataset with the predicted class for each new observation.

```{r}
source("R/predict_discrim.R")
pred <- predict_discrim(peng.lda, newdata = peng_new[, 3:6]) |>
  print()
```

```{r peng-plots-setup}
#| echo: false
ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))
source("R/penguin/penguin-colors.R")

theme_penguins <- list(
  scale_color_penguins(shade="dark"),
  scale_fill_penguins(shade="dark"))
legend_inside <- function(position) {
  theme(legend.position = "inside",
        legend.position.inside = position)
  }
```

Using this you can simply plot the original Penguin data for two variables and use `geom_label()` to plot the observations of the
new penguins in this space. In @fig-peng-new-data, I do this in two separate plots, one for the bill length and depth variables, and another
for `flipper_length` vs. `body_mass`. The key thing is that the call to `geom_label()` uses the predicted `species` and
coordinates for the new penguins from the `pred` dataset obtained from `predict_discrim()`.

```{r}
#| label: fig-peng-new-data
#| fig-height: 5
#| fig-width: 10
#| out-width: "100%"
#| fig-cap: "Plots of the Penguin data, showing the locations of the newly classified observations from island Z."
p1 <- ggplot(peng, 
       aes(x = bill_length, y = bill_depth,
           color = species, shape = species, fill=species)) +
  geom_point(size=2) +
  stat_ellipse(geom = "polygon", level = 0.95, alpha = 0.4) +
  geom_label(data = pred, label=row.names(pred), 
             fill="white", size = 5, fontface="bold") +
  theme_penguins +
  legend_inside(c(0.87, 0.15))

p2 <- ggplot(peng, 
       aes(x = flipper_length, y = body_mass,
           color = species, shape = species, fill=species)) +
  geom_point(size=2) +
  stat_ellipse(geom = "polygon", level = 0.95, alpha = 0.4) +
  geom_label(data = pred, label=row.names(pred), 
             fill="white", size = 5, fontface="bold") +
  theme_penguins +
  legend_inside(c(0.87, 0.15))

p1 + p2
```

In @fig-peng-new-data, Betsy is well within the data ellipses for their species in both plots.
Abe looks very much like an Adelie in the panel for the bill variables, but more like a Gentoo
in terms of flipper length and body mass.
Chloe, classed as a Gentoo is at the margin of the 95% ellipses. Dave, classified as a Chinstrap, looks more like
a Gentoo in terms of bill length and depth, but is in the region occupied by Adelie and Chinstraps in the
right panel. Emma is outside the data ellipses for all three species in both plots.

These plots are just two of the $4 \times 3 / 2 = 6$ possible pairwise plots that would appear in a scatterplot matrix.

### Visualizing classification in discriminant space

One main virtue of discriminant analysis is that it allows us to see the data in the reduced-rank space that shows the
greatest differences among the groups. If you add points for the newly  classified observations, you can see
better why they are classified as they were.
The `r package("MASS")` has a `plot.lda()` method, but the result is far too
ugly to be useful. Here's what you get with the default settings:

```{r echo=-1}
#| label: fig-peng-new-discrim
#| fig-height: 5
#| fig-width: 6
#| out-width: "60%"
#| fig-cap: "Plot of the Penguin observations in the space of the two discriminant dimensions. The default plot method uses the classes of the observations as labels."
op <- par(mar = c(4, 4, 1, 1)+.5)
plot(peng.lda)
```

But, we can turn this sow's ear into a silk purse!
My goal here is to illustrate plotting in discriminant space and show how the basic plot from `plot.lda()`
can be made more informative. First, `predict_discrim()` can also return the discriminant scores for the new observations
via the argument `scores = TRUE`. I'll use this dataset to add labels to the plot.

```{r}
pred <- predict_discrim(peng.lda, 
                        newdata = peng_new[, 3:6],
                        scores = TRUE)
print(pred[, -(1:4)])
```

Then, I want to use more informative axis labels, showing the percent of between-group variance associated with each dimension.
These come from the (poorly named) `svd` component of the `peng.lda` object.

```{r}
svd <- peng.lda$svd
var <- 100 * round(svd^2/sum(svd^2), 3)
labs <- glue::glue("Discriminant dimension {1:2} ({var}%)") |>
  print()
```
 
Next, you can override how `plot.lda()` plots the observations by creating a panel function that simply plots the points
rather than their observation classes,

```{r panel-pts}
panel.pts <- function(x, y, ...) points(x, y, ...)
```

Finally, call `plot.lda()` with this panel function, set the color and shape of the point symbols to match our
Penguin theme and use the axis labels set above in `labs`. You can then add points and labels for the newly classified penguins.

```{r echo = -1}
#| label: fig-peng-new-discrim2
#| fig-height: 5
#| fig-width: 6
#| out-width: "60%"
#| fig-cap: "A customized plot of the Penguin observations in the space of the two discriminant dimensions...."
op <- par(mar = c(4, 4, 1, 1)+.5)
panel.pts <- function(x, y, ...) points(x, y, ...)
col <- peng.colors()
plot(peng.lda, 
     panel = panel.pts,
     col = col[peng$species],
     pch = (15:17)[peng$species],
     xlab = labs[1],
     ylab = labs[2],
     ylim = c(-6, 6),
     cex.lab = 1.3
)


# plot the new observations and label them with their row.names in the predicted data
with(pred,{
     points(LD1, LD2, pch = 16, col = "black", cex = 2)
     text(LD1, LD2, label=row.names(pred),
          col = col[species], cex = 1.3, font = 2,
          pos = c(3, 1, 1, 1, 3),
          xpd = TRUE)
  })
```

Compared with the separate 2D views in data space (@fig-peng-new-data),
this view accounts for 100% of the variance due to separation of the species groups.
You can see that Betsy and Dave are rather close to the other Chinstraps.
Abe, Chloe and Emma are rather far from the centers of the species they are
classified as, yet they are closer to their groups than to any of the two others.

While @fig-peng-new-discrim2 is a substantial improvement over @fig-peng-new-discrim, it is harder to go much
further, because `plot.lda()` method conceals what it does to make the plot using the `"lda"` object.
You have to dive into that object to get the information needed to add additional graphical information.

For example, if you also wanted to show data ellipses or other bivariate summaries in this plot, it would
be necessary to get the predicted classes for each observation in `peng` dataset, together with the
discriminant scores, `LD1` and `LD2`, obtained using `scores=TRUE` in the call to `predict_discrim()`.
Data ellipses could then be added added using `car::dataEllipse()` as follows (not shown)

```{r peng-discrim-ellipses}
#| eval: false
pred_all <- predict_discrim(peng.lda, scores=TRUE)

# add data ellipses
dataEllipse(LD2 ~ LD1 | species, data = pred_all,
            levels = 0.68, fill=TRUE, fill.alpha = 0.1,
            group.labels = NULL,
            add = TRUE, plot.points = FALSE,
            col = col)
```



### Visualizing prediction regions

It also aids understanding and interpretation to visualize the boundaries that separate the prediction
into one group rather than another. Our penguins live in a 4-dimensional data space, and the boundaries
of the predicted classification regions are of one less dimension---3D hyperplanes here.

Any point in this data space can be classified into one of the group using a `predict()` method. Therefore,
to visualize the prediction regions, you can calculate the prediction over a grid of values of the
$y$ variables, and then plot those using colored tiles. The steps are:

* Construct a $k \times k$ grid of values of the two focal variables $y_1, y_2$ to be plotted:
  * For each focal $y$ variable, divide it's range into $k$ intervals, as in `seq(min(y)), max(y), k)`
  * Set each non-focal variable to its's mean.
  * Use a `predict()` method to get the predicted classification group for each point. 

* For base R graphs, you can use `image(x, y, z, ...)` to plot this grid of the `(x, y)`, where `z` is a $k \times k$-length vector of the
predicted class of an observation at this point. In `ggplot2` this is easier, using `geom_tile().

* Boundaries of the regions are the contours of the maximum of the posterior probabilities. You can plot these using `contour(x, y, z, ...)`
in base R or `geom_contour()` in `ggplot2`.

* On top of this, you can plot the data observations, data ellipses for the groups or anything else.

This idea is essentially the same as used in _effect plots_ ---
calculate predicted values over a range of the variables shown, while controlling those not shown at a typical value.

**`partimat()`**

The function `klar::partimat()` produces such "partition" plots for `lda()`, `qda()` as well as a number of other classification methods, including recursive partioning trees (`rpart::rpart()`), naive Bayes (`e1071::naiveBayes()`) and other methods. The graphs produced are
generally UGLY; but they are easy to produce and convey a sense of the method. Once we have the idea in mind, we can try to make
prettier, more useful versions.

`partimat()` takes a model formula for the classification, here `species ~ .` for the penguin data. It produces a classification plot
for every combination of two $y$ variables in the data, and can show them either in a scatterplot matrix format (`plot.matrix = TRUE`), or a rectangular display of only the unique pairs. The latter takes less space, and so allows higher resolution in the individual plots.
The `method` argument asks for `lda()` to determine the result shown in @fig-peng-partimat1.

<!-- fig.code: R/peng.lda.R -->

```{r}
#| label: fig-peng-partimat1
#| fig-height: 5
#| fig-width: 7.5
#| out-width: "90%"
#| fig-cap: "Partition plots for the penguin data. Each pairwise scatterplot ..."
peng |>
  dplyr::select(species, bill_length:body_mass) |>
  partimat(species ~ ., data = _, 
         method = "lda",
         plot.matrix = FALSE,
         image.colors = scales::alpha(col, alpha = 0.4),
         main = "Penguin Partition Plot"
  )
```

In each pairwise plot of the penguin variables shown in @fig-peng-partimat1, the background is colored according to the species that
any observation there (holding others constant) would be classified as. This fails in software design because there is little
flexibility in what else is shown to represent the observations. The penguin data values are shown using the first letter of
their species, colored black if that bird is classified correctly, `r red` otherwise.[^partimat-encoding]

[^partimat-encoding]: In fairness, `klaR::partimat()` was designed to emphasize the classification accuracy shown in each pairwise plot,
also printed as an error rate for each plot. You can see, for example that the plot of `body_mass` against `bill_depth` in row 2, column 2
of @fig-peng-partimat1 shows only two distinct regions, for Adelie and Gentoo. All the Chinstraps appear mixed in with the Adelies
and, giving an error rate of 0.204 in this plot. Two other panels also show high error rates.


