## What We Have Learned

This chapter introduced the fundamental building blocks of linear modeling in R, focusing on the versatile `lm()` 
function as our primary tool for fitting and understanding linear relationships in data. The goal for the chapter
is to help you understand the mechanics of translating between the algebraic formulation of a linear model and your research
questions using the tools in the `lm()` family.

Here are the essential takeaways:
  
* **The `lm()` function is your Swiss Army knife for linear modeling** -- Whether you're fitting simple regression, multiple regression, 
ANOVA, or ANCOVA models, `lm()` provides a unified interface through R's elegant formula syntax. The beauty lies in how 
a model foprmula like `y ~ x1 + x2 + x1:x2` captures complex relationships with intuitive notation.

<!-- Jack-of-all-trades? polymath? -->

* **Model objects contain a wealth of information** -- An `"lm"` object isn't just coefficients; it's a comprehensive container 
holding fitted values, residuals, the design matrix, and diagnostic information. Learning to extract and manipulate these 
components with functions like `coef()`, `fitted()`, `residuals()`, and `model.matrix()` unlocks the full power of linear modeling.

* **Model matrices reveal the algebraic heart of your analysis** -- Understanding how R transforms your formula into a 
design matrix via `model.matrix()` is fundamental to grasping what linear models actually compute. 
The journey from a formula like `y ~ treatment + block` to a matrix of 0s and 1s illuminates how categorical predictors 
are used in mathematical operations to calculate fitted values.

* **Contrasts control how factors speak to your research questions** -- The choice between treatment, sum, or Helmert contrasts 
isn't just technical housekeeping -- it determines which comparisons your model coefficients represent. 
Using `contrasts()` and `C()` strategically means your model output directly answers your scientific questions 
rather than leaving you to decode cryptic parameter estimates.

<!--
* **Factor coding schemes have profound interpretational consequences** -- Whether you use treatment contrasts 
(comparing to a reference level) or sum contrasts (comparing to overall mean) fundamentally changes what your 
intercept and slopes mean. Understanding these choices helps you avoid the common trap of misinterpreting coefficients in factorial designs.

* **The design matrix bridges statistical theory and computational reality** -- Examining the model matrix with `model.matrix()` 
demystifies how R handles missing cells, interaction terms, and nested factors. What seems like magic in the output 
becomes transparent when you see the actual numbers being multiplied and summed in the linear algebra.
-->

However fitting a model is just the first step.  While `summary()` and `anova()` provide essential numerical summaries, diagnostic plots created with `plot(lm_object)` expose patterns in residuals. Other plots help toidentify influential observations, and validate model assumptions. The interplay between numerical and graphical summaries is where true understanding emerges. This is the topic of @sec-linear-models-plots.