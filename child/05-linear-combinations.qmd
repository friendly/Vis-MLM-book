## Linear combinations

All methods of multivariate statistics involve a simple idea: Finding weighted sums---_linear combinations_--- of
observed variables to optimize some criterion---maximizing a measure of goodness-of-fit, like $R^2$ or minimizing
a measure of badness-of-fit like sums of squares of residuals. Methods differ according to whether:

* All variables belong to **one set** (say, $\mathbf{X}$), not distinguished as to whether they are responses or predictors, as in PCA and factor analysis, 
vs. **two sets** where one set  is considered outcome, dependent variables, to be explained by predictors, 
independent variables ($\mathbf{X}$),
as in multiple regression, multivariate analysis of variance, discriminant analysis and canonical correlation analysis.

* The variables in $\mathbf{X}$ and $\mathbf{Y}$ are discrete, **categorical factors** like sex and level of education or **quantitative** variables like salary and number of years of experience.

#### PCA {.unnumbered}
For example, @fig-lin-comb-pca illustrates PCA (as we saw in @sec-pca-biplot) as finding weights to maximize the variance of linear combinations, $v_1, v_2, ...$,
\begin{eqnarray*}
\mathbf{v}_1 & = & a_1 \mathbf{x}_1 + a_2 \mathbf{x}_2 + a_3 \mathbf{x}_3 + a_4 \mathbf{x}_4 \\
\mathbf{v}_2 & = & b_1 \mathbf{x}_1 + b_2 \mathbf{x}_2 + b_3 \mathbf{x}_3 + b_4 \mathbf{x}_4 \\
\vdots & = & \vdots \; , \\
\end{eqnarray*}

subject to all $\mathbf{v}_i, \mathbf{v}_j$ being uncorrelated, $\mathbf{v}_i \;\perp\; \mathbf{v}_j$.

```{r}
#| label: fig-lin-comb-pca
#| echo: false
#| out-width: "100%"
#| fig-cap: "Principal components analysis as linear combinations to maximize variance accounted for. Left: diagram of PCA showing two uncorrelated linear combinations, v1 and v2. Right: Geometry of PCA."
knitr::include_graphics("images/lin-comb-pca.png")
```

#### Multiple regression {.unnumbered}

An analogous diagram for multiple regression is shown in @fig-lin-comb-mra. Here, we find the weights $b_1, b_2, \dots$
to maximize the $R^2$ of $\mathbf{y}$ with the predicted values $\widehat{\mathbf{y}}$,

$$
\widehat{\mathbf{y}} = b_1 \mathbf{x}_1 + b_2 \mathbf{x}_2 + b_3 \mathbf{x}_3 \period
$$
In the vector diagram at the right, saying that the fitted vector $\widehat{\mathbf{y}}$ is a linear combination of
$\mathbf{x}_1$ and $\mathbf{x}_2$ means that it lies in the plane that they define. 
The fitted vector is the orthogonal projection of $\mathbf{y}$ on this plane, and
the least squares weights $b_1$ and $b_2$ give the maximum possible correlation $r^2 (\mathbf{y}, \widehat{\mathbf{y}})$.

```{r}
#| label: fig-lin-comb-mra
#| echo: false
#| out-width: "100%"
#| fig-cap: "Multiple regression as a linear combination to maximize the squared correlation with the predicted values $\\hat{\\mathbf{y}}$. Right: vector geometry of multiple regression for two predictors."
knitr::include_graphics("images/lin-comb-mra.png")
```

The vector of residuals,
$\mathbf{e} = \mathbf{y} -\widehat{\mathbf{y}}$ is orthogonal to that plane ($\mathbf{y}$ and
$\mathbf{e}$ are uncorrelated), and the least squares solution also minimizes length 
$\parallel \mathbf{e} \parallel = \sqrt(\Sigma e_i^2)$.


#### Multivariate regression {.unnumbered}

Multivariate multiple regression does the same thing for each response variable, $\mathbf{y}_1$ and $\mathbf{y}_2$,
as shown in @fig-lin-comb3. It finds the weights to maximize the correlation between _each_ $\mathbf{y}_j$
and the corresponding predicted value $\widehat{\mathbf{y}}_j$.

\begin{eqnarray*}
\widehat{\mathbf{y}}_1 & = & a_1 \mathbf{x}_1 + a_2 \mathbf{x}_2 + a_3 \mathbf{x}_3 \\
\widehat{\mathbf{y}}_2 & = & b_1 \mathbf{x}_1 + b_2 \mathbf{x}_2 + b_3 \mathbf{x}_3 \\
\end{eqnarray*}


```{r}
#| label: fig-lin-comb3
#| echo: false
#| out-width: "40%"
#| fig-cap: "Multivariate multiple regression as linear combinations to maximize the R squared for each response variable separately."
knitr::include_graphics("images/lin-comb3.png")
```

The weights $a_1, a_2, a_3$ and $b_1, b_2, b_3$ are the same as would be found in separate multiple regressions
for the response variables. 
However, the multivariate tests used here take the correlations among the $\mathbf{y}$s, and can be more powerful
than fitting separate univariate response models.

#### Canonical correlation analysis {.unnumbered}

Finally, canonical correlation analysis uses a different approach to fitting relations between a set of
responses, $\mathbf{y}_1, \mathbf{y}_2, \dots$ and a set of predictors, $\mathbf{x}_1, \mathbf{x}_2, \dots$. ...

```{r}
#| label: fig-lin-comb4
#| echo: false
#| out-width: "40%"
#| fig-cap: "Canonical correlation analyis finds uncorrelated linear combinations of the responses which maximize the R squared with linear combinations of the predictors."
knitr::include_graphics("images/lin-comb4.png")
```
