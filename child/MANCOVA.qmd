## ANCOVA $\rightarrow$ MANCOVA {#sec-ANCOVA-MANCOVA}

**TODO**: Consider moving this to @sec-vis-mlm and use much of the heplots MMRA vignette.

In univariate linear models, analysis of covariance (ANCOVA) is most often used in a situation 
where we want to compare the mean response, $\bar{y}_j$ for different groups (defined by one or more factors), but where there are one or more quantitative predictors $\mathbf{x}_1, \dots$ that should be taken into account for our comparisons to make sense.
The simplest case is when $\mathbf{x}$ is a pre-test score on the same measure, or when it is
a background measure like age or level of education that we want to control for,
to adjust for differences among the groups.

More generally, ANCOVA and its' multivariate MANCOVA brother
are used for situations where the model matrix $\mathbf{X}$ contains
a mixture of factor variables and quantitative predictors, called "covariates". 
In this wider context, there are two flavors of analysis with different emphasis on the
factors or the covariates:

* **true ANCOVA/MANOVA**: Attention is centered on the differences between the group means, but controlling for any difference in the covariate(s). This requires assuming that the slopes
for the groups are all the same.

* **homogeneity of regression**: Here the focus is on the regression relations between the $\mathbf{y}$s
and the predictor $\mathbf{x}$s, but we might also want to determine if the regression slopes are the same for
all groups defined by the factors.

In the ANCOVA flavor, the model fits additive effects of the group factor(s) and the covariate(s),
while the homogeneity of regression flavor adds interaction terms between groups and the $\mathbf{x}$s.  The test for homogeneity of regression is the added effect of the interaction terms:

```
mod1 <- lm(y ~ Group + x)            # ANCOVA model
mod2 <- lm(y ~ Group + x + Group:x)  # allow separate slopes
mod2 <- lm(y ~ Group * x)            # same as above

anova(mod1, mod2)            # test homogeneity of regression
```

<!-- **TODO**: Complete this, explaining how MANCOVA requires equal slopes for covariates, while homogeneity of regression tests that. ... -->

@fig-ANCOVA-ex illustrates these cases for a hypothetical two-group design studying the
the effect of an exercise program treatment on weight, recorded pre- ($x$) and post- ($y$)
compared to a control group given no treatment. In panel (a) the slopes for the two groups
are approximately equal, so the effect of treatment can be estimated by the difference
in the fitted values of $\hat{y}_i$ at the average value of $x$. In panel (b), the slope for
the treated group is considerably greater than that for the control group, so the difference
between the groups varies with $x$.

```{r}
#| label: fig-ANCOVA-ex
#| echo: false
#| out-width: "100%"
#| fig-cap: "Two possible outcome patterns for a two-group design assessing the effect of a treatment on weight, measured pre- and post-treatment. (a) Additive effects of Group and $x$; (b) Different slopes for the two groups. Plus signs show the means $(\\bar{x}_i, \\bar{y}_i)$ for the two groups."
knitr::include_graphics("images/ANCOVA-ex.png")
```


<!-- Useful univariate example: 
data('teengamb', package='faraway')
Analysis in https://bookdown.org/dereksonderegger/571/5-ANCOVA-Chapter.html
-->

### Example: Paired-associate tasks and academic performance {#sec-PA-tasks}

To what extent can simple tests of paired-associate learning[^paired-associate] predict measures of aptitude and achievement in kindergarten children? This was the question behind an experiment
by William Rohwer at the University of California, Berkeley.

[^paired-associate]: Paired-associate learning are among the simplest tests of memory and learning.
The subject is given a list of pairs of words or nonsense syllables, like "banana - house" or
"YYZ - Toronto" to learn. On subsequent trials she is given the stimulus term of each pair ("banana", "YYZ") and asked to reply with the correct response ("house", "Toronto").

There were three outcome measures, one verbal and two visually-based:

* A student achievement test (`SAT`), 
* Peabody Picture Vocabulary test (`PPVT`),
* Raven Progressive Matrices test (`Raven`). 

Four paired-associate tasks were used, which differed in the
syntactic and semantic relationship between the stimulus and response terms in each pair. 
These are called *named* (`n`), *still* (`s`), *named still* (`ns`), 
*named action* (`na`), and *sentence still* (`ss`).

Rohwer's data, taken  from @Timm:75, is given in `heplots::Rohwer`. But there's a MANCOVA wrinkle: Performance on the academic tasks is well-known to vary with socioeconomic status of the parents or the school they attend. A simple design was to collect data from children in two
schools, one in a low SES neighborhood ($n=37$) and the other an upper-class high SES one ($n=32$). The data look like this:

<!-- Rohwer & PA tasks: track down a reference, and the PA tasks, n, na, ss, ... 
See: https://files.eric.ed.gov/fulltext/ED053807.pdf.
Maybe its:
Rohwer, W.D., Jr., & Levin, J.R. Action, meaning and stimulus selection
in paired-associate learning. Journal
of Verbal Learning and Verbal Behavior,
1968, 7: 137-141.
-->

```{r, rohwer-some}
data(Rohwer, package = "heplots")
set.seed(42)
Rohwer |> dplyr::sample_n(6)
```

Following the scheme for reshaping the data used in @fig-schooldata-scats, a set of scatterplots of
each predictor against each response will give a useful initial look at the data. There's a lot to
see here, so the plot in @fig-Rohwer-scats focuses attention on the regression lines for the two groups
and their data ellipses.

```{r}
#| label: fig-Rohwer-scats
#| code-fold: true
#| fig-width: 10
#| fig-height: 6
#| out-width: "100%"
#| fig-cap: "Scatterplots of each of the three response variables against each of the five predictors in the `Rohwer` dataset."
yvars <- c("SAT", "PPVT", "Raven" )      # outcome variables
xvars <- c("n", "s", "ns", "na", "ss")   # predictors

Rohwer_long <- Rohwer %>%
  dplyr::select(-group) |>
  tidyr::pivot_longer(cols = all_of(xvars), 
                      names_to = "xvar", values_to = "x") |>
  tidyr::pivot_longer(cols = all_of(yvars), 
                      names_to = "yvar", values_to = "y") |>
  dplyr::mutate(xvar = factor(xvar, levels = xvars),
                yvar = factor(yvar, levels = yvars))

ggplot(Rohwer_long, aes(x, y, color = SES, shape = SES, fill = SES)) +
  geom_jitter(size=0.8) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              formula = y ~ x, 
              linewidth = 1.5) +
  stat_ellipse(geom = "polygon", alpha = 0.1) +
  labs(x = "Predictor (PA task)",
       y = "Response (Academic)") +
  facet_grid(yvar ~ xvar,            # plot matrix of Y by X
             scales = "free") +
  theme_bw(base_size = 16) +
  theme(legend.position = "bottom")
```

You can see here that the high-SES group generally performs better than the low group. The regression
lines have similar slopes in some of the panels, but not all. The low SES group also appears to have larger variance on most of the PA tasks.

#### MANCOVA model {.unnumbered}

Nevertheless, I fit the MANCOVA model that allows a test of different means for the two SES groups on the responses, but constrains the slopes for the PA covariates to be equal. Only two of the PA tasks (`na` and `ns`) show individually significant effects in the multivariate tests.

```{r}
# Make SES == 'Lo' the reference category
Rohwer$SES <- relevel(Rohwer$SES, ref = "Lo")

Rohwer.mod1 <- lm(cbind(SAT, PPVT, Raven) ~ SES + n + s + ns + na + ss, 
                  data=Rohwer)
Anova(Rohwer.mod1)
```

You can also examine the tests of the univariate ANCOVA models for each of the responses using `glance()`
or `heplots::uniStats()`. All are significantly related, but the `PPVT` measure has the largest $R^2$ by far.

```{r}
uniStats(Rohwer.mod1)
```

To help interpret these effects, bivariate coefficient plots for the paired associate tasks are shown in @fig-rohwer-mod1-coef.
(The coefficients for the group variable `SES` are on a different scale and so are omitted here.)
From this you can see that the named still and named action tasks have opposite signs: contrary to expectations,
`ns` is negatively associated with the measures of aptitude and achievement (when the other predictors are adjusted for).
```{r echo = -1}
#| label: fig-rohwer-mod1-coef
#| fig-width: 10
#| fig-height: 5
#| out-width: "100%"
#| fig-cap: "Bivariate coefficient plots for the MANCOVA model with confidence ellipses of 68% coverage."
op <- par(mar = c(4,4,1,1)+.1, mfrow = c(1,2))
coefplot(Rohwer.mod1, parm = 2:6,
         fill = TRUE,
         level = 0.68,
         cex.lab = 1.5)
coefplot(Rohwer.mod1, parm = 2:6, variables = c(1,3),
         fill = TRUE,
         level = 0.68,
         cex.lab = 1.5)
```

**TODO**: For interpretation, it would be nice to know how many items were used for each of the PA tasks. The range of `na` goes up to 35, but others are less.

#### Adjusted means {.unnumbered}

From the analysis of covariance perspective, interest is often centered on estimating the differences between
the group means, but adjusting or controlling for differences on the covariates. From the table of means
below, you can see that the high SES group performs better on all three response variables, but this group
also has higher scores on the paired associate tasks.

```{r rohwer-means}
means <- Rohwer |>
  group_by(SES) |>
  summarise_all(mean) |>
  print()
```

The adjusted mean differences are simply the values estimated by the coefficients for `SES` in the model.
These are smaller than the differences between the observed means.
```{r}
means[2, 3:5] - means[1, 3:5]  

# adjusted means
coef(Rohwer.mod1)[2,]
```

**TODO**: do this with a CI for the effects

#### Homogeneity of regression {.unnumbered}

The MANCOVA model, `Rohwer.mod1`, has relatively simple interpretations
(a large effect of `SES`, with `ns` and `na` as the major predictors)
but the test of the `SES` effect relies on the assumption of homogeneity of slopes for the predictors.
We can test this assumption as follows, by adding interactions of `SES`
with each of the covariates:
  
  
```{r}
Rohwer.mod2 <- lm(cbind(SAT, PPVT, Raven) ~ SES * (n + s + ns + na + ss),
                  data=Rohwer)
Anova(Rohwer.mod2)
```

It appears from the above that there is only weak evidence of unequal slopes
from the separate `SES:` terms; only that for `SES:na` is individually significant.
The evidence for heterogeneity is
stronger, however, when these terms are tested _collectively_ using 
`linearHypothesis()`. I use a small `grep()` trick here to find the interaction terms, which have a ":" in their names.

```{r, rohwer-mod2-test}
# test interaction terms jointly
coefs <- rownames(coef(Rohwer.mod2)) 
interactions <- coefs[grep(":", coefs)]

print(linearHypothesis(Rohwer.mod2, interactions), SSP=FALSE)
```




#### Separate models {.unnumbered}

Model `Rohwer.mod2` with all interaction terms essentially fits a separate slope for each of the low and high SES groups
for all responses with each of the predictor PA tasks. This is similar in spirit to what we would get if we fit a separate  multivariate regression model for each of the groups, but parameterized differently: The heterogeneous regression model gives, for the interaction
terms estimates of the difference in slopes between groups, while the separate-regressions
approach gives separate slope estimates for each of the groups. These are equivalent, in the
sense that the estimates for each approach can be derived from the other.

They are not equivalent in testing however, because the full model uses a combined
pooled within-group error covariance, allows hypotheses about equality of slopes and intercepts to be tested directly and has greater power because it uses the total
sample size.
Here, I simply illustrate the mechanics of fitting separate models
using the `subset` argument to `lm()`. 

```{r, rohwer-separate}
Rohwer.sesHi <- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, 
                   data=Rohwer, subset = SES=="Hi")
Anova(Rohwer.sesHi)

Rohwer.sesLo <- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, 
                   data=Rohwer, subset = SES=="Lo")
Anova(Rohwer.sesLo)
```

The strength of evidence for the predictors `na` and `ns` is weaker here than when tested
in the full heterogeneous model.