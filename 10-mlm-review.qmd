```{r include=FALSE}
source("R/common.R")
knitr::opts_chunk$set(fig.path = "figs/ch10/")
```

{{< include latex/latex-commands.qmd >}}

# Multivariate Linear Model {#sec-mlm-review}

@sec-Hotelling introduced the essential ideas of multivariate analysis in the context of a two-group design
using Hotelling's $T^2$. Here, I extend this to the to the general
Multivariate Linear Model (MLM). This can be understood as a simple extension of the univariate linear model, with the main difference being that there are multiple response variables considered together, instead of just one, analysed alone.
These outcomes might reflect several different ways or scales for measuring an underlying theoretical construct,
or they might represent different aspects of some phenomenon that are better understood when studied jointly.

For example, in the first case of different measures, there are numerous psychological scales used to assess depression or anxiety
and it may be important to include more than one measure to ensure that the construct has been measured adequately.
In the second case of various aspects, student "aptitude" or "achievement" reflects competency in different various subjects
(reading, math, history, science, ...) that are better studied together.



In this context, there are multiple techniques that can be applied depending on the structure of the variables at hand. For instance, with one or more continuous predictors and multiple response variables, one could use multivariate multiple regression (MMRA) to obtain estimates useful for prediction. Instead, if the predictors are categorical, multivariate analysis of variance (MANOVA) can be applied to test for differences between groups. Again, this is akin to multiple regression and ANOVA in the univariate context -- the same underlying model is utilized, but the tests for terms in the model are multivariate ones for the collection of all response variables, rather than univariate ones for a single response.

<!-- **TODO** Use `\Epsilon` = $\Epsilon$ here, which is defined as `\boldsymbol{\large\varepsilon}` for residuals. -->
<!-- Could also use a larger version, `\boldsymbol{\Large\varepsilon}` = -->
<!-- $\boldsymbol{\Large\varepsilon}$ if that makes a difference. -->

Before considering the details and examples that apply to MANOVA and MMRA, it is useful to consider the 
features of the multivariate linear model of which these cases are examples.

**Packages**

In this chapter I use the following packages. Load them now:

```{r load-pkgs}
library(broom)
library(car)
library(dplyr)
library(ggplot2)
library(heplots)
library(tidyr)
```

## Structure of the MLM
In each of these cases, the underlying MLM is given most compactly using the matrix equation, <!-- $$ --> 
<!-- \newcommand{\sizedmat}[2]{\mathord{\mathop{\mathbf{#1}\limits_{(#2)}}} --> 
<!-- $$ -->

$$
\sizedmat{Y}{n \times p} = 
\sizedmat{X}{n \times (q+1)} \, \sizedmat{B}{(q+1) \times p} + \sizedmat{\Epsilon}{n \times p} \:\: ,
$${#eq-mlm}

where

-   $\mathbf{Y} = (\mathbf{y}_1 , \mathbf{y}_2, \dots , \mathbf{y}_j, \dots, \mathbf{y}_p )$ is the matrix of $n$ observations on $p$ responses;
-   $\mathbf{X}$ is the model matrix with columns $\mathbf{x}_i$ for $q$ regressors, which typically includes an initial column $\mathbf{x}_0$ of 1s for the intercept;
-   $\mathbf{B} = ( \mathbf{b}_1 , \mathbf{b}_2 , \dots \mathbf{b}_j \dots, \mathbf{b}_p )$ is a matrix of regression coefficients, one column $\mathbf{b}_j$ for each response variable; 
-   $\Epsilon$ is a matrix of errors in predicting $\mathbf{Y}$.

When it is useful to refer symbolically to the _rows_ $\mathbf{b}_i^\top$ of coefficients for the regressors 
$x_0, x_1, x_2, \dots, x_i, \dots x_q$, these are expressed as  $\mathbf{B} = ( \mathbf{b}_0^\top^\top , \mathbf{b}_1^\top , \dots \mathbf{b}_j^\top \dots, \mathbf{b}_p )^\top$, or more explicitly

$$
\sizedmat{B}{(q+1) \times p} =
\begin{pmatrix} 
  b_{0}^\top \\ 
  b_{1}^\top  \\ 
  \vdots \\
  b_{i}^\top  \\
  \vdots \\
  b_{q}^\top  \\ 
\end{pmatrix}
$$

The structure of the model matrix $\mathbf{X}$ is the same as the univariate linear model, and may contain, therefore, 

* **quantitative** predictors, such as `age`, `income`, years of `education`
* **transformed predictors** like $\sqrt{\text{age}}$ or $\log{(\text{income})}$
* **polynomial terms**: $\text{age}^2$, $\text{age}^3, \dots$ (using `poly(age, k)` in R)
* **categorical predictors** ("factors"), such as treatment (Control, Drug A, drug B), or sex; internally a factor with `k` levels is transformed to `k-1` dummy (0, 1) variables, representing comparisons with a reference level, typically the first.
* **interaction terms**, involving either quantitative or categorical predictors, e.g., `age * sex`, `treatment * sex`.

### Assumptions
Just as in univariate models, the assumptions of the multivariate linear model almost entirely concern the behavior of the errors (residuals). 
Let $\mathbf{\epsilon}_{i}^{\prime}$ represent the $i$th row of $\Epsilon$. Then it is assumed that:

* **Normality**: The residuals, $\mathbf{\epsilon}_{i}^{\prime}$ are distributed as multivariate normal, 
$\mathcal{N}_{p}(\mathbf{0},\boldsymbol{\Sigma})$, 
where $\mathbf{\Sigma}$ 
is a non-singular error-covariance matrix. 
Statistical tests of multivariate normality of the residuals include the Shapiro-Wilk [@ShapiroWilk1965]
and Mardia [@Mardia:1970:MMS]
tests (in the **MVN** package);
<!-- mshapiro.test( )[in the mvnormtest package] can be used to perform the Shapiro-Wilk test for multivariate normality -->
however this is often better assessed visually using a $\chi^2$ QQ plot
of Mahalanobis squared distance against their corresponding $\chi^2_p$ values using `heplots::cqplot()`.
* **Homoscedasticity**: The error-covariance matrix $\mathbf{\Sigma}$ is constant across all observations and grouping factors.
Graphical methods to determine if this assumption is met are described in @sec-eqcov.
* **Independence**: $\mathbf{\epsilon}_{i}^{\prime}$ and $\mathbf{\epsilon}_{j}^{\prime}$ are independent for $i\neq j$, so knowing the data for case $i$ gives no information about case $j$ (as would be true if the data consisted of pairs of husbands and wives); 
* The predictors, $\mathbf{X}$, are fixed and measured without error or at least they are independent of the errors, $\Epsilon$. 

These statements are simply the
multivariate analogs of the assumptions of normality, constant variance and independence
of the errors in univariate models. Note that it is unnecessary to assume that the predictors 
(regressors, columns of $\mathbf{X}$) are normally distributed. 

Implicit in the above is perhaps the most important assumption---that the model has been correctly specified. This means:

* **Linearity**: The form of the relations between each $\mathbf{y}$ and the $\mathbf{x}$s is correct. Typically this means that the relations are _linear_, but if not, we have specified a correct transformation of $\mathbf{y}$ and/or $\mathbf{x}$.

* **Completeness**: No relevant predictors have been omitted from the model. 

* **Additive effects**:  The combined effect of different predictors is the sum of their individual effects.

## Fitting the model

The least squares (and also maximum likelihood) solution for the coefficients $\mathbf{B}$ is given by

<!-- $$ -->
<!-- \widehat{\mathbf{B}} = (\mathbf{X}\trans \mathbf{X})^{-1} \mathbf{X}\trans \mathbf{Y} \period -->
<!-- $$ -->


$$
\widehat{\mathbf{B}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} \period
$$


This is precisely the same as fitting the separate responses $\mathbf{y}_1 , \mathbf{y}_2 , \dots , \mathbf{y}_p$,
and placing the estimated coefficients $\widehat{\mathbf{b}}_i$ as columns in $\widehat{\mathbf{B}}$

$$
\widehat{\mathbf{B}} = [ \widehat{\mathbf{b}}_1, \widehat{\mathbf{b}}_2, \dots , \widehat{\mathbf{b}}_p] \period
$$
In R, we fit the multivariate linear model with `lm()` simply by giving a collection of response variables `y1, y2, ...`
on the left-hand side of the model formula, wrapped in `cbind()` which combines them to form a matrix response.

```{r mlm-lm}
#| eval: false
lm( cbind(y1, y2, y3) ~ x1 + x2 + ..., data=)
```

In the presence of possible outliers, robust methods are available for univariate linear models (e.g., `MASS::rlm()). So too, `heplots::robmlm()` provides robust
estimation in the multivariate case.

### Sums of squares

In univariate response models, statistical tests and model summaries (like $R^2$) are based on the familiar 
decomposition of the total sum of squares $SS_T$ into regression or hypothesis ($SS_H$) and error ($SS_E$)
sums of squares. In the multivariate linear model each of these becomes a $p \times p$ matrix $SSP$
containing sums of squares for the $p$ responses on the diagonal and sums of cross products in the off-diagonal.
elements. For the MLM this is expressed as

\begin{eqnarray*}
\underset{(p\times p)}{\mathbf{SSP}_{T}}
& = && \mathbf{Y}^{\prime} \mathbf{Y} - n \overline{\mathbf{y}}\,\overline{\mathbf{y}}^{\prime} \\
& = && \left(\widehat {\mathbf{Y}}^{\prime}\widehat{\mathbf{Y}} - n\overline{\mathbf{y}}\,\overline{\mathbf{y}}^{\prime} \right) + \widehat{\Epsilon^{\prime}}\widehat{\Epsilon} \\
& = &&  \mathbf{SSP}_{H} + \mathbf{SSP}_{E} \\
& \equiv && \mathbf{H} + \mathbf{E} \comma
\end{eqnarray*}

where
$\overline{\mathbf{y}}$ is the $(p\times 1)$ vector of means for the response variables; $\widehat{\mathbf{Y}} = \mathbf{X}\widehat{\mathbf{B}}$ is the matrix of fitted values; and $\widehat{\Epsilon} = \mat{Y} -\widehat{\mat{Y}}$ is the matrix of residuals. 
This is the decomposition that we visualize in HE plots,
where the size and direction of $\mathbf{H}$ and $\mathbf{E}$ can be represented as ellipsoids.


The univariate $F$ test statistic, 
$$F = \frac{\text{SS}_H/\text{df}_h}{\text{SS}_E/\text{df}_e} = \frac{\mathsf{Var}(H)}{\mathsf{Var}(E)} \comma
$$
assesses "how big" $\text{SS}_H$ is, relative to $\text{SS}_E$, the variance accounted for by a hypothesized model or model terms relative to
error variance.
In the multivariate analog $\H$ and $\E$ are both $p \times p$ matrices, 
and $\H$ "divided by" $\E$ becomes $\H \E^{-1}$. The answer, "how big" is expressed in terms of the $p$ eigenvalues 
$\lambda_i, i = 1, 2, \dots p$ of $\H \E^{-1}$. These are the values $\lambda$ for which
$$
\det{\H \E^{-1} - \lambda \mat{I}} = 0 \period
$$
The solution gives the $\lambda_i$ as the eigenvalues, with vectors $\mathbf{v}_i$ as the corresponding
eigenvectors,
$$
\H \E^{-1} \; \lambda_i = \lambda_i \mathbf{v}_i \period
$${#eq-he-eigen}

However, when the hypothesized model terms have $\text{df}_h$ degrees of freedom (columns of the $\mathbf{X}$ matrix for that term), $\H$ is of rank $\text{df}_h$,
so only $s=\min(p, \text{df}_h)$ eigenvalues can be non-zero. For example, a test for
a hypothesis about a single quantitative predictor $\mathbf{x}$, has $\text{df}_h = 1$ degree of freedom
and $\rank{H} = 1$; for a factor with $g$ groups, $\text{df}_h = \rank{H} = g-1$.

<!-- This -->
<!-- has a direct multivariate analog in terms of -->
<!-- the $s=\min(p, df_h)$ non-zero latent roots (eigenvalues), $\lambda_i$, of $\mat{H}\mat{E}^{-1}$ -->
<!-- ($\mathbf{H}$ "divided by" $\mathbf{E}$); that is, the values $\lambda_i$ that solve -->

<!-- $$ -->
<!--  \det{ ( \mathbf{H}\mathbf{E}^{-1} - \lambda \mathbf{I} )} = 0 \Longrightarrow  -->
<!--  \mathbf{H}\mathbf{E}^{-1} \lambda_i = \lambda_i \mathbf{v}_i \period -->
<!-- $$ -->

The overall multivariate test for the model in @eq-mlm is essentially a test of the hypothesis
$\mathcal{H}_0: \mathbf{B} = 0$ (excluding the row for the intercept). 
Equivalently, this is a test based on the _incremental_ $\mathbf{SSP}_{H}$ for the hypothesized
terms in the model---that is, the difference between the $\mathbf{SSP}_{H}$ for the full model
and the null, intercept-only model.
The same idea can be applied to test the difference between any pair of _nested_ models---the added contribution of terms in a larger model relative to a smaller model containing a subset of terms.

The eigenvectors $\mathbf{v}_i$ in @eq-he-eigen are also important. These are the weights for the
variables in a linear combination 
$v_{i1} \mathbf{y}_1 + v_{i2} \mathbf{y}_2 + \cdots + v_{ip} \mathbf{y}_p$ which produces the largest
univariate $F$ statistic for the $i$-th dimension. We exploit this in canonical discriminant analysis
and the corresponding canonical HE plots (**ref**).

### Test statistics

In the univariate case, the overall $F$-test of $\mathcal{H}_0: \boldsymbol{\beta} = \mathbf{0}$
is the uniformly most powerful invariant test when the assumptions are met. There is nothing better.
This is not the case in the MLM.

The reason is that when there are $p > 1$ response variables, and we are testing
a hypothesis comprising $\text{df}_h >1$ coefficients or degrees of freedom, there are $s > 1$
possible dimensions in which $\H$ can be large relative to $\E$, each measured by the
eigenvalue $\lambda_i$. There are several test
statistics that combine these into a single measure, shown in @tbl-mstats.


<!-- $$ -->
<!-- \begin{align*} -->
<!-- \text{Wilks's Lambda} \quad & \Lambda = \prod^s_i \frac{1}{1+\lambda_i} \quad\quad  && \eta^2 = 1-\Lambda^{1/s} \\ -->
<!-- \text{Pillai trace} \quad &   V = \sum^s_i \frac{\lambda_i}{1+\lambda_i} \quad  && \eta^2 = \frac{V}{s}     \\ -->
<!-- \text{Hotelling-Lawley trace} \quad &  H = \sum^s_i \lambda_i \quad\quad   && \eta^2 = \frac{H}{H+s}   \\ -->
<!-- \text{Roy's maximum root} \quad &  R = \lambda_1   \quad\quad         &&  \eta^2 = \frac{\lambda_1}{1+\lambda_1}  \\ -->
<!-- \end{align*} -->
<!-- $$ -->


::: {#tbl-mstats}

```{=latex}
\begin{center}
\begin{tabular}{|l|l|l|l|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  Criterion & Formula &  Partial $\eta^2$   \\
  \hline
  Wilks's $\Lambda$ & $\Lambda = \prod^s_i \frac{1}{1+\lambda_i}$ &  $\eta^2 = 1-\Lambda^{1/s}$   \\
  Pillai trace & $V = \sum^s_i \frac{\lambda_i}{1+\lambda_i}$ &  $\eta^2 = \frac{V}{s} $   \\
  Hotelling-Lawley trace & $H = \sum^s_i \lambda_i$ & $\eta^2 = \frac{H}{H+s}$   \\
  Roy maximum root & $R = \lambda_1$  &  $ \eta^2 = \frac{\lambda_1}{1+\lambda_1}$  \\
  \hline
\end{tabular}
\end{center}
```
Test statistics for multivariate tests combine the size of dimensions of $\mathbf{H}\mathbf{E}^{-1}$ into a single measure.
:::



These correspond to different kinds of "means" of the $\lambda_i$: arithmetic, geometric, harmonic
and supremum. See @Friendly-etal:ellipses:2013 for the geometry behind these measures.

Each of these statistics have different sampling distributions under the null hypothesis, but they
can all be converted to $F$ statistics, which are exact when $s \le 2$, and approximations otherwise.
As well, each has an analog of the $R^2$-like partial $\eta^2$ measure, giving the partial association
accounted for by each term in the MLM.

### Testing contrasts and linear hypotheses

Even more generally, these multivariate tests apply to _every_ linear hypothesis concerning the coefficients
in $\mathbf{B}$.  Suppose we want to test the hypothesis that a subset of rows (predictors)
and/or columns (responses) simultaneously have null effects.
This can be expressed in the general linear test,
$$
\mathcal{H}_0 : \mathbf{C}_{h \times q} \, \mathbf{B}_{q \times p} = \mathbf{0}_{h \times p} \comma
$$
where $\mathbf{C}$ is a  full rank $h \le q$ hypothesis matrix of constants, that selects 
subsets or linear combinations (contrasts) of the coefficients in $\mathbf{B}$ to be tested
in a $h$ degree-of-freedom hypothesis.

In this case, the SSP matrix for the hypothesis has the form
$$
\mathbf{H}  =
 (\mathbf{C} \widehat{\mathbf{B}})\trans \,
 [\mathbf{C} (\mathbf{X}\trans \mathbf{X} )^{-1} \mathbf{C}\trans]^{-1} \,
 (\mathbf{C} \widehat{\mathbf{B}})
$${#eq-hmat}

where there are $s = \min(h, p)$ non-zero eigenvalues of $\mat{H}\mat{E}^{-1}$.
In @eq-hmat, $\mathbf{H}$ measures the (Mahalanobis) squared distances (and cross products) among
the linear combinations $\mathbf{C} \widehat{\mathbf{B}}$ from the origin under the null hypothesis.
<!-- An animated display of these ideas and the relations between data ellipses and HE plots can be seen at -->
<!-- [https://www.datavis.ca/gallery/animation/manova/](https://www.datavis.ca/gallery/animation/manova/). -->

For example, with three responses $y_1, y_2, y_3$ and three predictors $x_1, x_2, x_3$, we can test the hypothesis
that neither $x_2$ nor $x_3$ contribute at all to the predicting the $y$s in terms of the hypothesis that
the coefficients for the corresponding rows of $\mathbf{B}$ are zero using a 1-row $\mathbf{C}$ matrix that
simply selects those rows:

<!-- $$ -->
<!-- \mathcal{H}_0 : \mathbf{C} \mathbf{B} =  -->
<!-- \begin{bmatrix} -->
<!-- 0 & 1 & 1 & 0 -->
<!-- \end{bmatrix} -->
<!-- \begin{pmatrix} -->
<!-- 	\beta_{0,y_1} & \beta_{0,y_2} & \beta_{0,y_3} \\  -->
<!-- 	\beta_{1,y_1} & \beta_{1,y_2} & \beta_{1,y_3} \\  -->
<!-- 	\beta_{2,y_1} & \beta_{2,y_2} & \beta_{2,y_3} \\  -->
<!-- 	\beta_{3,y_1} & \beta_{3,y_2} & \beta_{3,y_3}  -->
<!-- \end{pmatrix} -->
<!-- = -->
<!-- \begin{bmatrix} -->
<!-- 	\beta_{1,y_1} & \beta_{1,y_2} & \beta_{1,y_3} \\  -->
<!-- 	\beta_{2,y_1} & \beta_{2,y_2} & \beta_{2,y_3} \\  -->
<!-- \end{bmatrix} -->
<!-- =  -->
<!-- \mathbf{0}_{(2 \times 3)} -->
<!-- $$ -->

\begin{eqnarray*}
\mathcal{H}_0 : \mathbf{C} \mathbf{B} & = 
\begin{bmatrix}
0 & 1 & 1 & 0
\end{bmatrix}
\begin{pmatrix}
	\beta_{0,y_1} & \beta_{0,y_2} & \beta_{0,y_3} \\ 
	\beta_{1,y_1} & \beta_{1,y_2} & \beta_{1,y_3} \\ 
	\beta_{2,y_1} & \beta_{2,y_2} & \beta_{2,y_3} \\ 
	\beta_{3,y_1} & \beta_{3,y_2} & \beta_{3,y_3} 
\end{pmatrix} \\ \\
& = 
\begin{bmatrix}
	\beta_{1,y_1} & \beta_{1,y_2} & \beta_{1,y_3} \\ 
	\beta_{2,y_1} & \beta_{2,y_2} & \beta_{2,y_3} \\ 
\end{bmatrix}
= 
\mathbf{0}_{(2 \times 3)}
\end{eqnarray*}



## ANOVA $\rightarrow$ MANOVA

Multivariate analysis of variance (MANOVA) generalizes the familiar ANOVA model to situations where there are two or more response variables.
Unlike ANOVA, which focuses on discerning statistical differences in one continuous dependent variable influenced by an independent variable (or grouping variable), MANOVA considers several dependent variables at once. It integrates these variables into a single, composite variable through a weighted linear combination, allowing for a comprehensive analysis of how these dependent variables collectively vary with respect to the levels of the independent variable. Essentially, MANOVA investigates whether the grouping variable explains significant variations in the combined dependent variables.

The situation is illustrated in @fig-manova-diagram where there are two response measures, $Y_1$ and $Y_2$
with data collected for three groups. For concreteness, $Y_1$ might be a score on a math test and $Y_2$ might be a
reading score. Let's also say that group 1 has been studying Shakespeare, while group 2 has concentrated on physics,
but group 3 has done nothing beyond the normal curriculum.


```{r}
#| label: fig-manova-diagram
#| echo: false
#| out-width: "75%"
#| fig-cap: "Data from simple MANOVA design involving three groups and two response measures, $Y_1$ and $Y_2$, summarized by their data ellipses."
knitr::include_graphics(here::here("images", "manova-diagram-anno.png"))
```

As shown in the figure, the centroids, $(\mu_{1g}, \mu_{2g})$, clearly differ---the data ellipses barely overlap.
A multivariate analysis would show a highly difference among groups.
From a rough visual inspection, it seems that means differ on the math test $Y_1$, with the
physics group out-performing the other two. On the reading test $Y_2$ however it might turn out that the three
group means don't differ significantly in an ANOVA, although the Shakespeare group comes out best by a small amount.
Doing separate ANOVAs on these variables would miss what is so obvious from @fig-manova-diagram.

@fig-manova-response-dimensions illustrates a second important advantage of performing
a multivariate analysis over separate ANOVAS: that of determining the number of dimensions or aspects
along which groups differ. In the panel on the left, the means of the three groups increase nearly
linearly on the combination of $Y_1$ and $Y_2$, to their differences can be ascribed to a single
dimension. For example, the groups here might be patients diagnosed as normal, mild schizophrenia
and profound schizophrenia, and the measures could be tests of memory and attention.
The obvious multivariate interpretation from the figure is that of increasing impairment
of cognitive functioning across the groups. Note also the positive association within each group:
those who perform better on the memory task also do better on attention.

```{r}
#| label: fig-manova-response-dimensions
#| echo: false
#| out-width: "100%"
#| fig-cap: "A simple MANOVA design involving three groups and two response measures, $Y_1$ and $Y_2$, but with different patterns of the differences among the group means. The red arrows suggest interpretations in terms of dimensions or aspects of the response variables."
knitr::include_graphics(here::here("images", "manova-response-dimensions-anno.png"))
```

In contrast, the right panel of @fig-manova-response-dimensions shows a situation where the group means
have a low correlation. Data like this might arise in a study of parental competency, where there are
are measure of the degree of caring ($Y_1$) and time spent in play ($Y_2$) by fathers and groups consisting of fathers of children with no disability, or a physical disability or a mental ability.
As can be seen in @fig-manova-response-dimensions fathers of the disabled children differ from those
of the not disabled group in two different directions corresponding to being higher on either
$Y_1$ or $Y_2$. The `r colorize("red")` arrows suggest that the differences among groups could be
interpreted in terms of two uncorrelated dimensions, perhaps labeled overall competency and emphasis
on physical activity. (The pattern in @fig-manova-response-dimensions (right) is contrived for the sake of illustration; it does not reflect the data analyzed in the example below.)

### Example: Father parenting data

I use a simple example of a three-group multivariate design
to illustrate the basic ideas of fitting MLMs in R and testing hypotheses.
Visualization methods using HE plots are discussed in @REF.

The dataset `heplots::Parenting` come from an exercise (10B) in @Meyers-etal:2006 and are probably contrived, but are modeled on a real study in which
fathers were assessed on three subscales of a *Perceived Parenting Competence Scale*,

* `caring`, caretaking responsibilities;
* `emotion`, emotional support provided to the child; and
* `play`, recreational time spent with the child.

The dataset `Parenting` comprises 60 fathers selected from three `group`s of $n = 20$ each: (a) fathers of a child with _no disabilities_ (`"Normal"`); (b) fathers with a _physically_ disabled child; (c) fathers with a _mentally_ disabled child. 
The design is thus a three-group MANOVA, with three response variables. 

<!-- NOTE: Use **typical** rather than "normal" -->

The main questions concern whether the group means differ on these scales, and the nature of these differences. That is, do the means differ significantly on all three measures?
Is there a consistent order of groups across these three aspects of parenting?

More specific questions are: (a) Do the fathers of typical children differ from the other two groups
on average? (b) Do the physical and mental groups differ? These questions can be tested using contrasts,
and are specified by assigning a matrix to `contrasts(Parenting$group)`; each column is
a contrast whose values sum to zero. They are given labels `"group1"` (normal vs. other)
and `"group2"` (physical vs. mental) in some output.

```{r parenting-contrasts}
data(Parenting, package="heplots")
C <- matrix(c(1, -.5, -.5,
              0,  1,  -1), 
            nrow = 3, ncol = 2) |> print()
contrasts(Parenting$group) <- C
```

#### Exploratory plots {.unnumbered}

Before setting up a model and testing, it is well-advised to examine the data graphically.
The simplest plots are side-by-side boxplots (or violin plots)
for the three responses. With `ggplot2`,
this is easily done by reshaping the data to long format and using faceting.
In @fig-parenting-boxpl, I've also plotted the group means with white dots.

```{r}
#| label: fig-parenting-boxpl
#| out-width: "90%"
#| code-fold: true
#| code-summary: "See the ggplot code"
#| fig-cap: Faceted boxplots of scores on the three parenting scales, showing also the mean for each. 
parenting_long <- Parenting |>
  tidyr::pivot_longer(cols=caring:play, names_to = "variable")

ggplot(parenting_long, 
       aes(x=group, y=value, fill=group)) +
  geom_boxplot(outlier.size=2.5, 
               alpha=.5, 
               outlier.alpha = 0.9) + 
  stat_summary(fun=mean, 
               color="white", 
               geom="point", 
               size=2) +
  scale_fill_hue(direction = -1) +     # reverse default colors
  labs(y = "Scale value", x = "Group") +
  facet_wrap(~ variable) +
  theme_bw(base_size = 14) + 
  theme(legend.position="top") +
  theme(axis.text.x = element_text(angle = 15,
                                   hjust = 1)) 
```

In this figure, differences among the groups on `play` are most apparent, with fathers of non-disabled
children scoring highest. Differences among the groups on `emotion` are very small, but one high outlier for the fathers of mentally disabled children is apparent. On `caring`, fathers of children with a physical disability stand out as highest.

For exploratory purposes, you might also make a scatterplot matrix. Here, because the MLM assumes
homogeneity of the variances and covariance matrices $\mathbf{S}_i$, I show only the data ellipses in scatterplot matrix format, using `heplots:covEllipses()` (with 50% coverage, for clarity):

```{r}
#| label: fig-parenting-covEllipses
#| out-width: "90%"
#| fig-cap: Bivariate data ellipses for pairs of the three responses, showing the means, correlations and variances for the three groups.
colors <- scales::hue_pal()(3) |> rev()  # match color use in ggplot
covEllipses(cbind(caring, play, emotion) ~ group, data=Parenting,
  variables = 1:3,
  fill = TRUE, fill.alpha = 0.2,
  pooled = FALSE,
  level = 0.50, 
  col = colors)
```

If the covariance matrices were all the same, the data ellipses would have roughly the same size and orientation,
but that is not the  case in @fig-parenting-covEllipses. The normal group shows greater variability
overall and the correlations among the measures differ somewhat from group to group. 
We'll assess later whether this makes a difference in the conclusions that can be drawn (@sec-eqcov).
The group centroids also differ, but the pattern is not particularly clear.
We'll see an easier to understand view in HE plots and their canonical discriminant cousins.

#### Testing the model {.nunumbered}

Let's proceed to fit the multivariate model predicting all three scales from the `group` factor.
`lm()` for a multivariate response returns an object of class `"mlm"`, for which there are many
methods (use `methods(class="mlm") to find them).

```{r parenting-mod}
parenting.mod <- lm(cbind(caring, play, emotion) ~ group, 
                    data=Parenting) |> print()
```

The coefficients in this model are the values of the contrasts set up above. `group1` is the mean of the typical
group minus the average of the other two, which is negative on `caring` and `emotion` but positive for `play`.
`group2` is the difference in means for the physical vs. mental groups.

Before doing multivariate tests, it is useful to see what would happen if we ran univariate ANOVAs on each
of the responses. This can be extracted from an MLM using `stats::summary.aov()`:

```{r parenting-summary-aov}
summary.aov(parenting.mod)
```
If you like, you can also extract the univariate model fit statistics from the `"mlm"`

From this, one might conclude that there are differences only in `caring` and `play` and therefore ignore
`emotion`, but this would be short-sighted. 
`car::Anova()` gives the overall multivariate test $\mathcal{H}_0: \mathbf{B} = 0$ of the `group` effect. Note that
this has a much smaller $p$-value than any of the univariate $F$ tests.

```{r parenting-Anova}
Anova(parenting.mod)
```

`Anova()` returns an object of class `"Anova.mlm"` which has various methods.
The `summary()` method for this gives more details, including all four test statistics.
These tests have $s = \min(p, \text{df}_h) = \min(3,2) = 2$ dimensions and the $F$ approximations are not
equivalent here. All four tests are highly significant
```{r}
parenting.summary <- Anova(parenting.mod) |>  summary() 
print(parenting.summary, SSP=FALSE)
```

The `summary()` method by default prints the SSH = $\mathbf{H}$ and SSE = $\mathbf{E}$ matrices, but I suppressed
them above. They can be extracted from the object using `purrr::pluck()`:

```{r par-summary-pluck}
H <- parenting.summary |> 
  purrr::pluck("multivariate.tests", "group", "SSPH") |> 
  print()
E <- parenting.summary |> 
  purrr::pluck("multivariate.tests", "group", "SSPE") |> 
  print()
```


#### Linear hypotheses & contrasts {.unnumbered}

With three or more groups or with a more complex MANOVA design, contrasts provide a way of
testing questions of substantive interest regarding differences among group means.

The test of the contrast comparing the typical group to the average of the others is the test 
using the contrast $c_1 = (1, -\frac12, -\frac12)$ which produces the coefficients labeled
`"group1"`. The function `car::linearHypothesis()` carries out the multivariate test
that these are all zero. This is a 1 df test, and all four test statistics produce the same
$F$ and $p$-values.

```{r parenting-lin-hyp1}
coef(parenting.mod)["group1",]
linearHypothesis(parenting.mod, "group1") |> 
  print(SSP=FALSE)
```

Similarly, the difference between the physical and mental groups uses the contrast $c_2 = (0, 1, -1)$
and the test that these means are equal is given by `linearHypothesis()` applied to 
`group2`. 
```{r parenting-lin-hyp2}
coef(parenting.mod)["group2",]
linearHypothesis(parenting.mod, "group2") |> 
  print(SSP=FALSE)
```

`linearHypothesis()` is very general. The second argument (`hypothesis.matrix`) corresponds to 
$\mathbf{C}$, and can be specified as numeric matrix giving the linear combinations of coefficients by rows to be tested, or a character vector giving the hypothesis in symbolic form.

Because the contrasts used here are orthogonal, they comprise the overall test of 
$\mathbf{B} = \mathbf{0}$ which implies that the means of the three groups are all equal.
The test below gives the same results as `Anova(parenting.mod)`.

```{r parenting-lin-hyp3}
linearHypothesis(parenting.mod, c("group1", "group2")) |> 
  print(SSP=FALSE)
```





## MRA $\rightarrow$ MMRA

## ANCOVA $\rightarrow$ MANCOVA

## Repeated measures designs
