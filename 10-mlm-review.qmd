```{r include=FALSE}
source("R/common.R")
```

{{< include latex/latex-commands.qmd >}}

# Multivariate Linear Model {#sec-mlm-review}

The general Multivariate Linear Model (MLM) can be understood as a simple extension of the univariate linear model, with the main difference being that there are multiple response variables considered together, instead of just one, analysed alone.
These outcomes might reflect several different ways or scales for measuring an underlying theoretical construct,
or they might represent different aspects of some phenomenon that are better understood when studied jointly.

For example, in the first case, there are numerous psychological scales used to assess depression or anxiety
and it may be important to include more than one measure to ensure that the construct has been measured adequately.
In the second case, student "aptitude" or "achievement" reflects competency in different various subjects
(reading, math, history, science, ...) that are better studied together.



In this context, there are multiple techniques that can be applied depending on the structure of the variables at hand. For instance, with one or more continuous predictors and multiple response variables, one could use multivariate regression to obtain estimates useful for prediction. Instead, if the predictors are categorical, multivariate analysis of variance (MANOVA) can be applied to test for differences between groups. Again, this is akin to multiple regression and ANOVA in the univariate context -- the same underlying model is utilized, but the tests for terms in the model are multivariate ones for the collection of all response variables, rather than univariate ones for a single response.

<!-- **TODO** Use `\Epsilon` = $\Epsilon$ here, which is defined as `\boldsymbol{\large\varepsilon}` for residuals. -->
<!-- Could also use a larger version, `\boldsymbol{\Large\varepsilon}` = -->
<!-- $\boldsymbol{\Large\varepsilon}$ if that makes a difference. -->

## Structure of the MLM
In each of these cases, the underlying MLM is given most compactly using the matrix equation, <!-- $$ --> 
<!-- \newcommand{\sizedmat}[2]{\mathord{\mathop{\mathbf{#1}\limits_{(#2)}}} --> 
<!-- $$ -->

$$
\sizedmat{Y}{n \times p} = 
\sizedmat{X}{n \times q} \, \sizedmat{B}{q \times p} + \sizedmat{\Epsilon}{n \times p} \:\: ,
$$

where

-   $\mathbf{Y} = (\mathbf{y}_1 , \mathbf{y}_2, \dots , \mathbf{y}_p )$ is the matrix of $n$ observations on $p$ responses;
-   $\mathbf{X}$ is the model matrix with columns for $q$ regressors, which typically includes an initial column of 1s for the intercept;
-   $\mathbf{B} = ( \mathbf{b}_1 , \mathbf{b}_2 , \dots , \mathbf{b}_p )$ is a matrix of regression coefficients, one column for each response variable; and 
-   $\Epsilon$ is a matrix of errors in predicting $\mathbf{Y}$.

The structure of the model matrix $\mathbf{X}$ is the same as the univariate linear model, and may contain, therefore, 

* quantitative predictors, such as `age`, `income`, years of `education`
* transformed predictors like $\sqrt{\text{age}}$ or $\log{\text{income}}$
* polynomial terms: $\text{age}^2$, $\text{age}^3, \dots$ (using `poly(age, k)` in R)
* categorical predictors ("factors"), such as treatment (Control, Drug A, drug B), or sex; internally a factor with `k` levels is transformed to `k-1` dummy (0, 1) variables, representing comparisons with a reference level, typically the first.
* interaction terms, involving either quantitative or categorical predictors, e.g., `age * sex`, `treatment * sex`.

### Assumptions
Just as in univariate models, the assumptions of the multivariate linear model entirely concern the behavior of the errors (residuals). 
Let $\mathbf{\epsilon}_{i}^{\prime}$ represent the $i$th row of $\Epsilon$. Then it is assumed that:

* The residuals, $\mathbf{\epsilon}_{i}^{\prime}$ are distributed as multivariate normal, 
$\mathcal{N}_{p}(\mathbf{0},\boldsymbol{\Sigma})$, 
where $\mathbf{\Sigma}$ 
is a non-singular error-covariance matrix. This can be assessed visually using a $\chi^2$ QQ plot
of Mahalanobis squared distance against their corresponding $\chi^2_p$ values.
* The error-covariance matrix $\mathbf{\Sigma}$ is constant across all observations and grouping factors.
Graphical methods to determine if this assumption is met are described in @sec-eqcov.
* $\mathbf{\epsilon}_{i}^{\prime}$ and $\mathbf{\epsilon}_{j}^{\prime}$ are independent for $i\neq j$; and 
* The predictors, $\mathbf{X}$, are fixed or independent of $\Epsilon$. 

These statements are simply the
multivariate analogs of the assumptions of normality, constant variance and independence
of the errors in univariate models.

### Fitting the model

The least squares (and also maximum likelihood) solution for the coefficients $\mathbf{B}$ is given by

$$
\widehat{\mathbf{B}} = (\mathbf{X}\trans \mathbf{X})^{-1} \mathbf{X}\trans \mathbf{Y} \period
$$

This is precisely the same as fitting the separate responses $\mathbf{y}_1 , \mathbf{y}_2 , \dots , \mathbf{y}_p$,
and placing the estimated coefficients $\widehat{\mathbf{b}}_i$ as columns in $\widehat{\mathbf{B}}$

$$
\widehat{\mathbf{B}} = [ \widehat{\mathbf{b}}_1, \widehat{\mathbf{b}}_2, \dots , \widehat{\mathbf{b}}_p] \period
$$
In R, we fit the multivariate linear model simply by giving a collection of response variables `y1, y2, ...`
on the left-hand side of the model formula, wrapped in `cbind()` which combines them to form a matrix response.

```{r mlm-lm}
#| eval: false
lm( cbind(y1, y2, y3) ~ x1 + x2 + ..., data=)
```

### Sums of squares

In univariate response models, statistical tests and model summaries (like $R^2$) are based on the familiar 
decomposition of the total sum of squares $SS_T$ into regression or hypothesis ($SS_H$) and error ($SS_E$)
sums of squares. In the multivariate linear model each of these becomes a $p \times p$ matrix $SSP$
containing sums of squares for the $p$ responses on the diagonal and sums of cross products in the off-diagonal.
elements. For the MLM this is expressed as

\begin{eqnarray*}
\underset{(p\times p)}{\mathbf{SSP}_{T}}
& = & \mathbf{Y}^{\prime} \mathbf{Y} - n \overline{\mathbf{y}}\,\overline{\mathbf{y}}^{\prime} \\
& = & \left(\widehat {\mathbf{Y}}^{\prime}\widehat{\mathbf{Y}} - n\overline{\mathbf{y}}\,\overline{\mathbf{y}}^{\prime} \right) + \widehat{\Epsilon}^{\prime}\widehat{\Epsilon} \\
& = &  \mathbf{SSP}_{H} + \mathbf{SSP}_{E} 
& \equiv & \mathbf{H} + \mathbf{E} \comma
\end{eqnarray*}

<!-- $$ -->
<!-- \underset{(p\times p)}{\mathbf{SSP}_{T}} -->
<!--  =  \mathbf{Y}^{\prime} \mathbf{Y} - n \overline{\mathbf{y}}\,\overline{\mathbf{y}}^{\prime} \\ -->
<!-- $$ -->
<!-- $$ -->
<!--  =  \left(\widehat {\mathbf{Y}}^{\prime}\widehat{\mathbf{Y}} - n\overline{\mathbf{y}}\,\overline{\mathbf{y}}\trans \right) + \widehat{\Epsilon}^{\prime}\widehat{\Epsilon} \\ -->
<!-- $$ -->

where
$\overline{\mathbf{y}}$ is the $(p\times 1)$ vector of means for the response variables; $\widehat{\mathbf{Y}} = \mathbf{X}\widehat{\mathbf{B}}$ is the matrix of fitted values; and $\widehat{\Epsilon} = \mat{Y} -\widehat{\mat{Y}}$ is the matrix of residuals. 
This is the decomposition that we visualize in HE plots,
where the size and direction of $\mathbf{H}$ and $\mathbf{E}$ can be represented as ellipsoids.

The univariate $F$ test statistic, $F = \frac{SS_H/df_h}{SS_E/df_e}$
assesses "how big" $SS_H$ is, relative to $SS_E$. 
This
has a direct multivariate analog in terms of
the $s=\min(p, df_h)$ non-zero latent roots (eigenvalues), $\lambda_i$, of $\mat{H}\mat{E}^{-1}$
($\mathbf{H}$ "divided by" $\mathbf{E}$); that is, the values $\lambda_i$ that solve

$$
 \det{ \mathbf{H}\mathbf{E}^{-1} - \lambda \mathbf{I} } = 0 \Longrightarrow 
 \mathbf{H}\mathbf{E}^{-1} \lambda_i = \lambda_i \mathbf{v}_i \period
$$
The importance of this is that when there are $\p >1$ response variables, and we are testing
a hypothesis comprising $df_h >1$ coefficients or degrees of freedom, there $s > 1$
possible dimensions 

## ANOVA $\rightarrow$ MANOVA



## MRA -> MMRA

## ANCOVA -> MANCOVA

## Repeated measures designs
