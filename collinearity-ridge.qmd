```{r include=FALSE}
source("R/common.R")
```

# Collinearity & Ridge Regression {#sec-collin}

**Packages**
In this chapter we use the following packages. Load them now.
```{r}
library(car)
library(VisCollin)
library(genridge)
```


> Some of my collinearity diagnostics have large values, or small values, or whatever they are not supposed to be
> * What is bad?
> * If bad, what can I do about it?

In univariate multiple regression models, we usually hope to have high correlations between the outcome $y$ and each of the
predictors, $x_1, x_2, \dots$, but high correlations _among_ the predictors can cause problems
in estimating and testing their effects. The quote above shows the a typical quandary of some researchers in trying
do understand these problems and and take steps to resolve them.
This chapter illustrates the problems of collinearity,
describes diagnostic measures to asses its effects, 
and presents some novel visual tools for these purposes using the **VisCollin** package.

One class of solutions for collinearity involves _regularization methods_ such as ridge regression. 
Another collection of graphical methods, generalized ridge trace plots, implemented in the **genridge** package,
sheds further light on what is accomplished by this technique.

## What is collinearity?

Recall the standard classical linear model for a response variable $y$ with a collection of predictors
in $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_p)$

$$
\begin{eqnarray*}
\mathbf{y} & =&  \beta_0 + \beta_1 \mathbf{x}_1 + \beta_2 \mathbf{x}_2 + \cdots + \beta_p \mathbf{x}_p + \mathbf{\epsilon} \\
         & = & \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon} \; ,
\end{eqnarray*}
$$
for which the ordinary least squares solution is:

$$
\widehat{\mathbf{b}} = (\mathbf{X}^T \mathbf{X})^{-1} \; \mathbf{X}^T \mathbf{y} \; ,
$$
with sampling variances and covariances $\text{Var} (\widehat{\mathbf{b}}) = \sigma^2 \times (\mathbf{X}^T \mathbf{X})^{-1}$ and $\sigma^2$ is the variance of the residuals $\mathbf{\epsilon}$, estimated by the
mean squared error (MSE).

In the limiting case, when one $x_i$ is _perfectly_
predictable from the other $x$s, i.e., $R^2 (x_i | \text{other }x) = 1$, 

* there is no _unique_ solution for the regression coefficients 
$\mathbf{b} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X} \mathbf{y}$;
* the standard errors $s (b_i)$ of the estimated coefficients are infinite and _t_ statistics $t_i = b_i / s (b_i)$
are 0.

This extreme case reflects a situation when one or more predictors are effectively redundant, for example
when you include two variables $x$ and $y$ and their sum $z = x + y$ in a model, or use _ipsatized_
scores that sum to a constant. More generally, collinearity refers to the case when there are very high
**multiple correlations** among the predictors, such as $R^2 (x_i | \text{other }x) \ge 0.9$. 
Note that you can't tell simply by looking at the simple correlations. A large correlation
$r_{ij}$ is _sufficient_ for collinearity, but not _necessary_ --- you can have variables
$x_1, x_2, x_3$ for which the pairwise correlation are low, but the multiple correlation is high.

The consequences are:

* The estimated coefficients have large standard errors, $s(\hat{b_j})$. They are multiplied by
the square root of the variance inflation factor, $\sqrt{\text{VIF}}$, discussed below.
* This deflates the $t$-statistics, $t = \hat{b_j} / s(\hat{b_j})$ by the same factor.
* Thus you may find a situation where an overall model is highly significant (large $F$-statistic), while
no (or few) of the individual predictors are. This is a puzzlement!
* Beyond this, the least squares solution may have poor numerical accurracy [@Longley:1967], because
the solution depends on the determinant $|\,\mathbf{X}^T \mathbf{X}\,|$, which approaches 0
as multiple correlations increase.
* As well, recall that the coefficients $\hat{b}$ are **partial coefficients**, meaning the estimated
change $\Delta y$ in $y$ when $x$ changes by one unit $\Delta x$, but **holding all other variables
constant**. Then, the model may be trying to estimate something that does not occur in the data.

### Visualizing collinearity

Collinearity can be illustrated in data space for two predictors in terms of the stability of the
regression plane for a linear model `Y = X1 + X2`. In @fig-collin-demo (adapted from @Fox:2016:ARA, Fig. 13.2):

(a) shows a case where
$X_1$ and $X_2$ are uncorrelated as can be seen in their scatter in the horizontal plane (`+` symbols).
The regression plane is well-supported; a small change in Y for one observation won't make much difference.

(b) In panel (b), $X_1$ and $X_2$ have a perfect correlation, $r (x_1, x_2) = 1.0$. The regression plane
is not unique; in fact there are an infinite number of planes that fit the data equally well. Note that,
if all we care about is prediction (not the coefficients), we could use $X_1$ or $X_2$, or both, or
any weighted sum of them in a model and get the same predicted values.

(c) Shows a typical case where there is a strong correlation between $X_1$ and $X_2$. The regression plane
here is unique, but is not well determined. A small change in Y **can** make quite a difference
in the fitted value or coefficients, depending on the values of $X_1$ and $X_2$.
Where $X_1$ and $X_2$ are far from their near linear relation in the botom plane,
you can imagine that it is easy to tilt the plane substantially by a small change in $Y$.

```{r}
#| label: fig-collin-demo,
#| echo: false
#| out-width: 100%
#| fig.cap: "Effect of collinearity on the least squares regression plane. 
#|      (a) Small correlation between predictors;
#|      (b) Perfect correlation ;
#|      (c) Very strong correlation.
#|      The black points show the data Y values, white points are the fitted values in the regression plane,
#|      and + signs represent the values of X1 and X2.
#|      _Source_: Adapted from @Fox:2016:ARA, Fig. 13.2"
knitr::include_graphics("images/collin-demo.png")
```

### Data space and $\beta$ space
It is also useful to visualize collinearity by comparing the representation in **data space** with the
analogous view of the confidence ellipses for coefficients in **beta space**. To do so, we generate
data from a known model $y = 3 x_1 + 3 x_2 + \epsilon$ with $\epsilon \sim \mathcal{N} (0, 100)$
and various true correlations between $x_1$ and $x_2$, $\rho_{12} = (0, 0.8, 0.97)$ [^1].

[^1]: This example is adapted from one by John Fox (2022), [Collinearity Diagnostics](https://socialsciences.mcmaster.ca/jfox/Courses/SORA-TABA/slides-collinearity.pdf)

::: {.column-margin}
Working file: `R/collin-data-beta.R`
:::

First, we use `MASS:mvrnorm()` to construct a list of data frames `XY` with specified values
for the means and covariance matrices and a corresponding list of models, `mods`.

```{r collin-data-beta-gen}
#| code-fold: show
library(MASS)
library(car)

set.seed(421)            # reproducibility
N <- 200                 # sample size
mu <- c(0, 0)            # means
s <- c(1, 1)             # standard deviations
rho <- c(0, 0.8, 0.97)   # correlations
beta <- c(3, 3)          # true coefficients

# Specify a covariance matrix, with standard deviations s[1], s[2] and correlation r
Cov <- function(s, r){
  matrix(c(s[1],    r * prod(s),
         r * prod(s), s[2]), nrow = 2, ncol = 2)
}

# Generate a dataframe of X, y for each rho
# Fit the model for each
XY <- vector(mode ="list", length = length(rho))
mods <- vector(mode ="list", length = length(rho))
for (i in seq_along(rho)) {
  r <- rho[i]
  X <- mvrnorm(N, mu, Sigma = Cov(s, r))
  colnames(X) <- c("x1", "x2")
  y <- beta[1] * X[,1] + beta[2] * X[,2] + rnorm(N, 0, 10)

  XY[[i]] <- data.frame(X, y=y)
  mods[[i]] <- lm(y ~ x1 + x2, data=XY[[i]])
}
```
The estimated coefficients in these models are:

```{r collin-data-beta-coefs}
coefs <- sapply(mods, coef)
colnames(coefs) <- c("Intercept", "b1", "b2")
coefs
```



Then, we define a function to plot the data ellipse (`car::dataEllipse()`) for each data frame and confidence ellipse
(`car::dataEllipse()`) in the corresponding fitted model. In this figure, I specify the x, y limits for each plot
so that the relative sizes of these ellipses are comparable, so that variance inflation can be assesed visually.

```{r}
#| label: collin-data-beta
#| code-fold: show
#| out-width: "100%"
#| fig-show: "hold"
#| fig.cap: '95% Ddata ellipses for x1, x2 and the corresponding 95% confidence ellipses for their coefficients. 
#|    In the confidence ellipse plots, reference lines show the value (0,0) for the null hypothesis and "+" marks 
#|    the true values for the coefficients. This figure adapts an example by John Fox (2022).'
#|    
do_plots <- function(XY, mod, r) {
  X <- as.matrix(XY[, 1:2])
  dataEllipse(X,
              levels= 0.95,
              col = "darkgreen",
              fill = TRUE, fill.alpha = 0.05,
              xlim = c(-3, 3),
              ylim = c(-3, 3), asp = 1)
  text(0, 3, bquote(rho == .(r)), cex = 2, pos = NULL)

  confidenceEllipse(mod,
                    col = "red",
                    fill = TRUE, fill.alpha = 0.1,
                    xlab = "x1 coefficient",
                    ylab = "x2 coefficient",
                    xlim = c(-5, 10),
                    ylim = c(-5, 10),
                    asp = 1)
  points(beta[1], beta[2], pch = "+", cex=2)
  abline(v=0, h=0, lwd=2)
}

op <- par(mar = c(4,4,1,1)+0.1,
          mfcol = c(2, 3),
          cex.lab = 1.5)

for (i in seq_along(rho)) {
  do_plots(XY[[i]], mods[[i]], rho[i])
}
par(op)
```

Recall (Section #sec-data-beta) that the confidence ellipse for $(\beta_1, \beta_2)$ is just a 90 degree rotation
(and rescaling) of the data ellipse for $(x_1, x_2)$: it is wide (more variance) in any direction where the
data ellipse is narrow. 

The shadows of the confidence ellipses on the coordinate axes in @fig-collin-data-beta represent the 
standard errors of the coefficients, and get larger with increasing $\rho$. This is the effect of variance
inflation, described in the following section.



## Measuring collinearity: Variance inflation factors

How can we measure the effect of collinearity? The essential idea is to compare, for each predictor the variance
$s^2 (\widehat{b_j})$ that the coefficient that $x_j$ would have if it was totally unrelated to the other
predictors to the actual variance it has in the given model.

For two predictors such as shown in @fig-collin-data-beta the sampling variance of $x_1$ can be expressed
as

$$
s^2 (\widehat{b_1}) = \frac{MSE}{(n-1) \; s^2(x_1)} \; \times \; \left[ \frac{1}{1-r^2_{12}} \right]
$$
The first term here is the variance of $b_1$ when the two predictors are uncorrelated.
The term in brackets represents the **variance inflation factor** [@Marquandt:1970], the amount by which the 
variance of the coefficient is multiplied as a consequence of the correlation $r_{12}$ of
the predictors.  As $r_{12} \rightarrow 1$, the variances approaches infinity.

More generally, with any number of predictors, this relation has a similar form, replacing
the simple correlation $r_{12}$ with the multiple correlation predicting $x_j$ from all others,

$$
s^2 (\widehat{b_j}) = \frac{MSE}{(n-1) \; s^2(x_j)} \; \times \; \left[ \frac{1}{1-R^2_{j | \text{others}}} \right]
$$
So, we have that the variance inflation factors are:

$$
\text{VIF}_j = \frac{1}{1-R^2_{j \,|\, \text{others}}} 
$$
In practice, it is often easier to think in terms of the square root, $\sqrt{\text{VIF}_j}$ as the
multiplier of the standard errors. The denominator, $1-R^2_{j | \text{others}}$ is sometimes called
**tolerance**, a term I don't find particularly useful.

For the cases shown in @fig-collin-data-beta the VIFs and their square roots are:

```{r collin-data-beta-vif}
vifs <- sapply(mods, car::vif)
colnames(vifs) <- paste("rho:", rho)
vifs

sqrt(vifs)
```


Note that when there are terms in the model with more than one df, such as education with four levels
(and hence 3 df) or a polynomial term specified as `poly(x, 3)`, the standard VIF calculation
gives results that vary with how those terms are coded in the model. @FoxMonette:92 define
_generalized_, GVIFs as the inflation in the squared area of the confidence ellipse for the coefficients
of such terms, relative to what would be obtained with uncorrelated data. Visually, this can be seen
by comparing the areas of the ellipses in the bottom row of @fig-collin-data-beta.

**Example**: This example uses the `cars` data set in the `VisCollin` package
containing various measures of size and performance on 406 models of automobiles from 1982. Interest is focused on predicting gas mileage, `mpg`.
```{r cars}
data(cars, package = "VisCollin")
str(cars)
```

We fit a model predicting gas mileage (`mpg`) from the number of cylinders, engine displacement, horsepower, weight,
time to
accelerate from 0 -- 60 mph and model year (1970--1982). Perhaps surprisingly, only `weight` and `year` appear to
significantly predict gas mileage. What's going on here?

```{r cars-mod}
cars.mod <- lm (mpg ~ cylinder + engine + horse + weight + accel + year, 
                data=cars)
Anova(cars.mod)
```

We check the variance inflation factors, using `car::vif()`. We see that most predictors have very high
VIFs, indicating moderately severe multicollinearity.

```{r cars-vif}
vif(cars.mod)

sqrt(vif(cars.mod))
```

According to $\sqrt{\text{VIF}}$, the standard error of `cylinder` has been
multiplied by 3.26 and it's $t$-value divided by this number,
compared with the case when all predictors are
uncorrelated. `engine`, `horse` and `weight` suffer a similar fate.

::: {.callout-tip}
## Connection with inverse of correlation matrix

In the linear regression model with standardized predictors,
the covariance matrix of the estimated intercept-excluding
parameter vector $\mathbf{b}^\star$  has the
simpler form,
$$
\mathcal{V} (\mathbf{b}^\star) = \frac{\sigma^2}{n-1} \mathbf{R}^{-1}_{X} \; .
$$
where 
$\mathbf{R}_{X}$ is the correlation matrix among the predictors.
 It
can then be seen that the VIF$_j$ are just the diagonal entries of
$\mathbf{R}^{-1}_{X}$.

More generally, the matrix $\mathbf{R}^{-1}_{X} = (r^{ij})$, when standardized to a correlation matrix
as $-r^{ij} / \sqrt{r^{ii} \; r^{jj}}$ gives the matrix of all partial correlations,
$r_{ij \,|\, \textrm{others}}$.
}

:::

### Collinearity diagnostics

OK, we now know that large VIF$_j$ indicate predictor coefficients whose estimation 
is degraded due to large $R^2_{j \,|\, \text{others}}$.
But for this to be useful, we need to determine: 

* how many dimensions in the space of the predictors are associated with nearly collinear relations?
* which predictors are most strongly implicated in each of these?

Answers to these questions are provided using measures developed by Belsley and colleagues
[@Belsley-etal:80; @Belsley:91a].
These measures are based on the eigenvalues $\lambda_1, \lambda_2, \dots \lambda_p$
of the correlation matrix $R_{X}$ of the predictors (preferably centered and scaled, and not including the constant term
for the intercept), and the corresponding eigenvectors in the columns of $\mathbf{V}_{p \times p}$, given by
the the eigen decomposition
$$
\mathbf{R}_{X} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^T
$$
By elementary matrix algebra, the eigen decomposition of $\mathbf{R}_{XX}^{-1}$ is then 
$$
\mathbf{R}_{X}^{-1} = \mathbf{V} \mathbf{\Lambda}^{-1} \mathbf{V}^T \; ,
$$ {#eq-rxinv-eigen}
so, $\mathbf{R}_{X}$ and $\mathbf{R}_{XX}^{-1}$ have the same eigenvectors, and the eigenvalues
of $\mathbf{R}_{X}^{-1}$ are just $\lambda_i^{-1}$.
Using @eq-rxinv-eigen, the variance inflation factors may be expressed as
$$
\text{VIF}_j = \sum_{k=1}^p \frac{V^2_{jk}}{\lambda_k} \; ,
$$
which shows that only the _small_ eigenvalues contribute to variance inflation,
but only for those predictors that have large eigenvector coefficients on those small components.
These facts lead to the following diagnostic statistics for collinearity:

* **Condition indices**: 
The smallest of the eigenvalues, those for which $\lambda_j \approx 0$,
indicate collinearity and the number of small values indicates the number of near collinear relations.
Because the sum of the eigenvalues, $\Sigma \lambda_i = p$ increases with the number
of predictors $p$, it is useful to scale them
all in relation to the largest.  This leads to _condition indices_, defined as
$\kappa_j = \sqrt{ \lambda_1 / \lambda_j}$. These have the property that the resulting numbers
have common interpretations regardless of the number of predictors.

  + For completely uncorrelated predictors, all $\kappa_j = 1$.
  + $\kappa_j \rightarrow \infty$ as any $\lambda_k \rightarrow 0$.

* **Variance decomposition proportions**:
Large VIFs indicate variables that are involved in _some_ nearly collinear
relations, but they don't indicate _which_ other variable(s) each is involved with.
For this purpose, Belsley et. al. (1980) and Belsley (1991) proposed calculation of
the proportions of variance of each variable associated with each principal component
as a decomposition of the coefficient variance for each dimension.

These measures can be calculated using `VisCollin::colldiag()`.
For the current model, the usual display contains both the condition indices and
variance proportions. However, even for a small example, it is often difficult to know
what numbers to pay attention to.
```{r colldiag1}
(cd <- colldiag(cars.mod, center=TRUE))
```
@Belsley:91a recommends that the sources of collinearity be diagnosed 
(a) only for those components with large $\kappa_j$, and
(b) for those components for which the variance proportion is large (say, $\ge 0.5$) on _two_ or more predictors.
The print method for `"colldiag"` objects has a `fuzz` argument controlling this.

```{r colldiag2}
print(cd, fuzz = 0.5)
```

The mystery is solved: There are two nearly collinear relations among the predictors, corresponding to the two
smallest dimensions. 

* Dimension 5 reflects the high correlation between horsepower and weight,
* Dimension 6 reflects the high correlation between number of cylinders and engine displacement.

Note that the high variance proportion for `year` (0.787) on the second component creates no problem and
should be ignored because (a) the condition index is low and (b) it shares nothing with other predictors.


### Tableplots

### Collinearity biplots

## Remedies for collinearity


## Ridge regression

### What is ridge regression?

### Univariate ridge trace plots

### Bivariate ridge trace plots




```{r}
#| echo: false
cat("Writing packages to ", .pkg_file, "\n")
write_pkgs(file = .pkg_file)
```

## References {.unnumbered}
