{
  "hash": "a1e5d046cb4d339615d5f0346dd43c73",
  "result": {
    "engine": "knitr",
    "markdown": "---\neditor: \n  markdown: \n    wrap: 72\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n\n::: {.content-visible unless-format=\"pdf\"}\n<!--- For HTML Only --->\n\n\n<!-- \\require{newcommand} -->\n\\renewcommand*{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\trans}{^\\mathsf{T}}\n\\newcommand*{\\mat}[1]{\\mathbf{#1}}\n\\newcommand*{\\diag}[1]{\\mathrm{diag}\\, #1}\n<!-- %\\renewcommand*{\\det}[1]{\\mathrm{det} (#1)} -->\n<!-- %\\renewcommand*{\\det}[1]{|#1|} -->\n\\renewcommand*{\\det}[1]{\\mathrm{det}(#1)}\n\\newcommand*{\\rank}[1]{\\mathrm{rank} (\\mathbf{#1})}\n\\newcommand*{\\trace}[1]{\\mathrm{tr} (\\mathbf{#1})}\n\\newcommand*{\\dev}[1]{(#1 - \\bar{#1})}\n\\newcommand*{\\inv}[1]{\\mat{#1}^{-1}}\n\\newcommand*{\\half}[1]{\\mat{#1}^{1/2}}\n\\newcommand*{\\invhalf}[1]{\\mat{#1}^{-1/2}}\n\\newcommand*{\\nvec}[2]{{#1}_{1}, {#1}_{2},\\ldots,{#1}_{#2}}\n\\newcommand*{\\Beta}{\\boldsymbol{B}}\n\\newcommand*{\\Epsilon}{\\boldsymbol{\\Large\\varepsilon}}\n\\newcommand*{\\period}{\\:\\: .}\n\\newcommand*{\\comma}{\\:\\: ,}\n\\newcommand*{\\given}{\\, | \\,}\n\\newcommand*{\\Real}[1]{\\mathbb{R}^{#1}}\n\\newcommand*{\\degree}[1]{{#1}^{\\circ}}\n<!-- \\newcommand{\\sizedmat}[2]{\\mathord{\\mathop{\\mat{#1}}\\limits_{(#2)}}} -->\n\\newcommand{\\sizedmat}[2]{\\mathord{\\mathop{\\mat{#1}}\\limits_{#2}}}\n\n\\renewcommand*{\\H}{\\mathbf{H}}               \n\\newcommand*{\\E}{\\mathbf{E}}\n\\newcommand*{\\widebar}[1]{\\overline{#1}}\n\n\\newcommand{\\Var}{\\mathsf{Var}}\n\\newcommand{\\Cov}{\\mathsf{Cov}}\n\\newcommand{\\HO}{\\mathcal{H}_0}\n\n<!-- \\newcommand*{\\E}{\\mathcal{E}} -->\n\\newcommand*{\\V}{\\mathcal{V}}\n\n<!-- Index generation -->\n\\newcommand{\\IX}[1]{\\index{#1}#1}\n\\newcommand{\\ix}[1]{\\index{#1}}\n\\newcommand{\\ixmain}[1]{\\index{#1|textbf}}\n\n\\newcommand{\\ixon}[1]{\\index{#1|(}}\n\\newcommand{\\ixoff}[1]{\\index{#1|)}}\n\n<!-- % R packages:  indexed under both package name and packages! -->\n\\newcommand{\\ixp}[1]{%\n   \\index{#1@\\textsf{#1} package}%\n   \\index{package!#1@\\textsf{#1}}%\n\t}\n\n<!-- % data sets:  -->\n\\newcommand{\\ixd}[1]{%\n        \\index{data sets!#1}}\n\n\n<!-- % R stuff -->\n\\newcommand{\\pkg}[1]{\\textsf{#1}}\n\\newcommand{\\Rpackage}[1]{\\pkg{#1} package}\n\n\n\n\n:::\n\n# Plots of Multivariate Data {#sec-multivariate_plots}\n\n> There is no excuse for failing to plot and look. \n>\n> The greatest value of a picture is when it forces us to notice what we never expected to see.\n> --- John W. Tukey, _Exploratory Data Analysis_, 1977\n\n\n\nThese quotes from John Tukey remind us that data analysis should nearly always\nstart with graphs to help us understand the main features of our\ndata. It is important to understand the general _patterns_ and _trends_: Are relationships increasing or decreasing? Are they approximately linear or non-linear? But it is also important to spot\n_anomalies_: \"unusual\" observations, groups of points that seem to differ from the rest, and so forth.\nAs we saw with Anscombe's quartet (@sec-anscombe) numerical summaries hide features that are immediately apparent in a plot.\n\nThis chapter introduces a\ntoolbox of basic graphical methods for visualizing multivariate\ndatasets. It starts with some simple techniques to enhance the basic\nscatterplot with graphical _annotations_ such as fitted lines, curves and data\nellipses to _summarize_ the relation between two variables.\n\nTo visualize more than two variables, we can view all pairs of variables\nin a scatterplot matrix or shift gears entirely to show multiple\nvariables along a set of parallel axes. As the number of variables\nincreases, we may need to suppress details with stronger summaries\nfor a high-level reconnaissance of our data terrain, as we do by zooming\nout on a map. For example, we can simply remove the data points or make them nearly transparent\nto focus on the visual summaries provided by fitted lines or other graphical summaries.\n\n**Packages**\n\nIn this chapter I use the following packages. Load them now:\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(car)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(corrplot)\nlibrary(corrgram)\nlibrary(GGally)\nlibrary(ggdensity)\nlibrary(patchwork)\nlibrary(ggpcp)\nlibrary(tourr)\n```\n:::\n\n\n\n\n\n## Bivariate summaries {#sec-bivariate_summaries}\n\nThe basic scatterplot is the workhorse of multivariate data\nvisualization, showing how one variable, $y$, often an outcome to be\nexplained by or varies with another, $x$. It is a building block for\nmany useful techniques, so it is helpful to understand how it can be\nused as a tool for thinking in a wider, multivariate context.\n\nThe essential idea is that we can start with a simple version of the\nscatterplot and add annotations to show interesting features more\nclearly. We consider the following here:\n\n-   **Smoothers**: Showing overall trends, perhaps in several forms, as\n    visual summaries such as fitted regression lines or curves and\n    nonparametric smoothers.\n-   **Stratifiers**: Using color, shape or other features to identify\n    subgroups; more generally, *conditioning* on other variables in\n    multi-panel displays;\n-   **Data ellipses**: A compact 2D visual summary of bivariate linear\n    relations and uncertainty assuming normality; more generally,\n    contour plots of bivariate density.\n\n**Example: Academic salaries**\n\nLet's start with data on the academic salaries of faculty members\ncollected at a U.S. college for the purpose of assessing salary\ndifferences between male and female faculty members, and perhaps address\nanomalies in compensation. The dataset `carData::Salaries` gives data on\nnine-month salaries and other variables for 397 faculty members in the\n2008-2009 academic year.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(Salaries, package = \"carData\")\nstr(Salaries)\n#> 'data.frame':\t397 obs. of  6 variables:\n#>  $ rank         : Factor w/ 3 levels \"AsstProf\",\"AssocProf\",..: 3 3 1 3 3 2 3 3 3 3 ...\n#>  $ discipline   : Factor w/ 2 levels \"A\",\"B\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ yrs.since.phd: int  19 20 4 45 40 6 30 45 21 18 ...\n#>  $ yrs.service  : int  18 16 3 39 41 6 23 45 20 18 ...\n#>  $ sex          : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 1 ...\n#>  $ salary       : int  139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ...\n```\n:::\n\n\n\n\n\nThe most obvious, but perhaps naive, predictor of `salary` is\n`years.since.phd`. For simplicity, I'll refer to this as years of\n\"experience.\" Before looking at differences between males and females,\nwe would want consider faculty `rank` (related also to `yrs.service`)\nand `discipline`, recorded here as `\"A\"` (\"theoretical\" departments) or\n`\"B\"` (\"applied\" departments). But, for a basic plot, we will ignore these\nfor now to focus on what can be learned from plot annotations.\n\n<!-- figure-code: `R/Salaries-scatterplots.R`  -->\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ngg1 <- ggplot(Salaries, \n       aes(x = yrs.since.phd, y = salary)) +\n  geom_jitter(size = 2) +\n  scale_y_continuous(labels = scales::dollar_format(\n    prefix=\"$\", scale = 0.001, suffix = \"K\")) +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") \n\ngg1 + geom_rug(position = \"jitter\", alpha = 1/4)\n```\n\n::: {.cell-output-display}\n![Naive scatterplot of Salary vs. years since PhD, ignoring other variables, and without graphical annotations.](figs/ch03/fig-Salaries-scat-1.pdf){#fig-Salaries-scat fig-align='center' fig-pos='H' width=90%}\n:::\n:::\n\n\n\n\n\nThere is quite a lot we can see \"just by looking\" at this simple plot,\nbut the main things are:\n\n-   Salary increases generally from 0 - 40 years since the PhD, but then maybe begins to drop off (partial retirement?);\n-   Variability in salary increases among those with the same\n    experience, a \"fan-shaped\" pattern that signals a violation of\n    homogeneity of variance in simple regression;\n-   Data beyond 50 years is thin, but there are some quite low salaries\n    there. Adding rug plots to the X and Y axes is a simple but effective way to show the\n    marginal distributions of the observations. Jitter and transparency helps to avoid overplotting\n    due to discrete values.\n\n### Smoothers\n\nSmoothers are among the most useful graphical annotations you can add to\nsuch plots, giving a visual summary of how $y$ changes with $x$. The\nmost common smoother is a line showing the linear regression for $y$\ngiven $x$, expressed in math notation as\n$\\mathbb{E} (y | x) = b_0 + b_1 x$. If there is doubt that a linear\nrelation is an adequate summary, you can try a quadratic or other\npolynomial smoothers.\n\nIn **ggplot2**, these are easily added to a plot using `geom_smooth()`\nwith `method = \"lm\"`, and a model `formula`, which (by default) is\n`y ~ x` for a linear relation or `y ~ poly(x, k)` for a polynomial of\ndegree $k$.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\ngg1 + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\", \n              color = \"red\", fill= \"pink\",\n              linewidth = 2) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x,2)\", \n              color = \"darkgreen\", fill = \"lightgreen\",\n              linewidth = 2) \n```\n\n::: {.cell-output-display}\n![Scatterplot of Salary vs. years since PhD, showing \\textcolor{red}{linear} and \\textcolor{darkgreen}{quadratic} smooths with 95% confidence bands.](figs/ch03/fig-Salaries-lm-1.pdf){#fig-Salaries-lm fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\n<!--# UA: This is a fantastic graph. My only (very minor) suggestion is to replace one of the colours because the most common type of colour vision deficiency makes it hard to tell the difference between red and green. So, maybe green and purple (purple would match the inline code highlight colour)-->\n\n\nThis serves to highlight some of our impressions from the basic\nscatterplot shown in @fig-Salaries-scat, making them more apparent. And\nthat's precisely the point: the regression smoother draws attention to a\npossible pattern that we can consider as a visual summary of the data.\nYou can think of this as showing what a linear (or quadratic) regression\n\"sees\" in the data. Statistical tests <!--# (secref?) --> can help you decide if there is more evidence for a quadratic fit compared to the simpler linear relation. <!--# UA: Great paragraph!-->\n\n\nIt is useful to also show some indication of *uncertainty* (or\ninversely, *precision*) associated with the predicted values. Both\nthe linear and quadratic trends are shown in @fig-Salaries-lm with 95%\npointwise confidence bands.[^pointwise] These are necessarily narrower in the center\nof the range of $x$ where there is typically more data; they get wider\ntoward the highest values of experience where the data are thinner.\n\n[^pointwise]: \nConfidence bands allow us to visualize the uncertainty around a fitted regression curve,\nwhich can be of two types: _pointwise intervals_ or _simultaneous intervals_.\nThe default setting in ``ggplot2::geom_smooth()` calculates pointwise intervals \n(using `stats::predict.lm(..., interval=\"confidence\")` at a confidence level $1-\\alpha$ for the predicted response at _each value_ $x_i$ of a predictor, and have the frequentist interpretation that over repeated sampling only $100\\;\\alpha$ of the predictions at $x_i$ will be outside that interval. \nIn contrast, simultaneous intervals are calculated so that $1 - \\alpha$ is the probability that _all of them_ cover their corresponding true values simultaneously. These are necessarily wider than pointwise intervals.\nCommonly used methods for constructing simultaneous confidence bands in regression are the Bonferroni and Scheffé methods, which control the family-wise error rate over all values of $x_i$.\nSee [](https://en.wikipedia.org/wiki/Confidence_and_prediction_bands) for precise definitions of these terms.\nThese are different from a _prediction band_, which is used to represent the uncertainty about the value of a **new** data-point on the curve, but subject to the additional variance reflected in one observation.\n\n\n#### Non-parametric smoothers {.unnumbered}\n\nThe most generally useful idea is a smoother that tracks an average\nvalue, $\\mathbb{E} (y | x)$, of $y$ as $x$ varies across its' range\n*without* assuming any particular functional form, and so avoiding the\nnecessity to choose among `y ~ poly(x, 1)`, or `y ~ poly(x, 2)`, or\n`y ~ poly(x, 3)`, etc.\n\nNon-parametric smoothers attempt to estimate $\\mathbb{E} (y | x) = f(x)$\nwhere $f(x)$ is some smooth function. These typically use a collection\nof weighted *local regressions* for each $x_i$ within a window centered\nat that value. In the method called *lowess* or *loess* [@Cleveland:79;\n@ClevelandDevlin:88], a weight function is applied, giving greatest\nweight to $x_i$ and a weight of 0 outside a window containing a certain fraction, $s$, called *span*, of the nearest neighbors of $x_i$. The fraction, $s$, is usually within the range $1/3 \\le s \\le 2/3$, and it determines the\nsmoothness of the resulting curve; smaller values produce a wigglier\ncurve and larger values giving a smoother fit (an optimal\nspan can be determined by $k$-fold cross-validation to minimize a\nmeasure of overall error of approximation).\n\nNon-parametric regression is a broad topic; see @Fox:2016:ARA, Ch. 18 for\na more general treatment including smoothing splines, and @Wood:2006 for generalized additive models,\nfit using `method = \"gam\"` in **ggplot2**, which is the default when the\nlargest group has more than 1,000 observations.\n\n@fig-Salaries-loess shows the addition of a loess smooth to the plot in\n@fig-Salaries-lm, suppressing the confidence band for the linear\nregression. The loess fit is nearly coincident with the quadratic fit,\nbut has a slightly wider confidence band.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\ngg1 + \n  geom_smooth(method = \"loess\", formula = \"y ~ x\", \n              color = \"blue\", fill = scales::muted(\"blue\"),\n              linewidth = 2) +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE,\n              color = \"red\",\n              linewidth = 2) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x,2)\", \n              color = \"darkgreen\", fill = \"lightgreen\",\n              linewidth = 2) \n```\n\n::: {.cell-output-display}\n![Scatterplot of Salary vs. years since PhD, adding the loess smooth. The loess smooth curve and confidence band in \\textcolor{green}{green} is nearly indistinguishable from a quadratic fit in \\textcolor{blue}{blue}.](figs/ch03/fig-Salaries-loess-1.pdf){#fig-Salaries-loess fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\nBut now comes an important question: is it reasonable that academic\nsalary should increase up to about 40 years since the PhD degree and\nthen decline? The predicted salary for someone still working 50 years\nafter earning their degree is about the same as a person at 15 years.\nWhat else is going on here?\n\n\n\n\n\n### Stratifiers\n\nVery often, we have a main relationship of interest, but various groups\nin the data are identified by discrete factors (like faculty `rank` and\n`sex`, their type of `discipline` here), or there are quantitative\npredictors for which the main relation might vary. In the language of\nstatistical models such effects are *interaction* terms, as in\n`y ~ group + x + group:x`, where the term `group:x` fits a different\nslope for each group and the grouping variable is often called a\n*moderator* variable. Common moderator variables are ethnicity, health\nstatus, social class and level of education. Moderators can also be\ncontinuous variables as in `y ~ x1 + x2 + x1:x2`.\n\nI call these *stratifiers*, recognizing that we should consider breaking\ndown the overall relation to see whether and how it changes over such\n\"other\" variables. Such variables are most often factors, but we can cut\na continuous variable into ranges (*shingles*) and do the same\ngraphically. There are two general stratifying graphical techniques:\n\n-   **Grouping**: Identify subgroups in the data by assigning different\n    visual attributes, such as color, shape, line style, etc. within a\n    single plot. This is quite natural for factors; quantitative\n    predictors can be accommodated by cutting their range into ordered\n    intervals. Grouping has the\n    advantage that the levels of a grouping variable can be shown within\n    the same plot, facilitating direct comparison.\n\n-   **Conditioning**: Showing subgroups in different plot panels. This\n    has the advantages that relations for the individual groups more\n    easily discerned and one can easily stratify by two (or more) other\n    variables jointly, but visual comparison is more difficult because\n    the eye must scan from one panel to another.\n\n::: {.callout-note title=\"History Corner\"}\nRecognition of the roles of visual grouping by factors within a panel\nand conditioning in multi-panel displays was an important advance in the\ndevelopment of modern statistical graphics. It began at A.T.&T. Bell\nLabs in Murray Hill, NJ in conjunction with the **S** language, the\nmother of R.\n\nConditioning displays (originally called *coplots*\n[@ChambersHastie1991]) are simply a collection of 1D, 2D or 3D plots\nseparate panels for subsets of the data broken down by one or more\nfactors, or, for quantitative variables, subdivided into a factor with\nseveral overlapping intervals (*shingles*). The first implementation was\nin *Trellis* plots [@Becker:1996:VDC;@Cleveland:85].\n\nTrellis displays were extended in the **lattice** package [@R-lattice],\nwhich offered:\n\n-   A **graphing syntax** similar to that used in statistical model\n    formulas: `y ~ x | g` conditions the data by the levels of `g`, with\n    `|` read as \"given\"; two or more conditioning are specified as\n    `y ~ x | g1 + g2 + ...`, with `+` read as \"and\".\n-   **Panel functions** define what is plotted in a given panel.\n    `panel.xyplot()` is the default for scatterplots, plotting points,\n    but you can add `panel.lmline()` for regression lines,\n    `latticeExtra::panel.smoother()` for loess smooths and a wide\n    variety of others.\n\nThe **car** package [@R-car] supports this graphing syntax in many of\nits functions. **ggplot2** does not; it uses aesthetics (`aes()`), which\nmap variables in the data to visual characteristics in displays.\n:::\n\nThe most obvious variable that affects academic salary is `rank`,\nbecause faculty typically get an increase in salary with a promotion\nthat carries through in their future salary. What can we see if we group\nby `rank` and fit a separate smoothed curve for each?\n\nIn `ggplot2` thinking, grouping is accomplished simply by adding an\naesthetic, such as `color = rank`. What happens then is that points,\nlines, smooths and other `geom_*()` inherit the feature that they are\ndifferentiated by color. In the case of `geom_smooth()`, we get a\nseparate fit for each subset of the data, according to `rank`.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\n# make some re-useable pieces to avoid repetitions\nscale_salary <-   scale_y_continuous(\n  labels = scales::dollar_format(prefix=\"$\", \n                                 scale = 0.001, \n                                 suffix = \"K\")) \n# position the legend inside the plot\nlegend_pos <- theme(legend.position = \"inside\",\n                    legend.position.inside = c(.1, 0.95), \n                    legend.justification = c(0, 1))\n\nggplot(Salaries, \n       aes(x = yrs.since.phd, y = salary, \n           color = rank, shape = rank)) +\n  geom_point() +\n  scale_salary +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  geom_smooth(aes(fill = rank),\n                  method = \"loess\", formula = \"y ~ x\", \n                  linewidth = 2)  +\n  legend_pos\n```\n\n::: {.cell-output-display}\n![Scatterplot of Salary vs. years since PhD, grouped by rank.](figs/ch03/fig-Salaries-rank-1.pdf){#fig-Salaries-rank fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\nWell, there is a different story here. Salaries generally occupy\nseparate vertical levels, increasing with academic rank. The horizontal extents\nof the smoothed curves show their ranges. Within each rank there is some\ninitial increase after promotion, and then some tendency to decline with\nincreasing years. But, by and large, years since the PhD doesn't make\nas much difference once we've taken academic rank into account.\n\nWhat about the `discipline` which is classified, perhaps peculiarly, as\n\"theoretical\" vs. \"applied\"? The values are just `\"A\"` and `\"B\"`,\nso I map these to more meaningful labels before making the plot.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nSalaries <- Salaries |>\n  mutate(discipline = \n           factor(discipline, \n                  labels = c(\"A: Theoretical\", \"B: Applied\")))\n\nSalaries |>\n  ggplot(aes(x = yrs.since.phd, y = salary, color = discipline)) +\n    geom_point() +\n  scale_salary +\n  geom_smooth(aes(fill = discipline ),\n                method = \"loess\", formula = \"y ~ x\", \n                linewidth = 2) + \n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  legend_pos \n```\n\n::: {.cell-output-display}\n![Scatterplot of Salary vs. years since PhD, grouped by discipline.](figs/ch03/fig-Salaries-discipline-1.pdf){#fig-Salaries-discipline fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\nThe story in @fig-Salaries-discipline is again different. Faculty in\napplied disciplines on average earn about 10,000\\$ more per year on\naverage than their theoretical colleagues. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSalaries |>\n  group_by(discipline) |>\n  summarize(mean = mean(salary)) \n#> # A tibble: 2 x 2\n#>   discipline        mean\n#>   <fct>            <dbl>\n#> 1 A: Theoretical 108548.\n#> 2 B: Applied     118029.\n```\n:::\n\n\n\n\n\nFor both groups, there is an approximately linear relation up to about\n30--40 years, but the smoothed curves then diverge into the region\nwhere the data is thinner.\n\nThis result is more surprising than\ndifferences among faculty ranks. The effect of annotation with\nsmoothed curves as visual summaries is apparent, and provides a stimulus\nto think about _why_ these differences (if they are real) exist\nbetween theoretical and applied professors, and maybe _should_\ntheoreticians be paid more!\n\n\n### Conditioning\n\nThe previous plots use grouping by color to plot the data for different\nsubsets inside the same plot window, making comparison among groups\neasier, because they can be directly compared along a common vertical\nscale [^03-multivariate_plots-1]. This gets messy, however, when there are\nmore than just a few levels, or worse---when there are two (or more)\nvariables for which we want to show separate effects. In such cases, we\ncan plot separate panels using the `ggplot2` concept of *faceting*.\nThere are two options: `facet_wrap()` takes one or more conditioning\nvariables and produces a ribbon of plots for each combination of levels;\n`facet_grid(row ~ col)` takes two or more conditioning variables and\narranges the plots in a 2D array identified by the `row` and `col`\nvariables.\n\n[^03-multivariate_plots-1]: The classic study by\n    @ClevelandMcGill:84b;@ClevelandMcGill:85 shows that judgements of\n    magnitude along a common scale are more accurate than those along\n    separate, aligned scales.\n\nLet's look at salary broken down by the combinations of discipline and\nrank. Here, I chose to stratify using color by rank within each of\npanels faceting by discipline. Because there is more going on in this\nplot, a linear smooth is used to represent the trend.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nSalaries |>\n  ggplot(aes(x = yrs.since.phd, y = salary, \n             color = rank, shape = rank)) +\n  geom_point() +\n  scale_salary +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  geom_smooth(aes(fill = rank),\n              method = \"lm\", formula = \"y ~ x\", \n              linewidth = 2, alpha = 1/4) +\n  facet_wrap(~ discipline) +\n  legend_pos\n```\n\n::: {.cell-output-display}\n![Scatterplot of Salary vs. years since PhD, grouped by rank, with separate panels for discipline.](figs/ch03/fig-Salaries-faceted-1.pdf){#fig-Salaries-faceted fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\nOnce both of these factors are taken into account, there does not seem\nto be much impact of years of service. Salaries in theoretical\ndisciplines are noticeably greater than those in applied disciplines at\nall ranks, and there are even greater differences among ranks.\n\nFinally, to shed light on the question that motivated this example---\nare there anomalous differences in salary for men and women--- we can\nlook at differences in salary according to sex, when discipline and rank\nare taken into account. To do this graphically, condition by both\nvariables, but use `facet_grid(discipline ~ rank)` to arrange their\ncombinations in a grid whose rows are the levels of `discipline` and columns are those of `rank`. I want to make the comparison of\nmales and females most direct, so I use `color = sex` to stratify the\npanels. The smoothed regression lines and error bands are calculated\nseparately for each combination of discipline, rank and sex.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nSalaries |>\n  ggplot(aes(x = yrs.since.phd, y = salary, color = sex)) +\n  geom_point() +\n  scale_salary +\n  labs(x = \"Years since PhD\",\n       y = \"Salary\") +\n  geom_smooth(aes(fill = sex),\n              method = \"lm\", formula = \"y ~ x\",\n              linewidth = 2, alpha = 1/4) +\n  facet_grid(discipline ~ rank) +\n  theme_bw(base_size = 14) + \n  legend_pos\n```\n\n::: {.cell-output-display}\n![Scatterplot of Salary vs. years since PhD, grouped by sex, faceted by discipline and rank.](figs/ch03/fig-Salaries-facet-sex-1.pdf){#fig-Salaries-facet-sex fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\n\n## Data Ellipses {#sec-data-ellipse}\n\nThe _data ellipse_ [@Monette:90], or _concentration ellipse_ [@Dempster:69] is a\nremarkably simple and effective display for viewing and understanding\nbivariate relationships in multivariate data.\nThe data ellipse is typically used to add a visual summary to a scatterplot,\nthat shows all together the means, standard deviations, correlation,\nand slope of the regression line for\ntwo variables, perhaps stratified by another variable.\nUnder the classical assumption that the data are bivariate normally distributed,\nthe data ellipse is also a **sufficient** visual summary, in the sense that\nit captures **all** relevant features of the data.\nSee @Friendly-etal:ellipses:2013 for a complete discussion of the role of\nellipsoids in statistical data visualization.\n\nIt is based on the idea that in a bivariate normal distribution, the contours\nof equal probability form a series of concentric ellipses. If the variables were\nuncorrelated and had the same variances, these would be circles, and Euclidean\ndistance would measure the distance of each observation from the mean.\nWhen the variables are correlated, a different measure, _Mahalanobis distance_\nis the proper measure of how far a point is from the mean, taking the correlation\ninto account.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![2D data with curves of constant distance from the centroid. The blue solid ellipse shows a contour of constant Mahalanobis distance, taking the correlation into account; the dashed red circle is a contour of equal Euclidean distance. Given the data points,  Which of the points **A** and **B** is further from the mean (X)? _Source_: Re-drawn from [Ou Zhang](https://ouzhang.rbind.io/2020/11/16/outliers-part4/)](images/mahalanobis.png){#fig-mahalanobis fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\n<!--\nThis doesn't work\n#| fig-cap: !expr paste(\"2D data with curves of constant distance from the centroid. The\", colorize('blue'), \"solid ellipse shows a contour of constant Mahalanobis distance, taking the correlation into account; the dashed\", colorize('blue'), \"circle is a contour of equal Euclidean distance. Given the data points,  Which of the points **A** and **B** is further from the mean (X)? _Source_: Re-drawn from [Ou Zhang](https://ouzhang.rbind.io/2020/11/16/outliers-part4/)\")\n-->\n\nTo illustrate, @fig-mahalanobis shows a scatterplot with labels for two points, \"A\" and \"B\".\nWhich is further from the mean, \"X\"? \nA contour of constant Euclidean distance, shown by the \\textcolor{red}{red} dashed circle,\nignores the apparent negative correlation, so point \"A\" is further.\nThe \\textcolor{blue}{blue} ellipse for Mahalanobis distance \ntakes the correlation into account, so point \"B\" has a greater distance from the mean.\n\nMathematically, Euclidean (squared) distance for $p$ variables, $j = 1, 2, \\dots , p$,\nis just a generalization of\nthe square of a univariate standardized ($z$) score, $z^2 = [(y - \\bar{y}) / s]^2$,\n\n$$\nD_E^2 (\\mathbf{y}) = \\sum_j^p z_j^2 = \\mathbf{z}^\\textsf{T}  \\mathbf{z} = (\\mathbf{y} - \\bar{\\mathbf{y}})^\\textsf{T} \\operatorname{diag}(\\mathbf{S})^{-1} (\\mathbf{y} - \\bar{\\mathbf{y}}) \\; ,\n$$\nwhere $\\mathbf{S}$ is the sample variance-covariance matrix,\n$\\mathbf{S} = ({n-1})^{-1} \\sum_{i=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})^\\textsf{T} (\\mathbf{y}_i - \\bar{\\mathbf{y}})$.\n\nMahalanobis' distance takes the correlations into account simply by using the covariances\nas well as the variances,\n$$\nD_M^2 (\\mathbf{y}) = (\\mathbf{y} - \\bar{\\mathbf{y}})^\\mathsf{T} S^{-1} (\\mathbf{y} - \\bar{\\mathbf{y}}) \\; .\n$$ {#eq-Dsq}\n\nIn @eq-Dsq, the inverse $S^{-1}$ serves to \"divide\" the matrix $(\\mathbf{y} - \\bar{\\mathbf{y}})^\\mathsf{T} (\\mathbf{y} - \\bar{\\mathbf{y}})$ of squared distances \nby the variances (and covariances) of the variables, as in the univariate case.\n\nFor $p$ variables, the data _ellipsoid_ $\\mathcal{E}_c$ of\nsize $c$ is a $p$-dimensional ellipse,\ndefined as the set of points $\\mathbf{y} = (y_1, y_2, \\dots y_p)$\nwhose squared Mahalanobis distance, $D_M^2 ( \\mathbf{y} )$ is less than or equal\nto $c^2$,\n$$\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{S}) := \\{ D_M^2 (\\mathbf{y}) \\le c^2 \\} \\; .\n$$\nA computational definition recognizes that the boundary of the ellipsoid can be found by transforming\na unit sphere centered at the origin, $\\text{radius}(\\mathcal{P}) : \\{ \\mathbf{x}^\\textsf{T} \\mathbf{x}= 1\\}$, by $\\mathbf{S}^{1/2}$\nand then shifting that to centroid of the data,\n\n$$\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{S}) = \\bar{\\mathbf{y}} \\; \\oplus \\; \\mathbf{S}^{1/2} \\, \\mathcal{P} \n$$\nwhere $\\mathbf{S}^{1/2}$ represents a rotation and scaling and $\\oplus$ represents translation to a new centroid. The matrix $\\mathbf{S}^{1/2}$ is commonly computed\nas the Choleski factor of $\\mathbf{S}$. Slightly abusing notation and taking the unit sphere as given (like an identity matrix),\nwe can write the data ellipsoid as simply\n\n$$\n\\mathcal{E}_c (\\bar{\\mathbf{y}}, \\mathbf{S}) = \\bar{\\mathbf{y}} \\; \\oplus \\; \\sqrt{\\mathbf{S}} \\period\n$$\n\nWhen $\\mathbf{y}$ is (at least approximately) bivariate normal,\n$D_M^2(\\mathbf{y})$ has a large-sample $\\chi^2_2$ distribution\n($\\chi^2$ with 2 df),\nso taking $c^2 = \\chi^2_2 (0.68) = 2.28$ gives a \"1 standard deviation\nbivariate ellipse,\"\nan analog of the standard interval $\\bar{y} \\pm 1 s$, while\n$c^2 = \\chi^2_2 (0.95) = 5.99 \\approx 6$ gives a data ellipse of\n95\\% coverage.\n\n\n### Ellipse properties\n\nThe essential ideas of correlation and regression and their relation to ellipses go back to\n@Galton:1886.\nGalton's goal was to predict (or explain) how a heritable trait, $Y$, (e.g.,\nheight) of children was related to that of their parents, $X$.\nHe made a semi-graphic table of the frequencies of 928 observations of the average\nheight of father and mother versus the height of their child, shown in @fig-galton-corr.\nHe then drew smoothed contour lines of equal frequencies and had the wonderful\nvisual insight that these formed concentric shapes that were tolerably close to ellipses.\n\nHe then calculated summaries,  $\\text{Ave}(Y | X)$, and, for symmetry, $\\text{Ave}(X | Y)$, and plotted these as lines of means on his diagram. Lo and behold, he had a second visual\ninsight: the lines of means of ($Y | X$) and ($X | Y$) corresponded approximately to\nthe loci of  horizontal and vertical tangents to the concentric ellipses. \nTo complete the picture, he added lines showing the major and minor axes of the\nfamily of ellipses (which turned out to be the principal components) with the result shown in @fig-galton-corr.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Galton's 1886 diagram, showing the relationship of height of children to the average of their parents' height. The diagram is essentially an overlay of a geometrical interpretation on a bivariate grouped frequency distribution, shown as numbers.](images/galton-corr.jpg){#fig-galton-corr fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\nFor two variables, $x$ and $y$, the remarkable properties of the data ellipse are illustrated in @fig-galton-ellipse-r, a modern reconstruction of Galton's diagram.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sunflower plot of Galton's data on heights of parents and their children (in.), with 40%, 68% and 95% data ellipses and the regression lines of $y$ on $x$ (black) and $x$ on $y$ (grey).](images/galton-ellipse-r.jpg){#fig-galton-ellipse-r fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n* The ellipses have the mean vector $(\\bar{x}, \\bar{y})$ as their center.\n\n* The lengths of arms of the \\textcolor{blue}{blue} dashed central cross \nshow the standard deviations of the variables, which correspond to the shadows of the ellipse covering 40\\% of the data. These are the bivariate analogs of \nthe standard intervals $\\bar{x} \\pm 1 s_x$ and $\\bar{y} \\pm 1 s_y$.\n\n* More generally, shadows (projections) on the coordinate axes, or any linear combination of them,\ngive any standard interval, \n  $\\bar{x} \\pm k s_x$ and $\\bar{y} \\pm k s_y$.\n  Those with $k=1, 1.5, 2.45$, have\n  bivariate coverage 40%, 68% and 95% respectively, corresponding to these quantiles of the $\\chi^2$ distribution\n  with 2 degrees of freedom, i.e., \n  $\\chi^2_2 (.40) \\approx 1^2$, \n  $\\chi^2_2 (.68) \\approx 1.5^2$, and\n  $\\chi^2_2 (.95) \\approx 2.45$.\n  The shadows of the 68% ellipse are the bivariate analog of a univariate  $\\bar{x} \\pm 1 s_x$ interval.\n  \n  <!--# and univariate coverage 68\\%, 87\\% and 98.6\\% respectively. -->\n\n* The regression line predicting $y$ from $x$ goes through the points where the ellipses have vertical tangents. The _other_ regression line, predicting $x$ from $y$ goes through the points of horizontal\ntangency.\n\n* The correlation $r(x, y)$ is the ratio of the vertical segment from the mean of $y$ to the regression line to the vertical segment going to the top of the ellipse as shown at the right of the figure. It is\n$r = 0.46$ in this example.\n\n* The residual standard deviation, $s_e = \\sqrt{MSE} = \\sqrt{\\Sigma (y - \\bar{y})^2 / n-2}$, \nis the half-length of the ellipse at the mean $\\bar{x}$. \n\n\n\nBecause Galton's values of `parent` and `child` height were recorded in class intervals of 1 in.,\nthey are shown as sunflower symbols in @fig-galton-ellipse-r,\nwith multiple 'petals' reflecting the number of observations\nat each location. This plot (except for annotations) is constructed using `sunflowerplot()` and\n`car::dataEllipse()` for the ellipses.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(Galton, package = \"HistData\")\n\nsunflowerplot(parent ~ child, data=Galton, \n      xlim=c(61,75), \n      ylim=c(61,75), \n      seg.col=\"black\", \n    \txlab=\"Child height\", \n      ylab=\"Mid Parent height\")\n\ny.x <- lm(parent ~ child, data=Galton)     # regression of y on x\nabline(y.x, lwd=2)\nx.y <- lm(child ~ parent, data=Galton)     # regression of x on y\ncc <- coef(x.y)\nabline(-cc[1]/cc[2], 1/cc[2], lwd=2, col=\"gray\")\n\nwith(Galton, \n     car::dataEllipse(child, parent, \n         plot.points=FALSE, \n         levels=c(0.40, 0.68, 0.95), \n         lty=1:3)\n    )\n```\n:::\n\n\n\n\n\nFinally, as Galton noted in his diagram, the principal major and minor axes of the ellipse have important statistical properties. @Pearson:1901 would later show that\ntheir directions are determined by the eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2, \\dots$ of the covariance matrix $\\mathbf{S}$ and their radii by the\nsquare roots, $\\sqrt{\\mathbf{v}_1}, \\sqrt{\\mathbf{v}_1}, \\dots$ of the corresponding\neigenvalues.\n\n\n\n### R functions for data ellipses\n\nA number of packages provide functions for drawing data ellipses in a scatterplot, with various features.\n\n* `car::scatterplot()`: uses base R graphics to draw 2D scatterplots, with a wide variety of plot enhancements including linear and non-parametric smoothers (loess, gam), a formula method, e.g., `y ~ x | group`, and marking points and lines using symbol shape,\ncolor, etc. Importantly, the **car** package generally allows automatic identification of \"noteworthy\" points by their labels in the plot using a variety of methods. For example, `method = \"mahal\"` labels cases with the most extreme Mahalanobis distances;\n`method = \"r\"` selects points according to their value of `abs(y)`, which is\nappropriate in residual plots.\n* `car::dataEllipse()`: plots  classical or robust data (using `MASS::cov/trob()`) ellipses for one or more groups, with the same facilities for point identification.\n* `heplots::covEllipses()`: draws classical or robust data ellipses for one or more groups in a one-way design and optionally for the pooled total sample, where the focus is on homogeneity of within-group covariance matrices.\n* `ggplot2::stat_ellipse()`: uses the calculation methods of `car::dataEllipse()` to add unfilled (`geom = \"path\"`) or filled (`geom = polygon\"`) data ellipses in a `ggplot` scatterplot, using inherited aesthetics.\n\n### Example: Canadian occupational prestige {#sec-prestige}\n\nThese examples use the data on the prestige of 102 occupational categories and other measures from the\n1971 Canadian Census, recorded in `carData::Prestige`.[^prestige-src]\nOur interest is in understanding how `prestige` (the @PineoPorter2008 prestige score for an occupational category, derived from a social survey)\nis related to census measures of the average education, income, percent women of incumbents in those occupations.\nOccupation `type` is a factor with levels `\"bc\"` (blue collar), `\"wc\"` (white collar) and `\"prof\"` (professional).\n\n[^prestige-src]: The dataset was collected by Bernard Blishen, William Carroll and Catherine Moore, but apparently unpublished. A version updated to the 1981 census is described in @Blishen-etal-1987.\n\n<!-- figure-code: R/prestige.R -->\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(Prestige, package=\"carData\")\n# `type` is really an ordered factor. Make it so.\nPrestige$type <- ordered(Prestige$type,\n                         levels=c(\"bc\", \"wc\", \"prof\"))\nstr(Prestige)\n#> 'data.frame':\t102 obs. of  6 variables:\n#>  $ education: num  13.1 12.3 12.8 11.4 14.6 ...\n#>  $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...\n#>  $ women    : num  11.16 4.02 15.7 9.11 11.68 ...\n#>  $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...\n#>  $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...\n#>  $ type     : Ord.factor w/ 3 levels \"bc\"<\"wc\"<\"prof\": 3 3 3 3 3 3 3 3 3 3 ...\n```\n:::\n\n\n\n\n\nI first illustrate the relation between `income` and `prestige` in @fig-Prestige-scatterplot-income1\nusing `car::scatterplot()`\nwith many of its bells and whistles, including marginal boxplots for the variables,\nthe linear regression line, loess smooth and the 68% data ellipse.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscatterplot(prestige ~ income, data=Prestige,\n  pch = 16, cex.lab = 1.25,\n  regLine = list(col = \"red\", lwd=3),\n  smooth = list(smoother=loessLine, \n                lty.smooth = 1, lwd.smooth=3,\n                col.smooth = \"darkgreen\", \n                col.var = \"darkgreen\"),\n  ellipse = list(levels = 0.68),\n  id = list(n=4, method = \"mahal\", col=\"black\", cex=1.2))\n#> general.managers          lawyers        ministers       physicians \n#>                2               17               20               24\n```\n\n::: {.cell-output-display}\n![Scatterplot of prestige vs. income, showing the linear regression line (\\textcolor{red}{red}), the loess smooth with a confidence envelope (\\textcolor{darkgreen}{darkgreen}) and a 68% data ellipse. Points with the 4 largest $D^2$ values are labeled.](figs/ch03/fig-Prestige-scatterplot-income1-1.pdf){#fig-Prestige-scatterplot-income1 fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\nThere is a lot that can be seen here:\n\n* `income` is positively skewed, as is often the case.\n* The loess smooth, on the scale of income, shows `prestige` increasing up to $15,000 (these are 1971 incomes), and then leveling off.\n* The data ellipse, centered at the means encloses approximately 68% of the data points. It adds visual information about the correlation and precision of the linear regression; but here, the non-linear trend for higher incomes strongly suggests a different approach.\n* The four points identified by their labels are those with the largest Mahalanobis distances. `scatterplot()` prints their labels to the console.\n\n@fig-Prestige-scatterplot-educ1 shows a similar plot for education, which\nfrom the boxplot appears to be reasonably symmetric. The smoothed curve is quite\nclose to the linear regression, according to which `prestige` increases\non average \n`coef(lm(prestige ~ education, data=Prestige))[\"education\"]` =\n5.361 with each year of education.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscatterplot(prestige ~ education, data=Prestige,\n  pch = 16, cex.lab = 1.25,\n  regLine = list(col = \"red\", lwd=3),\n  smooth = list(smoother=loessLine, \n                lty.smooth = 1, lwd.smooth=3,\n                col.smooth = \"darkgreen\", \n                col.var = \"darkgreen\"),\n  ellipse = list(levels = 0.68),\n  id = list(n=4, method = \"mahal\", col=\"black\", cex=1.2))\n#>  physicians file.clerks    newsboys     farmers \n#>          24          41          53          67\n```\n\n::: {.cell-output-display}\n![Scatterplot of prestige vs. education, showing the linear regression line (\\textcolor{red}{red}), the loess smooth with a confidence envelope (\\textcolor{darkgreen}{darkgreen}) and a 68% data ellipse.](figs/ch03/fig-Prestige-scatterplot-educ1-1.pdf){#fig-Prestige-scatterplot-educ1 fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\nIn this plot, farmers, newsboys, file.clerks and physicians are identified as\nnoteworthy, for being furthest from the mean by Mahalanobis distance.\nIn relation to their typical level of education, these are mostly\nunderstandable, but it is nice that farmers are rated of higher prestige\nthan their level of education would predict.\n\nNote that the `method` argument for point identification can take a vector\nof case numbers indicating the points to be labeled. So, to\nlabel the observations with large absolute standardized residuals\nin the linear model `m`, you can use `method = which(abs(rstandard(m)) > 2)`.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(prestige ~ education, data=Prestige)\nscatterplot(prestige ~ education, data=Prestige,\n            pch = 16, cex.lab = 1.25,\n            boxplots = FALSE,\n            regLine = list(col = \"red\", lwd=3),\n            smooth = list(smoother=loessLine,\n                          lty.smooth = 1, lwd.smooth=3,\n                          col.smooth = \"black\", \n                          col.var = \"darkgreen\"),\n            ellipse = list(levels = 0.68),\n            id = list(n=4, method = which(abs(rstandard(m))>2), \n                      col=\"black\", cex=1.2)) |> invisible()\n```\n\n::: {.cell-output-display}\n![Scatterplot of prestige vs. education, labeling points whose absolute standardized residual is > 2.](figs/ch03/fig-Prestige-scatterplot-educ2-1.pdf){#fig-Prestige-scatterplot-educ2 fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n\n#### Plotting on a log scale {#sec-log-scale}\n\nA typical remedy for the non-linear relationship of income to prestige is to plot income on a log scale. This usually makes sense, and expresses a belief that a **multiple** of\nor **percentage increase** in income has a constant impact on prestige, as opposed to\nthe **additive** interpretation for income itself.\n\nFor example, the slope of the linear regression line in @fig-Prestige-scatterplot-income1\nis given by  `coef(lm(prestige ~ income, data=Prestige))[\"income\"]` = \n0.003. Multiplying this by 1000\nsays that a $1000 increase in `income` is associated with with an average\nincrease of `prestige` of 2.9.\n\nIn the plot below, `scatterplot(..., log = \"x\")` re-scales the x-axis to the\n$\\log_e()$ scale. The slope, `coef(lm(prestige ~ log(income), data=Prestige))[\"log(income)\"]` =\n21.556 says that a 1%\nincrease in salary is associated with an average change of 21.55 / 100 \nin prestige.\n\n<!-- removed: #| source-line-numbers: \"2\" -->\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscatterplot(prestige ~ income, data=Prestige,\n  log = \"x\",\n  pch = 16, cex.lab = 1.25,\n  regLine = list(col = \"red\", lwd=3),\n  smooth = list(smoother=loessLine,\n                lty.smooth = 1, lwd.smooth=3,\n                col.smooth = \"darkgreen\", col.var = \"darkgreen\"),\n  ellipse = list(levels = 0.68),\n  id = list(n=4, method = \"mahal\", col=\"black\", cex=1.2))\n#> general.managers        ministers         newsboys      babysitters \n#>                2               20               53               63\n```\n\n::: {.cell-output-display}\n![Scatterplot of prestige vs. log(income).](figs/ch03/fig-Prestige-scatterplot2-1.pdf){#fig-Prestige-scatterplot2 fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\nThe smoothed curve in @fig-Prestige-scatterplot2\nexhibits a slight tendency to bend upwards, but a linear relation is a reasonable approximation.\n\n#### Stratifying {#sec-stratifying}\n\nBefore going further, it is instructive to ask what we could see in the relationship\nbetween income and prestige if we stratified by type of occupation, fitting\nseparate regressions and smooths for blue collar, white collar and professional\nincumbents in these occupations. \n\nThe formula `prestige ~ income | type` (read: income _given_ type)\nis a natural way to specify grouping by `type`; separate linear regressions\nand smooths are calculated for each group, applying the\ncolor and point shapes specified by the `col` and `pch` arguments.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscatterplot(prestige ~ income | type, data=Prestige,\n  col = c(\"blue\", \"red\", \"darkgreen\"),\n  pch = 15:17,\n  grid = FALSE,\n  legend = list(coords=\"bottomright\"),\n  regLine = list(lwd=3),\n  smooth=list(smoother=loessLine, \n              var=FALSE, lwd.smooth=2, lty.smooth=1))\n```\n\n::: {.cell-output-display}\n![Scatterplot of prestige vs. income, stratified by occupational type. This implies a different interpretation, where occupation type is a moderator variable.](figs/ch03/fig-Prestige-scatterplot3-1.pdf){#fig-Prestige-scatterplot3 fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\nThis visual analysis offers a different interpretation of the dependence of prestige\non income, which appeared to be non-linear when occupation type was ignored.\nInstead, @fig-Prestige-scatterplot3 suggests an *interaction* of income by type.\nIn a model formula this would be expressed as one of:\n\n```r\nlm(prestige ~ income + type + income:type, data = Prestige)\nlm(prestige ~ income * type, data = Prestige)\n```\n\nThese models signify that there are different slopes (and intercepts) for the three\noccupational types. In this interpretation, `type` is a moderator variable, with a different story.\nThe slopes of the fitted lines suggest that among blue collar workers, prestige\nincreases sharply with their income. For white collar and professional workers, there is still\nan increasing relation of prestige with income, but the effect of income (slope) diminishes with\nhigher occupational category. A different plot entails a different story.\n\n\n### Example: Penguins data {#sec-penguins}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Penguin species observed in the Palmer Archipelago. This is a cartoon, but it illustrates some features of penguin body size measurements, and the colors typically used for species.  Image: Allison Horst](images/penguins-horst.png){#fig-penguin-species fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\nThe `penguins` dataset from the **palmerpenguins** package [@R-palmerpenguins] provides further instructive examples of plots and analyses of multivariate data. The data consists of measurements of body size \n(flipper length, body mass, bill length and depth)\nof 344 penguins collected at the [Palmer Research Station](https://pallter.marine.rutgers.edu/) in Antarctica.\n\nThere were three different species of penguins (Adélie, Chinstrap & Gentoo)\ncollected from 3 islands in the Palmer Archipelago\nbetween 2007--2009 [@Gorman2014]. The purpose was to examine differences in size or appearance of these species,\nparticularly differences among the sexes (sexual dimorphism) in relation to foraging and habitat.\n\nHere, I use a slightly altered version of the dataset, `peng`, renaming variables to remove the units,\nmaking factors of character variables and deleting a few cases with missing data.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(penguins, package = \"palmerpenguins\")\npeng <- penguins |>\n  rename(\n    bill_length = bill_length_mm, \n    bill_depth = bill_depth_mm, \n    flipper_length = flipper_length_mm, \n    body_mass = body_mass_g\n  ) |>\n  mutate(species = as.factor(species),\n         island = as.factor(island),\n         sex = as.factor(substr(sex,1,1))) |>\n  tidyr::drop_na()\n\nstr(peng)\n#> tibble [333 x 8] (S3: tbl_df/tbl/data.frame)\n#>  $ species       : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ island        : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#>  $ bill_length   : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n#>  $ bill_depth    : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n#>  $ flipper_length: int [1:333] 181 186 195 193 190 181 195 182 191 198 ...\n#>  $ body_mass     : int [1:333] 3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ...\n#>  $ sex           : Factor w/ 2 levels \"f\",\"m\": 2 1 1 1 2 1 2 1 2 2 ...\n#>  $ year          : int [1:333] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n```\n:::\n\n\n\n\n\nThere are quite a few variables to choose for illustrating data ellipses in scatterplots. Here I focus on\nthe measures of their bills, `bill_length` and `bill_depth` (indicating curvature) and show how to \nuse `ggplot2` for these plots.\n\nI'll be using the penguins data quite a lot, so it is useful to set up custom colors like those\nused in @fig-penguin-species, and shown in @fig-peng-colors with their color codes. These are shades of:\n\n* \\textcolor{orange}{Adelie}: \\textcolor{orange}{orange}, \n* \\textcolor{purple}{Chinstrap}: \\textcolor{purple}{purple}, and\n* \\textcolor{green}{Gentoo}: \\textcolor{green}{green}. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Color palettes used for penguin species.](images/peng-colors.png){#fig-peng-colors fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nTo use these in `ggplot2` I define a function\n`peng.colors()` that allows shades of light, medium and dark and then functions\n`scale_*_penguins()` for color and fill.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npeng.colors <- function(shade=c(\"medium\", \"light\", \"dark\")) {\n  shade = match.arg(shade)\n  #             light      medium     dark\n  oranges <- c(\"#FDBF6F\", \"#F89D38\", \"#F37A00\")  # Adelie\n  purples <- c(\"#CAB2D6\", \"#9A78B8\", \"#6A3D9A\")  # Chinstrap\n  greens <-  c(\"#B2DF8A\", \"#73C05B\", \"#33a02c\")  # Gentoo\n  \n  cols.vec <- c(oranges, purples, greens)\n  cols.mat <- \n    matrix(cols.vec, 3, 3, \n           byrow = TRUE,\n           dimnames = list(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n                           shade = c(\"light\", \"medium\", \"dark\")))\n  # get shaded colors\n  cols.mat[, shade ]\n}\n\n# define color and fill scales\nscale_fill_penguins <- function(shade=c(\"medium\", \"light\", \"dark\"), ...){\n  shade = match.arg(shade)\n  ggplot2::discrete_scale(\n    \"fill\",\"penguins\",\n     scales:::manual_pal(values = peng.colors(shade)), ...)\n}\n\nscale_colour_penguins <- function(shade=c(\"medium\", \"light\", \"dark\"), ...){\n  shade = match.arg(shade)\n  ggplot2::discrete_scale(\n    \"colour\",\"penguins\",\n    scales:::manual_pal(values = peng.colors(shade)), ...)\n}\nscale_color_penguins <- scale_colour_penguins\n```\n:::\n\n\n\n\n\nThis is used to define a `theme_penguins()` function that I use to simply change the color and fill scales\nfor plots below.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntheme_penguins <- function(shade=c(\"medium\", \"light\", \"dark\"), ...) {\n  shade = match.arg(shade)\n  list(scale_color_penguins(shade=shade),\n       scale_fill_penguins(shade=shade)\n      )\n}\n```\n:::\n\n\n\n\n\n\nAn initial plot using `ggplot2` shown in @fig-peng-ggplot1 uses color and point shape to distinguish the three penguin species. I annotate the plot of points using the linear regression lines, loess smooths to check for non-linearity\nand 95% data ellipses to show precision of the linear relation.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, shape = species, fill=species)) +\n  geom_point(size=2) +\n  geom_smooth(method = \"lm\", formula = y ~ x,\n              se=FALSE, linewidth=2) +\n  geom_smooth(method = \"loess\",  formula = y ~ x,\n              linewidth = 1.5, se = FALSE, alpha=0.1) +\n  stat_ellipse(geom = \"polygon\", level = 0.95, alpha = 0.2) +\n  theme_penguins(\"dark\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.15))\n```\n\n::: {.cell-output-display}\n![Penguin bill length and bill depth according to species.](figs/ch03/fig-peng-ggplot1-1.pdf){#fig-peng-ggplot1 fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\nOverall, the three species occupy different regions of this 2D space and for each species the relation between bill length and depth appears reasonably linear. Given this, we can suppress plotting the\ndata points to get a visual summary of the data using the fitted regression lines and data ellipses,\nas shown in @fig-peng-ggplot2. \n\nThis idea, of **visual thinning** a graph to focus on what should be seen,\nbecomes increasingly useful as the data becomes more complex. The `ggplot2` framework encourages this,\nbecause we can think of various components as layers, to be included or not.\nHere I chose to include only the regression line and\nadd data ellipses of 40%, 68% and 95% coverage to highlight the increasing bivariate \ndensity around the group means.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, shape = species, fill=species)) +\n  geom_smooth(method = \"lm\",  se=FALSE, linewidth=2) +\n  stat_ellipse(geom = \"polygon\", level = 0.95, alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", level = 0.68, alpha = 0.2) +\n  stat_ellipse(geom = \"polygon\", level = 0.40, alpha = 0.2) +\n  theme_penguins(\"dark\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.15))\n```\n\n::: {.cell-output-display}\n![**Visual thinning**: Suppressing the data points gives a visual summary of the relation between bill length and bill depth using the regression line and data ellipses.](figs/ch03/fig-peng-ggplot2-1.pdf){#fig-peng-ggplot2 fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\n#### Nonparamtric bivariate density plots\n\nWhile I emphasize data ellipses (because I like their beautiful geometry), other visual summaries of the bivariate density are possible and often useful. \n\nFor a single variable, `stats::density()` and `ggplot2::geom_density()`\ncalculate a smoothed estimate of the density using nonparametric kernel methods [@Silverman:86]\nwhose smoothness\nis controlled by a bandwidth parameter, analogous to the span in a loess smoother.\nThis idea extends to two (and more) variables [@Scott1992]. \nFor bivariate data,\n`MASS::kde2d()` estimates the density on a square $n \\times n$ grid over the\nranges of the variables.\n\n`ggplot2` provides `geom_density_2d()` which uses `MASS::kde2d()` and displays these as contours---\nhorizontal slices of the 3D surface at equally-spaced heights and projects these onto the 2D plane.\nThe **ggdensity** package [@R-ggdensity] extends this with `geom_hdr()`,\ncomputing the high density regions that bound given levels of probability\nand maps these to the `alpha` transparency aesthetic. \nA `method` argument allows you to specify various nonparametric (`method =\"kde\"` is the default)\nand parametric (`method =\"mvnorm\"` gives normal data ellipses) ways to estimate the underlying bivariate distribution.\n\n@fig-peng-ggdensity shows these side-by-side for comparison.\nWith `geom_density_2d()` you can specify either the number of contour `bins` or the\nwidth of these bins (`binwidth`). For `geom_hdr()`, the `probs` argument gives a result that\nis easier to understand.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(ggdensity)\nlibrary(patchwork)\np1 <- ggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species)) +\n  geom_smooth(method = \"lm\",  se=FALSE, linewidth=2) +\n  geom_density_2d(linewidth = 1.1, bins = 8) +\n  ggtitle(\"geom_density_2d\") +\n  theme_bw(base_size = 14) + \n  theme_penguins() +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.85, 0.15))\n\np2 <- ggplot(peng, \n       aes(x = bill_length, y = bill_depth,\n           color = species, fill = species)) +\n  geom_smooth(method = \"lm\",  se=FALSE, linewidth=2) +\n  geom_hdr(probs = c(0.95, 0.68, 0.4), show.legend = FALSE) +\n  ggtitle(\"ggdensity::geom_hdr\") +\n  theme_bw(base_size = 14) +\n  theme_penguins() +\n  theme(legend.position = \"none\")\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![**Bivariate densities** show the contours of the 3D surface representing the frequency in the joint distribution of bill length and bill depth.](figs/ch03/fig-peng-ggdensity-1.pdf){#fig-peng-ggdensity fig-align='center' fig-pos='H' width=120%}\n:::\n:::\n\n\n\n\n\n### Simpson's paradox: marginal and conditional relationships\n\nBecause it provides a visual representation of means, variances, and correlations,\nthe data ellipse is ideally suited as a tool for illustrating and\nexplicating various phenomena that occur in the analysis of linear models.\nOne class of simple, but important, examples concerns the difference between the _marginal relationship_ between variables, ignoring some important factor or covariate, and the _conditional_ relationship, adjusting (controlling) for that variable.\n\nSimpson's [-@Simpson:51] paradox  occurs when the marginal and\nconditional relationships differ in direction. That is, the overall correlation\nin a model `y ~ x` might be negative, while the within-group correlations\nin separate models for each group `y[g] ~ x[g]` might be positive, or vice versa.\n\nThis may be seen in the plots\nof bill length against bill depth for the penguin data shown in @fig-peng-simpsons. Ignoring penguin species, the marginal, total-sample correlation is slightly negative\nas seen in panel (a). The individual-sample ellipses in panel (b) show\nthat the conditional, within-species correlations are all positive, with\napproximately equal regression slopes.  However the group means have a negative\nrelationship, accounting for the negative marginal correlation when species is ignored. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Marginal (a), conditional (b), and pooled within-sample (c) relationships of bill length and depth in the Penguins data. Each plot shows the 68% data ellipse and regression line(s) with 95% confidence bands.](images/peng-simpsons.png){#fig-peng-simpsons fig-align='center' width=120%}\n:::\n:::\n\n\n\n\n\nThe regression line in panel (a) is that for the linear model\n`lm(bill_depth ~ bill_length)`, while the separate lines in panel (b)\nare those for the model `lm(bill_depth ~ bill_length * species)` which\nallows a different slope and intercept for each species.\n\nA correct analysis of the (conditional) relationship between these variables, controlling or adjusting for mean\ndifferences among species, is based on the pooled within-sample covariance matrix,\na weighted average of the individual within-group $\\mat{S}_i$,\n$$\n\\mat{S}_{\\textrm{within}}  =\n\\sum_{i=1}^g\n(n_i - 1) \\mat{S}_i \\, / \\, (N - g)\n\\comma\n$$\nwhere $N = \\sum n_i$. The result is shown in\npanel (c) of @fig-peng-simpsons.\n\nIn this graph, the data for each species were first transformed to deviations from the species means on both variables and then translated back to the grand means.\nYou can also see here that the shapes and sizes of the individual data ellipses are roughly comparable, but perhaps not identical. This visual idea of centering groups to a common mean will become\nimportant in @sec-eqcov when we want to test the assumption of equality of\nerror covariances in multivariate models.\n\nThe `ggplot2` code for the panels in this figure are shown below. Note that for components that will be the same across panels, you can define elements\n(e.g., `labels`, `theme_penguins()`, `legend_position`) once, and then re-use\nthese across several graphs.\n\n::: {.panel-tabset}\n\n## (a) Ignoring species\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlabels <- labs(\n  x = \"Bill length (mm)\",\n  y = \"Bill depth (mm)\",\n  color = \"Species\",\n  shape = \"Species\",\n  fill = \"Species\") \n\nplt1 <- ggplot(data = peng,\n               aes(x = bill_length,\n                   y = bill_depth)) +\n  geom_point(size = 1.5) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              se = TRUE, color = \"gray50\") +\n  stat_ellipse(level = 0.68, linewidth = 1.1) +\n  ggtitle(\"Ignoring species\") +\n  labels\n\nplt1\n```\n:::\n\n\n\n\n\n## (b) By species\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlegend_position <-\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.83, 0.16))\n\nplt2 <- ggplot(data = peng,\n               aes(x = bill_length,\n                   y = bill_depth,\n                   color = species,\n                   shape = species,\n                   fill = species)) +\n  geom_point(size = 1.5,\n             alpha = 0.8) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              se = TRUE, alpha = 0.3) +\n  stat_ellipse(level = 0.68, linewidth = 1.1) +\n  ggtitle(\"By species\") +\n  labels +\n  theme_penguins(\"dark\") +\n  legend_position \n\nplt2\n```\n:::\n\n\n\n\n\n## (c) Within species\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# center within groups, translate to grand means\nmeans <- colMeans(peng[, 3:4])\npeng.centered <- peng |>\n  group_by(species) |>\n  mutate(bill_length = means[1] + scale(bill_length, scale = FALSE),\n         bill_depth  = means[2] + scale(bill_depth, scale = FALSE))\n\nplt3 <- ggplot(data = peng.centered,\n               aes(x = bill_length,\n                   y = bill_depth,\n                   color = species,\n                   shape = species,\n                   fill = species)) +\n  geom_point(size = 1.5,\n             alpha = 0.8) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              se = TRUE, alpha = 0.3) +\n  stat_ellipse(level = 0.68, linewidth = 1.1) +\n  labels +\n  ggtitle(\"Within species\") +\n  theme_penguins(\"dark\") +\n  legend_position \n\nplt3\n```\n:::\n\n\n\n\n\n:::\n\n\n## Scatterplot matrices {#sec-scatmat}\n\nGoing beyond bivariate scatterplots, a *pairs* plot (or *scatterplot\nmatrix*) displays all possible $p \\times p$ pairs of $p$ variables in a\nmatrix-like display where variables $(x_i, x_j)$ are shown in a plot for\nrow $i$, column $j$. This idea, due to @Hartigan:75b, uses small\nmultiple plots, so that the eye can easily scan across a row or down a\ncolumn to see how a given variable is related to all the others.\n\nThe most basic version is provided by `pairs()` in base R. When one\nvariable is considered as an outcome or response, it is usually helpful\nto put this in the first row and column. For the `Prestige` data, in\naddition to income and education, we also have a measure of % women in\neach occupational category.\n\nPlotting these together gives @fig-prestige-pairs. In such plots, the\ndiagonal cells give labels for the variables, but they are also a guide\nto interpreting what is shown. In each row, say row 2 for `income`,\nincome is the vertical $y$ variable in plots against other variables. In\neach column, say column 3 for `education`, education is the horizontal\n$x$ variable.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npairs(~ prestige + income + education + women,\n      data=Prestige)\n```\n\n::: {.cell-output-display}\n![Scatterplot matrix of the variables in the Prestige dataset produced by `pairs()`](figs/ch03/fig-prestige-pairs-1.pdf){#fig-prestige-pairs fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\nThe plots in the first row show what we have seen before for the\nrelations between prestige and income and education, adding to those the\nplot of prestige vs. % women. Plots in the first column show the same\ndata, but with $x$ and $y$ interchanged.\n\nBut this basic `pairs()` plot is very limited. A more feature-rich\nversion is provided by `car::scatterplotMatrix()` which can add the\nregression lines, loess smooths and data ellipses for each pair, as\nshown in @fig-prestige-spm1.\n\nThe diagonal panels show density curves for the distribution of each\nvariable; for example, the distribution of `education` appears to be\nmulti-modal and that of `women` shows that most of the occupations have\na low percentage of women.\n\nThe combination of the regression line with the loess smoothed curve,\nbut without their confidence envelopes, provides about the right amount\nof detail to take in at a glance where the relations are non-linear.\nWe've already seen (@fig-Prestige-scatterplot-income1) the non-linear\nrelation between prestige and income (row 1, column 2) when occupational\ntype is ignored. But all relations with income in column 2 are\nnon-linear, reinforcing our idea (@sec-log-scale) that effects of income\nshould be assessed on a log scale.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscatterplotMatrix(~ prestige + income + education + women,\n  data=Prestige,\n  regLine = list(method=lm, lty=1, lwd=2, col=\"black\"),\n  smooth=list(smoother=loessLine, spread=FALSE,\n              lty.smooth=1, lwd.smooth=3, col.smooth=\"red\"),\n  ellipse=list(levels=0.68, fill.alpha=0.1))\n```\n\n::: {.cell-output-display}\n![Scatterplot matrix of the variables in the Prestige dataset from `car::scatterplotMatrix()`.](figs/ch03/fig-prestige-spm1-1.pdf){#fig-prestige-spm1 fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\n`scatterplotMatrix()` can also label points using the `id =` argument\n(though this can get messy) and can stratify the observations by a\ngrouping variable with different symbols and colors. For example,\n@fig-prestige-spm2 uses the syntax\n`~ prestige + education + income + women | type` to provide separate\nregression lines, smoothed curves and data ellipses for the three types\nof occupations. (The default colors are somewhat garish, so I use\n`scales::hue_pal()` to mimic the discrete color scale used in\n`ggplot2`).\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscatterplotMatrix(~ prestige + income + education + women | type,\n  data = Prestige,\n  col = scales::hue_pal()(3),\n  pch = 15:17,\n  smooth=list(smoother=loessLine, spread=FALSE,\n              lty.smooth=1, lwd.smooth=3, col.smooth=\"black\"),\n  ellipse=list(levels=0.68, fill.alpha=0.1))\n```\n\n::: {.cell-output-display}\n![Scatterplot matrix of the variables in the Prestige dataset from `car::scatterplotMatrix()`, stratified by type of occupation.](figs/ch03/fig-prestige-spm2-1.pdf){#fig-prestige-spm2 fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\nIt is now easy to see why education is multi-modal: blue collar, white\ncollar and professional occupations have largely non-overlapping years\nof education. As well, the distribution of % women is much higher in the\nwhite collar category.\n\nFor the `penguins` data, given what we've seen before in\n@fig-peng-ggplot1 and @fig-peng-ggplot2, we may wish to suppress details\nof the points (`plot.points = FALSE`) and loess smooths\n(`smooth = FALSE`) to focus attention on the similarity of regression\nlines and data ellipses for the three penguin species. In @fig-peng-spm,\nI've chosen to show boxplots rather than density curves in the diagonal\npanels in order to highlight differences in the means and interquartile\nranges of the species, and to show 68% and 95% data ellipses in the\noff-diagonal panels.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscatterplotMatrix(~ bill_length + bill_depth + flipper_length + body_mass | species,\n  data = peng, \n  col = peng.colors(\"medium\"), \n  legend=FALSE,\n  ellipse = list(levels = c(0.68, 0.95), \n                 fill.alpha = 0.1),\n  regLine = list(lwd=3),\n  diagonal = list(method = \"boxplot\"),\n  smooth = FALSE,\n  plot.points = FALSE,\n  cex.labels=1) \n```\n\n::: {.cell-output-display}\n![Scatterplot matrix of the variables in the penguins dataset, stratified by species.](figs/ch03/fig-peng-spm-1.pdf){#fig-peng-spm fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n<!--# I added cex.labels=1 in the code above because the diag labels were on top of the boxplots, I think it looks a bit better now, but still slightly obscured-->\nIt can be seen that the species are widely separated in most of the\nbivariate plots. As well, the regression lines for species have similar\nslopes and the data ellipses have similar size and shape in most of the\nplots. From the boxplots, we can also see that\n\\textcolor{orange}{Adelie} penguins have shorter bill lengths than\nthe others, while \\textcolor{green}{Gentoo} penguins have smaller\nbill depth, but longer flippers and are heavier than\n\\textcolor{purple}{Chinstrap} and \\textcolor{orange}{Adelie}\npenguins.\n\n::: {.callout-note title=\"Looking ahead\"}\n\n@fig-peng-spm provides a reasonably complete visual summary of the data\nin relation to multivariate models that ask \"do the species differ in\ntheir means on these body size measures?\" This corresponds to the MANOVA\nmodel,\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npeng.mod <- lm(cbind(bill_length, bill_depth, flipper_length, body_mass) ~ species, \n               data=peng)\n```\n:::\n\n\n\n\n\nHypothesis-error (HE) plots, described in @sec-vis-mlm provide a better\nsummary of the evidence for the MANOVA test of differences among means\non all variables together. These give an $\\mathbf{H}$ ellipse reflecting\nthe differences among means, to be compared with an $\\mathbf{E}$ ellipse\nreflecting within-group variation and a visual test of significance.\n\nA related question is \"how well are the penguin species distinguished by\nthese body size measures?\" Here, the relevant model is linear\ndiscriminant analysis (LDA), where `species` plays the role of the\nresponse in the model,\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npeng.lda <- MASS:lda( species ~ cbind(bill_length, bill_depth, flipper_length, body_mass), \n               data=peng)\n```\n:::\n\n\n\n\n\nBoth MANOVA and LDA depend on the assumption that the variances and\ncorrelations between the variables are the same for all groups. This assumption can be\ntested and visualized using the methods in @sec-eqcov.\n:::\n\n### Visual thinning\n\nWhat can you do if there are even more variables than in these examples?\nIf what you want is a high-level, zoomed-out display summarizing the\npairwise relations more strongly, you can apply the idea of visual\nthinning to show only the most important features.\n\nThis example uses data on the rate of various crimes in the 50 U.S.\nstates from the United States Statistical Abstracts, 1970, used by\n@Hartigan:75 and @Friendly:91. These are ordered in the dataset roughly\nby seriousness of crime or from crimes of violence to property crimes.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(crime, package = \"ggbiplot\")\nstr(crime)\n#> 'data.frame':\t50 obs. of  10 variables:\n#>  $ state   : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n#>  $ murder  : num  14.2 10.8 9.5 8.8 11.5 6.3 4.2 6 10.2 11.7 ...\n#>  $ rape    : num  25.2 51.6 34.2 27.6 49.4 42 16.8 24.9 39.6 31.1 ...\n#>  $ robbery : num  96.8 96.8 138.2 83.2 287 ...\n#>  $ assault : num  278 284 312 203 358 ...\n#>  $ burglary: num  1136 1332 2346 973 2139 ...\n#>  $ larceny : num  1882 3370 4467 1862 3500 ...\n#>  $ auto    : num  281 753 440 183 664 ...\n#>  $ st      : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n#>  $ region  : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n```\n:::\n\n\n\n\n\n<!-- **TODO**: This dataset is actually `data(crime, package = \"ggbiplot\")` but this depends on my new version, not yet on CRAN. -->\n\n@fig-crime-spm displays the scatterplot matrix for these seven\nvariables, using only the regression line and data ellipse to show the\nlinear relation and the loess smooth to show potential non-linearity.<!--# I believe you used non-linear, hyphenated earlier so I added the - --> To\nmake this even more schematic, the axis tick marks and labels are also\nremoved using the `par()` settings `xaxt = \"n\", yaxt = \"n\"`.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncrime |>\n  select(where(is.numeric)) |>\n  scatterplotMatrix(\n    plot.points = FALSE,\n    ellipse = list(levels = 0.68, fill=FALSE),\n    smooth = list(spread = FALSE, \n                  lwd.smooth=2, lty.smooth = 1, col.smooth = \"red\"),\n    cex.labels = 2,\n    xaxt = \"n\", yaxt = \"n\")\n```\n\n::: {.cell-output-display}\n![**Visual thinning**: Scatterplot matrix of the crime data, showing only high-level summaries of the linear and nonlinear relations betgween each pair of variables.](figs/ch03/fig-crime-spm-1.pdf){#fig-crime-spm fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\nWe can see that all pairwise correlations are positive, pairs closer to\nthe main diagonal tend to be more highly correlated and in most cases\nthe nonparametric smooth doesn't differ much from the linear regression\nline. Exceptions to this appear mainly in the columns for `robbery` and\n`auto` (auto theft).\n\n### Corrgrams {#sec-corrgram}\n\nWhat if you want to summarize the data even further, for example to show\nonly the value of the correlation for each pair of variables? A\n**corrgram** [@Friendly:02:corrgram] is a visual display of a\ncorrelation matrix, where the correlation can be rendered in a variety\nof ways to show the direction and magnitude: circular \"pac-man\" (or pie)\nsymbols, ellipses, colored vars or shaded rectangles, as shown in\n@fig-corrgram-renderings.\n\nAnother aspect is that of **effect ordering** [@FriendlyKwan:03:effect],\nordering the levels of factors and variables in graphic displays to make\nimportant features most apparent. For variables, this means that we can\narrange the variables in a matrix-like display in such a way as to make\nthe pattern of relationships easiest to see. Methods to achieve this\ninclude using principal components and cluster analysis to put the most\nrelated variables together as described in @sec-pca-biplot.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![**Corrgrams**: Some renderings for the value of a correlation in a corrgram display, conveying sign and magnitude in different ways.](images/corrgram-renderings.png){#fig-corrgram-renderings fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\nIn R, these diagrams can be created using the **corrgram** [@R-corrgram]\nand **corrplot** [@R-corrplot] packages, with different features.\n`corrgram::corrgram()` is closest to @Friendly:02:corrgram, in that it\nallows different rendering functions for the lower, upper and diagonal\npanels as illustrated in @fig-corrgram-renderings. For example, a\ncorrgram similar to @fig-crime-spm can be produced as follows (not shown\nhere):\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncrime |>\n  select(where(is.numeric)) |>\n  corrgram(lower.panel = panel.ellipse,\n           upper.panel = panel.ellipse,\n           diag.panel = panel.density)\n```\n:::\n\n\n\n\n\n`corrplot::corrplot()` provides the rendering methods\n`c(\"circle\", \"square\", \"ellipse\", \"number\", \"shade\", \"color\", \"pie\")`,\nbut only one can be used at a time. The function\n`corrplot::corrplot.mixed()` allows different options to be selected for\nthe lower and upper triangles. The iconic shape is colored with a\ngradient in relation to the correlation value.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncrime |>\n  select(where(is.numeric)) |>\n  cor() |>\n  corrplot.mixed(\n           lower = \"ellipse\",\n           upper = \"pie\",\n           tl.col = \"black\",\n           tl.srt = 0,\n           addCoef.col = \"black\",\n           addCoefasPercent = TRUE)\n```\n\n::: {.cell-output-display}\n![Corrplot of the `crime` data, showing the correlation between each pair of variables with an ellipse (lower) and a pie chart symbol (upper), all shaded in proportion to the correlation value, also shown numerically.](figs/ch03/fig-crime-corrplot-1.pdf){#fig-crime-corrplot fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\nThe combination of renderings shown in @fig-crime-corrplot is\ninstructive. Small differences among correlation values are easier to\nsee with the pie symbols than with the ellipses; for example, compare\nthe values for murder with larceny and auto theft in row 1, columns 6-7\nwith those in column 1, rows 6-7, where the former are easier to\ndistinguish. The shading color adds another visual cue.\n\nVariations of corrgrams are worthy replacements for a numeric table of\ncorrelations, which are often presented in publications only for\narchival value. Including the numeric value (rounded here, for\npresentation purposes), makes this an attractive alternative to boring\ntables of correlations.\n\n**TODO**: Add example showing correlation ordering -- e.g., `mtcars`\ndata.<!--# How's something like the code I added below? -->\n\n\n\n\n\n\n\n\n\n\n\n## Generalized pairs plots {#sec-ggpairs}\n\nWhen a dataset contains one or more discrete variables, the traditional\npairs plot cannot cope, using only color and/or point symbols to\nrepresent categorical variables. In the context of mosaic displays and\nloglinear models, representing $n$-way frequency tables by rectangular\ntiles depicting cell frequencies, I [@Friendly:94a] proposed an analog\nof the scatterplot matrix using mosaic plots for each pair of variables.\nThe **vcd** package [@R-vcd] implements very general `pairs()` methods\nfor `\"table\"` objects. See my book *Discrete Data Analysis with R*\n[@FriendlyMeyer:2016:DDAR] and the **vcdExtra** [@R-vcdExtra] package\nfor mosaic plots and mosaic matrices.\n\nFor example, we can tabulate the distributions of penguin species by sex\nand the island where they were observed using `xtabs()`. `ftable()`\nprints this three-way table more compactly. (In this example, and what\nfollows in the chapter, I've changed the labels for sex from (\"f\", \"m\")\nto (\"Female\", \"Male\")).\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# use better labels for sex\npeng <- peng |>\n  mutate(sex = factor(sex, labels = c(\"Female\", \"Male\")))\npeng.table <- xtabs(~ species + sex + island, data = peng)\n\nftable(peng.table)\n#>                  island Biscoe Dream Torgersen\n#> species   sex                                 \n#> Adelie    Female            22    27        24\n#>           Male              22    28        23\n#> Chinstrap Female             0    34         0\n#>           Male               0    34         0\n#> Gentoo    Female            58     0         0\n#>           Male              61     0         0\n```\n:::\n\n\n\n\n\nWe can see immediately that the penguin species differ by island: only\nAdelie were observed on all three islands; Biscoe Island had no\nChinstraps and Dream Island had no Gentoos.\n\n`vcd::pairs()` produces all pairwise mosaic plots, as shown in\n@fig-peng-mosaic. The diagonal panels show the one-way frequencies by\nwidth of the divided bars. Each off-diagonal panel shows the bivariate\ncounts, breaking down each column variable by splitting the bars in\nproportion to a second variable. Consequently, the frequency of each\ncell is represented by its' area. The purpose is to show the **pattern\nof association** between each pair, and so, the tiles in the mosaic are\nshaded according to the signed standardized residual,\n$d_{ij} = (n_{ij} - \\hat{n}_{ij}) / \\sqrt{\\hat{n}_{ij}}$ in a simple\n$\\chi^2 = \\Sigma_{ij} \\; d_{ij}^2$ test for association---\n\\textcolor{blue}{blue} where the observed frequency $n_{ij}$ is\nsignificantly greater than expected $\\hat{n}_{ij}$ under independence,\nand \\textcolor{red}{red} where it is less than expected. The tiles\nare unshaded when $| d_{ij} | < 2$.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(vcd)\npairs(peng.table, shade = TRUE,\n      lower_panel_args = list(labeling = labeling_values()),\n      upper_panel_args = list(labeling = labeling_values()))\n```\n\n::: {.cell-output-display}\n![Mosaic pairs plot for the combinations of species, sex and island. Diagnonal plots show the marginal frequency of each variable by the width of each rectangle. Off-diagonal mosaic plots subdivide by the conditional frequency of the second variable, shown numerically in the tiles. ](figs/ch03/fig-peng-mosaic-1.pdf){#fig-peng-mosaic fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\nThe shading patterns in cells (1,3) and (3,1) of @fig-peng-mosaic show\nwhat we've seen before in the table of frequencies: The distribution of\nspecies varies across island because on each island one or more species\ndid not occur. Row 2 and column 2 show that sex is nearly exactly\nproportional among species and islands, indicating independence,\n$\\text{sex} \\perp \\{\\text{species}, \\text{island}\\}$. More importantly,\nmosaic pairs plots can show, at a glance, all (bivariate) associations\namong multivariate categorical variables.\n\nThe next step, by John Emerson and others [@Emerson-etal:2013] was to\nrecognize that combinations of continuous and discrete, categorical\nvariables could be plotted in different ways.\n\n-   Two continuous variables can be shown as a standard scatterplot of\n    points and/or bivariate density contours, or simply by numeric\n    summaries such as a correlation value;\n-   A pair of one continuous and one categorical variable can be shown\n    as side-by-side boxplots or violin plots, histograms or density\n    plots;\n-   Two categorical variables could be shown in a mosaic plot or by\n    grouped bar plots.\n\nIn the **ggplot2** framework, these displays are implemented using the `ggpairs()` function from the **GGally** package [@R-GGally]. This allows different plot types to be shown in the lower and upper triangles and in\nthe diagonal cells of the plot matrix. As well, aesthetics such as color\nand shape can be used within the plots to distinguish groups directly.\nAs illustrated below, you can define custom functions to control exactly\nwhat is plotted in any panel.\n\nThe basic, default plot shows scatterplots for pairs of continuous\nvariables in the lower triangle and the values of correlations in the\nupper triangle. A combination of a discrete and continuous variables is\nplotted as histograms in the lower triangle and boxplots in the upper\ntriangle. @fig-peng-ggpairs1 includes `sex` to illustrate the\ncombinations.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nggpairs(peng, columns=c(3:6, 7),\n        aes(color=species, alpha=0.5),\n        progress = FALSE) +\n  theme_penguins() +\n  theme(axis.text.x = element_text(angle = -45))\n```\n\n::: {.cell-output-display}\n![Basic `ggpairs()` plot of penguin size variables and sex, stratified by species.](figs/ch03/fig-peng-ggpairs1-1.pdf){#fig-peng-ggpairs1 fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\n<!--# I changed the x-axis labels' sizes because they overlapped when rendered -->\n\nTo my eye, printing the values of correlations in the upper triangle is\noften a waste of graphic space. But this example shows something\npeculiar and interesting if you look closely: In all pairs among the\npenguin size measurements, there are positive correlations within each\nspecies, as we can see in @fig-peng-spm. Yet, in three of these panels,\nthe overall correlation ignoring species is negative. For example, the\noverall correlation between bill depth and flipper length is\n$r = -0.579$ in row 2, column 3; the scatterplot in the diagonally\nopposite cell, row 3, column 2 shows the data. These cases, of differing\nsigns for an overall correlation, ignoring a group variable and the\nwithin group correlations are examples of **Simpson's Paradox**,\nexplored later in Chapter XX. <!--# TODO: add chapter number when known -->\n\nThe last row and column, for `sex` in @fig-peng-ggpairs1, provides an\ninitial glance at the issue of sex differences among penguin species\nthat motivated the collection of these data. We can go further by also\nexamining differences among species and island, but first we need to\nunderstand how to display exactly what we want for each pairwise plot.\n\n`ggpairs()` is extremely general in that for each of the `lower`,\n`upper` and `diag` sections you can assign any of a large number of\nbuilt-in functions (of the form `ggally_NAME`), or your own custom\nfunction for what is plotted, depending on the types of variables in\neach plot.\n\n-   `continuous`: both X and Y are continuous variables, supply this as\n    the `NAME` part of a `ggally_NAME()` function or the name of a\n    custom function;\n-   `combo`: one X of and Y variable is discrete while the other is\n    continuous, using the same convention;\n-   `discrete`: both X and Y are discrete variables.\n\nThe defaults, which were used in @fig-peng-ggpairs1, are:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nupper = list(continuous = \"cor\",          # correlation values\n             combo = \"box_no_facet\",      # boxplots \n             discrete = \"count\")          # rectangles ~ count\nlower = list(continuous = \"points\",       # just data points\n             combo = \"facethist\",         # faceted histograms\n             discrete = \"facetbar\")       # faceted bar plots\ndiag  = list(continuous = \"densityDiag\",  # density plots\n             discrete = \"barDiag\")        # bar plots\n```\n:::\n\n\n\n\n\nThus, `ggpairs()` uses `ggally_cor()` to print the correlation values\nfor pairs of continuous variables in the upper triangle, and uses\n`ggally_points()` to plot scatterplots of points in the lower portion.\nThe diagonal panels as shown as density plots (`ggally_densityDiag()`)\nfor continuous variables but as bar plots (`ggally_barDiag()`) for\ndiscrete factors.\n\nSee the vignette,\n[ggally_plots](https://ggobi.github.io/ggally/articles/ggally_plots.html)\nfor an illustrated list of available high-level plots. For our purpose\nhere, which is to illustrate enhanced displays, note that for\nscatterplots of continuous variables, there are two functions which plot\nthe points and also add a smoother, `_lm` or `_loess`.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nls(getNamespace(\"GGally\")) |> \n  stringr::str_subset(\"^ggally_smooth_\")\n#> [1] \"ggally_smooth_lm\"    \"ggally_smooth_loess\"\n```\n:::\n\n\n\n\n\nA customized display for scatterplots of continuous variables can be any\nfunction that takes `data` and `mapping` arguments and returns a\n`\"ggplot\"` object. The `mapping` argument supplies the aesthetics, e.g.,\n`aes(color=species, alpha=0.5)`, but only if you wish to override what\nis supplied in the `ggpairs()` call.\n\nHere is a function, `my_panel()` that plots the data points,\nregression line and loess smooth:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmy_panel <- function(data, mapping, ...){\n  p <- ggplot(data = data, mapping = mapping) + \n    geom_point() + \n    geom_smooth(method=lm, formula = y ~ x, se = FALSE, ...) +\n    geom_smooth(method=loess, formula = y ~ x, se = FALSE, ...)\n  p\n}\n```\n:::\n\n\n\n\n\nFor this example, I want only simple summaries of for the scatterplots, so\nI don't want to plot the data points, but do want to add the regression\nline and a data ellipse.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmy_panel1 <- function(data, mapping, ...){\n  p <- ggplot(data = data, mapping = mapping) + \n     geom_smooth(method=lm, formula = y ~ x, se = FALSE, ...) +\n     stat_ellipse(geom = \"polygon\", level = 0.68, ...)\n  p\n}\n```\n:::\n\n\n\n\n\nThen, to show what can be done, @fig-peng-ggpairs7 uses `my_panel1()`\nfor the scatterplots in the 4 x 4 block of plots in the upper left. The\ncombination of the continuous body size measures and the discrete\nfactors `species`, `island` and `sex` are shown in upper triangle by\nboxplots but by faceted histograms in the lower portion. The factors are\nshown as rectangles with area proportional to count (poor-man's mosaic\nplots) above the diagonal and as faceted bar plots below.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nggpairs(peng, columns=c(3:6, 1, 2, 7),\n        mapping = aes(color=species, fill = species, alpha=0.2),\n        lower = list(continuous = my_panel1),\n        upper = list(continuous = my_panel1),\n        progress = FALSE) +\n  theme_penguins() +\n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank()) + \n  theme(axis.text.x = element_text(angle = -45))\n```\n\n::: {.cell-output-display}\n![Customized `ggpairs()` plot of penguin size variables, together with species, island and sex.](figs/ch03/fig-peng-ggpairs7-1.pdf){#fig-peng-ggpairs7 fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\n<!--# I changed the x-axis labels' sizes and angle because they overlapped when rendered -->\n\nThere is certainly a lot going on in @fig-peng-ggpairs7, but it does\nshow a high-level overview of all the variables (except `year`) in the\npenguins dataset.\n\n## Parallel coordinate plots {#sec-parcoord}\n\nAs we have seen above, scatterplot matrices and generalized pairs plots\nextend data visualization to multivariate data, but these variables\nshare one 2D space, so resolution decreases as the number of variable\nincrease. You need a very large screen or sheet of paper to see more\nthan, say 5-6 variables with any clarity.\n\nParallel coordinate plots are an attractive alternative, with which we\ncan visualize an arbitrary number of variables to get a visual summary\nof a potentially high-dimensional dataset, and perhaps recognize\noutliers and clusters in the data in a different way. In these plots,\neach variable is shown on a separate, parallel axis. A multivariate\nobservation is then plotted by connecting their respective values on\neach axis with lines across all the axes.\n\nThe geometry of parallel coordinates is interesting, because what is a\npoint in $n$-dimensional (Euclidean) *data* space becomes a line in the\n*projective* parallel coordinate space with $n$ axes, and vice-versa:\nlines in parallel coordinate space correspond to points in data space.\nThus, a collection of points in data space map to lines that intersect\nin a point in projective space. What this does is to map $n$-dimensional\nrelations into 2D patterns we can see in a parallel coordinates plot.\n\n::: {.callout-note title=\"History Corner\"}\n> Those who don't know history are doomed to plagarize it ---The author\n\nThe theory of projective geometry originated with the French\nmathematician Maurice d'Ocagne [-@Ocagne:1885] who sought a way to\nprovide graphic calculation of mathematical functions with alignment\ndiagrams or *nomograms* using parallel axes with different scales. A\nthree-variable equation, for example, could be solved using three\nparallel axes, where known values could be marked on their scales, a\nline drawn between them, and an unknown read on its scale at the point\nwhere the line intersects that scale.\n\nHenry Gannet (1880), in work preceding the *Statistical Atlas of the\nUnited States* for the 1890 Census [@Gannett:1898], is widely credited\nwith being the first to use parallel coordinates plots to show data, in\nhis case, to show the [rank ordering of US\nstates](https://www.davidrumsey.com/luna/servlet/detail/RUMSEY~8~1~32803~1152181)\nby 10 measures including population, occupations, wealth, manufacturing,\nagriculture and so on.\n\nHowever, both d'Ocagne and Gannet were far preceded in this by\nAndre-Michel Guerry [-@Guerry:1833] who used this method to show how the\nrank order of various crimes changed with age of the accused. See\n@Friendly2022, Figure 7 for his version and for an appreciation of the\nremarkable contributions of this amateur statistician to the history of\ndata visualization.\n\n<!-- **TODO**: Revise the _History_ section of the Wikipedia page for [Parallel coordinates](https://en.wikipedia.org/wiki/Parallel_coordinates). -->\n\nThe use of parallel coordinates for display of multidimensional data was\nrediscovered by Alfred Inselberg [-@Inselberg:1985] and extended by\nEdward Wegman [-@Wegman:1990], neither of whom recognized the earlier\nhistory. Somewhat earlier, David Andrews [-@Andrews:72] proposed mapping\nmultivariate observations to smooth Fourrier functions composed of\nalternating $\\sin()$ and $\\cos()$ terms. And in my book, *SAS System for\nStatistical Graphics* [@Friendly:91], I implemented what I called\n[*profile\nplots*](https://blogs.sas.com/content/iml/2022/11/14/profile-plots-sas.html)\nwithout knowing their earlier history as parallel coordinate plots.\n:::\n\nParallel coordinate plots present a challenge for graphic developers, in\nthat they require a different way to think about plot construction for\nmultiple variables, which can be quantitative, as in the original idea,\nor categorical factors, all to be shown along parallel axes.\n\nHere, I use the **ggpcp** package [@R-ggpcp], best described in\n@VanderPlas2023, who also review the modern history. This takes some\ngetting used to, because they develop `pcp_*()` extensions of the\n`ggplot2` grammar of graphics framework to allow:\n\n-   `pcp_select()`: selections of the variables to be plotted and their\n    horizontal order on parallel axes,\n-   `pcp_scale()`: methods for scaling of the variables to each axis,\n-   `pcp_arrange()`: methods for breaking ties in factor variables to\n    space them out.\n\nThen, it provides `geom_pcp_*()` functions to control the display of\naxes with appropriate aesthetics, labels for categorical factors and so\nforth. @fig-peng-ggpcp1 illustrates this type of display, using sex and\nspecies in addition to the quantitative variables for the penguin data.\n\n<!-- WARN: 03-multivariate_plots.html: Unable to resolve crossref @fig-peng-ggpcp1 \n     Does this have something to do with cache?\n--> \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\npeng |>\n  pcp_select(bill_length:body_mass, sex, species) |>\n  pcp_scale(method = \"uniminmax\") |>\n  pcp_arrange() |>\n  ggplot(aes_pcp()) +\n  geom_pcp_axes() +\n  geom_pcp(aes(colour = species), alpha = 0.8, overplot = \"none\") +\n  geom_pcp_labels() +\n  scale_colour_manual(values = peng.colors()) +\n  labs(x = \"\", y = \"\") +\n  theme(axis.title.y = element_blank(), axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(), legend.position = \"none\")\n```\n:::\n\n\n\n\n<!-- cHEATING HERE because ggcpc plots take so long -->\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figs/fig-peng-ggpcp1-1.png){fig-align='center' width=18in}\n:::\n:::\n\n\n\n\n\n\nRearranging the order of variables and the ordering of factor levels can\nmake a difference in what we can see in such plots. For a simple example\n(following @VanderPlas2023), we reorder the levels of species and\nislands to make it clearer which species occur on each island.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\npeng1 <- peng |>\n  mutate(species = factor(species, levels = c(\"Chinstrap\", \"Adelie\", \"Gentoo\"))) |>\n  mutate(island = factor(island, levels = c(\"Dream\", \"Torgersen\", \"Biscoe\")))\n\npeng1 |>\n  pcp_select(species, island, bill_length:body_mass) |>\n  pcp_scale() |>\n  pcp_arrange(method = \"from-left\") |>\n  ggplot(aes_pcp()) +\n  geom_pcp_axes() +\n  geom_pcp(aes(colour = species), alpha = 0.6, overplot = \"none\") +\n  geom_pcp_boxes(fill = \"white\", alpha = 0.5) +\n  geom_pcp_labels() +\n  scale_colour_manual(values = peng.colors()[c(2,1,3)]) +\n  theme_bw() +\n  labs(x = \"\", y = \"\") +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        legend.position = \"none\") \n```\n:::\n\n\n\n\n<!-- cHEATING HERE because ggcpc plots take so long -->\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figs/fig-peng-ggpcp2-1.png){fig-align='center' width=18in}\n:::\n:::\n\n\n\n\n\nThis plot emphasizes the relation between penguin species and the island\nwhere they were observed and then shows the values of the quantitative\nbody size measurements.\n\n<!--# **TODO**: Use `-bill_depth` to reverse that scale .... -->\n\n\n\n\n\n\n## Animated tours\n\nIn the mid 17$^{th}$  to early 19$^{th}$-century the **Grand Tour**\nbecame a coming-of-age custom for young Europeans \n(mainly British nobility and landed gentry)\nof sufficient rank and means to undertake a journey to the principal sites of Europe\n(Paris, Geneva, Rome, Athens, ...) to complete their education by learning something of\nthe cultural legacies in history, art, and music from antiquity to the Renaissance.\nThereby, they could gain a wider appreciation of history and be prepared to play\na role in polite society or in their chosen endeavors.\n\nTravels in high-dimensional data space might be less thrilling than a journey\nfrom London through Paris and Millan to Rome. Yet, in both cases it is useful to\nthink of the path taken, and what might be seen along the way.\nBut there are different kinds of tours. We might simply take a meandering tour,\nexploring all the way, or want to plan a tour to see the most interesting \nsites in travel or have a tour guided by an expert.\nSimilarly in data space, we might travel randomly to see what we can find\nor be guided to find interesting features such as clusters, outliers or non-linear relations in data.\n\nFollowing the demonstration in PRIM-9 (@sec-discoveries) of exploring multidimensional\ndata space by rotation @Asimov:85 developed the idea of the _grand tour_,\na computer method for viewing multivariate statistical data via orthogonal projections onto an animated\nsequence of low-dimensional subspaces, like a movie.\nIn contrast to a scatterplot matrix which shows a static view of a data cloud projected onto\nall pairwise variable axes, a statistical tour is like the view of an eye moving smoothly in high-dimensional\nspace, capturing what it sees from a given location onto the 2-d plane of the computer screen.\n\nMore generally, statistical tours are a type of dynamic projections onto orthogonal axes (called a _basis_)\nthat embed data in a\n$p$−dimensional space into a $d$−dimensional viewing subspace. Typically, $d=2$, and \nthe result is displayed as scatterplots, together with vectors representing the projections of the data variables in this space.\nBut the projected data can be rendered in 1-d as densities or histograms, or in other number of dimensions as glyphs,\nor even as parallel coordinate plots.\nThe essential idea is that we can define, and animate, a _tour path_ as a smooth sequence of such projections\nover small changes to the projection basis, which gives the orientation of the data in the viewing space.\n\n### Projections\n\nThe idea of a projection is fundamental to touring methods and other visualizations of high-D data, so it is useful to understand\nwhat a projection is. Quite simply, you can think of a projection as the shadow of an object or cloud of points. \nThis is nicely illustrated by the cover image (@fig-cover-GBE) used for Douglas Hofstadter's [-@Hofstadter1979] _Gödel, Bach and Escher_\nwhich shows 3D solid shapes illuminated by light sources so their shadows form the letters G, B and E projected onto the planes\nformed by pairs of the three coordinate axes. The set of three 2D views is essentially the same that we see in a scatterplot matrix,\nwhere a 3D dataset is portrayed by the set of shadows of the points on planes formed by pairs of coordinate axes.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The cover image from @Hofstadter1979 illustrates how projections are shadows of an object cast by a light from a given direction.](images/Cover-GBE.png){#fig-cover-GBE fig-align='center' width=40%}\n:::\n:::\n\n\nIn the simplest case, a data point $\\mathbf{x} = (x_1, x_2)$ in two dimensions can be represented geometrically as a vector from the\norigin as shown in @fig-projection. This point can be projected on any one-dimensional axis $\\mathbf{p}$ by dropping a line\nperpendicular to $\\mathbf{p}$, which is the idea of a shadow. Mathematically, this is calculated as the product \n$\\mathbf{x}^\\mathsf{T} \\mathbf{p} = x_1 p_1 + x_2 p_2$ and suitably normalized to give the correct length. ...\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Projection of a point **x** onto a direction or axis **p**.](images/projection.png){#fig-projection fig-align='center' width=40%}\n:::\n:::\n\n\n\nMore generally, a projection of an $(n \\times p)$ data matrix $\\mathbf{X}$ representing $n$ observations in $p$\ndimensions onto a $d$-dimensional viewing space $\\mathbf{Y}_{n \\times d}$ is represented by a $p \\times d$ projection matrix $\\mathbf{P}$\nas $\\mathbf{Y} = \\mathbf{X} \\mathbf{P}$, where the columns of $\\mathbf{P}$ are orthogonal and of unit length,i.e., \n$\\mathbf{P}^\\mathsf{T} \\mathbf{P} = \\mathbf{I}_{(d \\times d)}$.\n\nFor example, to project a data matrix $\\mathbf{X}$ in three dimensions onto a 2D plane, we would multiply it by a $(3 \\times 2)$ orthonormal\nmatrix $\\mathbf{P}$. The matrix $\\mathbf{P}_1$ below simply selects the first two columns of $\\mathbf{X}$.[^ref-Cook]\n\n[^ref-Cook]: This example was modified from one used by @Cook-etal-2008.\n\n$$\n\\mathbf{X} =\n\\begin{bmatrix} \n    0 & 0 & 0 \\\\ \n    0 & 0 & 10 \\\\ \n    0 & 10 & 0 \\\\ \n    0 & 10 & 10 \\\\ \n    10 & 0 & 0 \\\\ \n    10 & 0 & 10 \\\\ \n    10 & 10 & 0 \\\\ \n    10 & 10 & 10 \\\\ \n \\end{bmatrix}_{8 \\times 3}\n ;\\;\n \\mathbf{P_1} =\n \\begin{bmatrix} \n    1 & 0 \\\\ \n    0 & 1 \\\\ \n    0 & 0 \\\\ \n \\end{bmatrix}_{3 \\times 2} \n \\;\\Rightarrow\\quad\n \\mathbf{Y} = \\mathbf{X} \\; \\mathbf{P_1} =\n \\begin{bmatrix} \n    0 & 0 \\\\ \n    0 & 0 \\\\ \n    0 & 10 \\\\ \n    0 & 10 \\\\ \n    10 & 0 \\\\ \n    10 & 0 \\\\ \n    10 & 10 \\\\ \n    10 & 10 \\\\ \n \\end{bmatrix}_{8 \\times 2} \n$$\nAn oblique projection using all three dimensions is given by $\\mathbf{P_2}$ below. This produces a new 2D view in $\\mathbf{Y}$:\n$$\n \\mathbf{P_2} =\n\\begin{bmatrix} \n    0.71 & -0.42 \\\\ \n    0.71 & 0.42 \\\\ \n    0 & 0.84 \\\\ \n \\end{bmatrix}_{3 \\times 2}\n \\quad\\Rightarrow\\quad\n \\mathbf{Y} = \\mathbf{X} \\; \\mathbf{P_2} =\n\\begin{bmatrix} \n    0 & 0 \\\\ \n    0 & 8.4 \\\\ \n    7.1 & 4.2 \\\\ \n    7.1 & 12.6 \\\\ \n    7.1 & -4.2 \\\\ \n    7.1 & 4.2 \\\\ \n    14.2 & 0 \\\\ \n    14.2 & 8.4 \\\\ \n \\end{bmatrix} \n$$\n\nThe columns in $\\mathbf{Y}$ are simply the linear combinations of those of $\\mathbf{X}$ using the weights in each column of $\\mathbf{P_2}$\n\n\n\\begin{eqnarray*}\n\\mathbf{y}_1 & = & 0.71 \\mathbf{x}_1 + 0.71 \\mathbf{x}_2 + 0 \\mathbf{x}_3\\\\\n\\mathbf{y}_2 & = & -0.42 \\mathbf{x}_1 + 0.42 \\mathbf{x}_2 + 0.84 \\mathbf{x}_3 \\\\\n\\end{eqnarray*}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nvals <- c(0, 10)\nX <- expand.grid(x1 = vals, x2=vals, x3=vals) |> as.matrix()\n\n# project on just x1, x2 plane\nP1 <- rbind(diag(2), c(0,0))\nY1 <- X %*% P1\n\n# oblique projection\nP2 <- matrix(c(0.71, 0.71, 0, -0.42, .42, 0.84), ncol=2)\nY2 <- X %*% P2\n```\n:::\n\n\nIn this example, the matrix $\\mathbf{X}$ consists of 8 points at the vertices of a cube of size 10, as shown in @fig-proj-combined (a).\nThe projections $\\mathbf{Y}_1 = \\mathbf{P}_1 \\mathbf{X}$ and $\\mathbf{Y}_2 = \\mathbf{P}_2 \\mathbf{X}$ are shown in panels (b) and (c).\nTo make it easier to relate the points in different views, shapes and colors are assigned so that each point has a unique combination\nof these attributes.[^pch]\n\n[^pch]: Plot shapes given by `pch = 15:18` correspond to: \nfilled square (15),\nfilled circle (16),\nfilled triangle point-up (17),\nfilled diamond (18).\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npch <- rep(15:18, times = 2)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"brown\")\ncol <- rep(colors, each = 2)\ndata.frame(X, pch, col)\n#>   x1 x2 x3 pch       col\n#> 1  0  0  0  15       red\n#> 2 10  0  0  16       red\n#> 3  0 10  0  17      blue\n#> 4 10 10  0  18      blue\n#> 5  0  0 10  15 darkgreen\n#> 6 10  0 10  16 darkgreen\n#> 7  0 10 10  17     brown\n#> 8 10 10 10  18     brown\n```\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![**Projection example**: (a) The 8 points in **X** form a cube of size 10; (b) the projection by **P1** is the view ignoring **x3** (two points coincide at each vertex); (c) the projection by **P2** is an oblique view.](images/proj-combined.png){#fig-proj-combined fig-align='center' width=100%}\n:::\n:::\n\nBut, if we are traveling in the projection space of $\\mathbf{Y}$, we need some signposts to tell us how the new dimensions relate to those\nof $\\mathbf{X}$. The answer is provided simply by plotting the rows of $\\mathbf{P}$ as vectors, as shown in @fig-proj-vectors.\nIn these plots, each row of $\\mathbf{P}_1$ and $\\mathbf{P}_2$ appears as a vector from the origin. It's direction shows the contribution each of \n$\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3$ make to the new coordinates $\\mathbf{y}_1$ and $\\mathbf{y}_2$.\n\nIn $\\mathbf{P}_1$, the projected variable $\\mathbf{y}_1$ is related only to $\\mathbf{x}_1$, while $\\mathbf{y}_2$ is related only to $\\mathbf{x}_2$\n$\\mathbf{x}_3$ makes no contribution, and appears at the origin. However in the projection given by $\\mathbf{P}_2$, $\\mathbf{x}_1$ and $\\mathbf{x}_2$ make the same contribution\nto $\\mathbf{y}_1$, while $\\mathbf{x}_3$ has no contribution to that horizontal axis.\nThe vertical axis, $\\mathbf{y}_2$ here is completely aligned with $\\mathbf{x}_3$; $\\mathbf{x}_1$ and $\\mathbf{x}_2$ have vertical components\nthat are half of that for $\\mathbf{x}_3$ in absolute value. \n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(matlib)\nop <- par(mar=c(4, 5, 1, 1)+.1)\nxlim <- ylim <- c(-1.1, 1.1)\naxes.x <- c(-1, 1, NA, 0, 0)\naxes.y <- c(0, 0, NA, -1, 1)\nlabs <- c(expression(x[1]), expression(x[2]), expression(x[3]))\nplot(xlim, ylim, type = \"n\", asp=1,\n     xlab = expression(y[1]), ylab = expression(y[2]),\n     cex.lab = 1.8)\ncircle(0, 0, 1, col = adjustcolor(\"skyblue\", alpha = 0.2))\nlines(axes.x, axes.y, col = \"grey\")\nvectors(P1, labels = labs, cex.lab = 1.8, lwd = 3, pos.lab = c(4, 2, 1))\n\nplot(xlim, ylim, type = \"n\", asp=1,\n     xlab = expression(y[1]), ylab = expression(y[2]),\n     cex.lab = 1.8)\ncircle(0, 0, 1, col = adjustcolor(\"skyblue\", alpha = 0.2))\nlines(axes.x, axes.y, col = \"grey\")\nvectors(P2, labels = labs, cex.lab = 1.8, lwd = 3)\npar(op)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![**Variable vectors**: Data variables viewed as vectors in the space of their projections. The angles of the **x** vectors with respect to the **y** coordinate axes show their relative contributions to each. The lengths of the **x** vectors show the relative degree to which they are represented in the space of **y**s. Left: the **P1** projection; right: the **P2** projection.](images/proj-vectors.png){#fig-proj-vectors fig-align='center' width=100%}\n:::\n:::\n\n#### Vector lengths\nIn @fig-proj-vectors, the **lengths** of the $\\mathbf{x}$ vectors reflect the relative degree to which each variable is represented in the space of the projection, and this is important for interpretation.\nFor the $\\mathbf{P}_1$ projection, $\\mathbf{x}_3$ is of length 0, while $\\mathbf{x}_1$ and $\\mathbf{x}_2$ fill the unit circle. In the projection given by $\\mathbf{P}_2$, all three $\\mathbf{x}$ are approximately\nthe same length. \n\nIn algebra, the length of a vector $\\mathbf{x}$ is \n$||\\mathbf{x}|| = (\\mathbf{x}^\\mathsf{T} \\mathbf{x})^{1/2} = \\sqrt{\\Sigma x_i^2}$, the Euclidean distance of the tip of the vector from the origin.\nIn R, we calculate the lengths of row vectors in a projection matrix by transposing and using `matlib::len()`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nP1 |> t() |> matlib::len()\n#> [1] 1 1 0\nP2 |> t() |> matlib::len()\n#> [1] 0.825 0.825 0.840\n```\n:::\n\n\n#### Joint-views\nTo interpret such projections, we want to see **both** the projected data and the signposts that tell us where we are\nin relation to the original variables.\nTo do this, we can overlay the variable vectors represented by the rows of the projection matrix $\\mathbf{P}$\nonto plots like @fig-proj-combined (b)\nand @fig-proj-combined (c) to see how the axes in a projection relate to those in the data. \nTo place these together on the same plot, we can either center the columns of $\\mathbf{Y}$ at their means or shift the \nthe columns of $\\mathbf{P}$ to `colMeans(Y)`. It is only the directions of the vectors that matters, so we are free\nto scale their lengths by any convenient factor.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nY2s <- scale(Y2, scale=FALSE)       # center Y2\nplot(Y2s, cex = 3, \n     asp = 1,\n     pch = pch, col = col,\n     xlab = expression(y[1]), ylab = expression(y[2]),\n     xlim = c(-10, 10), ylim = c(-10, 10), cex.lab = 1.8)\nr <- 7\nvecs <- (r*diag(3) %*% P2)\nvectors(vecs, labels = labs, cex.lab = 1.8, lwd = 2)\nvectors(-vecs, labels = NULL, lty = 1, angle = 1, col = \"gray\")\n```\n:::\n\nThe plot in @fig-proj-P2-vec illustrates this, centering $\\mathbf{Y}$, and multiplying the vectors in $\\mathbf{P}$ by 7.\nTo check your understanding, try to see if you can relate what is shown in this plot to the 3D plot in @fig-proj-combined (a).\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The **P2** projection of the data showing vectors for the original variables in the space of **Y**.](images/proj-P2-vec.png){#fig-proj-P2-vec fig-align='center' width=50%}\n:::\n:::\n\nThe idea of viewing low-dimensional projections of data together with vectors representing the contributions of the\noriginal variables to the dimensions shown in a display is also the basis of **biplot** techniques (@sec-biplot)\nwe will use in relation to principal components analysis.\n\n### Touring methods\nThe trick of statistical touring methods is to generate a smooth sequence of interpolated projections $\\mathbf{P}_{(t)}$\nindexed by time $t$, $\\mathbf{P}_{(1)}, \\mathbf{P}_{(2)}, \\mathbf{P}_{(3)}, \\dots, \\mathbf{P}_{(T)}$.\nThis gives a path of views $\\mathbf{Y}_{(t)} = \\mathbf{X} \\mathbf{P}_{(t)}$, that can be animated in successive frames, as\nshown schematically in @fig-peng-tourr-diagram.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![**Interpolations**: Illustration of a grand tour of interpolations of projection planes showing 2D scatterplots of the Penguin dataset. The seqeunce of views moves smoothly from an initial frame **P(1)** to a final frame **P(T)** where the penguin species are widely separated.](images/peng-tourr-diagram.png){#fig-peng-tourr-diagram fig-align='center' width=90%}\n:::\n:::\n\n\n\nAsimov's [-@Asimov:85] original idea of the grand tour was that of a random path, picking orthogonal projections $\\mathbf{P}_{(i)}$ at random.\nGiven enough time, the grand tour gives a space-filling path and would eventually show every possible projection of the data. But it does so smoothly, by interpolating from one projection to the next.\nIn the travel analogy, the path by road from London to Paris might go smoothly through Kent to Dover,\nthence via Amiens and Beauvais before reaching Paris. By air, the tour would follow a\nsmoother _geodesic_ path, and this is what the grand tour does. The sense in watching an animation of a statistical grand tour\nis that of continuous motion.\nThe grand tour algorithm is described\nin detail by @Buja-etal-2005 and @Cook-etal-2008.\n\n<!-- Projection pursuit @Cook-etal-1995 -> guided tour -->\n\n#### Guided tours\nThe next big idea was that rather than traveling randomly in projection space one could take a _guided tour_, following\na path that leads to \"interesting projections\", such as those that reveal clusters, gaps in data space or outliers.\nThis idea, called _projection pursuit_ [@Cook-etal-1995], works by defining a measure of interestingness of a data projection.\nIn a guided tour, the next projection is chosen to increase that index, so over time the projection moves toward one\nthat is maximizes that index.\n\nIn the time since @Asimov:85, there have been many implementations of touring visualization methods.\nXGobi [@Swayne-etal-1998] for X-Windows displays on Linux systems provided a test-bed for dynamic, interactive graphic\nmethods; it's successor, GGobi [@Swayne-etal-2003;@CookSwayne:2007] extended the range of touring methods\nto include a wider variety of projection pursuit indices.\n\n#### `tourr` package\nThe current state of art is best captured in the **tourr** package for R [@Wickham-etal-2011;@R-tourr]. \nIt defines a tour to consist of three components:\n\n* **data**: An $(n \\times p)$ numerical data matrix to be viewed.\n* **path**: A tour path function that produces a smoothed sequence of projection matrices $\\mathbf{P}_{(p \\times d)}$ in $d$. dimensions, for example `grand_tour(d = 2)` or `guided_tour(index = holes)`.\n* **display**: A function that renders the projected data, for example `display_xy()` for a scatterplot, `display_depth()` for a 3D plot with simulated depth, or `display_pcp()` for a parallel coordinates plots\n\nThis very nicely separates the aspects of a tour, and allows one to think of and define new tour path methods and display methods.\nThe package defines two general tour functions: `animate()` produces a real-time animation on a display device and \n`render()` saves image frames to disk, such as a `.gif` file.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanimate(data, tour_path, display_method)\nrender(data, tour_path, display_method)\n```\n:::\n\nThe **tourr** package provides a wide range of tour path methods and display methods:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tour path methods\ngrep(\"_tour$\", lsf.str(\"package:tourr\"), value = TRUE)\n#>  [1] \"dependence_tour\"     \"frozen_guided_tour\" \n#>  [3] \"frozen_tour\"         \"grand_tour\"         \n#>  [5] \"guided_anomaly_tour\" \"guided_section_tour\"\n#>  [7] \"guided_tour\"         \"little_tour\"        \n#>  [9] \"local_tour\"          \"new_tour\"           \n#> [11] \"planned_tour\"        \"planned2_tour\"      \n#> [13] \"radial_tour\"\n\n# display methods\ngrep(\"display_\", lsf.str(\"package:tourr\"), value = TRUE)\n#>  [1] \"display_andrews\"   \"display_density2d\" \"display_depth\"    \n#>  [4] \"display_dist\"      \"display_faces\"     \"display_groupxy\"  \n#>  [7] \"display_idx\"       \"display_image\"     \"display_pca\"      \n#> [10] \"display_pcp\"       \"display_sage\"      \"display_scatmat\"  \n#> [13] \"display_slice\"     \"display_stars\"     \"display_stereo\"   \n#> [16] \"display_trails\"    \"display_xy\"\n```\n:::\n\nTour path methods take a variety of optional arguments to specify the detailed behavior of the method.\nFor example, most allow you to specify the number of dimension (`d =`) of the projections.\nThe `guided_tour()` is of particular interest here.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nargs(guided_tour)\n#> function (index_f, d = 2, alpha = 0.5, cooling = 0.99, max.tries = 25, \n#>     max.i = Inf, search_f = search_geodesic, n_sample = 100, \n#>     ...) \n#> NULL\n```\n:::\n\nIn this, `index_f` specifies a function that the method tries to optimize on its path and package defines four indices:\n\n* Holes (`holes()`): This is sensitive to projections with separated clusters of points, with few points near the origin\n* Central mass (`cmass()`): Sensitive to projections with lots of points in the center, but perhaps with some outliers\n* Linear discriminant analysis (`lda_pp()`): For data with a grouping factor, optimizes a measure of separation of the group means as in MANOVA or linear discriminant analysis.\n* PDA analysis (`pda_pp()`): A penalized version of `lda_pp()` for cases of large $p$ relative to sample size $n$ [@LeeCook-2009].\n\nIn addition, there is now a `guided_anomaly_tour()`  that looks for the best projection of\nobservations that are outside the data ellipsoid, finding a view showing observations with large\nMahalanobis distances from the centroid.\n\n\n#### Penguin tours\n\nPenguins are a traveling species. They make yearly travels inland to breeding sites in early spring, repeating the patterns of their ancestors. Near the beginning of summer, adult penguins and their chicks return to the sea and spend the rest of the summer feeding there [@Black-etal-2018]. If they were also data scientists, they might wonder about the relations among among their cousins of different species and take a tour of their measurements...\n\n<!-- Cite: Black et al Time-lapse imagery of Ade´lie penguins, https://doi.org/10.1371/journal.pone.0193532 -->\n\n\nFor example, using the Penguins dataset, the following calls produce grand tours in 2, 3, and 4 dimensions.\nThe 2D tour is displayed as a scatterplot, the 3D tour using simulated depth as shown by variation in point size and transparency, and the 4D tour is shown using a parallel coordinate plot.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(peng, package = \"heplots\")\npeng_scaled <- scale(peng[,3:6])\ncolnames(peng_scaled) <- c(\"BL\", \"BD\", \"FL\", \"BM\")\n\nanimate(peng_scaled, grand_tour(d = 2), display_xy())\nanimate(peng_scaled, grand_tour(d = 3), display_depth())\nanimate(peng_scaled, grand_tour(d = 4), display_pcp())\n```\n:::\n\n::: {#fig-peng-tour-demo .cell layout-ncol=\"3\" layout-align=\"center\"}\n::: {.cell-output-display}\n![2D, scatterplot](images/tours/peng-tour-demo1.png){#fig-peng-tour-demo-1 fig-align='center' width=33%}\n:::\n\n::: {.cell-output-display}\n![3D, simulated depth](images/tours/peng-tour-demo2.png){#fig-peng-tour-demo-2 fig-align='center' width=33%}\n:::\n\n::: {.cell-output-display}\n![4D, parallel coordinates plot](images/tours/peng-tour-demo3.png){#fig-peng-tour-demo-3 fig-align='center' width=33%}\n:::\n\nGrand tours of the penguin dataset in 2, 3, and 4 dimensions using different `display_*()` methods.\n:::\n\nTo illustrate, I'll start with a grand tour designed to explore this 4D space of penguins.\nI'll abbreviate the variables to two characters,\n\"BL\" = `bill_length`, \"BD\" = `bill_depth`, \"FL\" = `flipper_length`, and \"BM\" = `body_mass`\nand identify the penguin species using point shape (`pch`) and color (`col`).\n\nAs you watch this pay attention to the separation of the species and any other interesting features. What do you see? \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(peng, package = \"heplots\")\npeng_scaled <- scale(peng[,3:6])\ncolnames(peng_scaled) <- c(\"BL\", \"BD\", \"FL\", \"BM\")\n\npch <- c(15, 16, 17)[peng$species] \ncex = 1.2\n\nset.seed(1234)\nanimate(peng_scaled,\n        tour_path = grand_tour(d=2),\n        display_xy(col = peng$species,\n                   palette = peng.colors(\"dark\"),\n                   pch = pch, cex = cex,\n                   axis.col = \"black\", \n                   axis.text.col = \"black\", \n                   axis.lwd = 1.5))\n```\n:::\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Animation of a grand tour of the Penguin data.](images/tours/peng-tourr-grand.gif){#fig-peng-tour-grand fig-align='center' width=75%}\n:::\n:::\n:::\n\n@fig-peng-tour-grand-frames shows three frames from this movie.\nThe first (a) is the initial frame that shows the projection in the plane of\nbill depth and bill length. The variable vectors indicate that bill length\ndifferentiates Adelie penguins from the others. In frame (b), the three species\nare widely separated, with bill depth distinguishing Gentoo from the others.\nIn frame (c) the three species are largely mixed, but two points stand out as\noutliers, with exceptionally long bills compared to the rest.\n\n::: {#fig-peng-tour-grand-frames .cell layout-ncol=\"3\" layout-align=\"center\"}\n::: {.cell-output-display}\n![Initial frame](images/tours/peng-grand-tour1.png){#fig-peng-tour-grand-frames-1 fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![Clusters](images/tours/peng-grand-tour2.png){#fig-peng-tour-grand-frames-2 fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![Outliers](images/tours/peng-grand-tour3.png){#fig-peng-tour-grand-frames-3 fig-align='center' width=100%}\n:::\n\nThree frames from the grand tour of the Penguin data. (a) The initial frame is the projection showing only BD and BL, where bill length conveniently separates Adelie from the other two species. (b) A frame that shows the three species more widely separated. (c) A frame that shows two outliers with very large bills.\n:::\n\n\n<!-- ::: {#fig-peng-tour-grand-frames layout-ncol=3} -->\n\n<!-- ![Initial frame](images/tours/peng-grand-tour1.png){#fig-peng-grand-tour1 width=140} -->\n\n<!-- ![Clusters](images/tours/peng-grand-tour2.png){#fig-peng-grand-tour2 width=140} -->\n<!-- ![Outliers](images/tours/peng-grand-tour3.png){#fig-peng-grand-tour3 width=140} -->\n\n<!-- Three frames from the grand tour of the Penguin data. (a) The initial frame is the projection showing only -->\n<!-- BD and BL, where bill length conveniently separates Adelie from the other two species. -->\n<!-- (b) A frame that shows the three species more widely separated. (c) A frame that shows two outliers. -->\n<!-- ::: -->\n\n\nLet's take the penguins on a guided tour, trying to find views that show the greatest separations among the \npenguin species; that is, a guided tour, optimizing the `lda_pp()` index.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1234)\nanimate(peng_scaled, \n        guided_tour(lda_pp(peng$species)),\n        display_xy(col = peng$species,\n                   palette = peng.colors(\"dark\"),\n                   pch = pch,\n                   cex = cex)\n)\n```\n:::\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Animation of a guided tour of the Penguin data, using a tour criterion designed to find an optimal separation among the penguin species. The animation shows three loops of the sequence of projections and stops when the LDA criterion cannot be improved.](images/tours/peng-tourr-lda.gif){#fig-peng-tour-lda fig-align='center' width=75%}\n:::\n:::\n:::\n\n**TODO**: I'm trying to balance what will/can be shown in the HTML version vs. the printed PDF.\nNeeds text here specifically for the PDF version.\n\n::: {#fig-peng-tour-guided .cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![Optimizing `lda_pp()`](images/tours/peng-tour-lda-final.png){#fig-peng-tour-guided-1 fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![Optimizing `anomaly_index()`](images/tours/peng-tour-anomalies-final.png){#fig-peng-tour-guided-2 fig-align='center' width=100%}\n:::\n\n**Guided tours**: These figures show the final frame in the animations of guided tours designed to find the projection that optimize an index. (a) The `lda_pp()` criterion optimizes the separation of the means for species relative to within-group variation. (b) The `anomalies_index()` optimizes the average Mahalanobis distance of points from the centroid\n:::\n\n\nThese examples are intended to highlight what is possible with dynamic graphics for exploring high-dimensional data visually. \n@CookLaa-mulgar extend the discussion of these methods from @CookSwayne:2007 (which used Ggobi)\nto the **tourr** package. They illustrate dimension reduction, various cluster analysis methods,\ntrees and random forests and some machine-learning techniques.\n\n<!-- **TODO**: Clean up these package references. I'm linking to the pkgdown sites, b/c I don't have them installed and can't easily get a `citation()`.  -->\n\nIdeally, we should be able interact with a tour, \n\n* pausing when we see something interesting and saving the view for later analysis; \n* selecting or highlighting unusual points, \n* changing tour methods or variables displayed on the fly, and so forth.\n\nSome packages that provide these capabilities are: \n[**detourr**](https://casperhart.github.io/detourr/index.html) [@R-detourr]\n[**liminal**](https://sa-lee.github.io/liminal/) [@R-liminal]\nand\n[**langevitour**](https://logarithmic.net/langevitour/index.html) [@R-langevitour;@Harrison2023]\nThe **loon** package [@R-loon]\nis a general toolkit that enables highly interactive data visualization.\nIt provides a **loon.tour** package [@R-loon-tour] for using touring methods within the `loon` environment.\n\n\n\n\n\n\n\n\n\n\n\n\n**Package summary**\n\nFor development, keep track of the packages used in each chapter.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n15  packages used here:\n car, carData, corrgram, corrplot, dplyr, GGally, ggdensity, ggpcp, ggplot2, grid, knitr, patchwork, tidyr, tourr, vcd\n:::\n\n\n\n\n\n<!-- ## References {.unnumbered} -->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}