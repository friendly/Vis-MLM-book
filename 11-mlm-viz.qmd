```{r include=FALSE}
source("R/common.R")
knitr::opts_chunk$set(fig.path = "figs/ch11/")
```

::: {.content-visible unless-format="pdf"}
{{< include latex/latex-commands.qmd >}}
:::

# Visualizing Multivariate Models {#sec-vis-mlm}

The methods discussed in @sec-mlm-review provide the basis for a rather complete multivariate analysis
of traditional univariate methods for the same designs. You can carry out multiple regression,
ANOVA, or indeed, any classical linear model with the standard collection of analysis tools you use for
a single outcome variable, but naturally extended in most cases to having several outcomes to analyse
together. The key points are:

- Everything you know about the usual univariate models-- regression coefficients, main effects and contrasts for factors, interactions of model terms,  ... applies here.

- By a rather clever design, called "matrix algebra" the separate univariate models can be combined, by turning vectors of responses, $\mathbf{y}_1, \mathbf{y}_2, \dots$ into a matrix $\mathbf{Y}$. Bingo! We get a
multivariate extension.

- You can treat this as a collection of separate models, one for each response, because the coefficients are the same.
But, the benefit of a multivariate approach is that you also get an overall multivariate test for each term in the model.

As nice as these mathematical and statistical ideas might be, the fact that the analysis is conducted for the response
variables _collectively_, means that it may be harder to interpret and explain what this means about the separate
responses. Here's where multivariate model visualization comes to the rescue!

* **HE plots**: The tests of multivariate models, including multivariate analysis of variance (MANOVA) for group differences and multivariate multiple regression (MMRA) can be easily visualized by plots of a hypothesis ("H") data ellipse for the fitted values, relative to the corresponding plot of the error ellipse ("E") of the residuals, which I call the HE plot framework.

* **contrasts**: For factors in MANOVA, contrasts and linear hypotheses (@sec-contrasts2) provide a way to decompose
an overall multivariate test into portions directed at meaningful _specific_ research questions
regarding **how** the groups differ.
\ix{contrasts}

* **CDA**: For more than a few response variables, these result can be projected onto a lower-dimensional "canonical" space providing an even simpler description, accounting for most of the "juice". Vectors for the response variables in this space show how these relate to the canonical dimension, facilitating interpretation. 

<!-- 
- refer to Huang: "_given the challenges and assumptions involved in appropriately performing and interpreting MANOVA results, researchers are better off using other less error-prone procedures._" 
-->

@Huang2019, in an article titled, "MANOVA: A Procedure Whose Time Has Passed?"
(and others) criticized these methods as (a) difficult to understand because they are framed in terms of linear combinations of the the responses;
(b) more complicated and limited in interpreting MANOVA effects and (c) unwieldy post hoc strategies often employed for interpretation. 

But that's just you should expect to happen if you rely on tables of coefficients or ANOVA summary tables, even with significance stars (`*` ... `***`) to help interpret what is important.
(You should be looking at effect sizes and practical significance, right?)

These difficulties in understanding multivariate models can, I believe, be cured by accessible
graphical methods for visualizing hypothesis tests and for visualizing what these
linear combinations reflect in terms of the observed variables.
The HE plot framework described below provides powerful graphic methods available in easily used software,
the `r pkg("heplots")` and `r pkg("candisc")` packages.

<!-- footnote?
I should warn you that these plots are somewhat novel and take some time to understand ....
-->


This chapter describes this framework and illustrates some concrete examples, first for
MANOVA designs which are conceptually and visually simpler, and then for MMRA
designs with quantitative predictors and finally for MANCOVA models.
Many more worked examples are available in vignettes for the `r package("heplots")`.[^heplots-vignettes]

[^heplots-vignettes]: See the heplots vignettes `vignette("HE_manova", package="heplots")`
and `vignette("HE_mmra", package="heplots")`.
<!-- [HE plot MANOVA Examples](http://friendly.github.io/heplots/articles/HE_manova.html) and [HE plot MMRA Examples](http://friendly.github.io/heplots/articles/HE_mmra.html). -->

**Packages**

In this chapter I use the following packages. Load them now.
```{r}
library(car)
library(heplots)
library(candisc)
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggpubr)
library(patchwork)
```

## HE plot framework {#sec-he-framework}

@sec-Hotelling illustrated the basic ideas of the framework for visualizing multivariate
linear models in the context of a simple two group design, using Hotelling's $T^2$.  The main ideas were illustrated in @fig-HE-framework.

Having described the statistical ideas behind the MLM in @sec-mlm-review, we can proceed to
extend this framework to larger designs. @fig-dogfood-quartet illustrates these ideas using the
simple one-way MANOVA design of the dogfood data from @sec-dogfood-data.

<!-- fig.code: R/dogfood/dogfood-quartet.R -->

```{r}
#| label: fig-dogfood-quartet
#| echo: false
#| fig-align: center
#| out-width: "100%"
#| fig-cap: "**Dogfood quartet**: Illustration of the conceptual ideas of the HE plot framework for the dogfood data. (a) Scatterplot of the data; (b) Summary using data ellipses; (c) HE plot shows the variation in the means in relation to pooled within group variance; (d) Transformation from data space to canonical space"
knitr::include_graphics("images/dogfood-quartet.png")
```
\ix{dogfood data}
\ix{datasets!dogfood}
\ix{quartets!dogfood}


* In (a) **data space**, each group is summarized in (b) by its **data ellipse**, representing the means and covariances.

* Variation against the hypothesis of equal means can be seen by the $\mathbf{H}$ ellipse in the (c) **HE plot**, representing the data ellipse of the fitted values. Error variance is shown in the $\mathbf{E}$ ellipse,
representing the pooled within-group covariance matrix, $\mathbf{S}_p$ and the data ellipse of the residuals from the model.
For the dogfood data, the group means have a negative relation: longer time to start eating is associated with a smaller amount eaten.

* The MANOVA (or Hotelling's $T^2$) is formally equivalent to a **discriminant analysis**, predicting
group membership from the response variables which can be seen in data space. (The main difference is 
emphasis and goals: MANOVA seeks to test differences among group means, while discriminant analysis aims
at classification of the observations into groups.)

* This effectively projects the $p$-dimensional
space of the predictors into the smaller (d) **canonical space** that shows the greatest differences among
the groups. As in a biplot, vectors show the relations of the response variables with the canonical dimensions.

<!--
```{r}
#| label: fig-HE-framework
#| echo: false
#| fig-align: center
#| out-width: "100%"
#| fig-cap: "The Hypothesis Error plot framework for a two-group design. Above: Data ellipses can be summarized in an HE plot showing the pooled within-group error ($\\mathbf{E}$) ellipse and the $\\mathbf{H}$ 'ellipse' for the group means.
#| Below: Observations projected on the line joining the means give discriminant scores which correpond to a one-dimensional canonical space, represented by a boxplot of their scores and arrows reflecting the variable weights."
knitr::include_graphics("images/HE-framework.png")
```
-->

<!--
Having described the statistical ideas behind the MLM in @sec-mlm-review, we can proceed to
extend this framework to larger designs. A conceptual overview is shown in @fig-arcmanov1 for a one-way MANOVA
design with 8 groups.

```{r}
#| label: fig-arcmanov1
#| echo: false
#| fig-align: center
#| out-width: "100%"
#| fig-cap: "Conceptual plots showing the essential ideas behind multivariate tests, in terms of the hypothesis
#| ($\\mathbf{H}$) and error ($\\mathbf{E}$) matrices for a 1-way MANOVA design with two response variables, $Y_1$ and $Y_2$"
knitr::include_graphics("images/arcmanov.png")
```
-->


For more complex models such as MANOVA with multiple factors or multivariate multivariate regression with several predictors,
there is one sum of squares and products matrix (SSP), and therefore one
$\mathbf{H}$ ellipse for _each term_ in the model. For example, in a two-way MANOVA design with the model
formula `(y1, y2) ~ A + B + A*B` and equal sample sizes in the groups, the total sum of squares accounted for by the model is the sum of their separate effects,

$$
\begin{aligned}
\mathbf{SSP}_{\text{Model}} & = \mathbf{SSP}_{A} + \mathbf{SSP}_{B} + \mathbf{SSP}_{AB} \\
                            & = \mathbf{H}_A + \mathbf{H}_B + \mathbf{H}_{AB} \period
\end{aligned}
$${#eq-HE-model}


All of these hypotheses can be overlaid in a single HE plot showing their effects together in a comprehensive view.

## HE plot construction {#sec-he-plot-construct}

The HE plot is constructed to allow a direct visualization of the "size" of hypothesized terms in
a multivariate linear model in relation to unexplained error variation.
These can be displayed in 2D or 3D plots, so I use the term "ellipsoid" below to cover all cases.

Error variation is represented by a standard 68\% data ellipsoid of the $\mathbf{E}$ matrix
of the residuals in $\Epsilon$. This is divided by the residual degrees of freedom, so the size of $\mathbf{E} / \text{df}_e$ is analogous to a mean square error
in univariate tests. The choice of 68\% coverage allows you to ``read'' the residual standard deviation as the half-length of the
shadow of the $\mathbf{E}$ ellipsoid on any axis (see @fig-galton-ellipse-r).

The $\mathbf{E}$ ellipsoid is then translated to the overall (grand) means $\bar{\mathbf{y}}$ of the variables plotted, which allows us to show the means for factor levels on the same scale, facilitating interpretation.
In the notation of @eq-ellE, the error ellipsoid $\mathcal{E}_c$ of size $c$ is given by

$$
\mathcal{E}_c (\bar{\mathbf{y}}, \mathbf{E}) = \bar{\mathbf{y}} \; \oplus \; c\,\mathbf{E}^{1/2} \comma
$$ {#eq-Ec}
where $c = \chi^2_2 (0.68)$ for 2D plots and $c = \chi^2_3 (0.68)$ for 3D plots of standard 68% coverage.[^small-n]
Ellipses of various coverage were shown in @fig-ellipses-coverage.

[^small-n]: In smallish samples ($n < 30$) we use the better approximations,
$c = \sqrt{2 F_{2, n-2}^{0.68}}$ for 2D plots and $c = \sqrt{3 F_{3, n-3}^{0.68}}$ for 3D. The difference between these is usually invisible in plots.


An ellipsoid representing variation in the means of a factor (or any other term reflected in a general linear hypothesis test, @eq-hmat) uses the corresponding $\mathbf{H}$ matrix is simply the data ellipse of the fitted values for that term. But there is a question of
the relative scaling of the $\mathbf{H}$ and $\mathbf{E}$ ellipsoids for interpretation.

Dividing the hypothesis matrix by the error degrees of freedom, giving
$\mathbf{H} / \text{df}_e$,
puts this on the same scale as the $\E$ ellipse.
<!-- , as shown in the left panel of \figref{fig:heplot-iris1}. -->
I refer to this as _effect size scaling_, because it is similar to an effect size index used in
univariate models, e.g., $ES = (\bar{y}_1 - \bar{y}_2) / s_e$ in a two-group, univariate design.
An alternative, _significance scaling_ (@sec-signif-scaling) provides a visual test of significance of
a model $\mathbf{H}$ term.

<!--
The notation $\bar{\mathbf{y}} \; \oplus$ in @eq-Ec means that the error ellipsoid is
shifted to be centered at the grand means of the response variables, where the $\mathbf{H}$
is also centered. This allows the HE plot to also show the group means, facilitating
interpretation.
-->

To illustrate this concretely, consider the HE plot for the `dogfood` shown in @fig-dogfood-quartet (c),
reproduced here as @fig-dogfood-HE.

```{r echo = -1}
#| label: fig-dogfood-HE
#| code-fold: true
#| code-summary: "Show the code"
#| fig-width: 6
#| fig-height: 6
#| out-width: "60%"
#| fig-cap: "HE plot for the `dogfood` data, showing the means of the four groups, which generates the $\\mathbf{H}$ ellipse for the effect of `formula`. The $\\mathbf{E}$ ellipse labeled 'Error' shows the within-group variances and covariance."
op <- par(mar= c(5, 5, 1, 1) +.1)
data(dogfood, package="heplots")
dogfood.mod <- lm(cbind(start, amount) ~ formula, data=dogfood)

heplot(dogfood.mod,  
       fill = TRUE, fill.alpha = 0.1, 
       cex.lab = 1.5, cex = 1.5,
       xlim = c(-1, 4.5),
       ylim = c(70, 100))
```

From the analysis in @sec-sums-of-squares, we found the $\mathbf{H}$ matrix for the `formula` effect in the `dogfood.mod` model to be
as shown below. The negative covariance, -70.94, reflects a correlation of -0.94 between the means of
start time and amount eaten.

```{r dogfood-H-mat}
dogfood.aov <- Anova(dogfood.mod) 
SSP_H <- dogfood.aov$SSP[[1]] |> print()
```

Similarly, the $\mathbf{E}$ matrix, shown below, reflects a slight positive correlation, 0.12, 
for dogs fed the same formula.

```{r}
SSP_E <- dogfood.aov$SSPE |> print()
```


<!-- ### Example: Iris data {#iris-ex} -->

::: {#exm-iris-data}
**Iris data**


Perhaps the most famous (or infamous) dataset in the history of multivariate data analysis is that of measurements on three species of Iris flowers collected by Edgar Anderson [-@Anderson:35] in the Gaspé Peninsula of Québec, Canada. 
Anderson wanted to quantify the outward appearance ("morphology": shape, structure, color, pattern, size) of species as a method to study variation within and between such groups. Although Anderson published in the obscure _Bulletin of the American Iris Society_, R. A. Fisher [-@Fisher:1936] saw this as a challenge and opportunity to introduce the method now called
discriminant analysis---how to find a weighted composite of variables to best discriminate among existing groups.

<!-- depending on use, make some of this a text box -->
::: {.callout-note title="History corner: Is the iris data racist?"}

I said "infamous" above because Fisher published in the _Annals of Eugenics_. He was an ardent eugenicist himself, 
and the work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups.
Through guilt by association, the Iris data, having mistakenly been called "Fisher's Iris Data",
has become deprecated, even called "racist data".[^stop-iris] The voices of the _Setosa_, _Versicolor_ and _Virginica_ of
Gaspé protest: we don't have a racist bone in our body and nor prejudice against any
other species, to no avail.

@Bodmer-etal-2021 present a careful account of Fisher's views on eugenics within the context
of his time and his contributions to modern statistical theory and practice.
Fisher's views on race were largely formed by Darwin and Galton,
but "nearly all of Fisher’s statements were about populations,
groups of populations, or the human species as a whole". Regardless, the `iris` data were
Anderson's and should not be blamed. After all, if Anderson had gave his car to Fisher,
would the car be tainted by Fisher's eugenicist leanings?

[^stop-iris]: For example, Megan Stodel in a blog post [Stop using iris](https://www.meganstodel.com/posts/no-to-iris/) says, "_It is clear to me that knowingly using work that was itself used in pursuit of racist ideals is totally unacceptable._" A Reddit discussion on this topic, [Is it socially acceptable to use the Iris dataset?](https://www.reddit.com/r/MachineLearning/comments/ii9urh/d_is_it_socially_acceptable_to_use_the_iris/) has some interesting replies. 
:::

<!--
```{r}
#| label: fig-iris-flowers
#| out-width: "100%"
#| fig-cap: "Three species of irises in the Anderson/Fisher data set. _Source_: P. I. Adegbite [Iris Flower Classification]( https://peaceadegbite1.medium.com/iris-flower-classification-60790e9718a1)"
knitr::include_graphics(here::here("images/iris-flowers-labeled.png"))
```

@fig-iris-flowers shows photos of the three iris species, _Setosa_, _Versicolor_ and _Virginica_. 
Each flower has three sepals and three petals. The sepals have brightly colored central sections.
Anderson recorded measurements of the  length and width (in cm.) of the sepals and petals on 50 flowers of each type.
-->

```{r}
#| label: fig-iris-diagram
#| echo: false
#| out-width: "70%"
#| fig-cap: "Diagram of an iris flower showing the measurements of petal and sepal size. Each flower has three sepals and three alternating petals. The sepals have brightly colored central sections. _Source_: @DeSilva2020"
knitr::include_graphics(here::here("images/iris-diagram.jpg"))
```

So that we understand what the measurements represent, @fig-iris-diagram superposes labels on a typical iris flower, having three sepals which alternate with three petals.
Sepals are like ostentatious petals, with attractive decorations in the central section. Length is the distance from the center
to the tip and width is the transverse dimension.

As always, it is useful to start with overview displays to see the data.
A scatterplot matrix (@fig-iris-spm) shows that _versicolor_ and _virginica_ are more
similar to each other than either is to _setosa_, both in their pairwise means (_setosa_ are smaller) and in the
slopes of regression lines.
Further, the ellipses  suggest that the
assumption of constant within-group covariance matrices is problematic: While the shapes and sizes of the concentration ellipses for _versicolor_ and _virginica_ are reasonably similar, the shapes and sizes of the ellipses for _setosa_ are different from the other two.

```{r}
#| label: fig-iris-spm
#| out-width: "100%"
#| fig-width: 10
#| fig-height: 10
#| fig-cap: "Scatterplot matrix of the `iris` dataset. The species are summarized by 68% data ellipses and linear regression lines in each pairwise plot."
iris_colors <-c("blue", "darkgreen", "brown4")
scatterplotMatrix(~ Sepal.Length + Sepal.Width + 
                    Petal.Length + Petal.Width | Species,
  data = iris,
  col = iris_colors,
  pch = 15:17,
  smooth=FALSE,
  regLine = TRUE,
  ellipse=list(levels=0.68, fill.alpha=0.1),
  diagonal = FALSE,
  legend = list(coords = "bottomleft", 
                cex = 1.3, pt.cex = 1.2))
```

:::

**TODO*: Should this be a numbered section?

### MANOVA model {#sec-iris-mod}
We proceed nevertheless to fit a multivariate one-way ANOVA model to the iris data.
The MANOVA model for these data addresses the question: "Do the means of the `Species` differ significantly for the sepal and petal variables taken together?" 
$$
\mathcal{H}_0 : \boldsymbol{\mu}_\textrm{setosa} = \boldsymbol{\mu}_\textrm{versicolor} = \boldsymbol{\mu}_\textrm{virginica}
$$


Because there are three species, the test involves $s = \min(p, g-1) =2$ degrees of freedom,
and we are entitled to represent this by two 1-df contrasts,  or sub-questions. From the separation among the groups
shown in @fig-iris-spm (or more botanical knowledge), it makes sense to compare:

* Setosa vs. others: $\mathbf{c}_1 = (1,\: -\frac12, \: -\frac12)$
* Versicolor vs. Virginica: : $\mathbf{c}_1 = (0,\: 1, \: -1)$

You can do this by putting these vectors as columns in a matrix and assigning this to the
`contrasts()` of `Species`. It is important to do this _before_ fitting with `lm()`,
because the contrasts in effect determine how the $\mathbf{X}$ matrix is setup, and hence the
names of the coefficients representing `Species`.

```{r}
C <- matrix(c(1,-1/2,-1/2,  
              0,   1,  -1), nrow=3, ncol=2)
contrasts(iris$Species) <- C
contrasts(iris$Species)
```

Now let's fit the model. As you would expect from @fig-iris-spm, the differences among groups are highly significant. 

```{r iris-mod}
iris.mod <- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~
                 Species, data=iris)
Anova(iris.mod)
```

As a quick follow-up, it is useful to examine the univariate tests for each of the iris
variables, using `heplots::glance()` or `heplots::uniStats()`. It is of interest that
the univariate $R^2$ values are much larger for the petal variables than the sepal length and width.[^rsq-anova].
For comparison, `heplots::etasq()` gives the overall $\eta^2$ proportion of variance accounted for in 
all responses.

[^rsq-anova]: Recall that $R^2$ for a linear model is the the proportion of variation in the response that is explained by the model, calculated as $R^2 = \text{SS}_H / \text{SS}_T = \text{SS}_H / (\text{SS}_H + \text{SS}_E )$. For a multivariate model, these are obtained from the diagonal elements of $\mathbf{H}$ and $\mathbf{E}$.

```{r iris-glance}
glance(iris.mod)

etasq(iris.mod)
```

But these statistics don't help to understand _how_ the species differ. For this, we turn
to HE plots.

## HE plots {#sec-he-plots}

The `heplot()` function takes a `"mlm"` object and produces an HE plot for one pair
of variables specified by the `variables` argument. By default, it plots the first two.
@fig-iris-HE1 shows the HE plots for the two sepal and the two petal variables.

<!-- duplicated the code b/c it doesn't print from the fig-iris-HE1 chunk -->
```{r code-iris-HE1}
#| eval: false
heplot(iris.mod, size = "effect",
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
heplot(iris.mod, size = "effect", variables = 3:4,
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
```

```{r}
#| echo: false
#| label: fig-iris-HE1
#| fig-width: 10
#| fig-height: 5
#| fig-show: hold
#| out-width: "100%"
#| fig-cap: "HE plots for the multivariate model `iris.mod`. The left panel shows the plot for the Sepal variables; the right panel plots the Petal variables."
op <- par(mar = c(4, 4, 1, 1) + .5,
          mfrow = c(1, 2))
heplot(iris.mod, size = "effect",
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
heplot(iris.mod, size = "effect", variables = 3:4,
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
```

The interpretation of the plots in @fig-iris-HE1 is as follows:

* For the Sepal variables, length and width are positively correlated _within_ species 
(the $\mathbf{E}$ = "Error" ellipsoid). The means of the groups (the $\mathbf{H}$ = "Species" ellipsoid), however, are negatively correlated. This plot is the HE plot representation of the data
shown in row 2, column 1 of @fig-iris-spm. It reflects the relative **shape** of the 
iris sepals: shorter and wider for _setosa_ than the other two species.

* For the Petal variables length and width are again positively correlated _within_ species,
but now the means of the groups are positively correlated: longer petals go with wider ones
across species. This reflects the relative **size** of the iris petals.
The analogous data plot appears in row 4, column 3 of @fig-iris-spm.

<!--
**Older stuff**
@fig-iris-HE0 shows two views of the relationship between sepal length and sepal width for the iris data. ...
```{r}
#| eval: false
op <- par(mar = c(4, 4, 1, 1) + .5,
          mfrow = c(1, 2))
col <-c("blue", "darkgreen", "brown")
clr <- c(col, "red")
covEllipses(cbind(Sepal.Length, Sepal.Width) ~ Species, data=iris,
      pooled = TRUE,
      fill=TRUE,
      fill.alpha = 0.1,
      lwd = 3,
      col = clr,
      cex = 1.5, cex.lab = 1.5,
      label.pos = c(3, 1, 3, 0),
      xlim = c(4,8), ylim = c(2,4))

iris.mod <- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~
                 Species, data=iris)
heplot(iris.mod, size = "effect",
       cex = 1.5, cex.lab = 1.5,
       fill=TRUE, fill.alpha=c(0.3,0.1),
       xlim = c(4,8), ylim = c(2,4))
par(op)
```

```{r}
#| label: fig-iris-HE0
#| out-width: "100%"
#| fig-cap: "Iris data: Data ellipses and HE plot"
knitr::include_graphics("images/iris-HE1.png")
```
-->


## Significance scaling {#sec-signif-scaling}

The geometry of ellipsoids and multivariate tests allow us to go further with another re-scaling of the $\mathbf{H}$ ellipsoid
that gives a _visual test of significance_ for any term in a MLM.
This is done simply by dividing $\mathbf{H} / df_e$ further
by the $\alpha$-critical value of the corresponding test statistic to show the strength of evidence against
the null hypothesis.

Among the various multivariate test statistics,
Roy's maximum root test, based on the largest eigenvalue $\lambda_1$ of $\mathbf{H} \mathbf{E}^{-1}$,
gives $\mathbf{H} / (\lambda_\alpha df_e)$
which has the visual property that the
scaled $\mathbf{H}$ ellipsoid will protrude _somewhere_ outside the standard $\mathbf{E}$ ellipsoid if and only if
Roy's test is significant at significance level $\alpha$. The critical value $\lambda_\alpha$ for Roy's
test is
$$
\lambda_\alpha = \left(\frac{\text{df}_1}{\text{df}_2}\right) \; F_{\text{df}_1, \text{df}_2}^{1-\alpha} \comma
$$
where $\text{df}_1 = \max(p, \text{df}_h)$ and $\text{df}_2 = \text{df}_h + \text{df}_e - \text{df}_1$.

For these data, the HE plot using
significance scaling is shown in the right panel of @fig-iris-HE2. The left panel is the same as that
shown for sepal width and length in @fig-iris-HE1, but with axis limits to make the two plots
directly comparable.

```{r}
#| label: fig-iris-HE2
#| echo: false
#| out-width: "100%"
#| fig-cap: "HE plots for sepal width and sepal length in the iris dataset. Left: _effect_ scaling of the $\\mathbf{H}$ matrix; right: _significance_ scaling, where protrusion of $\\mathbf{H}$ outside $\\mathbf{E}$ indicates a significant effect by Roy's test."
knitr::include_graphics("images/iris-HE2.png")
```

You can interpret the plot using effect scaling to indicate that the overall "size" of variation
of the group means is roughly the same as that of within-group variation for the two sepal variables.
Significance scaling weights the evidence against the null hypothesis that a given effect is zero.
Clearly, the species vary significantly on the sepal variables, and the direction of the $\mathbf{H}$
ellipse suggests that those whose sepals are longer are also less wide.

## Visualizing contrasts and linear hypotheses {#sec-he-vis-contrasts}

As described in @sec-contrasts, tests of linear hypotheses and contrasts represented by the
general linear test $\mathcal{H}_0: \mathbf{C} \;\mathbf{B} = \mathbf{0}$ provide a
powerful way to probe the _specific_ effects represented within the global null hypothesis,
$\mathcal{H}_0: \mathbf{B} = \mathbf{0}$, that all effects are zero.

In this example the contrasts $\mathbf{c}_1$ (`Species1`) and $\mathbf{c}_2$ (`Species2`)
among the iris species are orthogonal, i.e., 
$\mathbf{c}_1^\top \mathbf{c}_2 = 0$. Therefore, their tests are statistically independent, and their
$\mathbf{H}$ matrices are additive. They fully decompose the general question of differences
among the groups into two independent questions regarding the contrasts.

$$
\mathbf{H}_\text{Species} = \mathbf{H}_\text{Species1} + \mathbf{H}_\text{Species2}
$$ {#eq-H-Species}

`car::linearHypothesis()` is the means for testing these statistically, and `heplot()`
provides the way to show these tests visually. Using the contrasts set up in @sec-iris-mod,
$\mathbf{c}_1$, representing the difference between _setosa_ and the other species
is labeled `Species1` and the comparison of _versicolor_ with _virginica_ is `Species2`.
The coefficients for these in $\mathbf{B}$ give the differences in the means.
The line for `(Intercept)` gives grand means of the variables.

```{r iris-mod-coef}
coef(iris.mod)
```

Numerical tests of hypotheses using `linearHypothesis()` can be specified in a very general way:
A matrix (or vector) $\mathbf{C}$ giving linear combinations of coefficients by rows, or a character vector giving the hypothesis in symbolic form.
A character variable or vector tests whether the named coefficients are different from zero
for all responses.

```{r iris-linhyp}
linearHypothesis(iris.mod, "Species1") |> print(SSP=FALSE)

linearHypothesis(iris.mod, "Species2") |> print(SSP=FALSE)
```

The various test statistics are all equivalent here---they give the same $F$ statistics--- because they
they have 1 degree of freedom.

In passing, from @eq-H-Species, note that the _joint_ test of these contrasts is exactly equivalent to the overall test of `Species` (results not shown).

```{r}
#| eval: false
linearHypothesis(iris.mod, c("Species1", "Species2"))
```

We can show these contrasts in an HE plot by supplying a named list for the `hypotheses` argument.
The names are used as labels in the plot.
In the case of a 1-df multivariate test, the $\mathbf{H}$ ellipses plot as a degenerate line.

```{r}
#| label: fig-iris-contrasts
#| fig-width: 6
#| fig-height: 6
#| out-width: "60%"
#| fig-cap: "HE plot for sepal length and width in the `iris` data showing the tests of the two contrasts, using significance scaling."
hyp <- list("S:Vv" = "Species1", "V:v" = "Species2")
heplot(iris.mod, hypotheses=hyp,
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1),
       col = c("red", "blue", "darkgreen", "darkgreen"),
       lty = c(0,0,1,1), label.pos = c(3, 1, 2, 1),
       xlim = c(2, 10), ylim = c(1.4, 4.6))
```

This HE plot shows that, for the two sepal variables, the greatest between-species variation
is accounted for by the contrast (`S:Vv`) between _setosa_ and the others, for which the effect is very
large in relation to error variation.  The second contrast (`V:v`), between the _versicolor_ and _virginica_
species is relatively smaller, but still explains significant variation of the sepal variables among the species.

The directions of these hypotheses in a given plot show _how_ the group means differ in terms of a given contrast.[^HE-contrast-geometry] For example, the contrast `S:Vv` is the line that separates _setosa_ from the others and indicates that _setosa_ flowers have shorter but wider sepals.

[^HE-contrast-geometry]: That the $\mathbf{H}$ ellipses for the contrasts subtend that for the overall test of `Species` is no accident.
In fact, this is true in $p$-dimensional space for _any_ linear hypothesis, and orthogonal contrasts
have the additional geometric property that they form _conjugate axes_ for the overall $\mathbf{H}$ ellipsoid
relative to the $\mathbf{E}$ ellipsoid [@Friendly-etal:ellipses:2013].

## HE plot matrices {#sec-HEplot-matrices}

In base R graphics, 2D scatterplots are extended to all pairwise views of multivariate data
with a `pairs()` method. For multivariate linear models, the `r package("heplots")` defines a
`pairs.mlm()` method to display HE plots for all pairs of the response variables.

```{r}
#| label: fig-iris-pairs
#| fig-width: 8
#| fig-height: 8
#| out-width: "100%"
#| fig-cap: "All pairwise HE plots for the iris data."
pairs(iris.mod,
      fill=TRUE, fill.alpha=c(0.3, 0.1))
```

@fig-iris-pairs provides a fairly complete visualization of the results of the multivariate tests
and answers the question: **how** do the species differ?
Sepal length and the two petal variables have the group means nearly perfectly correlated, in the
order _setosa_ < _versicolor_ < _virginica_. For Sepal width, however, _setosa_ has the largest mean,
and so the $\mathbf{H}$ ellipses show a negative correlation in the second row and column.


## Low-D views: Canonical analysis {#sec-candisc}

The HE plot framework so far provides views of all the effects in a MLM in 
_variable_ space. We can view this in 2D for selected pairs 
of response variables, or for all pairwise views in scatterplot matrix format, as in @fig-iris-pairs.

There is also an `heplot3d()` function giving plots for three response variables together.
The 3D plots are interactive, in that they can be rotated and zoomed
by mouse control, and dynamic, in that they can be made to spin and saved as movies.
To save space, these plots are not shown here.

However in a one-way MANOVA design with more than three response variables, it is difficult
to visualize how the groups vary on _all_ responses together, and how the
different variables contribute to discrimination among groups.
In this situation, **canonical discriminant analysis** (CDA) is often used, to provide
a low-D visualization of between-group variation.

When the predictors are also continuous, the analogous term is **canonical correlation analysis** (CCA),
described in @sec-cancor.
The advantage in both cases is that we can also show the relations of the response variables to these dimensions,
similar to a biplot (@sec-biplot) for a PCA of purely quantitative variables. 

But, to be clear: dimensions and variance accounted for in canonical space describe the relationships
_between_ response variables and factors or quantitative predictors, whereas PCA is only striving to
account for the total variation in the space of all variables.

The key to CDA is the eigenvalue decomposition of the $\mathbf{H}$ relative to $\mathbf{E}$
$\mathbf{H}\mathbf{E}^{-1} \lambda_i = \lambda_i \mathbf{v}_i$
(@eq-he-eigen). The eigenvalues, $\lambda_i$, give the "size" of each $s$ 
orthogonal dimensions on which the multivariate tests are based (@sec-H-vs-E). But the corresponding
eigenvectors, $\mathbf{v}_i$, give the _weights_ for the response variables
in $s$ linear combinations that maximally discriminate among the groups or equivalently maximize the
(canonical) $R^2$ of a linear combination of the predictor $\mathbf{X}$s with a linear combination of the
response $\mathbf{Y}$s.

Thus, CDA amounts to a transformation of the $p$ responses,
$\mat{Y}_{n \times p}$
into scores $\mathbf{Z}$ in the canonical space,

$$
\mat{Z}_{n \times s} = \mat{Y} \; \mathbf{E}^{-1/2} \; \mat{V} \;,
$$ {#eq-canZ}

where $\mat{V}$ contains the eigenvectors of $\mathbf{H}\mathbf{E}^{-1}$
and $s=\min ( p, \textbf{df}_h )$ dimensions, the degrees of freedom for the hypothesis.
It is well-known (e.g., @Gittins:85)
that _canonical discriminant plots_ of the first two (or three, in 3D)
columns of $\mat{Z}$ corresponding to the largest canonical correlations
provide an optimal low-D display of the variation between groups relative
to variation within groups.

Canonical discriminant analysis is typically carried out in conjunction with a one-way MANOVA design.
The `r package("candisc", cite=TRUE)` generalizes this to multi-factor designs in the `candisc()` function.
For any given term in a `"mlm"`, the _generalized canonical discriminant analysis_ amounts to a standard discriminant analysis based on the $\mathbf{H}$ matrix for _that_ term in relation to the full-model $\mathbf{E}$ matrix.[^candiscList]

[^candiscList]: The related `candiscList()` function performs a generalized canonical discriminant analysis for _all_ terms in a multivariate linear model, returning a list of the results for each factor.

Tests based on the eigenvalues $\lambda_i$,
initially stated by @Bartlett1938, use  Wilks' $\Lambda$ likelihood ratio tests of these.
This allow you to determine the number of significant canonical dimensions, or the number of different
aspects to consider for the relations between the responses and predictors.
This is a big win over univariate analyses for each dependent variable separately as
follow-ups for a significant MANOVA result.

These take the form of _sequential_ global tests of the hypothesis that the canonical correlation in
the current row and _all that follow_ are zero. Thus, if you find that the second dimension is
insignificant, there is no need to look at any further down the list.
The canonical $R^2$, `CanRsq`, gives the R-squared value of fitting the $i$th response canonical variate to the corresponding $i$th canonical variate for the predictors.

For the iris data, we get the following printed summary:

```{r iris-can}
iris.can <- candisc(iris.mod) |> 
  print()
```

This analysis shows a very simple result: The differences among the iris species can
be nearly entirely accounted for by the first canonical dimension (99.1%).
Interestingly, the second dimension is also highly significant, even though it accounts for only 0.88%.

### Coeficients

The `coef()` method for "candisc" objects returns a matrix of weights for the response variables
in the canonical dimensions. By default, these are given for the response variables _standardized_
to $\bar{y}=0$ and $s^2_y = 1$. 

The `type` argument also allows for _raw score_ weights (`type = "raw"`)
used to compute scores for the observations  on the canonical variables `Can1`, `Can2`, ... .
Using `type = "structure"` gives the canonical structure coefficients, which are the correlations between each response and the
canonical scores.

```{r iris-can-coef}
coef(iris.can, type = "std")
coef(iris.can, type = "structure")
```

The standardized (or raw score) weights are interpreted in terms of their signs and magnitudes,
just as in coefficient weights in a multiple regression. From the numbers, `Can1` seems to be a contrast between the sepal and petal variables. For `Can2`, sepal length doesn't matter and the result
contrasts the two width variables against petal length.

I find it easier to interpret the correlations between the observed and canonical variables, given as
the canonical structure coefficients. These are easily visualized as vectors in canonical space
(similar to biplots for a PCA), as shown below (@fig-iris-candisc).

<!--
Thus, the two weighted sums for the canonical variates, using mean-centered standardized scores, can be shown as follows:

```{r can-vars}
#| eval: false
#| echo: false
#| results: asis
C <- coef(iris.can, type = "std")
names <- row.names(C)
names <- c("S.len", "S.wid", "P.len", "P.wid")
can1 <- paste("Can1 = ", paste0(round(C[,1], 2), " x ", names, collapse = " + "))
#can1 <- gsub("+ -", "-", can1)
cat(can1,"\n")
can2 <- paste("Can2 = ", paste0(round(C[,2], 2), " x ", names, collapse = " + ")) |> cat()
```


\begin{eqnarray*}
\text{Can}_1 & = 0.43 \cdot \text{S.len} + 0.52 \cdot \text{S.wid} - 0.95 \cdot \text{P.len} - 0.58 \cdot \text{P.length} \\
\text{Can}_2 & = 0.01 \cdot \text{S.len} + 0.74 \cdot \text{S.wid} + 0.4 \cdot \text{P.len} + 0.58 \cdot \text{P.length}
\end{eqnarray*}

-->

### Canonical scores plot

The `plot()` method for `"candisc"` objects gives a plot of these observation scores in any two dimensions.
The argument `ellipse=TRUE` overlays this with their standard data ellipses for each species, as shown in @fig-iris-candisc. 

Analogous to the biplot (@sec-biplot),
the response variables are shown as
vectors, using the structure coefficients. Thus, the relative size of the projection of these
vectors on the canonical axes reflects the correlation of the observed response on the canonical dimension.
For ease of interpretation I flipped the sign of the first canonical dimension (`rev.axes`), so that the positive `Can1` direction
corresponds to larger flowers.

<!-- **TODO**: `var.labels` not a graphical parameter -->

```{r echo=-1}
#| label: fig-iris-candisc
#| fig-width: 8
#| fig-height: 7
#| out-width: "80%"
#| fig-cap: "Plot of canonical scores for the iris data. Ellipses give 68% coverage data ellipses for the canonical scores. Variable vectors make angles with the Can1 and Can2 axes indicating their correlations."
op <- par(mar = c(4, 4, 4, 1) + .5)
vars <- names(iris)[1:4] |> 
  stringr::str_replace("\\.", "\n")
plot(iris.can,
     var.labels = vars,
     var.col = "black",
     var.lwd = 2,
     ellipse=TRUE,
     scale = 9,
     col = iris_colors,
     pch = 15:17,
     cex = 0.7, var.cex = 1.25,
     rev.axes = c(TRUE, FALSE),
     xlim = c(-10, 10),
     cex.lab = 1.5)
```

The interpretation of
this plot is simple: in canonical space, variation of the means for the iris species is
essentially one-dimensional (99.1% of the effect of `Species`), and this dimension corresponds
to overall size of the iris flowers: The Setosas have smaller flowers.  

All variables except for `Sepal.Width` are positively aligned with
this axis, but the two petal variables show the greatest discrimination.
The negative direction for `Sepal.Width` reflects the pattern seen in @fig-iris-pairs, where _setosa_
have wider sepals.

For the second dimension, look at the projections of the variable vectors on the `Can2` axis.
All are positive, but this is dominated by `Sepal.Width`. We could call this a flower shape dimension.



### Canonical HE plot
For a one-way design, the _canonical HE plot_
is simply the HE plot of the canonical scores in the analogous MLM model that substitutes $\mat{Z}$ for $\mat{Y}$. 
In effect, it is a more compact visual summary of the plot shown of canonical scores in @fig-iris-candisc.

This is shown in @fig-iris-HEcan for the `iris` data.
In canonical space, the residuals are always uncorrelated, so the $\mathbf{E}$ ellipse plots as a circle.
The $\mathbf{H}$ ellipse for `Species` here reflects a data ellipse for the fitted values--- group means--- shown as
labeled points in the plot. The differences among `Species` are so large that this plot uses `size = "effect"`
scaling, making the axes comparable to those in @fig-iris-candisc.[^can-signif-scaling]

[^can-signif-scaling]:  If significance scaling was used the interpretation of the canonical HE plot plot would the same as before: if the hypothesis ellipse extends beyond the error ellipse, then that dimension is significant.  

The vectors for each predictor are the same structure coefficients as in the ordinary canonical plot.
They can again be reflected for easier interpretation and scaled in length to fill the plot window.

<!-- now depends on candisc 0.9.1 -->

```{r echo=-1}
#| label: fig-iris-HEcan
#| fig-width: 8
#| fig-height: 8
#| out-width: "80%"
#| fig-cap: "Canonical HE plot for the iris data. Compared with @fig-iris-candisc, it substutes canonical $\\mathbf{H}$ and $\\mathbf{E}$ ellipses for the canonical scores shown there."
op <- par(mar = c(4, 4, 4, 1) + .5)
heplot(iris.can,
       size = "effect",
       scale = 8,
       var.labels = vars, var.col = "black",
       var.lwd = 2, var.cex = 1.25,
       fill = TRUE, fill.alpha = 0.2,
       rev.axes = c(TRUE, FALSE),
       xlim = c(-10, 10),
       cex.lab = 1.5)
```

The collection of plots shown for the iris data here can be seen as progressive visual summaries of the 
data and increased visual understanding of the morphology of Anderson's _Iris_ flowers:

* The scatterplot matrix in @fig-iris-spm shows the iris flowers in the data space of the sepal and petal variables.
* Canonical analysis substitutes for these the two linear combinations reflected in `Can1` and `Can2`. The plot in @fig-iris-candisc portrays _exactly_ the same relations among the species, but in the reduced canonical space of only two dimensions.
* The HE plot version, shown in @fig-iris-HEcan summarizes the separate data ellipses for the species with
pooled, within-group variance of the $\mathbf{E}$ matrix for the canonical variables, which are always uncorrelated.
The variation among the group means is reflected in the size and shape of the ellipse for the $\mathbf{E}$ matrix.

## Factorial MANOVA {#sec-HE-factorial}

When there are two or more factors, the overall model
is comprised of main effects and possible interactions as shown in @eq-HE-model.
A significant advantage of HE plots is that they show how the response variables are related
in their effects. Moreover, the main effects and interactions
can be overlaid in the same plot showing how each term contributes to
assessment of differences among the groups. 


::: {#exm-plastic2}
**Plastic film data**

I illustrate these points below for the Plastic film data analyzed earlier in @exm-plastic1.
The model contemplated there examined how the three response variables, resistance to `tear`, film `gloss` and the `opacity` of the film varied with the two experimental factors, `rate` of extrusion and amount of some `additive`, both at two levels, labeled `High` and `Low`. Both main effects and the interaction of rate and additive were fit in the model `plastic.mod`: 

```{r plastic-mod}
plastic.mod <- lm(cbind(tear, gloss, opacity) ~ rate * additive, 
                  data=Plastic)
```


We can visualize _all_ these
effects for pairs of variables in an HE plot, showing the "size" and
orientation of hypothesis variation ($\mathbf{H}$) for each model term,
in relation to error
variation ($\mathbf{E}$), as ellipsoids.
When, as here, the model terms have 1 degree of freedom, the
$\mathbf{H}$ ellipsoids for `rate`, `additive` and `rate:additive` each degenerate to a line.

@fig-plastic-HE1 shows the HE plot for the responses `tear` and `gloss`, the strongest
by univariate tests. This plot takes advantage of another feature of
`heplot()`: You can overlay plots using `add = TRUE`, as is done here to show both significance and effect size scaling in
a single plot.

```{r echo = -1}
#| label: fig-plastic-HE1
#| fig-width: 6
#| fig-height: 6
#| out-width: "70%"
#| fig-cap: "HE plot for effects on `tear` and `gloss` according to the factors `rate`, `additive` and their interaction, `rate:additive`. The thicker lines show effect size scaling; thinner lines show significance scaling."
par(mar = c(4,4,1,1)+.1)
colors = c("red", "darkblue", "darkgreen", "brown4")
heplot(plastic.mod, size="significance", 
       col=colors, cex=1.5,  cex.lab = 1.5,
       fill=TRUE, fill.alpha=0.1)
heplot(plastic.mod, size="effect", 
       col=colors, lwd=6,
       add=TRUE, term.labels=FALSE)
```

In this view, the main effect of extrusion `rate` is highly significant, with the high level giving larger tear resistance and lower gloss on average. High level of `additive` produces greater tear resistance and higher gloss.  The interaction
effect, `rate:additive`, while non-significant, points nearly entirely in the direction of `gloss`.
You can see this more directly in @fig-plastic-ggline, where the lines for `gloss` diverge.

But what if you also wanted to show the means for the _combinations_ of rate and additive in an HE plot?
By design, means for the levels of interaction terms are not shown in the HE plot,
because doing so in general can lead to messy displays.

We can add them here for the term `rate:additive` as shown in @fig-plastic-HE2. This uses `heplots::termMeans()`
to find the cell means for the combinations of the two factors and then `lines()` to connect the pairs of points
for the low and high levels of `additive`.

```{r}
#| label: fig-plastic-HE2
#| fig-width: 6
#| fig-height: 6
#| out-width: "70%"
#| fig-cap: "HE plot for effects on `tear` and `gloss` using significance scaling. To this is added points showing the means for the combinations of rate and additive."
par(mar = c(4,4,1,1)+.1)
heplot(plastic.mod, size="evidence", 
       col=colors, cex=1.5, cex.lab = 1.5, 
       lwd = c(1, 5),
       fill=TRUE, fill.alpha=0.05)

# add interaction means
intMeans <- termMeans(plastic.mod, 'rate:additive', 
                      abbrev.levels=3)
points(intMeans[,1], intMeans[,2], pch=18, cex=1.2, col="brown4")
text(intMeans[,1], intMeans[,2], rownames(intMeans), 
     adj=c(0.5, 1), col="brown4")
lines(intMeans[c(1,3),1], intMeans[c(1,3),2], 
      col="brown4", lwd = 3)
lines(intMeans[c(2,4),1], intMeans[c(2,4),2], 
      col="brown4", lwd = 3)

```

@fig-plastic-HE2 is somewhat more complicated to interpret than the simple line plots in @fig-plastic-ggline,
but has the advantage that it shows effects on the two response variables jointly.
:::

::: {#exm-MockJury2}
**MockJury: Manipulation check**

In @exm-MockJury1, I examined the effects of the attractiveness of photos of hypothetical women defendants
and the nature of a crime on the judgments made by members of a mock jury, on the rated seriousness of
the crime and the length of a prison sentence participants would give to a guilty defendant. This analysis
used the following model, fitting `Serious` and `Years` of sentence to the combinations of
`Attr` and `Crime`:

```{r jury-mod}
data(MockJury, package = "heplots")
jury.mod <- lm(cbind(Serious, Years) ~ Attr * Crime, 
                data=MockJury)
```


The photos were classified as "beautiful", of "average" beauty or "unattractive", and as a validity check
on this experimental manipulation, 1--9 ratings on twelve attributes were also collected. Of these,
the direct rating of physical attractiveness, `physattr`, was most important, but it was also of interest
to see how other ratings differentiated the photos. The rating scales are the following:

```{r jury-names}
names(MockJury)[-(1:4)] 
```

To keep the graphs below simple, I consider only a subset of the ratings here, and fit the full $3 \times 2$ MANOVA model
for `Attr` and `Crime` and their interaction. The main interest is in the attractiveness of the photo,
but the other terms are included in the model to control for them.[^control2]

[^control2]: As in univariate linear models, any factors or covariates ignored in the model formula have their effects
pooled with the error term, reducing the sensitivity of their tests.

```{r jury-mod1}
jury.mod1 <- lm(cbind(phyattr, exciting, sociable, happy, independent) ~ Attr * Crime, 
                  data=MockJury)
uniStats(jury.mod1)
```

The `pairs.mlm()` plot for this model would be a $5 \times 5$ display showing all pairwise HE plots for this model.
@fig-jury-HE selects two of these showing `phyattr` against `exciting` and `independent`.
Although the model includes the full factorial of `Attr` and `Crime`, I only want to show the effect of
`Attr` here, so I do this using the `terms` and `factor.means` arguments. Because the ratings are on the same 1--9 scale,
I also use `asp = 1` to make response variables visually comparable.

```{r echo = -1}
#| label: fig-jury-HE
#| fig-width: 10
#| fig-height: 5
#| out-width: "100%"
#| fig-cap: "Two pairwise HE plots showing the effect of the classified attractivenes of the photo and ratings of those photos. Left: `exciting` vs. `phyattr` ratings; right: `independent` vs. `phyattr` ratings."
op <- par(mar = c(4, 4, 1, 1)+0.5, mfrow = c(1, 2))
heplot(jury.mod1, 
       terms = "Attr", factor.means = "Attr",
       fill = TRUE, fill.alpha = 0.2,
       cex = 1.2, cex.lab = 1.6, asp = 1)

heplot(jury.mod1,
       variables = c(1,5),
       terms = "Attr", factor.means = "Attr",
       fill = TRUE, fill.alpha = 0.2,
       cex = 1.2, cex.lab = 1.6, asp = 1)
```

These plots show that the means of the ratings of `phyattr` of the photos are in the expected order 
(Beautiful > Average > Unattractive), though the largest difference is between Beautiful and the others in both panels.[^contrast]

In the left panel of @fig-jury-HE, the means for `exciting` are nearly perfectly correlated with those for `phyattr`,
and there is little difference between the Beautiful photos and the others.
The means for `independent` are slightly positively correlated with those for `phyattr`, but there is a wider
separation between Average and Unattractive photos. The right panel shows the same order of the means
for `phyattr`, but the photos of Average attractiveness are rated as highest on independence.

[^contrast]: You could test this comparison with a more focused 1 df linear hypothesis using the contrast
$(1, -\frac12, -\frac12)$ for the levels of `Attr`.

**Canonical analysis**

As before, a canonical analysis squeezes the juice in a collection of responses into fewer dimensions,
allowing you to see relationships not not apparent in all pairwise views.
With 2 degrees of freedom for `Attr`, the space for all the ratings is 2D. `candisc()` for this term[^candisc-terms] shows that
91% of the variation between photo groups is accounted for in one dimension, but there is still some significant variation
associated with the 2$^\text{nd}$ dimension.

[^candisc-terms]: For MANOVA designs there is a separate analysis for each term in the model because there are different weights for the response variables.

```{r jury-can}
jury.can1 <- candisc(jury.mod1, term = "Attr") |>
  print()
```

The 2D plot of canonical scores in @fig-jury-can gives a very simple description for this model.
Dimension `Can1` widely separates the photo groups, and this dimension is nearly perfectly aligned
with the `phyattr` ratings. Ratings for `exciting` are also strongly associated with this.
This finding is sufficient to claim the validity of the classification 
used in the experiment.

```{r echo=-1}
#| label: fig-jury-can
#| fig-width: 10
#| fig-height: 7
#| out-width: "80%"
#| fig-cap: !expr paste0("Canonical discriminant plot for the factor `Attr` in the model `jury.mod1` for the ratings of the photos classified as ", colorize("Beautiful", "blue"), ", ", colorize("Average", "darkgreen"), " or ", colorize("Unattractive", "red"), ".  Ellipses have 50% coverage for the canonical scores.  Variable vectors reflect the correlations of the rating scales with the canonical dimensions.")
op <- par(mar = c(4, 4, 1, 1)+0.5)
col <- c("blue", "darkgreen", "red")
plot(jury.can1, rev.axes = c(TRUE, TRUE),
     col = col,
     ellipse = TRUE, ellipse.prob = 0.5,
     lwd = 3,
     var.lwd = 2,
     var.cex = 1.4,
     var.col = "black",
     pch = 15:17,
     cex = 1.4,
     cex.lab = 1.5)
```

The second canonical dimension, `Can2` is also of some interest. The photo means differ here mainly between
those considered of Average beauty and those classified as Unattractive. The ratings for independent and happy
are most strongly associated with this dimension. Ratings on `sociable` are related to both dimensions, but more so with
`Can2`.

:::

## Quantitative predictors: MMRA {#sec-he-mmra}

The ideas behind HE plots extend naturally to multivariate multiple regression (MMRA).
<!--
and multivariate analysis of covariance (MANCOVA). In MMRA designs, the $\mathbf{X}$ matrix contains only quantitative predictors, while in MANCOVA designs, it contains a mixture of factors and quantitative predictors (covariates), but typically there is just one “group” factor.

In the MANCOVA case, there is often a subtle difference in emphasis: true MANCOVA analyses focus on the differences among groups defined by the factors, adjusting for (or controlling for) the quantitative covariates. Analyses concerned with homogeneity of regression focus on quantitative predictors and attempt to test whether the regression relations are the same for all groups defined by the factors.
-->
A purely visual feature of HE plots in these cases is that the $\mathbf{H}$ ellipse for a quantitative predictor
with 1 df appears as a degenerate line. But consequently, the angles between these for different
predictors has a simple interpretation as as the correlation between their predicted effects.
Moreover, it is easy to show visual overall tests of _joint_ linear hypotheses for two or more
predictors together.


**TODO**: Use for an exercise `heplots::Hernior`: Recovery from Elective Herniorrhaphy -> HE_mmra vignette 

<!-- ### Example: NLSY data {#sec-NLSY-HE} -->

::: {#exm-NLSY-HE}
**NLSY data**

Here I'll continue the analysis of the NLSY data from @sec-NLSY-mmra. In the model `NLSY.mod1`, I used
only father's income  and education to predict scores in reading and math, and both 
of these demographic variables were highly significant. @fig-NLSY-heplot1 shows what this looks like
in an HE plot.

```{r}
#| label: fig-NLSY-heplot1
#| out-width: "80%"
#| fig-cap: "HE plot for the simple model for the NLSY data fitting reading and math scores from income and education."
data(NLSY, package = "heplots")
NLSY.mod1 <- lm(cbind(read, math) ~ income + educ, 
                data = NLSY)

heplot(NLSY.mod1, 
  fill=TRUE, fill.alpha = 0.2, 
  cex = 1.5, cex.lab = 1.5,
  lwd=c(2, 3, 3),
  label.pos = c("bottom", "top", "top")
  )

```

Fathers income and education are positively correlated in their effects on the outcome scores.
From the angles in the plot, income is most related to the math score, while education is related
to both, but slightly more to the reading score.

The overall joint test for both predictors can then be visualized as the test of the linear hypothesis
$\mathcal{H}_0 : \mathbf{B} = [\boldsymbol{\beta}_\text{income}, \boldsymbol{\beta}_\text{educ}] = \mathbf{0}$.
For `heplot()`, we specify the names of the coefficients to be tested with the `hypotheses` argument.

```{r}
#| label: fig-NLSY-heplot2
#| out-width: "80%"
#| fig-cap: "HE plot adding the $\\mathbf{H}$ ellipse for the overall test that both predictors have no effect on the outcome scores."
coefs <- rownames(coef(NLSY.mod1))[-1] |> print()

heplot(NLSY.mod1, 
       hypotheses = list("Overall" = coefs),
       fill=TRUE, fill.alpha = 0.2, 
       cex = 1.5, cex.lab = 1.5,
       lwd=c(2, 3, 3, 2),
       label.pos = c("bottom", "top"))
```

The geometric relations of the $\mathbf{H}$ ellipses for the overall test and the
individual predictors in @fig-NLSY-heplot2 are worth noting here. Those for the
separate coefficients always lie within the overall ellipse. The contribution
for `income` makes the overall ellipse larger in the direction of the `math` score,
while the contribution of education makes it larger in both directions.
:::

<!-- ### Example: School data {#sec-schooldata-HE} -->

::: {#exm-schooldata-HE}
**School data: HE plots**

The `schooldata` dataset analyzed in @sec-schooldata-mmra can also be illuminated by the methods of this
chapter. There I fit the multivariate regression model predicting students scores on
reading, mathematics and a measure of self-esteem using as predictors measures of parents' education, occupation,
school visits, counseling help with school assignments and number of teachers per school.

But I also found two highly influential observations (cases 44, 59; see @fig-schoolmod-infl) whose effect on the
coefficients is rather large; so, I remove them from the analysis here.[^removed-cases]

[^removed-cases]: An alternative to fitting the model removing specific cases deemed troublesome is to use
a **robust** method, such as `heplots::roblm()`. This uses re-weighted least squares to down-weight observations
with large residuals or other problems. These methods are illustrated in @sec-robust-estimation.

```{r}
data(schooldata, package = "heplots")

bad <- c(44, 59)
OK <- (1:nrow(schooldata)) |> setdiff(bad)
school.mod2 <- lm(cbind(reading, mathematics, selfesteem) ~ ., 
                  data=schooldata[OK, ])
```

In this model, parent's education and occupation
and their visits to the schools were highly predictive of student's outcomes but
their counseling efforts and the number of teachers in the schools did not contribute much.
However, the nature of these relationships was largely uninterpreted in that analysis.

Here is where HE plots can help. You can think of this as a way to visualize what is entailed
in the coefficients for this model by showing the _magnitude_ of the predictor effects by their size
and their relations to the outcome variable by their _direction_. The table of raw score coefficients
isn't very helpful in this regard.

```{r}
coef(school.mod2)
```


@fig-school-heplot1 shows the HE plot for reading and mathematics scores in this model, using the default
significance scaling.
```{r echo = -1}
#| label: fig-school-heplot1
#| out-width: "80%"
#| fig-cap: "HE plot for reading and mathematics scores in the multivariate regression model for the school dataset. Predictor effects appear as lines whose lenght indicates the magnitude of the relationship and whose orientation reflects their correlations with the outcome variables shown in the plot."
op <- par(mar = c(4, 4, 1, 2) + .1)
heplot(school.mod2, 
       fill=TRUE, fill.alpha=0.1,
       cex = 1.5,
       cex.lab = 1.5,
       label.pos = c(rep("top", 4), "bottom", "bottom"))
```

In @fig-school-heplot1, you can readily see:

* Parent's occupation and education are both significant in this view, but what is more important is their
orientation. Both are positively associated with reading and math scores, but education is somewhat more
related to reading than to mathematics. 

* Number of teachers and degree of parental counseling have a
similar orientation, with teachers having a greater relation to mathematics scores.

* Visits to school and number of teachers are not significant in
this plot, but both are positively correlated with reading and math and are coincident in the plot. 

* The parent time counseling measure, while also insignificant,
tilts in the opposite direction, having different signs for reading and math.

In the `pairs()` plot for all three responses (@fig-school-heplot2), we see something different in the relations for self-esteem. While occupation has a large positive relation in all the plots in the third row and column,
education, counseling and teachers have negative relations in these plots, particularly with mathematics scores.

```{r}
#| label: fig-school-heplot2
#| fig-width: 9
#| fig-height: 9
#| out-width: "100%"
#| fig-cap: "Pairwise HE plots for the three outcome variables in the multivariate regression model for the school dataset."
pairs(school.mod2, 
      fill=TRUE, fill.alpha=0.1,
      var.cex = 2.5,
      cex = 1.3)
```

The analysis of this data is continued below in @exm-schooldata-cancor.
:::


## Canonical correlation analysis {#sec-cancor}

Just as we saw for MANOVA designs, a canonical analysis for multivariate regression involves finding
a low-D view of the relations between predictors and outcomes that maximally explains their
relations in terms of linear combinations of each. 
That is, the goal is to find  weights
for one set of variables, say $\mathbf{X}$ _not_ to predict each of the other set 
$\mathbf{Y} =[\mathbf{y}_1, \mathbf{y}_2, \dots]$ _individually_, but rather to also find weights
for the $\mathbf{y}$s which is most highly correlated with the linear combination of the $\mathbf{x}$s.

In this sense, canonical correlation analysis (CCA) is _symmetric_ in the $x$ and $y$ variables:
the $y$ set is not considered responses. Rather the goal is simply to explain the correlations
between the two sets.
For a thorough treatment of this topic, see @Gittins:85.

Geometrically, these linear combinations are vectors representing projections in the observation space of the $x$ and $y$ variables, and CCA can also be thought of as
minimizing the angle between these vectors or maximizing the cosine of this angle. This is illustrated in @fig-cancor-diagram.

```{r}
#| label: fig-cancor-diagram
#| echo: false
#| out-width: "100%"
#| fig-cap: "Diagram illustrating canonical correlation. For two $y$ variables, all linear combinations are vectors in their plane, and similarly for the $x$ variables. Maximizing the correlation between linear combinations of each is equivalent to making the angle $\\phi$ between them as small as possible, or maximizing $\\cos({\\theta})$, shown in the diagram at the right. The thick grey arrow indignates that the two planes should be overlaid at a common origin. _Source_: Re-drawn by Udi Alter following a Cross-Validated discussion by user 'ttnphns', https://bit.ly/4dgq2cp"
knitr::include_graphics("images/cancor-diagram-udi.png")
```



Specifically, we want to find one set of weights $\mathbf{a}_1$ for the $x$ variables and another
for the $y$ variables to give the linear combinations $\mathbf{u}_1$ and $\mathbf{v}_1$,

\begin{eqnarray*}
\mathbf{u}_1 & = \mathbf{X} \ \mathbf{a}_1 = a_{11} \mathbf{x}_1 + a_{12} \mathbf{x}_2 + \cdots + a_{11} \mathbf{x}_q \\
\mathbf{v}_1 & = \mathbf{Y} \ \mathbf{b}_1 = b_{11} \mathbf{y}_1 + b_{12} \mathbf{y}_1 + \cdots + b_{11} \mathbf{y}_p \; ,
\end{eqnarray*}

such that the correlation $\rho_1 = \textrm{corr}(\mathbf{u}_1, \mathbf{v}_1)$ is maximized, or
equivalently, minimizing the angle between them.

Using
$\mathbf{S}_{xx}$, $\mathbf{S}_{yy}$ to represent the covariance matrices of the $x$ and $y$ variables,
and $\mathbf{S}_{xy}$ for the cross-covariances between the two sets, the 
correlation between the linear combinations of each can be expressed as

\begin{eqnarray*}
\rho_1 & = \textrm{corr}(\mathbf{u}_1, \mathbf{v}_1) 
         = \textrm{corr}(\mathbf{X} \ \mathbf{a}_1, \mathbf{Y} \ \mathbf{b}_1) \\
       & = \frac{\mathbf{a}_1^\top \ \mathbf{S}_{xy} \ \mathbf{b}_1 }{\sqrt{\mathbf{a}_1^\top \ \mathbf{S}_{xx} \  \mathbf{a}_1 } \sqrt{\mathbf{b}_1^\top \ \mathbf{S}_{yy} \ \mathbf{b}_1 }}
\end{eqnarray*}

But, the $y$ variables lie in a $p$-dimensional (observation) space, and the $x$ in $q$ dimensions, so
what they have common is a space of $s = \min(p, q)$ dimensions. Therefore, we can find additional
pairs of canonical variables,


\begin{eqnarray*}
\mathbf{u}_2 = \mathbf{X} \ \mathbf{a}_2 & \quad\quad \mathbf{v}_2 = \mathbf{Y} \ \mathbf{b}_2 \\
                                         & \vdots \\
\mathbf{u}_s = \mathbf{X} \ \mathbf{a}_s & \quad\quad \mathbf{v}_s = \mathbf{Y} \ \mathbf{b}_s \\
\end{eqnarray*}


such that each pair $(\mathbf{u}_i, \mathbf{v}_i)$ has the maximum possible correlation
and all distinct pairs are uncorrelated:

\begin{eqnarray*} 
\rho_i & =\max _{\mathbf{a}_i, \mathbf{b}_i}\left\{\mathbf{u}_i^{\top} \mathbf{v}_i\right\} = \\ 
\left\|\mathbf{u}_i\right\| & =1, \quad\left\|\mathbf{v}_i\right\|=1, \\ 
\mathbf{u}_i^{{\top}} \mathbf{u}_j & =0, \quad \mathbf{v}_i^{\top} \mathbf{v}_j=0 \quad\quad \forall j \neq i: i, j \in\{1,2, \ldots, s\} \ .
\end{eqnarray*}

In words, the correlations among canonical variables are zero except when when they are associated with the same canonical correlation or the weights $(\mathbf{a}_i, \mathbf{b}_i)$ for the same pair.
Alternatively, all $p \times q$ correlations the variables in $\mathbf{Y}$ and $\mathbf{X}$
are fully summarized in the $s = \min(p, q)$ canonical correlations $\rho_i$ for $i = 1, 2, \dots, s$.

The solution, developed by @Hotelling1936, is a form of a generalized eigenvalue problem, that can be
stated in two equivalent ways,

<!-- $$ -->
<!-- \left(\mathbf{S}_{y y}^{-1} \ \mathbf{S}_{y x} \ \mathbf{S}_{x x}^{-1} \ \mathbf{S}_{x y}-\rho^2 \ \mathbf{I}\right) \mathbf{v} = \mathbf{0} -->
<!-- $$ -->


$$
\begin{aligned}
& \left(\mathbf{S}_{y x} \ \mathbf{S}_{x x}^{-1} \ \mathbf{S}_{x y} - \rho^2 \ \mathbf{S}_{y y}\right) \mathbf{b} = \mathbf{0} \\
& \left(\mathbf{S}_{x y} \ \mathbf{S}_{y y}^{-1} \ \mathbf{S}_{y x} - \rho^2 \ \mathbf{S}_{x x}\right) \mathbf{a} = \mathbf{0} \ .
\end{aligned}
$$
Both equations have the same form and have the same eigenvalues. And, given the eigenvectors for one of these equations, we can find the eigenvectors for the other.

<!-- **TODO**: Fill in details of canonical correlations -->


<!-- ### Canonical analysis -->

::: {#exm-schooldata-cancor}
**School data: Canonical analysis**


For the school data, with $p = 3$ responses and $q = 5$ predictors there are three possible sets of canonical variables. Together these account for 100% of the total linear relations between them. `heplots::cancor()`
gives the percentage associated with each of the eigenvalues and the canonical correlations.

For this dataset, the first canonical variates, with Can $R = 0.995$, 
accounts for 98.6%, so you might think that that is
sufficient. Yet the likelihood ratio tests show that the second set, with Can $R = 0.774$,
is also significant, even though it only accounts for 1.3%.

```{r school-can}
school.can2 <- cancor(
  cbind(reading, mathematics, selfesteem) ~
        education + occupation + visit + counseling + teacher,
  data=schooldata[OK, ])
school.can2
```

The virtue of CCA is that _all_ correlations between the X and Y variables are completely captured
in the correlations between the _pairs_ of canonical scores: The $p \times q$ correlations between
the sets are entirely represented by the $s = \min(p, q)$ canonical ones.
Whether the second dimension is useful here depends on whether it adds some interpretable increment
to what is going on in these relations. One could be justifiably happy with an explanation based on
the first dimension that accounts for nearly all the total association between the sets.


The class `"cancor"` object returned by `cancor()` contains the canonical coefficients, for which there
is a `coef()` method as in `candisc()`, and also a `scores()` method to return the scores on the canonical
variables, called `Xcan1`, `Xcan2`, ... and `Ycan1`, `Ycan2`.

```{r school-can-names}
names(school.can2)
```

You can use the `plot()` method or `heplot()` method to visualize and help interpret the results.
The `plot()` method plots the canonical `scores$X` against the `scores$Y` for a given dimension
(selected by the `which` argument). The `id.n` argument gives a way to flag noteworthy observations.

```{r echo=-1}
#| label: fig-school-can
#| fig-width: 10
#| fig-height: 5
#| out-width: "100%"
#| fig-cap: "Plots of canonical scores for the first two canonical dimensions of the `schooldata` dataset, omitting the two highly influential cases."
op <- par(mar = c(4,4,1,1) + .1, mfrow = c(1, 2))
plot(school.can2, 
     pch=16, id.n = 3,
     cex.lab = 1.5, id.cex = 1.5,
     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))
text(-2, 1.5, paste("Can R =", round(school.can2$cancor[1], 3)), 
     cex = 1.4, pos = 4)

plot(school.can2, which = 2, 
     pch=16, id.n = 3,
     cex.lab = 1.5, id.cex = 1.5,
     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))
text(-3, 3, paste("Can R =", round(school.can2$cancor[2], 3)), 
     cex = 1.4, pos = 4)
par(op)
```

It is worthwhile to look at an analogous plot of canonical scores for the original dataset including
the two highly influential cases. As you can see in @fig-school-can0, cases 44 and 59 are way outside
the range of the rest of the data. Their influence increases the canonical correlation to a near perfect 
$\rho = 0.997$.

```{r}
#| label: fig-school-can0
#| fig-width: 8
#| fig-height: 8
#| out-width: "70%"
#| fig-cap: "Plots of canonical scores on the first canonical dimension for the `schooldata`, including the influential cases, which stand out as so far frome the rest of the observations."
school.can <- cancor(cbind(reading, mathematics, selfesteem) ~
                       education + occupation + visit + counseling + teacher,
                     data=schooldata)
plot(school.can, 
     pch=16, id.n = 3,
     cex.lab = 1.5, id.cex = 1.5,
     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))
text(-5, 1, paste("Can R =", round(school.can$cancor[1], 3)), 
     cex = 1.4, pos = 4)
```


Plots of canonical scores tell us of the strength of the canonical dimensions, but do not help
interpreting the analysis in relation to the original variables.
The HE plot version for canonical correlation analysis re-fits a multivariate regression model
for the Y variables against the Xs, but substitutes the canonical scores for each, essentially
projecting the data into canonical space.

**TODO**: Check out signs of structure coefs from `cancor()`. Would be better to reflect the vectors
for `Ycan1`.


```{r}
#| label: fig-school-hecan
#| fig-width: 8
#| fig-height: 8
#| out-width: "70%"
#| fig-cap: "HE plot for the canonical correlation analysis of the schooldata. Vectors for the variables indicate their correlations with the canonical dimensions."
heplot(school.can2,
       fill = TRUE, fill.alpha = 0.2,
       var.col = "red", 
       asp = NA, scale = 0.25,
       cex.lab = 1.5, cex = 1.25,
       prefix="Y canonical dimension ")
```

The `r colorize("red")` variable vectors shown in these plots are intended only to show the correlations of Y variables with the canonical dimensions. The fact that they are so closely aligned reflects
the fact that the first dimension accounts for nearly all of their associations with the predictors.
The orientation of the $\mathbf{H}$ ellipses/lines reflects the projection of those from @fig-school-heplot2 into canonical space

Only their _relative lengths_ and angles with respect to the Y canonical dimensions have meaning in these plots. Relative lengths correspond to proportions of variance accounted for in the Y canonical dimensions plotted; angles between the variable vectors and the canonical axes correspond to the structure correlations. The absolute lengths of these vectors are arbitrary and are
typically manipulated by the `scale` argument to provide better visual resolution and labeling for the variables.

:::

## MANCOVA models {#sec-he-mancova}

HE plots for designs containing a collection of quantitative predictors and one or more factors
are quite simple in MANCOVA models where the effects are additive, i.e., don't involve interactions.
They are a bit more challenging when you allow _separate_ slopes for groups on all quantitative 
variables, because there get to be too many terms to usefully display. But these models are more complicated!

If the evidence for heterogeneity of regressions is not very strong, it is still useful
to fit the MANCOVA model and display it in an HE plot.

An alternative is to fit separate models for the groups and display these as HE plots.
As noted earlier (@sec-PA-tasks), this is not ideal for testing hypotheses, but provides a useful and informative display of the relations between the predictors and responses and the groups effect.
I illustrate these approaches for the Rohwer data, encountered in @sec-PA-tasks, below.

<!-- ### Example: Rohwer data -->

::: {#exm-rowher-mancova}
**Rowher data**

In @sec-PA-tasks I fit several models for Rohwer's data on the relations between paired-associate
tasks and scholastic performance. The first model was the MANCOVA model testing the difference
between the high and low SES groups, controlling for, or taking into account differences on 
the paired-associate task.

```{r}
Rohwer.mod1 <- lm(cbind(SAT, PPVT, Raven) ~ SES + n + s + ns + na + ss, 
                 data=Rohwer)
```


HE plots for this model for the pairs (SAT, PPVT) and (SAT, Raven) is shown in @fig-rohwer-HE-mod1-pairs.
The result of an overall test for _all_ predictors, $\mathcal{H}_0 : \mathbf{B} = \mathbf{0}$,
is added to the basic plot using the `hypotheses` argument.


```{r}
#| label: fig-rohwer-HE-mod1-pairs
#| fig-width: 10
#| fig-height: 9
#| out-width: "100%"
#| fig-cap: "All-pairs HE plot for SAT, PPVT and Raven using the MANCOVA model. The ellipses labeled ‘Regr’ show the test of the overall effect of the quantitative predictors."
colors <- c("red", "blue", rep("black",5), "#969696")
covariates <- rownames(coef(Rohwer.mod1))[-(1:2)]
pairs(Rohwer.mod1, 
       col=colors,
       hypotheses=list("Regr" = covariates),
       fill = TRUE, fill.alpha = 0.1,
       cex=1.5, cex.lab = 1.5, var.cex = 3,
       lwd=c(2, rep(3,5), 4))
```


The positive effect of SES on the outcome measures is seen in all pairwise plots: the high SES group is better on all responses.
The positive orientation of the `Regr` ellipses for the covariates shows that the predicted values for all three responses are positively correlated (more so for SAT and PPVT): higher performance on the paired associate tasks, in general, is associated
with higher academic performance. The two significant predictors, `na` and `ns` are the only
ones that extend outside the error ellipses, but their orientations differ.

#### Homogeneity of regression {.unnumbered}
A second model tested the assumption of homogeneity of regression by adding interactions of
SES with the PA tasks, allowing separate slopes for the two groups on each of the other predictors.

```{r}
Rohwer.mod2 <- lm(cbind(SAT, PPVT, Raven) ~ SES * (n + s + ns + na + ss),
                  data=Rohwer)
```

This model has 11 terms, excluding the intercept: `SES`, plus 5 main effects ($x$s) for the predictors
and 5 interactions (slope differences), too many for an understandable display.
To visualize this in an HE plot (@fig-rohwer-HE-mod2), I simplify, by showing the interaction terms
_collectively_ by a single ellipse, representing their joint effect, and specified as a linear
hypothesis called `slopes` that picks out the interaction effects.

The argument `terms` limits the $\mathbf{H}$ ellipses for the right-hand-side of the model which are
shown to just those terms specified. The combined effect of the interaction terms is
specified as an hypothesis (`slopes`) testing the interaction terms (which have a ":" in their name).
Because SES is “treatment-coded” in this model, the interaction terms reflect the difference in slopes
for the high SES group compared to the low. 

```{r}
#| label: fig-rohwer-HE-mod2
#| fig-width: 6
#| fig-height: 6
#| out-width: "70%"
#| fig-cap: "HE plot for SAT and PPVT using the heterogeneous regression model. The ellipse labeled ‘Regr’ shows the test of the covariates combined, and the ellipse labeled 'slopes' shows the combined difference in slopes between the two groups."
(coefs <- rownames(coef(Rohwer.mod2)))

colors <- c("red", "blue", rep("black",5), "#969696")
heplot(Rohwer.mod2, col=c(colors, "darkgreen"), 
       terms=c("SES", "n", "s", "ns", "na", "ss"), 
       hypotheses=list("Regr" = c("n", "s", "ns", "na", "ss"),
                       "Slopes" = coefs[grep(":", coefs)]),
       fill = TRUE, fill.alpha = 0.2, cex.lab = 1.5)
```

#### Separate models {.unnumbered}

When there is heterogeneity of regressions,
using submodels for each of the groups has the advantage that you can easily visualize the slopes for
the predictors in each of the groups, particularly if you overlay the individual HE plots.
In this example, I'm using the models `Rohwer.sesLo` and `Rohwer.sesLo` fit to each of the groups.

```{r rohwer-submodels}
Rohwer.sesLo <- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, 
                   data=Rohwer, subset = SES=="Lo")
Rohwer.sesHi <- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, 
                   data=Rohwer, subset = SES=="Hi")
```

Here I make use of the fact that several HE plots can be overlaid using the option `add=TRUE` as shown in @fig-rohwer-HE-lohi.
The axis limits may need adjustment in the first plot so that the second one will fit.



```{r}
#| label: fig-rohwer-HE-lohi
#| fig-width: 6
#| fig-height: 6
#| out-width: "70%"
#| fig-cap: "Overlaid HE plots for SAT and PPVT, for the low and high SES groups, when each group is fit separately."
heplot(Rohwer.sesLo, 
       xlim = c(0,100),               # adjust axis limits
       ylim = c(40,110), 
       col=c("red", "black"), 
       fill = TRUE, fill.alpha = 0.1,
       lwd=2, cex=1.2, cex.lab = 1.5)
heplot(Rohwer.sesHi, 
       add=TRUE, 
       col=c("brown", "black"), 
       grand.mean=TRUE, 
       error.ellipse=TRUE,            # not shown by default when add=TRUE
       fill = TRUE, fill.alpha = 0.1,
       lwd=2, cex=1.2)
```

We can readily see the difference in means for the two SES groups (Hi has greater scores on both variables) and it also appears that the slopes of the `s` and `n` predictor ellipses are shallower for the High than the Low group, indicating greater relation with the SAT score. As well, the error ellipses show that on these measures, error variation is somewhat smaller in the low SES group.

:::

<!--
## Chapter summary

**TODO**: Insert chapter summary

* This chapter explains and illustrates the use of HE plots for understanding the effects and significance tests of model terms in multivariate linear models for factor predictors (MANOVA), quantitative predictors
(MMRA) and models for mixtures of these types, that I've collectively called MANCOVA models.
-->

```{r child="summary/Ch11-summary.qmd"}
```


```{r}
#| echo: false
#cat("Writing packages to ", .pkg_file, "\n")
write_pkgs(file = .pkg_file, quiet = TRUE)
```

