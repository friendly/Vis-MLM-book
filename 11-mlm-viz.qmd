```{r include=FALSE}
source("R/common.R")
knitr::opts_chunk$set(fig.path = "figs/ch11/")
```

::: {.content-visible unless-format="pdf"}
{{< include latex/latex-commands.qmd >}}
:::

# Visualizing Multivariate Models {#sec-vis-mlm}

The methods discussed in @sec-mlm-review provide the basis for a rather complete multivariate analysis
of traditional univariate methods for the same designs. You can carry out multiple regression,
ANOVA, or indeed, any classical linear model with the standard collection of analysis tools you use for
a single outcome variable, but naturally extended in most cases to having several outcomes to analyse
together. The key points are:

- Everything you know about the usual univariate models-- regression coefficients, main effects and contrasts for factors, interactions of model terms,  ... applies here.

- By a rather clever design, called "matrix algebra" the separate univariate models can be combined, by turning vectors of responses, $\mathbf{y}_1, \mathbf{y}_2, \dots$ into a matrix $\mathbf{Y}$. Bingo! We get a
multivariate extension.

- You can treat this as a collection of separate models, one for each response, because the coefficients are the same.
But, the benefit of a multivariate approach is that you also get an overall multivariate test for each term in the model.

As nice as these mathematical and statistical ideas might be, the fact that the analysis is conducted for the response
variables _collectively_, means that it may be harder to interpret and explain what this means about the separate
responses. Here's where multivariate model visualization comes to the rescue!

* The tests of multivariate models, including multivariate analysis of variance (MANOVA) for group differences and multivariate multiple regression (MMRA) can be easily visualized by plots of a hypothesis ("H") data ellipse for the fitted values, relative to the corresponding plot of the error ellipse ("E") of the residuals, which I call the HE plot framework.

* For more than a few response variables, these result can be projected onto a lower-dimensional "canonical" space providing an even simpler description. Vectors for the response variables in this space show how these relate to the canonical dimension, facilitating interpretation. This answers the objection of @Huang2019 and others that multivariate models are difficult to understand because they are framed in terms of linear combinations of the the responses. The difficulty in understanding, I believe, can be cured by methods for visualizing what these
linear combinations reflect in terms of the observed variables.

<!-- 
- refer to Huang: "_given the challenges and assumptions involved in appropriately performing and interpreting MANOVA results, researchers are better off using other less error-prone procedures._" 
-->

**Packages**

In this chapter we use the following packages. I load them now.
```{r}
library(car)
library(heplots)
library(candisc)
library(ggplot2)
library(dplyr)
library(tidyr)
```

## HE plot framework {#sec-he-framework}

@sec-Hotelling illustrated the basic ideas of the framework for visualizing multivariate
linear models in the context of a simple two group design, using Hotelling's $T^2$.  The main ideas were illustrated in @fig-HE-framework.

Having described the statistical ideas behind the MLM in @sec-mlm-review, we can proceed to
extend this framework to larger designs. @fig-dogfood-quartet illustrates these ideas using the
simple one-way MANOVA design of the dogfood data from @sec-dogfood-data.

```{r}
#| label: fig-dogfood-quartet
#| echo: false
#| fig-align: center
#| out-width: "100%"
#| fig-cap: "**Dogfood quartet**: Illustration of the conceptual ideas of the HE plot framework for the dogfood data. (a) Scatterplot of the data; (b) Summary using data ellipses; (c) HE plot shows the variation in the means in relation to pooled within group variance; (d) Transformation from data space to canonical space"
knitr::include_graphics("images/dogfood-quartet.png")
```



* In data space (a), each group is summarized by its **data ellipse** (b), representing the means and covariances.

* Variation against the hypothesis of equal means can be seen by the $\mathbf{H}$ ellipse in the **HE plot** (c), representing the data ellipse of the fitted values. Error variance is shown in the $\mathbf{E}$ ellipse,
representing the pooled within-group covariance matrix, $\mathbf{S}_p$ and the data ellipse of the residuals from the model.
For the dogfood data, the group means have a negative relation: longer time to start eating is associated with a smaller amount eaten.

* The MANOVA (or Hotelling's $T^2$) is formally equivalent to a **discriminant analysis**, predicting
group membership from the response variables which can be seen in data space. (The main difference is 
emphasis and goals: MANOVA seeks to test differences among group means, while discriminant analysis aims
at classification of the observations into groups.)

* This effectively projects the $p$-dimensional
space of the predictors into the smaller **canonical space** (d) that shows the greatest differences among
the groups. As in a biplot, vectors show the relations of the response variables with the canonical dimensions.

<!--
```{r}
#| label: fig-HE-framework
#| echo: false
#| fig-align: center
#| out-width: "100%"
#| fig-cap: "The Hypothesis Error plot framework for a two-group design. Above: Data ellipses can be summarized in an HE plot showing the pooled within-group error ($\\mathbf{E}$) ellipse and the $\\mathbf{H}$ 'ellipse' for the group means.
#| Below: Observations projected on the line joining the means give discriminant scores which correpond to a one-dimensional canonical space, represented by a boxplot of their scores and arrows reflecting the variable weights."
knitr::include_graphics("images/HE-framework.png")
```
-->

<!--
Having described the statistical ideas behind the MLM in @sec-mlm-review, we can proceed to
extend this framework to larger designs. A conceptual overview is shown in @fig-arcmanov1 for a one-way MANOVA
design with 8 groups.

```{r}
#| label: fig-arcmanov1
#| echo: false
#| fig-align: center
#| out-width: "100%"
#| fig-cap: "Conceptual plots showing the essential ideas behind multivariate tests, in terms of the hypothesis
#| ($\\mathbf{H}$) and error ($\\mathbf{E}$) matrices for a 1-way MANOVA design with two response variables, $Y_1$ and $Y_2$"
knitr::include_graphics("images/arcmanov.png")
```
-->


For more complex models such as MANOVA with multiple factors or multivariate multivariate regression,
there is one sum of squares and products matrix (SSP), and therefore one
$\mathbf{H}$ ellipse for each term in the model. For example, in a two-way MANOVA design with the model
formula `(y1, y2) ~ A + B + A*B` and equal sample sizes in the groups, the total sum of squares accounted for by the model is
\begin{align*}
\mathbf{SSP}_{\text{Model}} & = \mathbf{SSP}_{A} + \mathbf{SSP}_{B} + \mathbf{SSP}_{AB} \\
                            & = \mathbf{H}_A + \mathbf{H}_B + \mathbf{H}_{AB} \period
\end{align*}


## HE plot construction

The HE plot is constructed to allow a direct visualization of the "size" of hypothesized terms in
a multivariate linear model in relation to unexplained error variation.
These can be displayed in 2D or 3D plots, so I use the term "ellipsoid" below to cover all cases.

Error variation is represented by a standard 68\% data ellipsoid of the $\mat{E}$ matrix
of the residuals in $\Epsilon$. This is divided by the residual degrees of freedom, so the size of $\mat{E} / \text{df}_e$ is analogous to a mean square error
in univariate tests. The choice of 68\% coverage allows you to ``read'' the residual standard deviation as the half-length of the
shadow of the $\mat{E}$ ellipsoid on any axis (see @fig-galton-ellipse-r).
The $\mat{E}$ ellipsoid is then translated to the overall (grand) means $\bar{\mathbf{y}}$ of the variables plotted, which allows us to show the means for factor levels on the same scale, facilitating interpretation.
In the notation of @eq-ellE, the error ellipsoid is given by
$$
\mathcal{E}_c (\bar{\mathbf{y}}, \mathbf{E}) = \bar{\mathbf{y}} \; \oplus \; c\,\mathbf{E}^{1/2} \comma
$$ {#eq-Ec}
where $c = \sqrt{2 F_{2, n-2}^{0.68}}$ for 2D plots and $c = \sqrt{3 F_{3, n-3}^{0.68}}$ for 3D for standard 68% coverage.

**TODO* Data ellipses are defined in @sec-data-ellipse, but the boundaries are given in there in terms of $\chi^2$ values.  Fix notation here.

An ellipsoid representing variation in the means of a factor (or any other term reflected in a general linear hypothesis test, @eq-hmat) in the $\mat{H}$ matrix is simply the data ellipse of the fitted values for that term. 

Dividing the hypothesis matrix by the error degrees of freedom, giving
$\mat{H} / \text{df}_e$,
puts this on the same scale as the $\E$ ellipse.
<!-- , as shown in the left panel of \figref{fig:heplot-iris1}. -->
I refer to this as _effect size scaling_, because it is similar to an effect size index used in
univariate models, e.g., $ES = (\bar{y}_1 - \bar{y}_2) / s_e$ in a two-group, univariate design.

<!--
The notation $\bar{\mathbf{y}} \; \oplus$ in @eq-Ec means that the error ellipsoid is
shifted to be centered at the grand means of the response variables, where the $\mat{H}$
is also centered. This allows the HE plot to also show the group means, facilitating
interpretation.
-->

**TODO**: Per MT comments, add illustration of ellipses with varying coverage, from: `R/ellipses-coverage.R`

### Example: Iris data {#iris-ex}

Perhaps the most famous (or infamous) dataset in the history of multivariate data analysis is that of measurements on three species of Iris flowers collected by Edgar Anderson [-@Anderson:35] in the Gaspé Peninsula of Québec, Canada. 
Anderson wanted to quantify the outward appearance ("morphology": shape, structure, color, pattern, size) of species as a method to study variation within and between such groups. Although Anderson published in the obscure _Bulletin of the American Iris Society_, R. A. Fisher [-@Fisher:1936] saw this as a challenge and opportunity to introduce the method now called
discriminant analysis---how to find a weighted composite of variables to best discriminate among existing groups.

<!-- depending on use, make some of this a text box -->
::: {.callout-note title="History corner"}

I said "infamous" above because Fisher published in the _Annals of Eugenics_, was an ardent eugenicist himself, 
and the work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups.
Through guilt by association, the Iris data, having mistakenly been called "Fisher's Iris Data",
has become deprecated, even called "racist data".[^stop-iris] The voices of the _Setosa_, _Versicolor_ and _Virginica_ of
Gaspé protest: we don't have a racist bone in out body and nor prejudice against any
other species.

@Bodmer-etal-2021 present a careful account of Fisher's views on eugenics within the context
of his time and his contributions to modern statistical theory and practice.
Fisher's views on race were largely formed by Darwin and Galton,
but "nearly all of Fisher’s statements were about populations,
groups of populations, or the human species as a whole". Regardless, the `iris` data were
Anderson's and should not be blamed. After all, if Anderson had lent his car to Fisher,
would the car be tainted by Fisher's eugenicist leanings?

[^stop-iris]: For example, Megan Stodel in a blog post [Stop using iris](https://www.meganstodel.com/posts/no-to-iris/) says, "_It is clear to me that knowingly using work that was itself used in pursuit of racist ideals is totally unacceptable._" A Reddit discussion on this topic, [Is it socially acceptable to use the Iris dataset?](https://www.reddit.com/r/MachineLearning/comments/ii9urh/d_is_it_socially_acceptable_to_use_the_iris/) has some interesting replies. 
:::

<!--
```{r}
#| label: fig-iris-flowers
#| out-width: "100%"
#| fig-cap: "Three species of irises in the Anderson/Fisher data set. _Source_: P. I. Adegbite [Iris Flower Classification]( https://peaceadegbite1.medium.com/iris-flower-classification-60790e9718a1)"
knitr::include_graphics(here::here("images/iris-flowers-labeled.png"))
```

@fig-iris-flowers shows photos of the three iris species, _Setosa_, _Versicolor_ and _Virginica_. 
Each flower has three sepals and three petals. The sepals have brightly colored central sections.
Anderson recorded measurements of the  length and width (in cm.) of the sepals and petals on 50 flowers of each type.
-->

```{r}
#| label: fig-iris-diagram
#| echo: false
#| out-width: "70%"
#| fig-cap: "Diagram of an iris flower showing the measurements of petal and sepal size. Each flower has three sepals and three alternating petals. The sepals have brightly colored central sections. _Source_: @DeSilva2020"
knitr::include_graphics(here::here("images/iris-diagram.jpg"))
```

So that we understand what the measurements represent, @fig-iris-diagram superposes labels on a typical iris flower.
Sepals are like ostentatious petals, with attractive decorations in the central section. Length is the distance from the center
to the tip and width is the transverse dimension.

As always, it is useful to start with overview displays to see the data.
A scatterplot matrix (@fig-iris-spm) shows that _versicolor_ and _virginica_ are more
similar to each other than either is to _setosa_, both in their pairwise means (_setosa_ are smaller) and in the
slopes of regression lines.
Further, the ellipses  suggest that the
assumption of constant within-group covariance matrices is problematic: While the shapes and sizes of the concentration ellipses for _versicolor_ and _virginica_ are reasonably similar, the shapes and sizes of the ellipses for _setosa_ are different from the other two.

```{r}
#| label: fig-iris-spm
#| out-width: "100%"
#| fig-width: 10
#| fig-height: 10
#| fig-cap: "Scatterplot matrix of the `iris` dataset. The species are summarized by 68% data ellipses and linear regression lines in each pairwise plot."
iris_colors <-c("blue", "darkgreen", "brown4")
scatterplotMatrix(~ Sepal.Length + Sepal.Width + 
                    Petal.Length + Petal.Width | Species,
  data = iris,
  col = iris_colors,
  pch = 15:17,
  smooth=FALSE,
  regLine = TRUE,
  ellipse=list(levels=0.68, fill.alpha=0.1),
  diagonal = FALSE,
  legend = list(coords = "bottomleft", 
                cex = 1.3, pt.cex = 1.2))
```

### MANOVA model {#sec-iris-mod}
We proceed nevertheless to fit a multivariate one-way ANOVA model to the iris data.
The MANOVA model for these data addresses the question: "Do the means of the `Species` differ significantly for the sepal and petal variables taken together?" 
$$
\mathcal{H}_0 : \mathbf{\mu}_\textrm{setosa} = \mathbf{\mu}_\textrm{versicolor} = \mathbf{\mu}_\textrm{virginica}
$$


Because there are three species, the test involves $s = \min(p, g-1) =2$ degrees of freedom,
and we are entitled to represent this by two 1-df contrasts,  or sub-questions. From the separation among the groups
shown in @fig-iris-spm (or more botanical knowledge), it makes sense to compare:

* Setosa vs. others: $\mathbf{c}_1 = (1,\: -\frac12, \: -\frac12)$
* Versicolor vs. Virginica: : $\mathbf{c}_1 = (0,\: 1, \: -1)$

You can do this by putting these vectors as columns in a matrix and assigning this to the
`contrasts()` of `Species`. It is important to do this _before_ fitting with `lm()`,
because the contrasts in effect determine how the $\mathbf{X}$ matrix is setup, and hence the
names of the coefficients representing `Species`.

```{r}
C <- matrix(c(1,-1/2,-1/2,  
              0,   1,  -1), nrow=3, ncol=2)
contrasts(iris$Species) <- C
contrasts(iris$Species)
```

Now let's fit the model. As you would expect from @fig-iris-spm, the differences among groups are highly significant. 

```{r iris-mod}
iris.mod <- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~
                 Species, data=iris)
Anova(iris.mod)
```

As a quick follow-up, it is useful to examine the univariate tests for each of the iris
variables, using `heplots::glance()` or `heplots::uniStats()`. It is of interest that
the univariate $R^2$ values are much larger for the petal variables than the sepal length and width.[^rsq-anova].
For comparison, `heplots::etasq()` gives the overall $\eta^2$ proportion of variance accounted for in 
all responses.

[^rsq-anova]: Recall that $R^2$ for a linear model is the the proportion of variation in the response that is explained by the model, calculated as $R^2 = \text{SS}_H / \text{SS}_T = \text{SS}_H / (\text{SS}_H + \text{SS}_E )$. For a multivariate model, these are obtained from the diagonal elements of $\mathbf{H}$ and $\mathbf{E}$.

```{r iris-glance}
glance(iris.mod)

etasq(iris.mod)
```

But these statistics don't help to understand _how_ the species differ. For this, we turn
to HE plots.

## HE plots

The `heplot()` function takes a `"mlm"` object and produces an HE plot for one pair
of variables specified by the `variables` argument. By default, it plots the first two.
@fig-iris-HE1 shows the HE plots for the two sepal and the two petal variables.

<!-- duplicated the code b/c it doesn't print from the fig-iris-HE1 chunk -->
```{r code-iris-HE1}
#| eval: false
heplot(iris.mod, size = "effect",
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
heplot(iris.mod, size = "effect", variables = 3:4,
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
```

```{r}
#| echo: false
#| label: fig-iris-HE1
#| fig-width: 10
#| fig-height: 5
#| fig-show: hold
#| out-width: "100%"
#| fig-cap: "HE plots for the multivariate model `iris.mod`. The left panel shows the plot for the Sepal variables; the right panel plots the Petal variables."
op <- par(mar = c(4, 4, 1, 1) + .5,
          mfrow = c(1, 2))
heplot(iris.mod, size = "effect",
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
heplot(iris.mod, size = "effect", variables = 3:4,
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
```

The interpretation of the plots in @fig-iris-HE1 is as follows:

* For the Sepal variables, length and width are positively correlated _within_ species 
(the $\mat{E}$ = "Error" ellipsoid). The means of the groups (the $\mat{H}$ = "Species" ellipsoid), however, are negatively correlated. This plot is the HE plot representation of the data
shown in row 2, column 1 of @fig-iris-spm. It reflects the relative **shape** of the 
iris sepals: shorter and wider for _setosa_ than the other two species.

* For the Petal variables length and width are again positively correlated _within_ species,
but now the means of the groups are positively correlated: longer petals go with wider ones
across species. This reflects the relative **size** of the iris petals.
The analogous data plot appears in row 4, column 3 of @fig-iris-spm.

<!--
**Older stuff**
@fig-iris-HE0 shows two views of the relationship between sepal length and sepal width for the iris data. ...
```{r}
#| eval: false
op <- par(mar = c(4, 4, 1, 1) + .5,
          mfrow = c(1, 2))
col <-c("blue", "darkgreen", "brown")
clr <- c(col, "red")
covEllipses(cbind(Sepal.Length, Sepal.Width) ~ Species, data=iris,
      pooled = TRUE,
      fill=TRUE,
      fill.alpha = 0.1,
      lwd = 3,
      col = clr,
      cex = 1.5, cex.lab = 1.5,
      label.pos = c(3, 1, 3, 0),
      xlim = c(4,8), ylim = c(2,4))

iris.mod <- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~
                 Species, data=iris)
heplot(iris.mod, size = "effect",
       cex = 1.5, cex.lab = 1.5,
       fill=TRUE, fill.alpha=c(0.3,0.1),
       xlim = c(4,8), ylim = c(2,4))
par(op)
```

```{r}
#| label: fig-iris-HE0
#| out-width: "100%"
#| fig-cap: "Iris data: Data ellipses and HE plot"
knitr::include_graphics("images/iris-HE1.png")
```
-->


## Significance scaling

The geometry of ellipsoids and multivariate tests allow us to go further with another re-scaling of the $\mat{H}$ ellipsoid
that gives a _visual test of significance_ for any term in a MLM.
This is done simply by dividing $\mat{H} / df_e$ further
by the $\alpha$-critical value of the corresponding test statistic to show the strength of evidence against
the null hypothesis.

Among the various multivariate test statistics,
Roy's maximum root test, based on the largest eigenvalue $\lambda_1$ of $\mat{H} \mat{E}^{-1}$,
gives $\mat{H} / (\lambda_\alpha df_e)$
which has the visual property that the
scaled $\mat{H}$ ellipsoid will protrude _somewhere_ outside the standard $\mat{E}$ ellipsoid if and only if
Roy's test is significant at significance level $\alpha$. The critical value $\lambda_\alpha$ for Roy's
test is
$$
\lambda_\alpha = \left(\frac{\text{df}_1}{\text{df}_2}\right) \; F_{\text{df}_1, \text{df}_2}^{1-\alpha} \comma
$$
where $\text{df}_1 = \max(p, \text{df}_h)$ and $\text{df}_2 = \text{df}_h + \text{df}_e - \text{df}_1$.

For these data, the HE plot using
significance scaling is shown in the right panel of @fig-iris-HE2. The left panel is the same as that
shown for sepal width and length in @fig-iris-HE1, but with axis limits to make the two plots
directly comparable.

```{r}
#| label: fig-iris-HE2
#| echo: false
#| out-width: "100%"
#| fig-cap: "HE plots for sepal width and sepal length in the iris dataset. Left: _effect_ scaling of the $\\mathbf{H}$ matrix; right: _significance_ scaling, where protrusion of $\\mathbf{H}$ outside $\\mathbf{E}$ indicates a significant effect by Roy's test."
knitr::include_graphics("images/iris-HE2.png")
```

You can interpret the plot using effect scaling to indicate that the overall "size" of variation
of the group means is roughly the same as that of within-group variation for the two sepal variables.
Significance scaling weights the evidence against the null hypothesis that a given effect is zero.

## Visualizing contrasts and linear hypotheses

As described in @sec-contrasts, tests of linear hypotheses and contrasts represented by the
general linear test $\mathcal{H}_0: \mathbf{C} \;\mathbf{B} = \mathbf{0}$ provide a
powerful way to probe the _specific_ effects represented within the global null hypothesis,
$\mathcal{H}_0: \mathbf{B} = \mathbf{0}$, that all effects are zero.

In this example the contrasts $\mathbf{c}_1$ (`Species1`) and $\mathbf{c}_2$ (`Species2`)
among the iris species are orthogonal, i.e., 
$\mathbf{c}_1^\top \mathbf{c}_2 = 0$. Therefore, their tests are statistically independent, and their
$\mathbf{H}$ matrices are additive. They fully decompose the general question of differences
among the groups into two independent questions regarding the contrasts.

$$
\mathbf{H}_\text{Species} = \mathbf{H}_\text{Species1} + \mathbf{H}_\text{Species2}
$$ {#eq-H-Species}

`car::linearHypothesis()` is the means for testing these statistically, and `heplot()`
provides the way to show these tests visually. Using the contrasts set up in @sec-iris-mod,
$\mathbf{c}_1$, representing the difference between _setosa_ and the other species
is labeled `Species1` and the comparison of _versicolor_ with _virginica_ is `Species2`.
The coefficients for these in $\mathbf{B}$ give the differences in the means.
The line for `(Intercept)` gives grand means of the variables.

```{r iris-mod-coef}
coef(iris.mod)
```

Numerical tests of hypotheses using `linearHypothesis()` can be specified in a very general way:
A matrix (or vector) $\mathbf{C}$ giving linear combinations of coefficients by rows, or a character vector giving the hypothesis in symbolic form.
A character variable or vector tests whether the named coefficients are different from zero
for all responses.

```{r iris-linhyp}
linearHypothesis(iris.mod, "Species1") |> print(SSP=FALSE)

linearHypothesis(iris.mod, "Species2") |> print(SSP=FALSE)
```

The various test statistics are all equivalent here---they give the same $F$ statistics--- because they
they have 1 degree of freedom.

In passing, from @eq-H-Species, note that the _joint_ test of these contrasts is exactly equivalent to the overall test of `Species` (results not shown).

```{r}
#| eval: false
linearHypothesis(iris.mod, c("Species1", "Species2"))
```

We can show these contrasts in an HE plot by supplying a named list for the `hypotheses` argument.
The names are used as labels in the plot.
In the case of a 1-df multivariate test, the $\mathbf{H}$ ellipses plot as a degenerate line.

```{r}
#| label: fig-iris-contrasts
#| fig-width: 6
#| fig-height: 6
#| out-width: "60%"
#| fig-cap: "HE plot for sepal length and width in the `iris` data showing the tests of the two contrasts"
hyp <- list("S:Vv" = "Species1", "V:v" = "Species2")
heplot(iris.mod, hypotheses=hyp,
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1),
       col = c("red", "blue", "darkgreen", "darkgreen"),
       lty = c(0,0,1,1), label.pos = c(3, 1, 2, 1),
       xlim = c(2, 10), ylim = c(1.4, 4.6))
```

This HE plot shows that, for the two sepal variables, the greatest between-species variation
is accounted for by the contrast (`S:Vv`) between _setosa_ and the others, for which the effect is very
large in relation to error variation.  The second contrast (`V:v`), between the _versicolor_ and _virginica_
species is relatively smaller, but still explains significant variation of the sepal variables among the species.

The directions of these hypotheses in a given plot show _how_ the group means differ in terms of a given contrast.[^HE-contrast-geometry] For example, the contrast `S:Vv` is the line that separates _setosa_ from the others and indicates that _setosa_ flowers have shorter but wider sepals.

[^HE-contrast-geometry]: That the $\mathbf{H}$ ellipses for the contrasts subtend that for the overall test of `Species` is no accident.
In fact, this is true in $p$-dimensional space for _any_ linear hypothesis, and orthogonal contrasts
have the additional geometric property that they form _conjugate axes_ for the overall $\mathbf{H}$ ellipsoid
relative to the $\mathbf{E}$ ellipsoid [@Friendly-etal:ellipses:2013].

## HE plot matrices {#sec-HEplot-matrices}

In base R graphics, 2D scatterplots are extended to all pairwise views of multivariate data
with a `pairs()` method. For multivariate linear models, the `r package("heplots")` defines a
`pairs.mlm()` method to display HE plots for all pairs of the response variables.

```{r}
#| label: fig-iris-pairs
#| fig-width: 8
#| fig-height: 8
#| out-width: "100%"
#| fig-cap: "All pairwise HE plots for the iris data."
pairs(iris.mod,
      fill=TRUE, fill.alpha=c(0.3, 0.1))
```

@fig-iris-pairs provides a fairly complete visualization of the results of the multivariate tests
and answers the question: **how** do the species differ?
Sepal length and the two petal variables have the group means nearly perfectly correlated, in the
order _setosa_ < _versicolor_ < _virginica_. For Sepal width, however, _setosa_ has the largest mean,
and so the $\mathbf{H}$ ellipses show a negative correlation in the second row and column.


## Low-D views: Canonical analysis {#sec-candisc}

The HE plot framework so far provides views of all the effects in a MLM in 
_variable_ space. We can view this in 2D for selected pairs 
of response variables, or for all pairwise views in scatterplot matrix format.
There is also an `heplot3d()` function giving plots for three response variables together.
The 3D plots are interactive, in that they can be rotated and zoomed
by mouse control, and dynamic, in that they can be made to spin and saved as movies.
To save space, these plots are not shown here.

However in a one-way MANOVA design with more than response three variables, it is difficult
to visualize how the groups vary on _all_ responses together, and how the
different variables contribute to discrimination among groups.
In this situation, **canonical discriminant analysis** (CDA) is often used, to provide
a low-D visualization of between-group variation.
When the predictors are also continuous, the analogous term is **canonical correlation analysis** (CCA).
The advantage here is that we can also show the relations of the response variables to these dimensions,
similar to a biplot (@sec-biplot) for a PCA of purely quantitative variables. 

The key to this is the eigenvalue decomposition, $\mat{H}\mat{E}^{-1} \lambda_i = \lambda_i \vec{v}_i$
(@eq-he-eigen)
of $\mathbf{H}$ relative to $\mathbf{E}$. The eigenvalues, $\lambda_i$, give the "size" of each $s$ 
orthogonal dimensions on which the multivariate tests are based. But the corresponding
eigenvectors, $\mathbf{v}_i$, give the _weights_ for the response variables
in $s$ linear combinations that maximally discriminate among the groups or equivalently maximize the
(canonical) $R^2$ of a linear combination of the predictor $\mathbf{X}$s with a linear combination of the
response $\mathbf{Y}$s.

Thus, CDA amounts to a transformation of the $p$ responses,
$\mat{Y}_{n \times p}$
into the canonical space,
$$
\mat{Z}_{n \times s} = \mat{Y} \; \mat{E}^{-1/2} \; \mat{V} \;,
$$
where $\mat{V}$ contains the eigenvectors of $\mat{H}\mat{E}^{-1}$
and $s=\min ( p, df_h )$.
It is well-known (e.g., @Gittins:85)
that \emph{canonical discriminant plots} of the first two (or three, in 3D)
columns of $\mat{Z}$ corresponding to the largest canonical correlations
provide an optimal low-D display of the variation between groups relative
to variation within groups.

Canonical discriminant analysis is typically carried out in conjunction with a one-way MANOVA design.
The `r package("candisc", cite=TRUE)` generalizes this to multi-factor designs in the `candisc()` function.
For any given term in a `"mlm"`, the _generalized canonical discriminant analysis_ amounts to a standard discriminant analysis based on the $\mat{H}$ matrix for _that_ term in relation to the full-model $\mat{E}$ matrix.[^candiscList]

[^candiscList]: `candiscList()` performs a generalized canonical discriminant analysis for _all_ terms in a multivariate linear model, returning a list of the results for each factor.

Tests based on the eigenvalues $\lambda_i$,
initially stated by @Bartlett1938, use  Wilks' $\Lambda$ likelihood ratio tests of these.
This allow you to determine the number of significant canonical dimensions, or the number of different
aspects to consider for the relations between the responses and predictors.
These take the form of _sequential_ global tests of the hypothesis that the canonical correlation in
the current row and all that follow are zero. Thus, if you find 
The canonical $R^2$, `CanRsq`, gives the R-squared value of fitting the $i$th response canonical variate to the corresponding $i$th canonical variate for the predictors.

For the iris data, we get the following printed summary:

```{r iris-can}
iris.can <- candisc(iris.mod) |> print()
```

This analysis shows a very simple result: The differences among the iris species can
be nearly entirely accounted for by the first canonical dimension (99.1%).
Interestingly, the second dimension is also highly significant, even though it accounts for only 0.88%.

### Coeficients

The `coef()` method for "candisc" objects returns a matrix of weights for the response variables
in the canonical dimensions. By default, these are given for the response variables _standardized_
to $\bar{y}=0$ and $s^2_y = 1$. The `type` argument also allows for raw score weights (`type = "raw"`)
used to compute the observation scores on `Can1`, `Can2`, ... .
`type = "structure"` gives the canonical structure coefficients, which are the correlations between each response and the
canonical scores.

```{r iris-can-coef}
coef(iris.can, type = "std")
coef(iris.can, type = "structure")
```

Thus, the two weighted sums for the canonical variates, using mean-centered standardized scores, can be shown as follows:

```{r can-vars}
#| eval: false
#| echo: false
#| results: asis
C <- coef(iris.can, type = "std")
names <- row.names(C)
names <- c("S.len", "S.wid", "P.len", "P.wid")
can1 <- paste("Can1 = ", paste0(round(C[,1], 2), " x ", names, collapse = " + "))
#can1 <- gsub("+ -", "-", can1)
cat(can1,"\n")
can2 <- paste("Can2 = ", paste0(round(C[,2], 2), " x ", names, collapse = " + ")) |> cat()
```


\begin{eqnarray*}
\text{Can}_1 & = 0.43 \cdot \text{S.length} + 0.52 \cdot \text{S.wid} - 0.95 \cdot \text{P.length} - 0.58 \cdot \text{P.length} \\
\text{Can}_2 & = 0.01 \cdot \text{S.length} + 0.74 \cdot \text{S.wid} + 0.4 \cdot \text{P.length} + 0.58 \cdot \text{P.length}
\end{eqnarray*}


### Canonical scores plot

The `plot()` method for `"candisc"` objects gives a plot of these observation scores. 
`ellipse=TRUE` overlays this with their standard data ellipses for each species, as shown in @fig-iris-candisc. 
The response variables are shown as
vectors, using the structure coefficients, as in a biplot. Thus, the relative size of the projection of these
vectors on the canonical axes reflects the correlation of the observed response on the canonical dimension.
For ease of interpretation I flipped the sign of the first canonical dimension, so that the positive `Can1` direction
corresponds to larger flowers.

<!-- **TODO**: `var.labels` not a graphical parameter -->

```{r echo=-1}
#| label: fig-iris-candisc
#| fig-width: 8
#| fig-height: 7
#| out-width: "80%"
#| fig-cap: "Plot of canonical scores for the iris data. Ellipses give 68% coverage data ellipses for the canonical scores. Variable vectors make angles with the Can1 and Can2 axes indicating their correlations."
op <- par(mar = c(4, 4, 4, 1) + .5)
vars <- names(iris)[1:4] |> 
  stringr::str_replace("\\.", "\n")
plot(iris.can,
     var.labels = vars,
     var.col = "black",
     var.lwd = 2,
     ellipse=TRUE,
     scale = 9,
     col = iris_colors,
     pch = 15:17,
     cex = 0.7, var.cex = 1.25,
     rev.axes = c(TRUE, FALSE),
     xlim = c(-10, 10),
     cex.lab = 1.5)
```

The interpretation of
this plot is simple: in canonical space, variation of the means for the iris species is
essentially one-dimensional (99.1% of the effect of `Species`), and this dimension corresponds
to overall size of the iris flowers.  All variables except for `Sepal.Width` are positively aligned with
this axis, but the two petal variables show the greatest discrimination.
The negative direction for `Sepal.Width` reflects the pattern seen in @fig-iris-pairs, where _setosa_
have wider sepals.

For the second dimension, look at the projections of the variable vectors on the `Can2` axis.
All are positive, but this is dominated by `Sepal.Width`. We could call this a flower shape dimension.



### Canonical HE plot
For a one-way design, the _canonical HE plot_
is simply the HE plot of the canonical scores in the analogous MLM model that substitutes $\mat{Z}$ for $\mat{Y}$. 
In effect, it is a more compact visual summary of the plot shown in @fig-iris-candisc.

This is shown in @fig-iris-HEcan for the `iris` data.
In canonical space, the residuals are always uncorrelated, so the $\mathbf{E}$ ellipse plots as a circle.
The $\mathbf{H}$ ellipse for `Species` here reflects a data ellipse for the fitted values--- group means--- shown as
labeled points in the plot. The differences among `Species` are so large that this plot uses `size = "effect"`
scaling, making the axes comparable to those in @fig-iris-candisc.[^can-signif-scaling]

[^can-signif-scaling]:  If significance scaling was used the interpretation of the canonical HE plot plot would the same as before: if the hypothesis ellipse extends beyond the error ellipse, then that dimension is significant.  

The vectors for each predictor are the same structure coefficients as in the ordinary canonical plot.
They can again be reflected for interpretation and scaled in length to fill the plot window.

<!-- now depends on candisc 0.9.1 -->

```{r echo=-1}
#| label: fig-iris-HEcan
#| fig-width: 8
#| fig-height: 8
#| out-width: "80%"
#| fig-cap: "Canonical HE plot for the iris data."
op <- par(mar = c(4, 4, 4, 1) + .5)
heplot(iris.can,
       size = "effect",
       scale = 8,
       var.labels = vars,
       var.col = "black",
       var.lwd = 2,
       var.cex = 1.25,
       fill = TRUE, fill.alpha = 0.2,
       rev.axes = c(TRUE, FALSE),
       xlim = c(-10, 10),
       cex.lab = 1.5)
```

The collection of plots shown for the iris data here can be seen as progressive visual summaries of the 
data:

* The scatterplot matrix in @fig-iris-spm shows the iris flowers in the data space of the sepal and petal variables.
* Canonical analysis substitutes for these the two linear combinations reflected in `Can1` and `Can2`. The plot in @fig-iris-candisc portrays _exactly_ the same relations among the species, but in the reduced canonical space of only two dimensions.
* The HE plot version, shown in @fig-iris-HEcan summarizes the separate data ellipses for the species with
pooled, within-group variance of the $\mathbf{E}$ matrix for the canonical variables, which are always uncorrelated.
The variation among the group means is reflected in the size and shape of the ellipse for the $\mathbf{E}$ matrix.


## Quantitative predictors: MMRA 

The ideas behind HE plots extend naturally to multivariate multiple regression (MMRA).
<!--
and multivariate analysis of covariance (MANCOVA). In MMRA designs, the $\mathbf{X}$ matrix contains only quantitative predictors, while in MANCOVA designs, it contains a mixture of factors and quantitative predictors (covariates), but typically there is just one “group” factor.

In the MANCOVA case, there is often a subtle difference in emphasis: true MANCOVA analyses focus on the differences among groups defined by the factors, adjusting for (or controlling for) the quantitative covariates. Analyses concerned with homogeneity of regression focus on quantitative predictors and attempt to test whether the regression relations are the same for all groups defined by the factors.
-->
A purely visual feature of HE plots in these cases is that the $\mathbf{H}$ ellipse for a quantitative predictor
with 1 df appears as a degenerate line. But consequently, the angles between these for different
predictors has a simple interpretation as as the correlation between their predicted effects.
Moreover, it is easy to show visual overall tests of _joint_ linear hypotheses for two or more
predictors together.


**TODO**: Use these examples below

* `heplots::NLSY`: National Longitudinal Survey of Youth Data
* `heplots::schooldata`: School Data -> `R/schooldata-ex.R`
* `heplots::Hernior`: Recovery from Elective Herniorrhaphy -> HE_mmra vignette

### Example: NLSY data {#sec-NLSY-HE}

<!-- **TODO**: The HE plots are nicer w/o using log(income) -->

Here I'll continue the analysis of the NLSY data from @sec-NLSY-mmra. In the model `NLSY.mod1` I used
only father's income  and education to predict scores in reading and math, and both 
of these demographic variables were highly significant.

```{r}
#| label: fig-NLSY-heplot1
#| out-width: "80%"
#| fig-cap: "HE plot for the simple model for the NLSY data fitting reading and math scores from income and education."
data(NLSY, package = "heplots")
NLSY.mod1 <- lm(cbind(read, math) ~ income + educ, 
                data = NLSY)

heplot(NLSY.mod1, 
  fill=TRUE, fill.alpha = 0.2, 
  cex = 1.5, cex.lab = 1.5,
  lwd=c(2, 3, 3),
  label.pos = c("bottom", "top", "top")
  )

```

Fathers income and education are positively correlated in their effects on the outcome scores.
From the angles in the plot, income is most related to the math score, while education is related
to both, but slightly more to the reading score.

The overall joint test for both predictors can then visualized as the test of the linear hypothesis
$\mathcal{H}_0 : \mathbf{B} = [\boldsymbol{\beta}_\text{income}, \boldsymbol{\beta}_\text{educ}] = \mathbf{0}$.
For `heplot()`, we specify the names of the coefficients to be tested with the `hypotheses` argument.

```{r}
#| label: fig-NLSY-heplot2
#| out-width: "80%"
#| fig-cap: "HE plot adding the $\\mathbf{H}$ ellipse for the overall test that both predictors have no effect on the outcome scores."
coefs <- rownames(coef(NLSY.mod1))[-1] |> print()

heplot(NLSY.mod1, 
       hypotheses = list("Overall" = coefs),
       fill=TRUE, fill.alpha = 0.2, 
       cex = 1.5, cex.lab = 1.5,
       lwd=c(2, 3, 3, 2),
       label.pos = c("bottom", "top"))
```

## Canonical correlation analysis {#sec-cancor}

Just as we saw for MANOVA designs, a canonical analysis for multivariate regression involves finding
a low-D view of the relations between predictors and outcomes that maximally explains their
relations in terms of linear combinations of each. 
That is, the goal is to find  weights
for one set of variables, say $\mathbf{X}$ _not_ to predict each of the other set 
$\mathbf{Y} =[\mathbf{y}_1, \mathbf{y}_2, \dots]$ _individually_, but rather to also find weights
for the $\mathbf{y}$s which is most highly correlated with the linear combination of the $\mathbf{x}$s.

In this sense, canonical correlation analysis (CCA) is _symmetric_ in the $x$ and $y$ variables:
the $y$ set is not considered responses. Rather the goal is simply to explain the correlations
between the two sets.
For a thorough treatment of this topic, see @Gittins:85.

Geometrically, these linear combinations are vectors representing projections in the observation space of the $x$ and $y$ variables, and CCA can also be thought of as
minimizing the angle between these vectors or maximizing the cosine of this angle. This is illustrated in @fig-cancor-diagram.

```{r}
#| label: fig-cancor-diagram
#| echo: false
#| out-width: "100%"
#| fig-cap: "Diagram illustrating canonical correlation. For two $y$ variables, all linear combinations are vectors in their plane, and similarly for the $x$ variables. Maximizing the correlation between linear combinations of each is equivalent to making the angle $\\phi$ between them as small as possible, or maximizing $\\cos({\\theta})$, shown in the diagram at the right. The thick grey arrow indignates that the two planes should be overlaid at a common origin. _Source_: Re-drawn by Udi Alter following a Cross-Validated discussion by user 'ttnphns', https://bit.ly/4dgq2cp"
knitr::include_graphics("images/cancor-diagram-udi.png")
```



Specifically, we want to find one set of weights $\mathbf{a}_1$ for the $x$ variables and another
for the $y$ variables to give the linear combinations $\mathbf{u}_1$ and $\mathbf{v}_1$,

\begin{eqnarray*}
\mathbf{u}_1 & = \mathbf{X} \ \mathbf{a}_1 = a_{11} \mathbf{x}_1 + a_{12} \mathbf{x}_2 + \cdots + a_{11} \mathbf{x}_q \\
\mathbf{v}_1 & = \mathbf{Y} \ \mathbf{b}_1 = b_{11} \mathbf{y}_1 + b_{12} \mathbf{y}_1 + \cdots + b_{11} \mathbf{y}_p \; ,
\end{eqnarray*}

such that the correlation $\rho_1 = \textrm{corr}(\mathbf{u}_1, \mathbf{v}_1)$ is maximized, or
equivalently, minimizing the angle between them.

Using
$\mathbf{S}_{xx}$, $\mathbf{S}_{yy}$ to represent the covariance matrices of the $x$ and $y$ variables,
and $\mathbf{S}_{xy}$ for the cross-covariances between the two sets, the 
correlation between the linear combinations of each can be expressed as

\begin{eqnarray*}
\rho_1 & = \textrm{corr}(\mathbf{u}_1, \mathbf{v}_1) 
         = \textrm{corr}(\mathbf{X} \ \mathbf{a}_1, \mathbf{Y} \ \mathbf{b}_1) \\
       & = \frac{\mathbf{a}_1^\top \ \mathbf{S}_{xy} \ \mathbf{b}_1 }{\sqrt{\mathbf{a}_1^\top \ \mathbf{S}_{xx} \  \mathbf{a}_1 } \sqrt{\mathbf{b}_1^\top \ \mathbf{S}_{yy} \ \mathbf{b}_1 }}
\end{eqnarray*}

But, the $y$ variables lie in a $p$-dimensional (observation) space, and the $x$ in $q$ dimensions, so
what they have common is a space of $s = \min(p, q)$ dimensions. Therefore, we can find additional
pairs of canonical variables,


\begin{eqnarray*}
\mathbf{u}_2 = \mathbf{X} \ \mathbf{a}_2 & \quad\quad \mathbf{v}_2 = \mathbf{Y} \ \mathbf{b}_2 \\
                                         & \vdots \\
\mathbf{u}_s = \mathbf{X} \ \mathbf{a}_s & \quad\quad \mathbf{v}_s = \mathbf{Y} \ \mathbf{b}_s \\
\end{eqnarray*}


such that each pair $(\mathbf{u}_i, \mathbf{v}_i)$ has the maximum possible correlation
and all distinct pairs are uncorrelated:

\begin{eqnarray*} 
\rho_i & =\max _{\mathbf{a}_i, \mathbf{b}_i}\left\{\mathbf{u}_i^{\top} \mathbf{v}_i\right\} = \\ 
\left\|\mathbf{u}_i\right\| & =1, \quad\left\|\mathbf{v}_i\right\|=1, \\ 
\mathbf{u}_i^{{\top}} \mathbf{u}_j & =0, \quad \mathbf{v}_i^{\top} \mathbf{v}_j=0 \quad\quad \forall j \neq i: i, j \in\{1,2, \ldots, s\} \ .
\end{eqnarray*}

In words, the correlations among canonical variables are zero except when when they are associated with the same canonical correlation or the weights $(\mathbf{a}_i, \mathbf{b}_i)$ for the same pair.
Alternatively, all $p \times q$ correlations the variables in $\mathbf{Y}$ and $\mathbf{X}$
are fully summarized in the $s = \min(p, q)$ canonical correlations $\rho_i$ for $i = 1, 2, \dots, s$.

The solution, developed by @Hotelling1936, is a form of a generalized eigenvalue problem, that can be
stated in two equivalent ways,

<!-- $$ -->
<!-- \left(\mathbf{S}_{y y}^{-1} \ \mathbf{S}_{y x} \ \mathbf{S}_{x x}^{-1} \ \mathbf{S}_{x y}-\rho^2 \ \mathbf{I}\right) \mathbf{v} = \mathbf{0} -->
<!-- $$ -->


$$
\begin{aligned}
& \left(\mathbf{S}_{y x} \ \mathbf{S}_{x x}^{-1} \ \mathbf{S}_{x y} - \rho^2 \ \mathbf{S}_{y y}\right) \mathbf{b} = \mathbf{0} \\
& \left(\mathbf{S}_{x y} \ \mathbf{S}_{y y}^{-1} \ \mathbf{S}_{y x} - \rho^2 \ \mathbf{S}_{x x}\right) \mathbf{a} = \mathbf{0} \ .
\end{aligned}
$$
Both equations have the same form and have the same eigenvalues. And, given the eigenvectors for one of these equations, we can find the eigenvectors for the other.

**TODO**: Fill in details of canonical correlations

### Example: School data {#sec-schooldata-HE}

<!-- **TODO**: This should be redone, omitting the influential cases, 44, 59 -->

The `schooldata` dataset analyzed in @sec-schooldata-mmra can also be illuminated by the methods of this
chapter. There I fit the multivariate regression model predicting students scores on
reading, mathematics and a measure of self-esteem using as predictors measures of parents' education, occupation,
school visits, counseling help with school assignments and number of teachers per school.
But I also found two highly influential observations (cases 44, 59; see @fig-schoolmod-infl) whose effect on the
coefficients is rather large; so, I remove them from the analysis here.[^removed-cases]

[^removed-cases]: An alternative to fitting the model removing specific cases deemed troublesome is to use
a **robust** method, such as `heplots::roblm()` which uses re-weighted least squares to down-weight observations
with large residuals or other problems.

```{r}
data(schooldata, package = "heplots")

bad <- c(44, 59)
OK <- (1:nrow(schooldata)) |> setdiff(bad)
school.mod2 <- lm(cbind(reading, mathematics, selfesteem) ~ ., 
                  data=schooldata[OK, ])
```

In this model, parent's education and occupation
and their visits to the schools were highly predictive of student's outcomes but
their counseling efforts and the number of teachers in the schools did not contribute much.
However, the nature of these relationships was largely uninterpreted in that analysis.

Here is where HE plots can help. You can think of this as a way to visualize what is entailed
in the coefficients for this model by showing the _magnitude_ of the predictor effects by their size
and their relations to the outcome variable by their _direction_. The table of raw score coefficients
isn't very helpful in this regard.

```{r}
coef(school.mod2)
```


@fig-school-heplot1 shows the HE plot for reading and mathematics scores in this model, using the default
significance scaling.
```{r echo = -1}
#| label: fig-school-heplot1
#| out-width: "80%"
#| fig-cap: "HE plot for reading and mathematics scores in the multivariate regression model..."
op <- par(mar = c(4, 4, 1, 2) + .1)
heplot(school.mod2, 
       fill=TRUE, fill.alpha=0.1,
       cex = 1.5,
       cex.lab = 1.5,
       label.pos = c(rep("top", 4), "bottom", "bottom"))
```

Parent's occupation and education are both significant in this view, but what is more important is their
orientation. Both are positively associated with reading and math scores, but education is somewhat more
related to reading than to mathematics. Number of teachers and degree of parental counseling have a
similar orientation, with teachers having a greater relation to mathematics scores.
Visits to school and number of teachers are not significant in
this plot, but both are positively correlated with reading and math and are coincident in the plot. The parent time counseling measure, while also insignificant,
tilts in the opposite direction, having different signs for reading and math.

In the `pairs()` plot for all three responses (@fig-school-heplot2), we see something different in the relations for self-esteem. While occupation has a large positive relation in all the plots in the third row and column,
education, counseling and teachers have negative relations in these plots, particularly with mathematics scores.

```{r}
#| label: fig-school-heplot2
#| fig-width: 9
#| fig-height: 9
#| out-width: "100%"
#| fig-cap: "Pairwise HE plots for the three outcome variables in the multivariate regression model ..."
pairs(school.mod2, 
      fill=TRUE, fill.alpha=0.1,
      var.cex = 2.5,
      cex = 1.3)
```


### Canonical analysis

With $p = 3$ responses and $q = 5$ predictors there are three possible sets of canonical variables
which together account for 100% of the total linear relations between them. `heplots::cancor()`
gives the percentage associated with each of the eigenvalues and the canonical correlations.

For this dataset, the first canonical variates, with Can $R = 0.995$, 
accounts for 98.6%, so you might think that that is
sufficient. Yet the likelihood ratio tests show that the second set, with Can $R = 0.774$,
is also significant, even though it only accounts for 1.3%.

```{r school-can}
# bad <- c(44, 59)
# OK <- (1:nrow(schooldata)) |> setdiff(bad)
school.can2 <- cancor(cbind(reading, mathematics, selfesteem) ~
                        education + occupation + visit + counseling + teacher,
                     data=schooldata[OK, ])
school.can2
```

The virtue of CCA is that _all_ correlations between the X and Y variables are completely captured
in the correlations between the _pairs_ of canonical scores: The $p \times q$ correlations between
the sets are entirely represented by the $s = \min(p, q)$ canonical ones.
Whether the second dimension is useful here depends on whether it adds some interpretable increment
to what is going on in these relations. One could be justifiably happy with an explanation based on
the first dimension that accounts for nearly all the total association between the sets.


The class `"cancor"` object returned by `cancor()` contains the canonical coefficients, for which there
is a `coef()` method as in `candisc()`, and also a `scores()` method to return the scores on the canonical
variables, called `Xcan1`, `Xcan2`, ... and `Ycan1`, `Ycan2`.

```{r school-can-names}
names(school.can2)
```

You can use the `plot()` method or `heplot()` method to visualize and help interpret the results.
The `plot()` method plots the canonical `scores$X` against the `scores$Y` for a given dimension
(selected by the `which` argument). The `id.n` argument gives a way to flag noteworthy observations.

```{r echo=-(1:2)}
#| label: fig-school-can
#| fig-width: 10
#| fig-height: 5
#| out-width: "100%"
#| fig-cap: "Plots of canonical scores for the first two canonical dimensions of the `schooldata` dataset, omitting the two highly influential cases."
op <- par(mar = c(4,4,1,1) + .1,
          mfrow = c(1, 2))
plot(school.can2, 
     pch=16, id.n = 3,
     cex.lab = 1.5, id.cex = 1.5,
     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))
text(-2, 1.5, paste("Can R =", round(school.can2$cancor[1], 3)), 
     cex = 1.4, pos = 4)

plot(school.can2, which = 2, 
     pch=16, id.n = 3,
     cex.lab = 1.5, id.cex = 1.5,
     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))
text(-3, 3, paste("Can R =", round(school.can2$cancor[2], 3)), 
     cex = 1.4, pos = 4)
par(op)
```

It is worthwhile to look at an analogous plot of canonical scores for the original dataset including
the two highly influential cases. As you can see in @fig-school-can0, cases 44 and 59 are way outside
the range of the rest of the data. Their influence increases the canonical correlation to a near perfect 
$\rho = 0.997$.

```{r}
#| label: fig-school-can0
#| fig-width: 8
#| fig-height: 8
#| out-width: "70%"
#| fig-cap: "Plots of canonical scores on the first canonical dimension for the `schooldata`, including the influential cases, which stand out as so far frome the rest of the observations."
school.can <- cancor(cbind(reading, mathematics, selfesteem) ~
                       education + occupation + visit + counseling + teacher,
                     data=schooldata)
plot(school.can, 
     pch=16, id.n = 3,
     cex.lab = 1.5, id.cex = 1.5,
     ellipse.args = list(fill = TRUE, fill.alpha = 0.1))
text(-5, 1, paste("Can R =", round(school.can$cancor[1], 3)), 
     cex = 1.4, pos = 4)
```


Plots of canonical scores tell us of the strength of the canonical dimensions, but do not help
interpreting the analysis in relation to the original variables.
The HE plot version for canonical correlation analysis re-fits a multivariate regression model
for the Y variables against the Xs, but substitutes the canonical scores for each, essentially
projecting the data into canonical space.

**TODO**: Check out signs of structure coefs from `cancor()`. Would be better to reflect the vectors
for `Ycan1`.


```{r}
#| label: fig-school-hecan
#| fig-width: 8
#| fig-height: 8
#| out-width: "70%"
#| fig-cap: "HE plot for the canonical correlation analysis of the schooldata. Vectors for the variables indicate their correlations with the canonical dimensions."
heplot(school.can2,
       fill = TRUE, fill.alpha = 0.2,
       var.col = "red", 
       asp = NA, scale = 0.25,
       cex.lab = 1.5, cex = 1.25,
       prefix="Y canonical dimension ")
```

The `r colorize("red")` variable vectors shown in these plots are intended only to show the correlations of Y variables with the canonical dimensions. The fact that they are so closely aligned reflects
the fact that the first dimension accounts for nearly all of their associations with the predictors.
The orientation of the $\mathbf{H}$ ellipses/lines reflects the projection of those from @fig-school-heplot2 into canonical space

Only their _relative lengths_ and angles with respect to the Y canonical dimensions have meaning in these plots. Relative lengths correspond to proportions of variance accounted for in the Y canonical dimensions plotted; angles between the variable vectors and the canonical axes correspond to the structure correlations. The absolute lengths of these vectors are arbitrary and are
typically manipulated by the `scale` argument to provide better visual resolution and labeling for the variables.

## MANCOVA models

HE plots for designs containing a collection of quantitative predictors and one or more factors
are quite simple in MANCOVA models where the effects are additive, i.e., don't involve interactions.
They are a bit more challenging when you allow _separate_ slopes for groups on all quantitative 
variables, because there get to be too many terms to usefully display. But these models are more complicated!

If the evidence for heterogeneity of regressions is not very strong, it is still useful
to fit the MANCOVA model and display it in an HE plot.

An alternative is to fit separate models for the groups and display these as HE plots.
As noted earlier (@sec-PA-tasks), this is not ideal for testing hypotheses, but provides a useful and informative display of the relations between the predictors and responses and the groups effect.
I illustrate these approaches for the Rohwer data, encountered in @sec-PA-tasks, below.

### Example: Rohwer data

In @sec-PA-tasks I fit several models for Rohwer's data on the relations between paired-associate
tasks and scholastic performance. The first model was the MANCOVA model testing the difference
between the high and low SES groups, controlling for, or taking into account differences on 
the paired-associate task.

```{r}
Rohwer.mod1 <- lm(cbind(SAT, PPVT, Raven) ~ SES + n + s + ns + na + ss, 
                 data=Rohwer)
```


HE plots for this model for the pairs (SAT, PPVT) and (SAT, Raven) is shown in @fig-rohwer-HE-mod1-pairs.
The result of an overall test for _all_ predictors, $\mathcal{H}_0 : \mathbf{B} = \mathbf{0}$,
is added to the basic plot using the `hypotheses` argument.


```{r}
#| label: fig-rohwer-HE-mod1-pairs
#| fig-width: 10
#| fig-height: 9
#| out-width: "100%"
#| fig-cap: "All-pairs HE plot for SAT, PPVT and Raven using the MANCOVA model. The ellipses labeled ‘Regr’ show the test of the overall effect of the quantitative predictors."
colors <- c("red", "blue", rep("black",5), "#969696")
covariates <- rownames(coef(Rohwer.mod1))[-(1:2)]
pairs(Rohwer.mod1, 
       col=colors,
       hypotheses=list("Regr" = covariates),
       fill = TRUE, fill.alpha = 0.1,
       cex=1.5, cex.lab = 1.5, var.cex = 3,
       lwd=c(2, rep(3,5), 4))
```


The positive effect of SES on the outcome measures is seen in all pairwise plots: the high SES group is better on all responses.
The positive orientation of the `Regr` ellipses for the covariates shows that the predicted values for all three responses are positively correlated (more so for SAT and PPVT): higher performance on the paired associate tasks, in general, is associated
with higher academic performance. The two significant predictors, `na` and `ns` are the only
ones that extend outside the error ellipses, but their orientations differ.

#### Homogeneity of regression {.unnumbered}
A second model tested the assumption of homogeneity of regression by adding interactions of
SES with the PA tasks, allowing separate slopes for the two groups on each of the other predictors.

```{r}
Rohwer.mod2 <- lm(cbind(SAT, PPVT, Raven) ~ SES * (n + s + ns + na + ss),
                  data=Rohwer)
```

This model has 11 terms, excluding the intercept: `SES`, plus 5 main effects ($x$s) for the predictors
and 5 interactions (slope differences), too many for an understandable display.
To visualize this in an HE plot (@fig-rohwer-HE-mod2), I simplify, by showing the interaction terms
_collectively_ by a single ellipse, representing their joint effect, and specified as a linear
hypothesis called `slopes` that picks out the interaction effects.

The argument `terms` limits the $\mathbf{H}$ ellipses for the right-hand-side of the model which are
shown to just those terms specified. The combined effect of the interaction terms is
specified as an hypothesis (`slopes`) testing the interaction terms (which have a ":" in their name).
Because SES is “treatment-coded” in this model, the interaction terms reflect the difference in slopes
for the high SES group compared to the low. 

```{r}
#| label: fig-rohwer-HE-mod2
#| fig-width: 6
#| fig-height: 6
#| out-width: "70%"
#| fig-cap: "HE plot for SAT and PPVT using the heterogeneous regression model. The ellipse labeled ‘Regr’ shows the test of the covariates combined, and the ellipse labeled 'slopes' shows the combined difference in slopes between the two groups."
(coefs <- rownames(coef(Rohwer.mod2)))

colors <- c("red", "blue", rep("black",5), "#969696")
heplot(Rohwer.mod2, col=c(colors, "darkgreen"), 
       terms=c("SES", "n", "s", "ns", "na", "ss"), 
       hypotheses=list("Regr" = c("n", "s", "ns", "na", "ss"),
                       "Slopes" = coefs[grep(":", coefs)]),
       fill = TRUE, fill.alpha = 0.2, cex.lab = 1.5)
```

#### Separate models {.unnumbered}

When there is heterogeneity of regressions,
using submodels for each of the groups has the advantage that you can easily visualize the slopes for
the predictors in each of the groups, particularly if you overlay the individual HE plots.
In this example, I'm using the models `Rohwer.sesLo` and `Rohwer.sesLo` fit to each of the groups.

```{r rohwer-submodels}
Rohwer.sesLo <- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, 
                   data=Rohwer, subset = SES=="Lo")
Rohwer.sesHi <- lm(cbind(SAT, PPVT, Raven) ~ n + s + ns + na + ss, 
                   data=Rohwer, subset = SES=="Hi")
```

Here I make use of the fact that several HE plots can be overlaid using the option `add=TRUE` as shown in @fig-rohwer-HE-lohi.
The axis limits may need adjustment in the first plot so that the second one will fit.



```{r}
#| label: fig-rohwer-HE-lohi
#| fig-width: 6
#| fig-height: 6
#| out-width: "70%"
#| fig-cap: "Overlaid HE plots for SAT and PPVT, for the low and high SES groups, when each group is fit separately."
heplot(Rohwer.sesLo, 
       xlim = c(0,100),               # adjust axis limits
       ylim = c(40,110), 
       col=c("red", "black"), 
       fill = TRUE, fill.alpha = 0.1,
       lwd=2, cex=1.2, cex.lab = 1.5)
heplot(Rohwer.sesHi, 
       add=TRUE, 
       col=c("brown", "black"), 
       grand.mean=TRUE, 
       error.ellipse=TRUE,            # not shown by default when add=TRUE
       fill = TRUE, fill.alpha = 0.1,
       lwd=2, cex=1.2)
```

We can readily see the difference in means for the two SES groups (Hi has greater scores on both variables) and it also appears that the slopes of the `s` and `n` predictor ellipses are shallower for the High than the Low group, indicating greater relation with the SAT score. As well, the error ellipses show that on these measures, error variation is somewhat smaller in the low SES group.

<!--
## Chapter summary

**TODO**: Insert chapter summary

* This chapter explains and illustrates the use of HE plots for understanding the effects and significance tests of model terms in multivariate linear models for factor predictors (MANOVA), quantitative predictors
(MMRA) and models for mixtures of these types, that I've collectively called MANCOVA models.
-->

```{r child="summary/Ch11-summary.qmd"}
```

<!--
**Packages**:

```{r}
#| echo: false
cat("Writing packages to ", .pkg_file, "\n")
write_pkgs(file = .pkg_file)
```
-->
