```{r include=FALSE}
source("R/common.R")
knitr::opts_chunk$set(fig.path = "figs/ch11/")
```

::: {.content-visible unless-format="pdf"}
{{< include latex/latex-commands.qmd >}}
:::

# Visualizing Multivariate Models {#sec-vis-mlm}

The methods discussed in @sec-mlm-review provide the basis for a rather complete multivariate analysis
of traditional univariate methods for the same designs. You can carry out multiple regression,
ANOVA, or indeed, any classical linear model with the standard collection of analysis tools you use for
a single outcome variable, but naturally extended in most cases to having several outcomes to analyse
together. The key points are:

- Everything you know about the usual univariate models-- regression coefficients, main effects and contrasts for factors, interactions of model terms,  ... applies here.

- By a rather clever design, called "matrix algebra" the separate univariate models can be combined, by turning vectors of responses, $\mathbf{y}_1, \mathbf{y}_2, \dots$ into a matrix $\mathbf{Y}$. Bingo! We get a
multivariate extension.

- You can treat this as a collection of separate models, one for each response, because the coefficents are the same.
But, the benefit of a multivariate approach is that you also get an overall multivariate test for each term in the model.

As nice as these mathematical and statistical ideas might be, the fact that the analysis is conducted for the response
variables _collectively_, means that it may be harder to interpret and explain what this means about the separate
responses. Here's where multivariate model visualization comes to the rescue!

* The tests of multivariate models, including multivariate analysis of variance (MANOVA) for group differences and multivariate multiple regression (MMRA) can be easily visualized by plots of a hypothesis ("H") data ellipse for the fitted values, relative to the corresponding plot of the error ellipse ("E") of the residuals, which I call the HE plot framework.

* For more than a few response variables, these result can be projected onto a lower-dimensional "canonical" space providing an even simpler description. Vectors for the response variables in this space show how these relate to the canonical dimension, facilitating interpretation. This answers the objection of @Huang2019 and others that multivariate models are difficult to understand
because they are framed in terms of linear combinations of the the responses.

- refer to Huang: "_given the challenges and assumptions involved in appropriately performing and interpreting MANOVA results, researchers are better off using other less error-prone procedures._"

**Packages**

In this chapter we use the following packages. I load them now.
```{r}
library(car)
library(heplots)
library(candisc)
library(ggplot2)
library(dplyr)
library(tidyr)
```

## HE plot framework {#sec-he-framework}

@sec-Hotelling illustrated the basic ideas of the framework for visualizing multivariate
linear models in the context of a simple two group design, using Hotelling's $T^2$.  The main ideas were illustrated in @fig-HE-framework.

Having described the statistical ideas behind the MLM in @sec-mlm-review, we can proceed to
extend this framework to larger designs. @fig-dogfood-quartet illustrates these ideas using the
simple one-way MANOVA design of the dogfood data from @sec-dogfood-data.

```{r}
#| label: fig-dogfood-quartet
#| echo: false
#| fig-align: center
#| out-width: "100%"
#| fig-cap: "**Dogfood quartet**: Illustration of the conceptual ideas of the HE plot framework for the dogfood data. (a) Scatterplot of the data; (b) Summary using data ellipses; (c) HE plot shows the variation in the means in relation to pooled within group variance; (d) Transformation from data space to canonical space"
knitr::include_graphics("images/dogfood-quartet.png")
```



* In data space (a), each group is summarized by its **data ellipse** (b), representing the means and covariances.

* Variation against the hypothesis of equal means can be seen by the $\mathbf{H}$ ellipse in the **HE plot** (c), representing the data ellipse of the fitted values. Error variance is shown in the $\mathbf{E}$ ellipse,
representing the pooled within-group covariance matrix, $\mathbf{S}_p$ and the data ellipse of the residuals from the model.
For the dogfood data, the group means have a negative relation: longer time to start eating is associated with a smaller amount eaten.

* The MANOVA (or Hotelling's $T^2$) is formally equivalent to a **discriminant analysis**, predicting
group membership from the response variables which can be seen in data space. (The main difference is 
emphasis and goals: MANOVA seeks to test differences among group means, while discriminant analysis aims
at classification of the observations into groups.)

* This effectively projects the $p$-dimensional
space of the predictors into the smaller **canonical space** (d) that shows the greatest differences among
the groups. As in a biplot, vectors show the relations of the response variables with the canonical dimensions.

<!--
```{r}
#| label: fig-HE-framework
#| echo: false
#| fig-align: center
#| out-width: "100%"
#| fig-cap: "The Hypothesis Error plot framework for a two-group design. Above: Data ellipses can be summarized in an HE plot showing the pooled within-group error ($\\mathbf{E}$) ellipse and the $\\mathbf{H}$ 'ellipse' for the group means.
#| Below: Observations projected on the line joining the means give discriminant scores which correpond to a one-dimensional canonical space, represented by a boxplot of their scores and arrows reflecting the variable weights."
knitr::include_graphics("images/HE-framework.png")
```
-->

<!--
Having described the statistical ideas behind the MLM in @sec-mlm-review, we can proceed to
extend this framework to larger designs. A conceptual overview is shown in @fig-arcmanov1 for a one-way MANOVA
design with 8 groups.

```{r}
#| label: fig-arcmanov1
#| echo: false
#| fig-align: center
#| out-width: "100%"
#| fig-cap: "Conceptual plots showing the essential ideas behind multivariate tests, in terms of the hypothesis
#| ($\\mathbf{H}$) and error ($\\mathbf{E}$) matrices for a 1-way MANOVA design with two response variables, $Y_1$ and $Y_2$"
knitr::include_graphics("images/arcmanov.png")
```
-->


For more complex models such as MANOVA with multiple factors or multivariate multivariate regression,
there is one sum of squares and products matrix (SSP), and therefore one
$\mathbf{H}$ ellipse for each term in the model. For example, in a two-way MANOVA design with the model
formula `(y1, y2) ~ A + B + A*B` and equal sample sizes in the groups, the total sum of squares accounted for by the model is
\begin{align*}
\mathbf{SSP}_{\text{Model}} & = \mathbf{SSP}_{A} + \mathbf{SSP}_{B} + \mathbf{SSP}_{AB} \\
                            & = \mathbf{H}_A + \mathbf{H}_B + \mathbf{H}_{AB} \period
\end{align*}


## HE plot construction

The HE plot is constructed to allow a direct visualization of the "size" of hypothesized terms in
a multivariate linear model in relation to unexplained error variation.
These can be displayed in 2D or 3D plots, so I use the term "ellipsoid" below to cover all cases.

Error variation is represented by a standard 68\% data ellipsoid of the $\mat{E}$ matrix
of the residuals in $\Epsilon$. This is divided by the residual degrees of freedom, so the size of $\mat{E} / \text{df}_e$ is analogous to a mean square error
in univariate tests. The choice of 68\% coverage allows you to ``read'' the residual standard deviation as the half-length of the
shadow of the $\mat{E}$ ellipsoid on any axis (see @fig-galton-ellipse-r).
The $\mat{E}$ ellipsoid is then translated to the overall (grand) means $\bar{\mathbf{y}}$ of the variables plotted, which allows us to show the means for factor levels on the same scale, facilitating interpretation.
In the notation of @eq-ellE, the error ellipsoid is given by
$$
\mathcal{E}_c (\bar{\mathbf{y}}, \mathbf{E}) = \bar{\mathbf{y}} \; \oplus \; c\,\mathbf{E}^{1/2} \comma
$$ {#eq-Ec}
where $c = \sqrt{2 F_{2, n-2}^{0.68}}$ for 2D plots and $c = \sqrt{3 F_{3, n-3}^{0.68}}$ for 3D for standard 68% coverage.

An ellipsoid representing variation in the means of a factor (or any other term reflected in a general linear hypothesis test, @eq-hmat) in the $\mat{H}$ matrix is simply the data ellipse of the fitted values for that term. 

Dividing the hypothesis matrix by the error degrees of freedom, giving
$\mat{H} / \text{df}_e$,
puts this on the same scale as the \E ellipse.
<!-- , as shown in the left panel of \figref{fig:heplot-iris1}. -->
I refer to this as _effect size scaling_, because it is similar to an effect size index used in
univariate models, e.g., $ES = (\bar{y}_1 - \bar{y}_2) / s_e$ in a two-group, univariate design.

<!--
The notation $\bar{\mathbf{y}} \; \oplus$ in @eq-Ec means that the error ellipsoid is
shifted to be centered at the grand means of the response variables, where the $\mat{H}$
is also centered. This allows the HE plot to also show the group means, facilitating
interpretation.
-->

### Example: Iris data {#iris-ex}

Perhaps the most famous (or infamous) dataset in the history of multivariate data analysis is that of measurements on three species of Iris flowers collected by Edgar Anderson [-@Anderson:35] in the Gaspé Peninsula of Québec, Canada. 
Anderson wanted to quantify the outward appearance ("morphology": shape, structure, color, pattern, size) of species as a method to study variation within and between such groups. Although Anderson published in the obscure _Bulletin of the American Iris Society_, R. A. Fisher [-@Fisher:1936] saw this as a challenge and opportunity to introduce the method now called
discriminant analysis---how to find a weighted composite of variables to best discriminate among existing groups.

<!-- depending on use, make some of this a text box -->
::: {.callout-note title="History corner"}

I said "infamous" above because Fisher published in the _Annals of Eugenics_, was an ardent eugenicist himself, 
and the work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups.
Through guilt by association, the Iris data, having mistakenly been called "Fisher's Iris Data",
has become deprecated, even called "racist data".[^stop-iris] The voices of the _Setosa_, _Versicolor_ and _Virginica_ of
Gaspé protest: we don't have a racist bone in out body and nor prejudice against any
other species.

@Bodmer-etal-2021 present a careful account of Fisher's views on eugenics within the context
of his time and his contributions to modern statistical theory and practice.
Fisher's views on race were largely formed by Darwin and Galton,
but "nearly all of Fisher’s statements were about populations,
groups of populations, or the human species as a whole". Regardless, the `iris` data were
Anderson's and should not be blamed. After all, if Anderson had lent his car to Fisher,
would the car be tainted by Fisher's eugenicist leanings?

[^stop-iris]: [Stop using iris](https://www.meganstodel.com/posts/no-to-iris/)
:::

<!--
```{r}
#| label: fig-iris-flowers
#| out-width: "100%"
#| fig-cap: "Three species of irises in the Anderson/Fisher data set. _Source_: P. I. Adegbite [Iris Flower Classification]( https://peaceadegbite1.medium.com/iris-flower-classification-60790e9718a1)"
knitr::include_graphics(here::here("images/iris-flowers-labeled.png"))
```

@fig-iris-flowers shows photos of the three iris species, _Setosa_, _Versicolor_ and _Virginica_. 
Each flower has three sepals and three petals. The sepals have brightly colored central sections.
Anderson recorded measurements of the  length and width (in cm.) of the sepals and petals on 50 flowers of each type.
-->

```{r}
#| label: fig-iris-diagram
#| echo: false
#| out-width: "70%"
#| fig-cap: "Diagram of an iris flower showing the measurements of petal and sepal size. Each flower has three sepals and three alternating petals. The sepals have brightly colored central sections. _Source_: @DeSilva2020"
knitr::include_graphics(here::here("images/iris-diagram.jpg"))
```

So that we understand what the measurements represent, @fig-iris-diagram superposes labels on a typical iris flower.
Sepals are like ostentatious petals, with attractive decorations in the central section. Length is the distance from the center
to the tip and width is the transverse dimension.

As always, it is useful to start with overview displays to see the data.
A scatterplot matrix (@fig-iris-spm) shows that _versicolor_ and _virginica_ are more
similar to each other than either is to _setosa_, both in their pairwise means (_setosa_ are smaller) and in the
slopes of regression lines.
Further, the ellipses  suggest that the
assumption of constant within-group covariance matrices is problematic: While the shapes and sizes of the concentration ellipses for _versicolor_ and _virginica_ are reasonably similar, the shapes and sizes of the ellipses for _setosa_ are different from the other two.

```{r}
#| label: fig-iris-spm
#| out-width: "100%"
#| fig-width: 10
#| fig-height: 10
#| fig-cap: "Scatterplot matrix of the `iris` dataset. The species are summarized by 68% data ellipses and linear regression lines in each pairwise plot."
iris_colors <-c("blue", "darkgreen", "brown4")
scatterplotMatrix(~ Sepal.Length + Sepal.Width + 
                    Petal.Length + Petal.Width | Species,
  data = iris,
  col = iris_colors,
  pch = 15:17,
  smooth=FALSE,
  regLine = TRUE,
  ellipse=list(levels=0.68, fill.alpha=0.1),
  diagonal = FALSE,
  legend = list(coords = "bottomleft", 
                cex = 1.3, pt.cex = 1.2))
```

### MANOVA model {#sec-iris-mod}
We proceed nevertheless to fit a multivariate one-way ANOVA model to the iris data.
The MANOVA model for these data addresses the question: "Do the means of the `Species` differ significantly for the sepal and petal variables taken together?" 
$$
\mathcal{H}_0 : \mathbf{\mu}_\textrm{setosa} = \mathbf{\mu}_\textrm{versicolor} = \mathbf{\mu}_\textrm{virginica}
$$


Because there are three species, the test involves $s = \min(p, g-1) =2$ degrees of freedom,
and we are entitled to represent this by two 1-df contrasts,  or sub-questions. From the separation among the groups
shown in @fig-iris-spm (or more botanical knowledge), it makes sense to compare:

* Setosa vs. others: $\mathbf{c}_1 = (1,\: -\frac12, \: -\frac12)$
* Versicolor vs. Virginica: : $\mathbf{c}_1 = (0,\: 1, \: -1)$

You can do this by putting these vectors as columns in a matrix and assigning this to the
`contrasts()` of `Species`. It is important to do this _before_ fitting with `lm()`,
because the contrasts in effect determine how the $\mathbf{X}$ matrix is setup, and hence the
names of the coefficients representing `Species`.

```{r}
C <- matrix(c(1,-1/2,-1/2,  
              0,   1,  -1), nrow=3, ncol=2)
contrasts(iris$Species) <- C
contrasts(iris$Species)
```

Now let's fit the model. As you would expect from @fig-iris-spm, the differences among groups are highly significant. 

```{r iris-mod}
iris.mod <- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~
                 Species, data=iris)
Anova(iris.mod)
```

As a quick follow-up, it is useful to examine the univariate tests for each of the iris
variables, using `heplots::glance()` or `heplots::uniStats()`. It is of interest that
the univariate $R^2$ values are much larger for the petal variables than the sepal length and width.[^rsq-anova].
For comparison, `heplots::etasq()` gives the overall $\eta^2$ proportion of variance accounted for in 
all responses.

[^rsq-anova]: Recall that $R^2$ for a linear model is the the proportion of variation in the response that is explained by the model, calculated as $R^2 = \text{SS}_H / \text{SS}_T = \text{SS}_H / (\text{SS}_H + \text{SS}_E )$. For a multivariate model, these are obtained from the diagonal elements of $\mathbf{H}$ and $\mathbf{E}$.

```{r iris-glance}
glance(iris.mod)

etasq(iris.mod)
```

But these statistics don't help to understand _how_ the species differ. For this, we turn
to HE plots.

## HE plots

The `heplot()` function takes a `"mlm"` object and produces an HE plot for one pair
of variables specified by the `variables` argument. By default, it plots the first two.
@fig-iris-HE1 shows the HE plots for the two sepal and the two petal variables.

<!-- duplicated the code b/c it doesn't print from the fig-iris-HE1 chunk -->
```{r code-iris-HE1}
#| eval: false
heplot(iris.mod, size = "effect",
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
heplot(iris.mod, size = "effect", variables = 3:4,
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
```

```{r}
#| echo: false
#| label: fig-iris-HE1
#| fig-width: 10
#| fig-height: 5
#| fig-show: hold
#| out-width: "100%"
#| fig-cap: "HE plots for the multivariate model `iris.mod`. The left panel shows the plot for the Sepal variables; the right panel plots the Petal variables."
op <- par(mar = c(4, 4, 1, 1) + .5,
          mfrow = c(1, 2))
heplot(iris.mod, size = "effect",
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
heplot(iris.mod, size = "effect", variables = 3:4,
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1))
```

The interpretation of the plots in @fig-iris-HE1 is as follows:

* For the Sepal variables, length and width are positively correlated _within_ species 
(the $\mat{E}$ = "Error" ellipsoid). The means of the groups (the $\mat{H}$ = "Species" ellipsoid), however, are negatively correlated. This plot is the HE plot representation of the data
shown in row 2, column 1 of @fig-iris-spm. It reflects the relative **shape** of the 
iris sepals: shorter and wider for _setosa_ than the other two species.

* For the Petal variables length and width are again positively correlated _within_ species,
but now the means of the groups are positively correlated: longer petals go with wider ones
across species. This reflects the relative **size** of the iris petals.
The analogous data plot appears in row 4, column 3 of @fig-iris-spm.

<!--
**Older stuff**
@fig-iris-HE0 shows two views of the relationship between sepal length and sepal width for the iris data. ...
```{r}
#| eval: false
op <- par(mar = c(4, 4, 1, 1) + .5,
          mfrow = c(1, 2))
col <-c("blue", "darkgreen", "brown")
clr <- c(col, "red")
covEllipses(cbind(Sepal.Length, Sepal.Width) ~ Species, data=iris,
      pooled = TRUE,
      fill=TRUE,
      fill.alpha = 0.1,
      lwd = 3,
      col = clr,
      cex = 1.5, cex.lab = 1.5,
      label.pos = c(3, 1, 3, 0),
      xlim = c(4,8), ylim = c(2,4))

iris.mod <- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~
                 Species, data=iris)
heplot(iris.mod, size = "effect",
       cex = 1.5, cex.lab = 1.5,
       fill=TRUE, fill.alpha=c(0.3,0.1),
       xlim = c(4,8), ylim = c(2,4))
par(op)
```

```{r}
#| label: fig-iris-HE0
#| out-width: "100%"
#| fig-cap: "Iris data: Data ellipses and HE plot"
knitr::include_graphics("images/iris-HE1.png")
```
-->


## Significance scaling

The geometry of ellipsoids and multivariate tests allow us to go further with another re-scaling of the $\mat{H}$ ellipsoid
that gives a _visual test of significance_ for any term in a MLM.
This is done simply by dividing $\mat{H} / df_e$ further
by the $\alpha$-critical value of the corresponding test statistic to show the strength of evidence against
the null hypothesis.

Among the various multivariate test statistics,
Roy's maximum root test, based on the largest eigenvalue $\lambda_1$ of $\mat{H} \mat{E}^{-1}$,
gives $\mat{H} / (\lambda_\alpha df_e)$
which has the visual property that the
scaled $\mat{H}$ ellipsoid will protrude _somewhere_ outside the standard $\mat{E}$ ellipsoid if and only if
Roy's test is significant at significance level $\alpha$. The critical value $\lambda_\alpha$ for Roy's
test is
$$
\lambda_\alpha = \left(\frac{\text{df}_1}{\text{df}_2}\right) \; F_{\text{df}_1, \text{df}_2}^{1-\alpha} \comma
$$
where $\text{df}_1 = \max(p, \text{df}_h)$ and $\text{df}_2 = \text{df}_h + \text{df}_e - \text{df}_1$.

For these data, the HE plot using
significance scaling is shown in the right panel of @fig-iris-HE2. The left panel is the same as that
shown for sepal width and length in @fig-iris-HE1, but with axis limits to make the two plots
directly comparable.

```{r}
#| label: fig-iris-HE2
#| echo: false
#| out-width: "100%"
#| fig-cap: "HE plots for sepal width and sepal length in the iris dataset. Left: _effect_ scaling of the $\\mathbf{H}$ matrix; right: _significance_ scaling, where protrusion of $\\mathbf{H}$ outside $\\mathbf{E}$ indicates a significant effect by Roy's test."
knitr::include_graphics("images/iris-HE2.png")
```

You can interpret the plot using effect scaling to indicate that the overall "size" of variation
of the group means is roughly the same as that of within-group variation for the two sepal variables.
Significance scaling weights the evidence against the null hypothesis that a given effect is zero.

## Visualizing contrasts and linear hypotheses

As described in @sec-contrasts, tests of linear hypotheses and contrasts represented by the
general linear test $\mathcal{H}_0: \mathbf{C} \;\mathbf{B} = \mathbf{0}$ provide a
powerful way to probe the _specific_ effects represented within the global null hypothesis,
$\mathcal{H}_0: \mathbf{B} = \mathbf{0}$, that all effects are zero.

In this example the contrasts $\mathbf{c}_1$ (`Species1`) and $\mathbf{c}_2$ (`Species2`)
among the iris species are orthogonal, i.e., 
$\mathbf{c}_1^\top \mathbf{c}_2 = 0$. Therefore, their tests are statistically independent, and their
$\mathbf{H}$ matrices are additive. They fully decompose the general question of differences
among the groups into two independent questions regarding the contrasts.

$$
\mathbf{H}_\text{Species} = \mathbf{H}_\text{Species1} + \mathbf{H}_\text{Species2}
$$ {#eq-H-Species}

`car::linearHypothesis()` is the means for testing these statistically, and `heplot()`
provides the way to show these tests visually. Using the contrasts set up in @sec-iris-mod,
$\mathbf{c}_1$, representing the difference between _setosa_ and the other species
is labeled `Species1` and the comparison of _versicolor_ with _virginica_ is `Species2`.
The coefficients for these in $\mathbf{B}$ give the differences in the means.
The line for `(Intercept)` gives grand means of the variables.

```{r iris-mod-coef}
coef(iris.mod)
```

Numerical tests of hypotheses using `linearHypothesis()` can be specified in a very general way:
A matrix (or vector) $\mathbf{C}$ giving linear combinations of coefficients by rows, or a character vector giving the hypothesis in symbolic form.
A character variable or vector tests whether the named coefficients are different from zero
for all responses.

```{r iris-linhyp}
linearHypothesis(iris.mod, "Species1") |> print(SSP=FALSE)

linearHypothesis(iris.mod, "Species2") |> print(SSP=FALSE)
```

The various test statistics are all equivalent here---they give the same $F$ statistics--- because they
they have 1 degree of freedom.

In passing, from @eq-H-Species, note that the _joint_ test of these contrasts is exactly equivalent to the overall test of `Species` (results not shown).

```{r}
#| eval: false
linearHypothesis(iris.mod, c("Species1", "Species2"))
```

We can show these contrasts in an HE plot by supplying a named list for the `hypotheses` argument.
The names are used as labels in the plot.
In the case of a 1-df multivariate test, the $\mathbf{H}$ ellipses plot as a degenerate line.

```{r}
#| label: fig-iris-contrasts
#| fig-width: 6
#| fig-height: 6
#| out-width: "60%"
#| fig-cap: "HE plot for sepal length and width in the `iris` data showing the tests of the two contrasts"
hyp <- list("S:Vv" = "Species1", "V:v" = "Species2")
heplot(iris.mod, hypotheses=hyp,
       cex = 1.5, cex.lab = 1.5,
       fill = TRUE, fill.alpha = c(0.3, 0.1),
       col = c("red", "blue", "darkgreen", "darkgreen"),
       lty = c(0,0,1,1), label.pos = c(3, 1, 2, 1),
       xlim = c(2, 10), ylim = c(1.4, 4.6))
```

This HE plot shows that, for the two sepal variables, the greatest between-species variation
is accounted for by the contrast (`S:Vv`) between _setosa_ and the others, for which the effect is very
large in relation to error variation.  The second contrast (`V:v`), between the _versicolor_ and _virginica_
species is relatively smaller, but still explains significant variation of the sepal variables among the species.

The directions of these hypotheses in a given plot show _how_ the group means differ in terms of a given contrast.[^HE-contrast-geometry] For example, the contrast `S:Vv` is the line that separates _setosa_ from the others and indicates that _setosa_ flowers have shorter but wider sepals.

[^HE-contrast-geometry]: That the $\mathbf{H}$ ellipses for the contrasts subtend that for the overall test of `Species` is no accident.
In fact, this is true in $p$-dimensional space for _any_ linear hypothesis, and orthogonal contrasts
have the additional geometric property that they form _conjugate axes_ for the overall $\mathbf{H}$ ellipsoid
relative to the $\mathbf{E}$ ellipsoid [@Friendly-etal:ellipses:2013].

## HE plot matrices

In base R graphics, 2D scatterplots are extended to all pairwise views of multivariate data
with a `pairs()` method. For multivariate linear models, the `r package("heplots")` defines a
`pairs.mlm()` method to display HE plots for all pairs of the response variables.

```{r}
#| label: fig-iris-pairs
#| fig-width: 8
#| fig-height: 8
#| out-width: "100%"
#| fig-cap: "All pairwise HE plots for the iris data."
pairs(iris.mod,
      fill=TRUE, fill.alpha=c(0.3, 0.1))
```

@fig-iris-pairs provides a fairly complete visualization of the results of the multivariate tests
and answers the question: **how** do the species differ?
Sepal length and the two petal variables have the group means nearly perfectly correlated, in the
order _setosa_ < _versicolor_ < _virginica_. For Sepal width, however, _setosa_ has the largest mean,
and so the $\mathbf{H}$ ellipses show a negative correlation in the second row and column.


## Low-D views: Canonical analysis {#sec-candisc}

The HE plot framework so far provides views of all the effects in a MLM in 
_variable_ space. We can view this in 2D for selected pairs 
of response variables, or for all pairwise views in scatterplot matrix format.
There is also an `heplot3d()` function giving plots for three response variables together.
The 3D plots are interactive, in that they can be rotated and zoomed
by mouse control, and dynamic, in that they can be made to spin and saved as movies.
To save space, these plots are not shown here.

However in a one-way MANOVA design with more than response three variables, it is difficult
to visualize how the groups vary on _all_ responses together, and how the
different variables contribute to discrimination among groups.
In this situation, **canonical discriminant analysis** (CDA) is often used, to provide
a low-D visualization of between-group variation.
When the predictors are also continuous, the analogous term is **canonical correlation analysis** (CCA).
The advantage here is that we can also show the relations of the response variables to these dimensions,
similar to a biplot (@sec-biplot) for a PCA of purely quantitative variables. 

The key to this is the eigenvalue decomposition, $\mat{H}\mat{E}^{-1} \lambda_i = \lambda_i \vec{v}_i$
(@eq-he-eigen)
of $\mathbf{H}$ relative to $\mathbf{E}$. The eigenvalues, $\lambda_i$, give the "size" of each $s$ 
orthogonal dimensions on which the multivariate tests are based. But the corresponding
eigenvectors, $\mathbf{v}_i$, give the _weights_ for the response variables
in $s$ linear combinations that maximally discriminate among the groups or equivalently maximize the
(canonical) $R^2$ of a linear combination of the predictor $\mathbf{X}$s with a linear combination of the
response $\mathbf{Y}$s.

Thus, CDA amounts to a transformation of the $p$ responses,
$\mat{Y}_{n \times p}$
into the canonical space,
$$
\mat{Z}_{n \times s} = \mat{Y} \; \mat{E}^{-1/2} \; \mat{V} \;,
$$
where $\mat{V}$ contains the eigenvectors of $\mat{H}\mat{E}^{-1}$
and $s=\min ( p, df_h )$.
It is well-known (e.g., @Gittins:85)
that \emph{canonical discriminant plots} of the first two (or three, in 3D)
columns of $\mat{Z}$ corresponding to the largest canonical correlations
provide an optimal low-D display of the variation between groups relative
to variation within groups.

Canonical discriminant analysis is typically carried out in conjunction with a one-way MANOVA design.
The `r package("candisc", cite=TRUE)` generalizes this to multi-factor designs in the `candisc()` function.
For any given term in a `"mlm"`, the _generalized canonical discriminant analysis_ amounts to a standard discriminant analysis based on the $\mat{H}$ matrix for _that_ term in relation to the full-model $\mat{E}$ matrix.[^candiscList]

[^candiscList]: `candiscList()` performs a generalized canonical discriminant analysis for _all_ terms in a multivariate linear model, returning a list of the results for each factor.

Tests based on the eigenvalues $\lambda_i$,
initially stated by @Bartlett1938, use  Wilks' $\Lambda$ likelihood ratio tests of these.
This allow you to determine the number of significant canonical dimensions, or the number of different
aspects to consider for the relations between the responses and predictors.
These take the form of _sequential_ global tests of the hypothesis that the canonical correlation in
the current row and all that follow are zero. Thus, if you find 
The canonical $R^2$, `CanRsq`, gives the R-squared value of fitting the $i$th response canonical variate to the corresponding $i$th canonical variate for the predictors.

For the iris data, we get the following printed summary:

```{r iris-can}
iris.can <- candisc(iris.mod) |> print()
```

This analysis shows a very simple result: The differences among the iris species can
be nearly entirely accounted for by the first canonical dimension (99.1%).
Interestingly, the second dimension is also highly significant, even though it accounts for only 0.88%.

### Coeficients

The `coef()` method for "candisc" objects returns a matrix of weights for the response variables
in the canonical dimensions. By default, these are given for the response variables standardized
to $\bar{y}=0$ and $s^2_y = 1$. The `type` argument also allows for raw score weights (`type = "raw"`)
used to compute the observation scores on `Can1`, `Can2`, ... .
`type = "raw"` gives the canonical structure coefficients, which are the correlations between each response and the
canonical scores.

```{r iris-can-coef}
coef(iris.can)
coef(iris.can, type = "structure")
```

Thus, the two weighted sums for the canonical variates, using mean-centered raw scores, can be shown as follows:

```{r can-vars}
#| echo: false
#| results: asis
C <- coef(iris.can, type = "raw")
names <- row.names(C)
can1 <- paste("Can1 = ", paste0(round(C[,1], 2), " x ", names, collapse = " + "))
#can1 <- gsub("+ -", "-", can1)
cat(can1)
can2 <- paste("Can2 = ", paste0(round(C[,2], 2), " x ", names, collapse = " + ")) |> cat()
```

### Canonical scores plot

The `plot()` method for `"candisc"` objects gives a plot of these observation scores. 
`ellipse=TRUE` overlays this with their standard data ellipses for each species, as shown in @fig-iris-candisc. 
The response variables are shown as
vectors, using the structure coefficients, as in a biplot. Thus, the relative size of the projection of these
vectors on the canonical axes reflects the correlation of the observed response on the canonical dimension.
For ease of interpretation I flipped the sign of the first canonical dimension, so that the positive `Can1` direction
corresponds to larger flowers.

```{r echo=-1}
#| label: fig-iris-candisc
#| fig-width: 8
#| fig-height: 8
#| out-width: "80%"
#| fig-cap: "Plot of canonical scores for the iris data."
op <- par(mar = c(4, 4, 4, 1) + .5)
vars <- names(iris)[1:4] |> 
  stringr::str_replace("\\.", "\n")
plot(iris.can,
     var.labels = vars,
     var.col = "black",
     var.lwd = 2,
     ellipse=TRUE,
     scale = 9,
     col = iris_colors,
     pch = 15:17,
     cex = 0.7,
     rev.axes = c(TRUE, FALSE),
     xlim = c(-10, 10))
```

The interpretation of
this plot is simple: in canonical space, variation of the means for the iris species is
essentially one-dimensional (99.1% of the effect of `Species`), and this dimension corresponds
to overall size of the iris flowers.  All variables except for `Sepal.Width` are positively aligned with
this axis, but the two petal variables show the greatest discrimination.
The negative direction for `Sepal.Width` reflects the pattern seen in @fig-iris-pairs, where _setosa_
have wider sepals.

For the second dimension, look at the horizontal projections of the variable vectors on the `Can2` axis.
All are positive, but this is dominated by `Sepal.Width`.



### Canonical HE plot
For a one-way design, the _canonical HE plot_
is simply the HE plot of the canonical scores in the analogous MLM model that substitutes $\mat{Z}$ for $\mat{Y}$. 
In effect, it is a more compact visual summary of the plot shown in @fig-iris-candisc.

This is shown in @fig-iris-HEcan for the `iris` data.
In canonical space, the residuals are always uncorrelated, so the $\mathbf{E}$ ellipse plots as a circle.
The $\mathbf{H}$ ellipse for `Species` here reflects a data ellipse for the fitted values--- group means--- shown as
labeled points in the plot. The differences among `Species` are so large that this plot uses `size = "effect"`
scaling, making the axes comparable to those in @fig-iris-candisc.[^can-signif-scaling]

[^can-signif-scaling]:  If significance scaling was used the interpretation of the canonical HE plot plot would the same as before: if the hypothesis ellipse extends beyond the error ellipse, then that dimension is significant.  

The vectors for each predictor are the same structure coefficients as in the ordinary canonical plot.
They can again be reflected for interpretation and scaled in length to fill the plot window.

<!-- now depends on candisc 0.9.1 -->

```{r echo=-1}
#| label: fig-iris-HEcan
#| fig-width: 8
#| fig-height: 8
#| out-width: "80%"
#| fig-cap: "Canonical HE plot for the iris data."
op <- par(mar = c(4, 4, 4, 1) + .5)
heplot(iris.can,
       size = "effect",
       scale = 8,
       var.labels = vars,
       var.col = "black",
       var.lwd = 2,
       fill = TRUE, fill.alpha = 0.2,
       rev.axes = c(TRUE, FALSE),
       xlim = c(-10, 10))
```

The collection of plots shown for the iris data here can be seen as progressive visual summaries of the 
data:

* The scatterplot matrix in @fig-iris-spm shows the iris flowers in the data space of the sepal and petal variables.
* Canonical analysis substitutes for these the two linear combinations reflected in `Can1` and `Can2`. The plot in @fig-iris-candisc portrays _exactly_ the same relations among the species, but in the reduced canonical space of only two dimensions.
* The HE plot version, shown in @fig-iris-HEcan summarizes the separate data ellipses for the species with
pooled, within-group variance of the $\mathbf{E}$ matrix for the canonical variables, which are always uncorrelated.
The variation among the group means is reflected in the size and shape of the ellipse for the $\mathbf{E}$ matrix.


## Quantitative predictors: MMRA and MANCOVA

The ideas behind HE plots extend naturally to multivariate multiple regression (MMRA) and multivariate analysis of covariance (MANCOVA). In MMRA designs, the $\mathbf{X}$ matrix contains only quantitative predictors, while in MANCOVA designs, it contains a mixture of factors and quantitative predictors (covariates), but typically there is just one “group” factor.

In the MANCOVA case, there is often a subtle difference in emphasis: true MANCOVA analyses focus on the differences among groups defined by the factors, adjusting for (or controlling for) the quantitative covariates. Analyses concerned with homogeneity of regression focus on quantitative predictors and attempt to test whether the regression relations are the same for all groups defined by the factors.

A purely visual feature of HE plots in these cases is that the $\mathbf{H}$ ellipse for a quantitative predictor
with 1 df appears as a degenerate line. But consequently, the angles between these for different
predictors has a simple interpretation as as the correlation between their predicted effects.
Moreover, it is easy to show visual overall tests of _joint_ linear hypotheses for two or more
predictors together.

* `heplots::NLSY`: National Longitudinal Survey of Youth Data
* `heplots::schooldata`: School Data -> `R/schooldata-ex.R`
* `heplots::Hernior`: Recovery from Elective Herniorrhaphy -> HE_mmra vignette

### Example: NLSY data

Here I'll continue the analysis of the NLSY data from @sec-NLSY-mmra. In the model `NLSY.mod1` I used
only father's income (as $\log_2()$) and education to predict scores in reading and math, and both 
of these demographic variables were highly significant.

```{r}
#| label: fig-NLSY-heplot1
#| out-width: "80%"
#| fig-cap: "HE plot for the simple model for the NLSY data fitting reading and math scores from income and education."
data(NLSY, package = "heplots")
NLSY.mod1 <- lm(cbind(read, math) ~ log2(income) + educ, 
           data= NLSY |> filter(income != 0))

heplot(NLSY.mod1, 
  fill=TRUE, fill.alpha = 0.2, 
  cex = 1.5, cex.lab = 1.5,
  lwd=c(2, 3, 3),
  label.pos = c("bottom", "top", "top")
  )

```

Fathers income and education are highly correlated in their effects on the outcome scores.
From the angles in the plot, income is slightly more related to the math score, while education
slightly more to the reading score.

The overall joint test for both predictors can then visualized as the test of the linear hypothesis
$\mathcal{H}_0 : \mathbf{B} = [\boldsymbol{\beta}_\text{income}, \boldsymbol{\beta}_\text{educ}] = \mathbf{0}$.
For `heplot()`, we specify the names of the coefficients to be tested with the `hypotheses` argument.

```{r}
#| label: fig-NLSY-heplot2
#| out-width: "80%"
#| fig-cap: "HE plot adding the $\\mathbf{H}$ ellipse for the overall test that both predictors have no effect on the outcome scores."
coefs <- rownames(coef(NLSY.mod1))[-1] |> print()

heplot(NLSY.mod1, 
       hypotheses = list("Overall" = coefs),
       fill=TRUE, fill.alpha = 0.2, 
       cex = 1.5, cex.lab = 1.5,
       lwd=c(2, 3, 3, 2),
       label.pos = c("bottom", "top"))
```



```{r}
#| echo: false
cat("Writing packages to ", .pkg_file, "\n")
write_pkgs(file = .pkg_file)
```
