```{r include=FALSE}
source("R/common.R")
knitr::opts_chunk$set(fig.path = "figs/Ch13/")
```

::: {.content-visible unless-format="pdf"}
{{< include latex/latex-commands.qmd >}}
:::
  
# Multiviate Influence and Robust Estimation

In the analysis of linear models, the identification and treatment of outliers and
influential observations represents one of the most critical yet challenging
aspects of statistical modeling. As you saw earlier (@sec-leverage), even a single "bad"
observation can completely alter the results of a linear model fit by ordinary
least squares.

Univariate influence diagnostics have
been well-established since the pioneering work of @Cook:77 and others
(@Belsley-etal:80;@CookWeisberg:82) and their wide implementation in R packages
such as `r pkg("stats")` and `r pkg("car")` makes these readily accessible
in statistical practice. If you seek statistical advice regarding a
perplexing model, the consultant may well ask:

> Did you make any influence or other diagnostic plots?

However, the
extension to multivariate response models introduces additional complexity that
goes far beyond simply applying univariate methods to each response variable
separately. The multivariate case requires consideration of the _joint influence_
structure across all responses simultaneously, accounting for the correlation
patterns among dependent variables and the potential for observations to be
influential in some linear combinations of responses while appearing benign when
examined multivariate. 

This multivariate perspective can reveal influence patterns
that would otherwise remain hidden, as an observation might exert substantial
leverage on the overall model fit through subtle but systematic effects across
multiple responses.

Detecting outliers and influential observations 
has now progressed to the point where the methods described below can usefully
be applied to multivariate linear models. But having found some troublesome
cases, the question arises, what to do about them?

**Packages**

In this chapter we use the following packages. Load them now
```{r pkg-load}
library(dplyr)
library(tidyr)
library(car)
library(heplots)
library(candisc)
library(mvinfluence)
library(ggplot2)
library(patchwork)
```

## Multivariate influence {#sec-multivariate-influence}

An elegant extension of the ideas behind leverage, studentized residuals and measures of
influence to the case of multivariate response data is due to @BarrettLing:92
(see also: @Barrett:2003). These methods have been implemented in the `r package("mvinfluence", cite=TRUE)`
which makes available several forms of influence plots to visualize the results.

As in the univariate case, the measures of multivariate influence stem from case-deletion idea
of comparing some statistic calculated from the full sample to that statistic calculated when
case $i$ is deleted. The Barrett-Ling approach generalized this to the case of deleting a set $I$ of
$m \ge 1$ cases. This can be useful because some cases can "mask" the influence of others
in the sense that when one is deleted, others become much more influential. However, in most
cases the default of deleting individual observations ($m=1$) is sufficient.

### Notation

It is useful to define some notation used to designate terms in the model calculated from the _complete_ dataset
versus those calculated with one or more observations _excluded_.
As before, let $\mathbf{X}$ be the model matrix in the multivariate linear model, 
$\mathbf{Y}_{n \times p} = \mathbf{X}_{n \times q} \; \mathbf{B}_{q \times p} + \mathbf{E}_{n \times p}$.
As we know, the usual least squares estimate of $\mathbf{B}$ is given by
$\mathbf{B} = (\mathbf{X}^\top \mathbf{X})^{-1}  \mathbf{X}^\top \mathbf{Y}$.

Then let: 

* $\mathbf{X}_I$ be the _submatrix_ of $\mathbf{X}$ whose $m$ rows are indexed by $I$,
* $\mathbf{X}_{(-I)}$ is the _complement_, the submatrix of $\mathbf{X}$ with the $m$ rows in $I$ deleted,

 
Matrices $\mathbf{Y}_I$, $\mathbf{Y}_{(-I)}$ are defined similarly, denoting the submatrix of $m$ rows of $\mathbf{Y}$
and the submatrix with those rows deleted, respectively.

The calculation of regression coefficients when the cases indexed by $I$ have been removed
has the form
$\mathbf{B}_{(-I)} = (\mathbf{X}_{(-I)}^\top \mathbf{X}_{(-I)})^{-1} \mathbf{X}_{(-I)}^\top \mathbf{Y}_{I}$.
The corresponding residuals are expressed as
$\mathbf{E}_{(-I)} = \mathbf{Y}_{(-I)} - \mathbf{X}_{(-I)} \mathbf{B}_{(-I)}$.
 
### Hat values and residuals
 
The influence measures defined by @BarrettLing:92 are functions of two matrices $\mathbf{H}_I$ and $\mathbf{Q}_I$
corresponding to hat values and residuals, 
defined as follows:

* For the full data set, the "hat matrix", $\mathbf{H}$, is given by
      $\mathbf{H} = \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top$,
* $\mathbf{H}_I$ is the $m \times m$ the submatrix of $\mathbf{H}$ corresponding to the index set $I$,
      $\mathbf{H}_I = \mathbf{X} (\mathbf{X}_I^\top \mathbf{X}_I)^{-1} \mathbf{X}^\top$,
* $\mathbf{Q}$ is the analog of $\mathbf{H}$ defined for the residual matrix $\mathbf{E}$, that is,
      $\mathbf{Q} = \mathbf{E} (\mathbf{E}^\top \mathbf{E})^{-1} \mathbf{E}^\top$, with corresponding submatrix
      $\mathbf{Q}_I = \mathbf{E} \, (\mathbf{E}_I^\top \mathbf{E}_I)^{-1} \, \mathbf{E}^\top$,

   
### Cook's distance
 
Multivariate analogs of all the usual influence diagnostics (Cook's D, CovRatio, ...) can be defined in terms of
$\mathbf{H}$ and $\mathbf{Q}$. For instance,
Cook's distance is defined for a univariate response by
$$
D_I = (\mathbf{b} - \mathbf{b}_{(-I)})^T (\mathbf{X}^T \mathbf{X}) (\mathbf{b} - \mathbf{b}_{(-I)}) / p s^2 \; ,
$$
a measure of the squared distance between the coefficients $\mathbf{b}$ for the full data set and those
$\mathbf{b}_{(-I)}$ 
obtained when the cases in $I$ are deleted.  

In the multivariate case, Cook's distance is obtained
by replacing the vector of coefficients $\mathbf{b}$ by $\mathrm{vec} (\mathbf{B})$, the result of stringing out
the coefficients for all responses in a single $(n \times p)$-length vector.

$$
D_I = \frac{1}{p} [\mathrm{vec} (\mathbf{B} - \mathbf{B}_{(-I)})]^T (S^{-1} \otimes \mathbf{X}^T \mathbf{X}) \mathrm{vec} (\mathbf{B} - \mathbf{B}_{(-I)})  \; ,
$$
where $\otimes$ is the Kronecker (direct) product and
$\mathbf{S} = \mathbf{E}^T \mathbf{E} / (n-p)$ is the covariance matrix of the residuals.


### Leverage and residual components

We gain further insight by considering how far we can generalize from the case for a
univariate response. When $m = 1$, Cook's distance can be re-written as a product of leverage and residual components as
$$
D_i = \left(\frac{n-p}{p} \right) \frac{h_{ii} q_{ii}}{(1 - h_{ii})^2  } \;.
$$

This suggests that we define a _leverage component_ $L_i$ and _residual component_ $R_i$ as

$$
L_i = \frac{h_{ii}}{1 - h_{ii}} \quad\quad R_i = \frac{q_{ii}}{1 - h_{ii}} \;.
$$

$R_i$ is the studentized residual here, and $D_i \propto L_i \times R_i$.

In the general, multivariate case there are analogous matrix expressions for $\mathbf{L}$ and $\mathbf{R}$.
When `m > 1`, the quantities $\mathbf{H}_I$, $\mathbf{Q}_I$, $\mathbf{L}_I$, and
$\mathbf{R}_I$ are $m \times m$ matrices.  Where scalar quantities are needed, the `r pkg("mvinfluence")`
functions apply
a function, `FUN`, either `det()` or `tr()` to calculate a measure of "size", as in

````
H <- sapply(x$H, FUN)
Q <- sapply(x$Q, FUN)
L <- sapply(x$L, FUN)
R <- sapply(x$R, FUN)
````

This is the same trick used in the calculation of the various multivariate test statistics like Wilks' Lambda and Pillai's trace. In this way, the full range of multivariate influence measures discussed by @Barrett:2003
can be calculated.


## The Misterious Case 9

To illustrate these ides, this example, from @Barrett:2003, considers the simplest case, of one predictor (`x`) and
two response variables, `y1` and `y2`.

```{r toy-data}
Toy <- tibble(
   case = 1:9,
   x =  c(1,    1,    2,    2,    3,    3,    4,    4,    10),
   y1 = c(0.10, 1.90, 1.00, 2.95, 2.10, 4.00, 2.95, 4.95, 10.00),
   y2 = c(0.10, 1.80, 1.00, 2.93, 2.00, 4.10, 3.05, 4.93, 10.00)
)
```

A quick peek (@fig-toy-scatmat) at the data indicates that `y1` and `y2` are nearly perfectly correlated with each other.
Both of these are also strongly linear with `x` and there is one extreme point (case 9). The data is pecuiliar,
but looking at these pairwise plots doesn't suggest that anything is terribly wrong. 
In the plots of `y1` and `y2` agains `x`, case 9 simply looks like a good leverage point (@sec-leverage).

```{r}
#| label: fig-toy-scatmat
#| fig-width: 8
#| fig-height: 8
#| out-width: "100%"
#| fig-cap: "Scatterplot matrix for the toy example."
scatterplotMatrix(~ y1 + y2 + x, data=Toy, 
  cex=2,
  col = "blue", pch = 16,
  id = list(n=1, cex=2), 
  regLine = list(lwd = 2, col="red"),
  smooth = FALSE)
```

For this example, we fit the univariate models with `y1` and `y2` separately and then the multivariate model.

```{r toy-models}
Toy.lm1 <- lm(y1 ~ x, data=Toy)
Toy.lm2 <- lm(y2 ~ x, data=Toy)
Toy.mlm <- lm(cbind(y1, y2) ~ x, data=Toy)
```

### Cook's D

First, let's examine the Cook's D statistics for the models. Note that the function `cooks.distance()`
invokes `stats::cooks.distance.lm()` for the univariate response models, but
invokes `mvinfluence::cooks.distance.mlm()` for the multivariate model.

```{r toy-cooks-distance}
df <- Toy
df$D1  <- cooks.distance(Toy.lm1)
df$D2  <- cooks.distance(Toy.lm2)
df$D12 <- cooks.distance(Toy.mlm)

df
```

The only thing remarkable here is for case 9:  The univariate Cook's Ds, `D1` and `D2` are very small,
yet the multivariate statistic, `D12=3.22` is over 10 times the next largest value.


Let's see how case 9 stands out in the influence plots (@fig-toy-inflplots). It has an extreme hat value. But, because it's residual is
very small, it does not have much influence on the fitted models for either `y1` or `y2`. Neither of these plots
suggest that anything is terribly wrong with the univariate models---none of the points are in the "danger" zones
of the upper- and lower-right corners.

```{r echo=-1}
#| label: fig-toy-inflplots
#| fig-width: 10
#| fig-height: 5
#| out-width: "100%"
#| fig-cap: "Influence plots for the univariate models for `y1` and `y2`"
par(mar = c(4, 4, 3, 1)+.1, mfrow = c(1,2))
ip1 <- car::influencePlot(Toy.lm1, 
                          id = list(cex=1.5), cex.lab = 1.5)
ip2 <- car::influencePlot(Toy.lm2, 
                          id = list(cex=1.5), cex.lab = 1.5)
```

**TODO**: Check how these are defined in mvinfluence

Contrast these results with what we get for the model for `y1` and `y2` jointly (@fig-toy-inflplot-mlm-stres)
In the multivariate version, `mvinfluence::influencePlot.mlm()` plots the squared studentized residual (denoted `R` in the output) against the hat value; this is referred to as a `type = "stres" plot.[^cutoffs] 
Case 9 stands in @fig-toy-inflplot-mlm-stres out as wildly influential on the joint regression model. But there's more: The cases in @fig-toy-inflplots with large Cook's D (bubble size)
have only tiny influence in the multivariate model.

[^cutoffs]: Similar to the univariate version, hat values greater than 2 or 3 times their average, $\bar{h} = p/n$ here,
are considered large in the multivariate case. Values of the squared studentized residual $R_i$
are calibrated by the Beta distribution, $\text{Beta}(\alpha=0.95, q/2, (n-p-q)/2)$.

```{r echo=-1}
#| label: fig-toy-inflplot-mlm-stres
#| out-width: "80%"
#| fig-cap: "Studentized residual influence plot for the multivariate model `(y1, y2) ~ x`. Dotted vertical lines mark large hat values, $H > {2, 3} p/n$. The dotted horizontal line marks large values of the squared studentized residual."
par(mar = c(4,4,1,1)+.1)
influencePlot(Toy.mlm, type = "stres",
              id.n=2, id.cex = 1.3,
              cex.lab = 1.5)
```

::: {.callout-note title="Theory into Practice"}
Chairman Mao said, "Theory into practice", but @Tukey:59 said that,
"The practical power of a statistical test is the product of its' statistical power and the probability of use". The story for multivariate influence here illustrates
a nice feature of the connections between statistical theory, graphic development
and implemented in software here. 

A statistical development proposes a new way of
thinking about a problem. People with a graphical bent look at this and think,
"How can I visualize this?". A software developer then solves the remaining problem
of how to incorporate that into easy-to-use functions or applications.
If only this was easy, but sometimes, all three roles appear within a given person.

:::

The general formulation of @Barrett:2003 suggests
an alternative form of the multivariate influence plot (@fig-toy-inflplot-mlm-LR) that uses the leverage (`L`) and residual (`R`) components (`type = "LR"`) directly.

Because influence is the product of leverage and residual, a plot of $\log(L)$ versus $\log(R)$ has the attractive property
that contours of constant Cook's distance fall on diagonal lines with slope = -1. 
Adjacent reference lines represent constant _multiples_ of influence.

```{r echo=-1}
#| label: fig-toy-inflplot-mlm-LR
#| out-width: "80%"
#| fig-cap: "LR plot of $\\log(L)$ versus $\\log(R)$ for the multivariate model `(y1, y2) ~ x`. Dotted lines show contours of constant Cook's distance."
par(mar = c(4,4,1,1)+.1)
influencePlot(Toy.mlm, type="LR",
              id.n=2, id.cex = 1.3,
              cex.lab = 1.5) -> infl
```

### DFBETAS

The DFBETAS statistics give the estimated change in the regression coefficients when each case is deleted in turn.
We can gain some insight as to why case 9 is unremarkable in the univariate regressions by plotting these, shown in
@fig-toy-dfbetas.
The values come from `stats::dfbetas()` and return the standardized values.

```{r}
#| label: fig-toy-dfbetas
#| code-fold: true
#| fig-width: 10
#| fig-height: 5
#| out-width: "100%"
#| fig-cap: ""
db1 <- as.data.frame(dfbetas(Toy.lm1))
gg1 <- ggplot(data = db1, aes(x=`(Intercept)`, y=x, label=rownames(db1))) +
  geom_point(size=1.5) +
  geom_label(size=6, fill="pink") +
  xlab(expression(paste("Deletion Intercept  ", b[0]))) +
  ylab(expression(paste("Deletion Slope  ", b[1]))) +
  ggtitle("dfbetas for y1") +
  theme_bw(base_size = 16)

db2 <- as.data.frame(dfbetas(Toy.lm2))
gg2 <- ggplot(data = db2, aes(x=`(Intercept)`, y=x, label=rownames(db2))) +
  geom_point(size=1.5) +
  geom_label(size=6, fill="pink") +
  xlab(expression(paste("Deletion Intercept  ", b[0]))) +
  ylab(expression(paste("Deletion Slope  ", b[1]))) +
  ggtitle("dfbetas for y2") +
  theme_bw(base_size = 16)

gg1 + gg2
```

The values for case 9 are nearly (0, 0) in both plots, indicating that deleting this case has negligible effect
in both _univariate_ regressions.
Yet, case 9 appeared very influential in the multivariate model. Why did this happen?

In this contrived example, the problem arose from the very high correlation between `y1` and `y2`,
$r = 0.9997$ as can be seen in the (y1, y2) panel in @fig-toy-scatmat.
Although each of the `y1` and `y2` values for the high-leverage cases are in-line with the univariate regressions (and thus have small univariate Cook's Ds), the ill-conditioning magnifies small discrepancies in their positions, making the multivariate Cook's D larger. And that's the solution to the
Mysterious Case 9.




## Example: NLSY data


## Robust Estimation {#sec-robust-estimation}

 

