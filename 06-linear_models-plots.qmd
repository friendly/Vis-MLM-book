```{r include=FALSE}
source("R/common.R")
knitr::opts_chunk$set(fig.path = "figs/ch06/")
```

# Plots for univariate response models {#sec-linear-models-plots}

For a univariate linear model fit using `lm()`, `glm()` and similar functions, the standard `plot()`
method gives basic versions of _diagnostic_ plots of residuals and other calculated quantities for assessing
possible violations of the model assumptions.
Some of these can be considerably enhanced using other packages.

Beyond this, 

* tables of model coefficients, standard errors and test statistics can often be usefully 
supplemented or even replaced by suitable _coeficient plots_ providing essentially the same information.

* when there are two or more predictors, standard plots of $y$ vs. each $x$ ignore the other predictors in the model.
_Added-variable plots_ (also called _partial regression plots_) allow you to visualize _partial_ relations
between the response and a predictor by adjusting (controlling for) all other predictors.

* in this same situation, you can more easily understand their separate impact
on the response by plotting the _marginal effects_ of one or more focal variables, averaging
over other variables not shown in a given plot. 

* when there are highly correlated predictors, some specialized plots are useful to
understand the nature of _multicolinearity_.

The classic reference on regression diagnostics is @Belsley-etal:80.
My favorite modern texts are the brief @Fox2020 and the more complete @FoxWeisberg:2018,
both of which are supported by the **car** package [@R-car].

## The "regression quartet"

For a fitted model, plotting the model object with `plot(model)` provides for any of six basic plots,
of which four are produced by default, giving rise to the term _regression quartet_ for this collection.
These are:

* **Residuals vs. Fitted**: For well-behaved data, the points should hover around a horizontal line at residual = 0,
with no obvious pattern or trend.

* **Normal Q-Q plot**: A plot of sorted standardized residuals $e_i$ (obtained from`rstudent(model)`) against the theoretical values those values would have in a standard normal $\mathcal{N}(0, 1)$ distribution.

* **Scale-Location**: Plots the square-root of the absolute values of the standardized residuals $\sqrt{| e_i |}$
as a measure of "scale" against the fitted values $\hat{y}_i$ as a measure of "location". This provides an assessment
of homogeneity of variance, which appears as a tendency for scale to vary with location.

* **Residuals vs. Leverage**: Plots standardized residuals against leverage to help identify possibly influential observations.
Leverage, or "hat" values (given by `hat(model)`) are proportional to the squared Mahalanobis distances of
the predictor values $\mathbf{x}_i$ from the means, and measure the potential of an observation to
change the fitted coefficients if that observation was deleted. Actual influence is measured by Cooks's distance
(`cooks.distance(model)`) and is proportional to the product of residual times leverage. Contours of constant
Cook's $D$ are added to the plot.

One key feature of these plots is providing **reference** lines or smoothed curves for ease of judging the
extent to which a plot conforms to the expected pattern; another is the **labeling** of observations which
deviate from an assumption.

The base-R `plot(model)` plots are done much better in a variety of packages.
I illustrate some versions from the **car** [@R-car] and **performance** [@Ludecke-etal-performance] packages,
part of the **easystats** [@R-easystats] suite of packages.

**Packages**:
```{r}
library(car)
library(ggplot2)
library(easystats)
library(modelsummary)
library(dplyr)
```


#### Example: Duncan's occupational prestige {.unnumbered}

In a classic study in sociology, @Duncan:61 used data from the U.S. Census in 1950 to study how one could
predict the prestige of occupational categories --- which is hard to measure ---
from available information in the census for those occupations. His data is available in `carData:Duncan`, and contains

* `type`: the category of occupation, one of `prof` (professional), `wc` (white collar) or `bc` (blue collar);
* `income`: the percentage of occupational incumbents with a reported income > $3500 (about $40,000 in current dollars);
* `education`: the percentage of occupational incumbents who were high school graduates;
* `prestige`: the percentage of respondents in a social survey who rated the occupation as “good” or better in prestige.

These variables are a bit quirky in they are measured in percents, 0-100, rather dollars for `income` and
years for `education`, but this common scale permitted Duncan to ask an interesting sociological question:
Assuming that both income and education predict prestige, are they equally important, as might be
assessed by testing the hypothesis $H_0: \beta_{\text{income}} = \beta_{\text{education}}$.

A quick look at the data shows the variables and a selection of the occupational categories, which are
the `row.names()` of the dataset.
```{r duncan}
data(Duncan, package = "carData")
set.seed(42)
car::some(Duncan)
```

Let's start by fitting a simple model using just income and education as predictors. The results look very good!
Both `income` and `education` are highly significant and the $R^2 = 0.828$ for the model indicates
that `prestige` is very well predicted by just these variables.

```{r duncan-mod}
duncan.mod <- lm(prestige ~ income + education, data=Duncan)
summary(duncan.mod)
```

Beyond this, Duncan was interested in the coefficients and whether income and education could be said
to have equal impacts on predicting occupational prestige. A nice display of model coefficients with
confidence intervals is provided by `parameters::model_parameters()`.

```{r duncan-coef}
parameters::model_parameters(duncan.mod)
```

We can also test Duncan's hypothesis that income and education have equal effects on prestige
with `car::linearHypothesis()`. This is constructed as a test of a restricted model in which
the two coefficients are forced to be equal against the unrestricted model. Duncan was very happy with
this result.
```{r duncan-linhyp}
car::linearHypothesis(duncan.mod, "income = education")
```

But, should Duncan be **so** happy? It is unlikely that he ran any model diagnostics or plotted
his model; we do so now. Here is the regression quartet for this model. Each plot shows
some trend lines, and importantly, labels some observations that stand out and might deserve attention.

```{r}
#| label: fig-duncan-plot-model
#| out-width: "100%"
#| fig-cap: "Regression quartet of diagnostic plots for the `Duncan` data. Several possibly unusual observations are labeled."
op <- par(mfrow = c(2,2), 
          mar = c(4,4,3,1)+.1)
plot(duncan.mod, lwd=2, pch=16)
par(op)
```

Some points to note:

* A few observations (minister, reporter, conductor) are flagged in multiple panels
* The red trend line in the scale-location plot indicates that residual variance is not constant, but rather increases from both ends. This is a consequence of the fact that `prestige` is measured as a percentage, bounded at [0, 100], and the standard deviation of a percentage $p$ is proportional to $\sqrt{p \times (1-p)}$.

#### Example: Canadian occupational prestige {#sec-prestige2}

<!-- **CUT THIS EXAMPLE** -->

**TODO**:  Already introduced in @sec-prestige Pair this down & integrate with that

These examples use the data on the prestige of 102 occupational categories and other measures from the
1971 Canadian Census, recorded in `carData::Prestige`.[^prestige-src]
It differs from the `Duncan` dataset primarily in that the main variables---prestige, income and education
were revamped to better reflect the underlying constructs in more meaningful units.

[^prestige-src]: The dataset was collected by Bernard Blishen, William Carroll and Catherine Moore, but apparently unpublished. A version updated to the 1981 census is described in @Blishen-etal-1987.

* `prestige`: Rather than a simple percentage of "good+" ratings, this uses a wider and more reliable scale
from @Pineo-Porter-1967 on a scale from 10--90.

* `income` is measured as the average income of incumbents in each occupation, in 1971 dollars, rather than
percent exceeding a given threshold ($3500)

* `education` is measured as the average education of occupational incumbents, years.

The dataset again includes `type` of occupation with the same levels `"bc"` (blue collar), `"wc"` (white collar) and `"prof"` (professional),
but in addition includes the percent of `women` in these
occupational categories.

Our interest again is in understanding how `prestige` 
is related to census measures of the average education, income, percent women of incumbents in those
occupations, but with attention to the scales of measurement and possibly more complex relationships ...

<!-- **TODO**: These data should be introduced earlier with descriptive plots, scatterplots, ... -->


```{r prestige}
data(Prestige, package="carData")
# `type` is really an ordered factor. Make it so.
# Prestige$type <- ordered(Prestige$type,
#                          levels=c("bc", "wc", "prof"))
str(Prestige)
```

We fit a main-effects model using all predictors (ignoring `census`, the Canadian Census occupational code):

```{r prestige-mod}
prestige.mod <- lm(prestige ~ education + income + women + type,
                   data=Prestige)
```

`plot(model)` produces four separate plots. For a quick look, I like to arrange them in a single 2x2 figure.

```{r fig-plot-prestige-mod}
#| out-width: "100%"
#| fig-show: hold
#| fig-cap: "Regression quartet of diagnostic plots for the `Prestige` data. Several possibly unusual observations are labeled."
op <- par(mfrow = c(2,2), 
          mar=c(4,4,3,1)+.1)
plot(prestige.mod, lwd=2, cex.lab=1.4)
par(op)
```



## Other Model plots


## Coefficient displays

The results of linear models are most often reported in tables and typically with "significance stars"
(`*, **, ***`) to indicate the outcome of hypothesis tests. These are useful for looking up precise values and you can use this format to compare a small number of competing models side-by-side.
However, as illustrated by @KastellecLeoni:2007, plots of coefficients can increase the clarity of presentation
and make it easier to draw correct conclusions. Yet, when you need to present tables, there is a variety of tools
in R that can help make them attractive in publications.

For illustration, I'll consider three models for the `Prestige` data of increasing complexity:

* `mod1` fits the main effects of the three quantitative predictors; 
* `mod2` adds the categorical variable `type` of occupation;
* `mod3` allows an interaction of `income` with `type`. 

```{r prestige-models}
mod1 <- lm(prestige ~ education + income + women,
           data=Prestige)
mod2 <- lm(prestige ~ education + women + income + type,
           data=Prestige)
mod3 <- lm(prestige ~ education + women + income * type,
           data=Prestige)
```

From our earlier analyses (@sec-prestige) we saw that the marginal relationship between `income` and `prestige` was nonlinear @fig-Prestige-scatterplot-income1), and was better represented in a linear model using
`log(income)` (@sec-log-scale) shown in @fig-Prestige-scatterplot2. However, this possibly non-linear relationship could also be explained by stratifying (@sec-stratifying) the data by `type` of occupation (@fig-Prestige-scatterplot3).




### Displaying coefficients

`summary()` gives the complete precis of a fitted model, with information about the estimated coefficients, redisuals and goodness-of fit statistics like $R^2$. But if you only want to see the coefficients,
standard errors, etc. `lmtest::coeftest()` gives these results in the familiar format for console output.
`broom::tidy()` places these in a tidy format common to many modeling functions which is useful for 
futher processing (e.g., comparing models).
```{r coeftest}
lmtest::coeftest(mod1)

broom::tidy(mod1)
```

The **modelsummary** package (@R-modelsummary) is an easy to use,
very general package to summarize data and statistical models in R. The main function
`modelsummary()` can produce highly customizable tables of coefficients
in a wide variety of output formats, including HTML, PDF, LaTeX, Markdown, and MS Word.
You can select the statistics displayed for any model term with the `estimate` and
`statistic` arguments. 
```{r}
#| label: tbl-modelsummary1
#| tbl-cap: Table of coefficients for the main effects model.
modelsummary(list("Model1" = mod1),
  coef_omit = "Intercept",
  shape = term ~ statistic,
  estimate = "{estimate} [{conf.low}, {conf.high}]",
  statistic = c("std.error", "p.value"),
  fmt = fmt_statistic("estimate" = 3, "conf.low" = 4, "conf.high" = 4),
  gof_omit = ".")
```

`gof_omit` allows you to omit or select the goodness-of-fit statistics and other model information
available from those listed by `get_gof()`:

```{r}
get_gof(mod1)
```



### Visualizing coefficients

`modelplot()` is the companion function. It allows you to plot model estimates and confidence intervals. It makes it easy to subset, rename, reorder, and customize plots using same mechanics as in `modelsummary()`.
```{r}
#| label: fig-modelplot1
#| out-width: "80%"
#| fig-cap: "Plot of coefficients and their standard error bar for the simple main effects model"
theme_set(theme_minimal(base_size = 14))

modelplot(mod1, coef_omit="Intercept", 
          color="red", size=1, linewidth=2) +
  labs(title="Raw coefficients for mod1")
```

But this plot is disappointing and misleading because it show the **raw** coefficients. 
From the plot, it looks like only `education` has a non-zero effect, but the effect of `income` is also
highly significant. The problem is that the magnitude of the coefficient $\hat{b}_{\text{education}}$
is more than 40,000 times that of the other coefficients, because education is measured years,
while income is measured in dollars. 
The 95% confidence interval for $\hat{b}_{\text{income}} = [0.0008, 0.0019]$, but this is invisible in the plot.

Before figuring out how to fix this issue, I show the comparable displays from `modelsummary()`
and `modelplot()` for all three models.
When you give `modelsummary()` a list of models, it displays their coefficients side-by-side
as shown in @tbl-modelsummary2. 



```{r}
#| label: tbl-modelsummary2
#| tbl-cap: Table of coefficients for three models.
models <- list("Model1" = mod1, "Model2" = mod2, "Model3" = mod3)
modelsummary(models,
     coef_omit = "Intercept",
     fmt = 2,
     stars = TRUE,
     shape = term ~ statistic,
     statistic = c("std.error", "p.value"),
     gof_map = c("rmse", "r.squared")
     )
```

Note that a factor predictor (like `type` here) with $d$ levels is represented by $d-1$ coefficients
in main effects and in interactions with quantitative variables. These levels are coded with _treatment contrasts_ by default. Also by default, the first level is set as the reference level in alphabetical order.
Here the reference level is blue collar (`bc`), so the coefficient `typeprof = 5.91`
indicates that professional occupations on average are rated 5.91 greater on the Prestige scale
than blue collar workers.

Note also that unlike the table,
the coefficients in @fig-modelplot1 are ordered from bottom to top, because the Y axis starts
at the lower left corner. In @fig-modelplot2  I use `scale_y_discrete()` to reverse the order.
```{r}
#| label: fig-modelplot2
#| out-width: "90%"
#| fig-cap: "Plot of raw coefficients and their confidence intervals for all three models"
modelplot(models, 
          coef_omit="Intercept", 
          size=1.3, linewidth=2) +
  ggtitle("Raw coefficients") +
  geom_vline(xintercept = 0, linewidth=1.5) +
  scale_y_discrete(limits=rev) +
  theme(legend.position = "inside",
        legend.position.inside = c(0.85, 0.2))
```

### More useful coefficient plots

The problem with plots of raw coefficients shown in @fig-modelplot1 and @fig-modelplot2
is that the coefficients for different predictors are not directly comparable because
they are measured in different units.

One alternative is to plot the _standardized coefficients_. These are interpreted
as the standardized change in prestige for a one standard deviation change in the
predictors.

One way to do this is to transform all variables to standardized ($z$) scores.
The syntax below uses `scale` to transform all the numeric variables.
Then, we re-fit the models using the standardized data.

```{r standardize}
Prestige_std <- Prestige |>
  as_tibble() |>
  mutate(across(where(is.numeric), scale))

mod1_std <- lm(prestige ~ education + income + women, data=Prestige_std)

mod2_std <- lm(prestige ~ education + women + income + type, data=Prestige_std)

mod3_std <- lm(prestige ~ education + women + income * type, data=Prestige_std)
```

The plot in @fig-modelplot3 now shows the significant effect of income in all three models.
As well, it offers a more sensitive comparison of the coefficients of other terms
across models; for example `women` is not significant in models 1 and 2, but becomes
significant in Model 3 when the interaction of `income * type` is included.
```{r}
#| label: fig-modelplot3
#| out-width: "90%"
#| fig-cap: "Plot of standardized coefficients and their confidence intervals for all three models"
models <- list("Model1" = mod1_std, "Model2" = mod2_std, "Model3" = mod3_std)
modelplot(models, 
          coef_omit="Intercept", size=1.3) +
  ggtitle("Standardized coefficients") +
  geom_vline(xintercept = 0, linewidth = 1.5) +
  scale_y_discrete(limits=rev) +
  theme(legend.position = "inside",
        legend.position.inside = c(0.85, 0.2))
```

It turns out there is an easier way to get plots of standardized coefficients.
`modelsummary()` extracts coefficients from model objects using the `parameters` package, 
and that package offers several options for standardization: 
See [model parameters documentation](https://easystats.github.io/parameters/reference/model_parameters.default.html).
We can pass the `standardize="refit"` (or other) argument directly to `modelsummary()` or `modelplot()`, and that argument will be forwarded to `parameters`. The plot produced by the code below is identical to @fig-modelplot3 and is not shown.

```{r}
#| eval: false
modelplot(list("mod1" = mod1, "mod2" = mod2, "mod3" = mod3),
          standardize = "refit",
          coef_omit="Intercept", size=1.3) +
  ggtitle("Standardized coefficients") +
  geom_vline(xintercept = 0, linewidth=1.5) +
  scale_y_discrete(limits=rev) +
  theme(legend.position = "inside",
        legend.position.inside = c(0.85, 0.2))
```



## Spread-level plot


## Added-variable plots {#sec-avplots}

As we've seen (**TODO**: Where) simple, _marginal_ scatterplots of a response $y$ against _each_ of several predictors $x_1, x_2, \dots$ can be misleading because each one ignores the other predictors.
However, the general linear model fit by `lm()` estimates all effects together and so the coefficients pertain to the _partial_ effect of a given predictor, _adjusting_ for the effects of all others.

The _added-variable_ plot (also called _partial regression_ plot) is the multivariate analog of a
a simple marginal scatterplot, designed to visualize directly the partial relation $y$ and the predictors
$x_1, x_2, \dots$ in a multiple regression model.

<!--
An added-variable plot displays a scatterplot of a transformation of an independent
variable (say, x1) and the dependent variable (y) that nets out the influence of all
the other independent variables. The fitted regression line between these transformed
variables has the same slope as the coefficient on x1 in the full regression model, which
includes all the independent variables.
An added-variable plot is a visually compelling method for showing a partial corre-
lation between x1 and y. A confidence interval shows how precisely the sample data fit
that correlation. An added-variable plot is the multivariate analogue of using a simple
scatterplot with a regression fit in a univariate context
-->


## Marginal effect plots

<!-- ## Outliers, leverage influence -->
```{r child="child/05-leverage.qmd"}

```


```{r}
#| echo: false
cat("Writing packages to ", .pkg_file, "\n")
write_pkgs(file = .pkg_file)
```

<!-- ## References {.unnumbered} -->
